quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Modifiability,"l or two that use `PythonScriptExecutor` to call into a Python machine-learning library, and do an assessment of maintainability, etc. `PythonScriptExecutor` will come with an attached set of conditions for its use, intended to address the most serious issues raised by the engine and support teams with having Python code in the GATK. We should document these conditions in the docs for `PythonScriptExecutor` when it's implemented:. 1. All tools that use `PythonScriptExecutor` must have a Java-based front-end, with standard GATK (barclay-based) arguments. We put a lot of development effort into our arg parser and into striving for user-interface consistency across tools, and cannot afford to duplicate this effort in Python. Geraldine (CC'd) and the rest of the support team can back me up on this one!. 2. An honest effort should be made to minimize the amount of code written in Python -- as much of each tool's work as possible should be done in Java. In particular, reading/writing final inputs and outputs should happen in Java. This is important for a number of reasons, including the engine team's goal of ensuring universal GCS support, consistent Google authentication handling, etc. Again, we really don't want to have to duplicate that work in Python, or for the tools that call into Python to be inconsistent with the rest of the toolkit. 3. All dependencies (Python and native) of Python libraries used will be clearly documented, and included in the default GATK docker image. I don't think I need to explain why this one is important :) . 4. Before we go any further down this path, we prototype one or two tools using `PythonScriptExecutor`, and do a fair assessment of maintainability and other concerns of the engine/support teams, such as whether it will even be possible to package all dependencies without conflicts. 5. Engine team will continue to search for Java-based solutions while this evaluation is ongoing, but this proposal at least unblocks the CNV team for now.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3501:1875,maintainab,maintainability,1875,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3501,1,['maintainab'],['maintainability']
Modifiability,"lassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 13:13:11.050 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:13:11.275 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:13:12.999 INFO CountReadsSpark - ------------------------------------------------------------; 13:13:13.000 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:13:13.000 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:13:13.000 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:13:13.001 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:13:13.001 INFO CountReadsSpark - Start Date/Time: December 21, 2018 1:13:11 PM EST; 13:13:13.001 INFO CountReadsSpark - ----------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:1713,variab,variables,1713,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,ldevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExce,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:3209,Config,ConfigureActionsProjectEvaluator,3209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['Config'],['ConfigureActionsProjectEvaluator']
Modifiability,le path: file:///home/pkus/mutect_test/simple_uniprot_Dec012014.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/simple_uniprot/hg38/simple_uniprot_Dec012014.tsv; > 15:16:41.540 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/Familial_Cancer_Genes.no_dupes.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/familial/hg38/Familial_Cancer_Genes.no_dupes.tsv; > 15:16:41.545 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; > 15:16:41.556 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/oreganno/hg38/oreganno.tsv; > 15:16:41.575 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/oreganno/hg38/oreganno.config; > 15:16:41.707 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/oreganno/hg38/oreganno.tsv; > 15:16:41.709 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/oreganno/hg38/oreganno.tsv; > WARNING 2020-07-17 15:16:41 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 15:16:41.717 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/dnaRepairGenes.20180524T145835.csv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/dna_repair_genes/hg38/dnaRepairGenes.20180524T145835.csv; > 15:16:41.723 INFO DataSourceUtils - Resolved data source file path: file://,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708:10284,config,config,10284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708,1,['config'],['config']
Modifiability,"le>; from . import timeseries; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/timeseries.py"", line 1, in <module>; import theano.tensor as tt; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.u",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:4769,config,config,4769,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['config'],['config']
Modifiability,"le>; from . import timeseries; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/timeseries.py"", line 1, in <module>; import theano.tensor as tt; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/confith.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782:6006,config,config,6006,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782,1,['config'],['config']
Modifiability,"led: time = Wed Apr 14 11:52:33 2021; , filename = '/SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20210411.GRCh37.gatkcnv.brs/work/92/579e5a48aa9e52cd0; e1df603266809/B00HOTD.counts.hdf5', file descriptor = 250, errno = 121, error message = 'Remote I/O error', buf = ; 0x2b6ebddf38e8, total read size = 384, bytes this sub-read = 384, bytes actually read = 18446744073709551615, offs; et = 712120; major: Low-level I/O; minor: Read failed; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5D.c line 826 in H5Dvlen_reclaim(); : invalid dataspace; major: Invalid arguments to routine; minor: Inappropriate type; 11:52:33.796 INFO GermlineCNVCaller - Shutting down engine; [April 14, 2021 11:52:33 AM CEST] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed t; ime: 0.90 minutes.; Runtime.totalMemory()=2374500352; Exception in thread ""main"" java.lang.InternalError: H5DreadVL_str: failed to read variable length strings; 	at ncsa.hdf.hdf5lib.H5.H5DreadVL(Native Method); 	at org.broadinstitute.hdf5.HDF5File.lambda$readStringArray$0(HDF5File.java:161); 	at org.broadinstitute.hdf5.HDF5File.readDataset(HDF5File.java:349); 	at org.broadinstitute.hdf5.HDF5File.readStringArray(HDF5File.java:150); 	at org.broadinstitute.hellbender.tools.copynumber.utils.HDF5Utils.readIntervals(HDF5Utils.java:62); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.HDF5SimpleCountCollection.lambda$new; $2(HDF5SimpleCountCollection.java:76); 	at htsjdk.samtools.util.Lazy.get(Lazy.java:25); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.HDF5SimpleCountCollection.getInterva; ls(HDF5SimpleCountCollection.java:85); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleCountCollection.readHDF5(Simpl; eCountCollection.java:119); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleCountCollection.readAndSubset(; Simp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7202:3805,variab,variable,3805,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202,1,['variab'],['variable']
Modifiability,lizer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/wgs.hg38/pipelines/hc/cram.test/GRCh38_full_analysis_set_plus_decoy_hla.fa.gz --spark-master yarn; 2019-01-09 13:35:04 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:05 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 13:35:09.640 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:35:09.799 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:11.507 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.508 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:35:11.508 INFO CountReadsSpark - For support and documentat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:1703,config,configuration,1703,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['config'],['configuration']
Modifiability,llRegion(Mutect2Engine.java:250); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:324); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:281); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx3000m -jar /root/gatk.jar Mutect2 -R gs://fc-0b0cb3ce-e2cb-4aef-a8b2-08e60d78e87c/Canis_lupus_familiaris_assembly3.fasta -I gs://fc-8268e82b-ed61-4e04-a8c9-a95a05c0952e/bda6f5ba-8928-45bf-a6b0-9fe67d8dd9a4/PreProcessingForVariantDiscovery_GATK4/cccdda67-56e1-4363-aa6c-46ce53ef8afd/call-GatherBamFiles/attempt-2/Abrams_cell.bam -tumor Abrams_1 --germline-resource gs://fc-0b0cb3ce-e2cb-4aef-a8b2-08e60d78e87c/canid_wgs_ref.1.0.no_samples.vcf.gz -pon gs://fc-afa03a31-404c-4a93-9f6a-31b673db5c69/b92f3c35-5813-455b-94dc-3de3b54f5f98/Mutect2_Panel/c9f21d8a-384e-4d17-a6f8-79a502698827/call-MergeVCFs/1-Mutect2_PON_2019-07-25T22-08-49.vcf -L gs://fc-afa03a31-404c-4a93-9f6a-31b673db5c69/f2138b33-3918-4f8a-9b87-1823a0084ac3/Mutect2/c4844164-ecad-4878-9e5d-cd134a7fb40d/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098:2782,variab,variable,2782,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098,1,['variab'],['variable']
Modifiability,"llbender/setup_gcnvkernel.py. I have installed gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1256, in check_mkl_openmp; import mkl; ModuleNotFoundError: No module named 'mkl'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:1299,config,configparser,1299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['config'],['configparser']
Modifiability,"ller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4011,Config,ConfigFactory,4011,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:4167,config,config,4167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['config'],['config']
Modifiability,"lot of threading output writers through the codebase and perhaps this is better handled by the ""--debug"" argument like it used to? Thoughts? . Notes: ; - It should be noted that by design all of the added changes to HaplotypeCaller are opt-in, barring errors in implementation.; - This code is measurably slower than vanilla HaplotypeCaller. In particular FRD is a very expensive step that corresponds to ~5-7% of the runtime. This is in part because it has to duplicate many of the steps in the genotyper based on the number of unique mapping qualities present at a site as well as the fact that it performs an O(n^2) number of operations at sites with many possible alleles. There are options to cut down on the cost of this algorithm that moderately impact the results relative to DRAGEN. . This implementation is intended to produce results close to the results on DRAGEN 3.4.12 without stripping away the major improvements made in GATK4, as a result there are a number of areas in which we know we are producing different results: ; - In GATK4 variants that overlap with an upstream deletion will have added to their alleles list a sybmolic '*' deletion alleles which are genotyped as part of the allele array in the genotyeper. This is not the case in DRAGEN and it interacts with FRD in such a way as to produce a number of variants that in DRAGEN would have been called as 0/1 heterozygous calls with capped QUAL scores, in gatk they are called as 1/2 calls with uncapped quality scores.; - While we have added the option to use the legacy assembly region creation code, it is not part of the expected pipeline for running DRAGEN. This includes a number of arguments that were done away with in the recent refactoring pass. ; - I have added hooks to revert the assembly engine to approximately its state in gatk3. While I don't stand by that change I think we should bundle the minor assembly engine changes together with our other assembly engine work to try to make a more convincing case.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6634:5372,refactor,refactoring,5372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6634,1,['refactor'],['refactoring']
Modifiability,"ls = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:16:36.298 INFO GenomicsDBImport - Inflater: IntelInflater; 16:16:36.298 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:16:36.298 INFO GenomicsDBImport - Requester pays: disabled; 16:16:36.298 INFO GenomicsDBImport - Initializing engine; 16:16:36.523 WARN GenomicsDBImport - genomicsdb-update-workspace-path was set, so ignoring specified intervals.The tool will use the intervals specified by the initial import; 16:16:37.372 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 16:16:37.372 DEBUG GenomeLocParser - chr1 (248956422 bp); 16:16:37.373 DEBUG GenomeLocParser - chr2 (242193529 bp); 16:16:37.373 DEBUG GenomeLocParser - chr3 (1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:5592,Config,ConfigFactory,5592,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"ls = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:05:38.396 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:05:38.396 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:05:38.396 INFO GermlineCNVCaller - Requester pays: disabled; 21:05:38.396 INFO GermlineCNVCaller - Initializing engine; 21:05:38.399 DEBUG ScriptExecutor - Executing:; 21:05:38.399 DEBUG ScriptExecutor - python; 21:05:38.399 DEBUG ScriptExecutor - -c; 21:05:38.399 DEBUG ScriptExecutor - import gcnvkernel; 21:06:10.792 DEBUG ScriptExecutor - Result: 0; 21:06:10.792 INFO GermlineCNVCaller - Done initializing engine; 21:06:10.826 INFO GermlineCNVCaller - Intervals specified...; log4j:WARN No appenders could be found for logger (org.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:4489,Config,ConfigFactory,4489,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"ls by specifying bin_length = 0.; -I removed the separation between coverage and allelic packages to make the package structure a bit simpler.; -@MartonKN should review, since he wrote PreprocessIntervals and is updating the caller. Added segmentation classes and tests for ModelSegments CNV pipeline.; -I added implementations of copy-ratio, allele-fraction, and ""multidimensional"" (joint) segmentation. All implementations are pretty boilerplate; they simply partition by contig and then call out to KernelSegmenter. Note that there is some logic in multidimensional segmentation that only uses the first het in each copy-ratio interval and if any are available, and imputes the alt-allele fraction to 0.5 if not.; -Makes sense for @mbabadi to review this, since he reviewed the KernelSegmenter PR. Added modeling classes and tests for ModelSegments CNV pipeline.; -Most of this code is copied from the old MCMC code. However, I've done some overall code cleanup and refactoring, especially to remove some overextraction of methods in the allele-fraction likelihoods (see #2860). I also added downsampling and scaling of likelihoods to cut down on runtime. Tests have been simplified and rewritten to use simulated data.; -@LeeTL1220 do you think you could take a look?. Added ModelSegments CLI.; -Mostly control flow to handle optional inputs and validation, but there is some ugly and not well documented code that essentially does the GetHetCoverage step. We'll refactor later, I filed #3915.; -@asmirnov239 can review. This is lower priority than the gCNV VCF writing. Deleted gCNV WDL and Cromwell tests.; -Trivial to review. Added WDL and Cromwell tests for ModelSegments CNV pipeline.; -This includes the cost optimizations from @meganshand and @jsotobroad (sorry guys, I wasn't sure how to track your contributions while fixing up commits!) I also added tests for both GC/no-GC pair workflows.; -@MartonKN should review to gain familiarity with the WDL. Note that this WDL has already been ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3913:1219,refactor,refactoring,1219,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3913,1,['refactor'],['refactoring']
Modifiability,"ls=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 19:01:43.728 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.1.0-10-g554a0e8-SNAPSHOT; 19:01:43.729 INFO HaplotypeCallerSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:01:43.729 INFO HaplotypeCallerSpark - Executing as hadoop@ip-xx.xx.xx.xx on Linux v4.9.85-38.58.amzn1.x86_64 amd64; 19:01:43.729 INFO HaplotypeCallerSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-b10; 19:01:43.729 INFO HaplotypeCallerSpark - Start Date/Time: April 8, 2019 7:01:43 PM UTC; 19:01:43.729 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869:1709,variab,variables,1709,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"lse --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 7, 2017 12:48:13 AM UTC] Executing as tianj@ip-xxx-xx-xx-xxx on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11; Version: 4.alpha.2-1100-g04dbeb2-SNAPSHOT; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:48:13.680 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, m",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:3838,variab,variable,3838,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['variab'],['variable']
Modifiability,"lse --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 8, 2017 9:14:13 AM CST] Executing as yaron@dn1 on Linux 4.4.0-31-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.alpha.2-281-g752d020-SNAPSHOT; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system proper",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:2947,variab,variable,2947,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['variab'],['variable']
Modifiability,"lse; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcnvkernel. 23:38:13.336 DEBUG ScriptExecutor - Result: 0; 23:38:13.341 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initializ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:5049,Config,ConfigFactory,5049,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"lse; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.CUSTOM_READER_FACTORY :; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.REFERENCE_FASTA : null; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tri",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:3400,Config,ConfigFactory,3400,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,"lts.NON_ZERO_BUFFER_SIZE : 131072; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.REFERENCE_FASTA : null; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4327,Config,ConfigFactory,4327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"lts.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:05:38.396 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:05:38.396 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:05:38.396 INFO GermlineCNVCaller - Requester pays: disabled; 21:05:38.396 INFO",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:4043,Config,ConfigFactory,4043,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"ly braces can wait for a separate pass (we will want to do those in this PR though). If you're not sure what to include or not just ask. I like the idea of keeping the GATK3 tests working as we go along. We should make a clear distinction between the old and new tests though. Ideally the GATK3 tests would be in a separate commit that we can just delete at the end, but that can get unwieldy if the files in the commit need to change as we go along. Alternatively you could isolate them into a separate directory. They should either be disabled or made dependent on a test method (see the `@Test` annotation properties `enabled` and `dependsOn`) that is easily toggled so they can be run locally, but don't run on the CI server. Otherwise the CI server build will always fail. In general, its really helpful to have the first commit in the PR contain the completely unmodified GATK3 source files. It makes it much easier for the reviewer to see what changed for the port. I noticed that you have 2 new plugins included in this. I'm not sure if that was suggested by someone on the GATK team (I'm wondering if we want to go down that path...) but I can tell you that the existing plugins required an enormous amount of test development and review iteration. If we do decide to make them plugins, I think it would be a good idea to do so in a separate PR. Also, if we choose to make an AbstractPlugin base class, we may want that to live in the Barclay repo. As @magicdgs points out, master already has your previous commits, so you should start by rebasing on that. Ideally, the branch would have the following commits before we start the first review cycle:. 1. A single commit containing the unmodified GATK3 source (unmodified with the exception that if a file is renamed for GATK4, its helpful to rename the GATK3 version in this commit so it's easy to compare in the next commit). This commit doesn't have to compile or run - its just to make the review process easier for us, and will be delete",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352:1604,plugin,plugins,1604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352,1,['plugin'],['plugins']
Modifiability,"ly. I am busy with graduation in recent days, and will work on visa application for the further postdoc position at Harvard Medical School. Thank you for the interests in our tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an opt",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:1033,enhance,enhancement,1033,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349,1,['enhance'],['enhancement']
Modifiability,"ly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set ever",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2734:1121,variab,variable,1121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734,1,['variab'],['variable']
Modifiability,"make a flexible coverage tool (DepthOfCoverage, DiagnoseTargets, ...)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/19:7,flexible,flexible,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/19,1,['flexible'],['flexible']
Modifiability,"makeBefore/makeAfter seems more flexible in that it makes before/after a property of the way the transformer is applied, rather than of the transformer itself. When we write the plugin descriptor, it can maintain two separate argument lists (for ""--preFilterTransformer ..."" and ""--postFilterTransformer ...""), and then merge them accordingly. It does complicate the tool structure, but its the price for flexibility, and the pattern is not that complex. I think we should be explicit about what ""pre"" and ""post"" are relative to in the method and argument names, i.e., makePreReadFilterTransformer and makePostReadFilterTransformer, or maybe even one makeReadFilterTransformer method that takes a pre/post argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-246000255:32,flexible,flexible,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-246000255,2,"['flexible', 'plugin']","['flexible', 'plugin']"
Modifiability,making the version number depend on the git hash using a gradle git plugin from https://github.com/ajoberstar/gradle-git. It seems like the top gradle-git integration library. There are lots of pre-baked things in it to help with releases and such that we can grow into.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/196:68,plugin,plugin,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/196,1,['plugin'],['plugin']
Modifiability,"mark duplicates in dataflow - based on the code by garrickevans . The main work is done in; `private static final class MarkDuplicatesDataflowTransform extends PTransform<PCollection<Read>, PCollection<Read>>` - the sigrature conforms to the main read processing pipeline. Limitations:; - no optical duplicates; - only integration tests (would be good to have unit tests that check dup detection logic on very specific reads - ideally those from picard's tests). @droazen please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/541:152,extend,extends,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/541,1,['extend'],['extends']
Modifiability,"mbler.java:4: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.3891049Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T00:09:07.3891593Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T00:09:07.3892257Z symbol: class RangeMap; 2022-08-16T00:09:07.3892601Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3893126Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T00:09:07.3893670Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T00:09:07.3894352Z symbol: class Range; 2022-08-16T00:09:07.3894678Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3897711Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T00:09:07.3902203Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T00:09:07.3902980Z symbol: class RangeMap; 2022-08-16T00:09:07.3903340Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3903864Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T00:09:07.3904505Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T00:09:07.3905250Z symbol: class Range; 2022-08-16T00:09:07.3905751Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3906273Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:101: error: cannot find symbol; 2022-08-16T00:09:07.3906908Z static VCFHeaderLine rangeToVCFHeaderLine(Range<Integer> genotypeQualityBand) ***; 2022-08-16T00:09:07.3907793Z symbol: class Range; 2022-08-16T00:09:07.3908125Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3910592Z src/main/java/org/broadinstitute/hellbender/to",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:6286,extend,extends,6286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['extend'],['extends']
Modifiability,"mbler.java:4: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.7690890Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T22:45:53.7738985Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T22:45:53.7739852Z symbol: class RangeMap; 2022-08-16T22:45:53.7740332Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7740892Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T22:45:53.7741707Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T22:45:53.7743523Z symbol: class Range; 2022-08-16T22:45:53.7743866Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7747579Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T22:45:53.7748444Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T22:45:53.7776218Z symbol: class RangeMap; 2022-08-16T22:45:53.7776715Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7777389Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T22:45:53.7778220Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T22:45:53.7779110Z symbol: class Range; 2022-08-16T22:45:53.7779574Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7780209Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:101: error: cannot find symbol; 2022-08-16T22:45:53.7780965Z static VCFHeaderLine rangeToVCFHeaderLine(Range<Integer> genotypeQualityBand) ***; 2022-08-16T22:45:53.7781896Z symbol: class Range; 2022-08-16T22:45:53.7782232Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7785096Z src/main/java/org/broadinstitute/hellbender/to",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:8324,extend,extends,8324,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,1,['extend'],['extends']
Modifiability,mbly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:15849,Extend,Extended,15849,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Extend'],['Extended']
Modifiability,"md5/%s; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.REFERENCE_FASTA : null; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:3697,Config,ConfigFactory,3697,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,"mentations of copy-ratio, allele-fraction, and ""multidimensional"" (joint) segmentation. All implementations are pretty boilerplate; they simply partition by contig and then call out to KernelSegmenter. Note that there is some logic in multidimensional segmentation that only uses the first het in each copy-ratio interval and if any are available, and imputes the alt-allele fraction to 0.5 if not.; -Makes sense for @mbabadi to review this, since he reviewed the KernelSegmenter PR. Added modeling classes and tests for ModelSegments CNV pipeline.; -Most of this code is copied from the old MCMC code. However, I've done some overall code cleanup and refactoring, especially to remove some overextraction of methods in the allele-fraction likelihoods (see #2860). I also added downsampling and scaling of likelihoods to cut down on runtime. Tests have been simplified and rewritten to use simulated data.; -@LeeTL1220 do you think you could take a look?. Added ModelSegments CLI.; -Mostly control flow to handle optional inputs and validation, but there is some ugly and not well documented code that essentially does the GetHetCoverage step. We'll refactor later, I filed #3915.; -@asmirnov239 can review. This is lower priority than the gCNV VCF writing. Deleted gCNV WDL and Cromwell tests.; -Trivial to review. Added WDL and Cromwell tests for ModelSegments CNV pipeline.; -This includes the cost optimizations from @meganshand and @jsotobroad (sorry guys, I wasn't sure how to track your contributions while fixing up commits!) I also added tests for both GC/no-GC pair workflows.; -@MartonKN should review to gain familiarity with the WDL. Note that this WDL has already been through many revisions from @meganshand, @jsotobroad, and @LeeTL1220, so hopefully there shouldn't be too much for you to find serious fault with. Note that I punted on adding MultidimensionalKernelSegmenterUnitTest and ModelSegmentsIntegrationTest. Filed #3916. Closes #2858. (FINALLY!); Closes #3825.; Closes #3661.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3913:1717,refactor,refactor,1717,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3913,1,['refactor'],['refactor']
Modifiability,"moving settings from SparkCommandLineProgram to SparkContextFactory; the settings that were being applied to SparkContext.getConf were pointless since getConf is a copy of the configuration; instead they're applied to the SparkConfig in SparkContextFactory.getSparkContext. fixes #1096. <!-- Reviewable:start -->. [<img src=""https://reviewable.io/review_button.png"" height=40 alt=""Review on Reviewable""/>](https://reviewable.io/reviews/broadinstitute/gatk/1097). <!-- Reviewable:end -->",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1097:176,config,configuration,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1097,1,['config'],['configuration']
Modifiability,mpl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3346,Plugin,PluginRegistry,3346,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['Plugin'],['PluginRegistry']
Modifiability,"ms that we will want to run the filter with more stringent parameters, as higher base error rates are causing homs to leak past the filter, which in turn affects the fit of the allele-fraction model (which only attempts to model hets) by biasing normal segments towards unbalanced, and 2) we now want to run ModelSegments separately on the normal to allow for the filtering of germline events. So we want to be more stringent with low-coverage normals without affecting our high-coverage tumors. For example, here's some hg38 NovaSeq FFPE WGS data from a ~40x normal:. ![download](https://user-images.githubusercontent.com/11076296/43977946-9bd0a1bc-9cb3-11e8-9d7f-016a99c1c173.png). Compare to an hg19 TCGA WGS ~40x normal:. ![download 1](https://user-images.githubusercontent.com/11076296/43978051-f8820770-9cb3-11e8-8e16-13b51792614f.png). The hom-ref tail in the first plot is much fatter and clearly leaks into the het cloud. Also curious is that the het cloud is far less binomial (or even beta-binomial---note also the absence of the tail extending to the origin). I am still not sure why the incoming data looks different. There are several confounding factors: NovaSeq vs. HiSeq, hg38 vs. hg19, AF > 2% gnomAD sites vs. AF > 10% 1000G sites, FFPE vs. frozen, etc. I have not seen enough examples/combinations to be able to say which are the most important factors. Changing the genotyping/filtering strategy can get around this change in the data without a corresponding change in the allele-fraction model for now, but getting the data to look as good as possible upstream would be even better. Another thought: would be nice if the strategy was easily compatible with an eventual implementation of multi-sample segmentation, which would require that the same sites are used in both the tumor and the normal. We would want to strike a balance between maximizing the number of sites and including questionable sites from the normal. Will add more details later. @davidbenjamin @LeeTL1220 @eit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3915#issuecomment-412189218:1597,extend,extending,1597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3915#issuecomment-412189218,1,['extend'],['extending']
Modifiability,"n Tools; #; # Only update this environment if there is a *VERY* good reason to do so!; # If the build is broken but could be fixed by doing something else, then do that thing instead.; # Ensuring the correct environment for canonical (or otherwise reasonable) usage of our standard Docker takes precedence over edge cases.; # If you break the environment, you are responsible for fixing it and also owe the last developer who left this in a reasonable state a beverage of their choice.; # (This may be yourself, and you'll appreciate that beverage while you tinker with dependencies!); #; # When changing dependencies or versions in this file, check to see if the ""supportedPythonPackages"" DataProvider; # used by the testGATKPythonEnvironmentPackagePresent test in PythonEnvironmentIntegrationTest needs to be updated; # to reflect the changes.; #; name: gatk; channels:; # if channels other than conda-forge are added and the channel order is changed (note that conda channel_priority is currently set to flexible),; # verify that key dependencies are installed from the correct channel and compiled against MKL; - conda-forge; - defaults; dependencies:. # core python dependencies; - conda-forge::python=3.6.10 # do not update; - pip=20.0.2 # specifying channel may cause a warning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely nec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:1270,flexible,flexible,1270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,1,['flexible'],['flexible']
Modifiability,"n occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1256, in check_mkl_openmp; import mkl; ModuleNotFoundError: No module named 'mkl'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/pymc3/__init__.py"", line 5, in <module>; from .distributions import *; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/py",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:1793,config,configparser,1793,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['config'],['configparser']
Modifiability,"n3.6/site-packages/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/confith.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinst",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782:6227,config,configparser,6227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782,1,['config'],['configparser']
Modifiability,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5302:4653,variab,variable,4653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302,2,"['Refactor', 'variab']","['Refactored', 'variable']"
Modifiability,"ncher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at htsjdk.samtools.metrics.MetricsFile.loadClass(MetricsFile.java:471); 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:353); 	... 8 more; ```. If it is replaced, the tool still errors but with a different error:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:170); 	at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:219); 	at java.util.ArrayList.forEach(ArrayList.java:1249); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:171); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); ```. It does not matter if I produce the pre-adapter metrics with the latest Picard jar v2.9.2. I get the same error. . I'm using a M2 callset from GATK3. Even so, I don't think I should get the above error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030:3915,adapt,adapter,3915,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030,1,['adapt'],['adapter']
Modifiability,ncotator/funcotator_dataSources.v1.6.20190124s/gencode_xrefseq/hg38/gencode_xrefseq_v90_38.tsv; > 12:28:17.939 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/hgnc_download_Nov302017.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/hgnc/hg38/hgnc_download_Nov302017.tsv; > 12:28:17.939 INFO Funcotator - Finalizing data sources (this step can be long if data sources are cloud-based)...; > 12:28:17.940 INFO DataSourceUtils - Setting lookahead cache for data source: chr1_b_bed : 100000; > 12:28:17.951 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.967 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.config; > 12:28:17.995 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.997 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > WARNING 2020-07-21 12:28:17 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 12:28:18.002 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; > 12:28:18.009 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > 12:28:18.020 INFO Fe,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:8858,config,config,8858,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['config'],['config']
Modifiability,"nd other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge::keras=2.2.4 # updated from pip-installed 2.2.0, which caused various conflicts/clobbers of conda-installed packages; # conda-installed 2.2.4 appears to be the most recent version with a consistent API and without conflicts/clobbers; # if you wish to update, note that versions of conda-forge::keras after 2.2.5; # undesirably set the environment variable KERAS_BACKEND = theano by default; - defaults::intel-openmp=2019.4; - conda-forge::scikit-learn=0.22.2; - conda-forge::matplotlib=3.2.1; - conda-forge::pandas=1.0.3. # core R dependencies; these should only be used for plotting and do not take precedence over core python dependencies!; - r-base=3.6.2; - r-data.table=1.12.8; - r-dplyr=0.8.5; - r-getopt=1.20.3; - r-ggplot2=3.3.0; - r-gplots=3.0.3; - r-gsalib=2.1; - r-optparse=1.6.4. # other python dependencies; these should be removed after functionality is moved into Java code; - biopython=1.76; - pyvcf=0.6.8; - bioconda::pysam=0.15.3 # using older conda-installed versions may result in libcrypto / openssl bugs. # pip installs should be avoided, as pip may not respect the dependencies found by the conda solver; - pip:; - gatkPythonPackageArchive.zip; ```. It seems to successfully create the environment. I'd still recommend updating the information on your README.md and the file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:3013,variab,variable,3013,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,1,['variab'],['variable']
Modifiability,"nd the channel order is changed (note that conda channel_priority is currently set to flexible),; # verify that key dependencies are installed from the correct channel and compiled against MKL; - conda-forge; - defaults; dependencies:. # core python dependencies; - conda-forge::python=3.6.10 # do not update; - pip=20.0.2 # specifying channel may cause a warning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge::keras=2.2.4 # updated from pip-installed 2.2.0, which caused various conflicts/clobbers of conda-installed packages; # conda-installed 2.2.4 appears to be the most recent version with a consistent API and without conflicts/clobbers; # if you wish to update, note that versions of conda-forge::keras after 2.2.5; # undesirably set the environment variable KERAS_BACKEND = theano by default; - defaults::intel-openmp=2019.4; - conda-forge::scikit-learn=0.22.2; - conda-forge::matplotlib=3.2.1; - conda-forge::pandas=1.0.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:2181,config,config,2181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,1,['config'],['config']
Modifiability,"nd, so it cannot localize gs; paths. In other words, the WDL tests in travis need a local instance of; the file and to use that path in the json. On Wed, Apr 10, 2019 at 3:49 PM Jonn Smith <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220>; >; > This seems to be running into a cromwell / WDL error:; >; > java.lang.IllegalArgumentException: Could not build the path ""gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, LinuxFileSystem. Failures: HTTP: gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt does not have an http or https scheme (IllegalArgumentException); > LinuxFileSystem: Cannot build a local path from gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt (RuntimeException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; > 	Could not build the path ""gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, LinuxFileSystem. Failures: HTTP: gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt does not have an http or https scheme (IllegalArgumentException); >; > Isn't cromwell supposed to handle gs:// URLs for localizing files? Do you; > have any thoughts?; >; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5872#issuecomment-481836556>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk7Wd-RgMx2g-UPLNrvjettNMf9ixks5vfkA3gaJpZM4clLLK>; > .; >. -- ; Lee Lichtenstein; Br",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5872#issuecomment-481853426:1069,config,configure,1069,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5872#issuecomment-481853426,1,['config'],['configure']
Modifiability,nds 11 --gvcf-gq-bands 12 --gvcf-gq-bands 13 --gvcf-gq-bands 14 --gvcf-gq-bands 15 --gvcf-gq-bands 16 --gvcf-gq-bands 17 --gvc; f-gq-bands 18 --gvcf-gq-bands 19 --gvcf-gq-bands 20 --gvcf-gq-bands 21 --gvcf-gq-bands 22 --gvcf-gq-bands 23 --gvcf-gq-bands 24 --gvcf-gq-bands 25 --gvcf-gq-bands 26 --gvcf-gq-bands 27 --g; vcf-gq-bands 28 --gvcf-gq-bands 29 --gvcf-gq-bands 30 --gvcf-gq-bands 31 --gvcf-gq-bands 32 --gvcf-gq-bands 33 --gvcf-gq-bands 34 --gvcf-gq-bands 35 --gvcf-gq-bands 36 --gvcf-gq-bands 37 -; -gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47; --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands ; 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --floor-blocks false --indel-size-to-eliminate-in-re; f-model 10 --disable-optimizations false --dragen-mode false --flow-mode NONE --apply-bqd false --apply-frd false --disable-spanning-event-genotyping false --transform-dragen-mapping-quali; ty false --mapping-quality-threshold-for-genotyping 20 --max-effective-depth-adjustment-for-frd 0 --just-determine-active-regions false --dont-genotype false --do-not-run-physical-phasing ; false --do-not-correct-overlapping-quality false --use-filtered-reads-for-annotations false --use-flow-aligner-for-stepwise-hc-filtering false --adaptive-pruning false --do-not-recover-dan; gling-branches false --recover-dangling-heads false --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 ; --min-dangling-branch-length 4 --recover-all-dangling-branches false --max-num-haplotypes-in-population 128 --min-pruning 2 --adaptive-pruning-initial-error-rate 0.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:4845,adapt,adaptive-pruning,4845,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,2,['adapt'],"['adaptive-pruning', 'adaptive-pruning-initial-error-rate']"
Modifiability,nection.getNewHttpClient(HttpURLConnection.java:1202); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:966); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:158); 	at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:489); 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:206); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1012); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:975); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1084); 	at org.apache.spark.SparkContext$$anonfun$,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369:7114,config,configure,7114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369,1,['config'],['configure']
Modifiability,"nfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcnvkernel. 23:38:13.336 DEBUG ScriptExecutor - Result: 0; 23:38:13.341 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:38:14.588 INFO GermlineCNVCaller - Intervals specified...; 23:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:5224,Config,ConfigFactory,5224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"nfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcnvkernel. 23:38:13.336 DEBUG ScriptExecutor - Result: 0; 23:38:13.341 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:38:14.588 INFO GermlineCNVCaller - Intervals specified...; 23:38:14.590 DEBUG GenomeLocParser - Prepared reference sequence cont",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:5290,Config,ConfigFactory,5290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"nflater; 16:36:22.399 INFO Funcotator - GCS max retries/reopens: 20; 16:36:22.399 INFO Funcotator - Requester pays: disabled; 16:36:22.399 INFO Funcotator - Initializing engine; 16:36:22.624 INFO FeatureManager - Using codec VCFCodec to read file file:///home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz; 16:36:22.842 INFO Funcotator - Done initializing engine; 16:36:22.842 INFO Funcotator - Validating sequence dictionaries...; 16:36:22.856 INFO Funcotator - Processing user transcripts/defaults/overrides...; 16:36:22.857 INFO Funcotator - Initializing data sources...; 16:36:22.859 INFO DataSourceUtils - Initializing data sources from directory: /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s; 16:36:22.871 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 16:36:22.871 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.871 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.891 INFO Funcotator - Shutting down engine; [January 10, 2024 at 4:36:22 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; ***********************************************************************. A USER ERROR has occurred: ERROR: Directory contains more than one config file: file:///home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg38/. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace. Any guidance to resolve the issue is appreciated.; Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8647:4863,config,config,4863,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647,1,['config'],['config']
Modifiability,"ng join() or detach(). This occurs when running JointGenotyping on 345 gvcfs created by GATK4 ExomeGermlineSingleSample; the workflow is running on an HPC cluster in Singularity (single node, 32 cores/node, 1002GB node memory) NOTE that I am able to successfully run JointGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80). The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used:v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the whole thing). (From main process stderr, picking from SplitInterval setting status to Done). \[2022-10-18 15:38:20,88\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.SplitIntervalList:NA:1\]: Status change from WaitingForReturnCode to Done. \[2022-10-18 15:38:25,47\] \[info\] WorkflowExecutionActor-9743b28a-3819-49a7-8598-b0c5267647ee \[9743b28a\]: Starting JointGenotyping.ImportGVCFs (10 shards). \[2022-10-18 15:38:33,03\] \[info\] Assigned new job execution tokens to the following groups: 9743b28a: 10. \[2022-10-18 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076:1550,config,config,1550,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076,1,['config'],['config']
Modifiability,"ng: PrintReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:6928,config,configuration,6928,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['config'],['configuration']
Modifiability,"ngest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics fo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:1975,refactor,refactored,1975,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['refactor'],['refactored']
Modifiability,"nshot 2018-01-17 13 22 27"" src=""https://user-images.githubusercontent.com/11543866/35060379-501cb8c8-fb8c-11e7-845e-a146fc2ced94.png"">. ## Major wants; - The is_bamout Boolian appears to be hardcoded to `false` in the script. Users need to be able to understand that this option can be changed without ambiguity. So this should become a proper optional variable. @LeeTL1220 tells me this can be overwritten. However, why leave this as misinterpretable to newbie WDL-scriptors? Especially since `wdltools inputs` doesn't include it as a variable at all in the generated inputs list. Please can we make this a proper optional argument that `wdltools inputs` will generate a variable for.; - [ ""${variants_for_contamination}"" == *.vcf ] does not allow *.vcf.gz files. It should accept either.; - Outputs should allow either .vcf or .vcf.gz compression by user-specification. Alternatively, if we want to keep it simple and hardcode, then the preference is for compressed files. Some of us prefer to save on storage.; - Need to be able to specify optional string args for SplitIntervals. I would like to be able to use the BALANCING_WITHOUT_INTERVAL_SUBDIVISION mode. Furthermore, I'd like for the tool to automatically interpret this mode, when not given an -L intervals list, to not split reference contigs. I.e. a contig is an interval. (Perhaps already the tool behavior?); - The version of Oncotator is not compatible with GRCh38. Please, can we have an option to switch this out with Funcotator? . ## Minor wants; - The JSON template in the repo should show the optional variables.; - Script calls for a Picard jar. I don't mind specifying this because I like controlling for the Picard version I use. However, users may want to call the Picard version within the GATK jar. I cannot fathom a simple way to allow switching this out in the script, but perhaps something like the gatk_override option could work. The goal would be to call the Picard tool from a Docker. This better enables provenance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4188:1747,variab,variables,1747,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4188,1,['variab'],['variables']
Modifiability,nternal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:2837,config,configuration,2837,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['config'],['configuration']
Modifiability,"nternally by discrete RV posterior update routines (""callers"") as a safety measure to stabilize self-consistency loops. For example, consider the mean-field treatment of two coupled Markov chains: the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum valu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4720:1221,variab,variables,1221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720,1,['variab'],['variables']
Modifiability,"ntervals of two jobs, and whether separating the jobs would impact calls. In this example, GenotypeGVCFs would run over 1:1050-1150. For example, if we had a multi-NT variant that spanned 1148-1052, we'd want that called correctly no matter what intervals were used for the jobs. I tried using running GenomicsDBImport with -L over a small region, or I ran SelectVariants on the gVCF first (which behaves a little differently), and then used that subset gVCF as input to GenomicsDBImport, where GenomicsDBImport is given the entire contig as the interval. The resulting workspaces will be slightly different, with the latter containing information over a wider region (GenomicsDBIport truncates start/end of the input records to just the target interval). . So if either of these workspaces is passed to GenotypeGVCFs, using --only-output-calls-starting-in-intervals and -L 1:1050-1150:. I think any upstream padding doesnt matter. If you have a multi-nucleotide polymorphism that starts upstream of 1050 but spans 1050, this job wouldnt be responsible for calling that. The prior job, which has an interval set upstream of this one should call it. I think GenomicsDbImport's behavior is fine here. If you have a multi-NT variant that starts within 1050-1150, but extends outside (i.e. deletion or insertion starting at 1148), this could be a problem. The GenomicsDB workspace created with the interval 1:1050-1150 lacks the information to score that, right? The workspace created using the more permissive SelectVariants->GenomicsDBImport contains that downstream information and presumably would make the same call as if GenotypeGVCFs was given the intact chromosome as input, right?. However, it seems that if I simply create the workspace with a reasonably padded interval (adding 1kb should be more than enough for Illumina, right?), and then run GenotypeGVCFs with the original, unpassed interval, then the resulting workspace should contain all available information and GenotypeGVCFs should be",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221558244:1317,polymorphi,polymorphism,1317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221558244,1,['polymorphi'],['polymorphism']
Modifiability,"nvs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/confith.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:192); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.onStartup(DetermineGermlineContigPloidy.java:240); at org.broadinstitute.h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782:6496,config,configdefaults,6496,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782,1,['config'],['configdefaults']
Modifiability,"o be special and left untouched by BQSR. Currently, there is no easy way to convert base qualities to two. The only instances I am aware of is (i) for SamToFastq, which then unaligns the reads and (ii) MergeBamAlignment, which isn't necessarily a part of everyone's workflow. Also, MergeBamAlignment's `CLIP_ADAPTERS` softclips XT tagged sequence, which then becomes fair game for our assembly-based callers. MarkIlluminaAdapters uses aligned reads to mark those with 3' adapter sequence with the XT tag. The XT tag values note the start of the 3' adapter sequence in the read. During MergeBamAlignment, one must especially request that this XT tag is retained in the merged output. Because our assembly-based callers throw out CIGAR strings from the aligner when reassembling reads, so as to use soft-clipped sequence that may contain true variants we wish to resolve, adapter sequence can be incorporated into the graph. This is not an issue for libraries with low levels of adapter read through and for germline calling as we prune nodes in the graph that have less than two reads supporting it. . However, for somatic cases and for libraries where there is considerable adapter read through, the current solution is to hard-clip adapter sequences out of reads or to toss these reads altogether so as not to increase the extent of spurious calls. The issue with hard-clipping is that our reads become malformed due to a mismatch in CIGAR string and sequence length. These the GATK engine filters. So the solution is to either correct the CIGAR strings or to go back and re-align the clipped reads or again to toss the reads. It would be great not to have to throw out reads that include some adapter sequence in somatic workflows that call down to the lowest allele fraction variants. It seems this would simply be a matter of a tool or feature that replaces adapter sequence marked with the XT tag with base qualities of 2 and special handling by our callers of sequence with base quality of two.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3540:1252,adapt,adapter,1252,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3540,5,['adapt'],['adapter']
Modifiability,"o go ahead and add this option, I would probably keep the directory structure of the GermlineCNVCaller output the same (i.e., with folders named ""SAMPLE_#""), and just check the sample_name.txt files at the PostprocessGermlineCNVCalls step. I don't think this should require GermlineCNVCaller code changes, right?. 4) We may require additional code at the WDL level if we want to both switch over to primarily using sample names but also get rid of bundling (i.e., by passing only the calls for each sample when needed). Locally, you can always just search all output for directories containing the appropriate sample_name.txt. But on the cloud, you'd want to make sure that the postprocessing step for a particular sample gets only its corresponding directories, which would have to happen at the WDL level; the check against sample_name.txt at the tool level would just be a formality. I can foresee headaches with globbing and funky sample names. I'm not sure I understand your point about extending PostprocessGermlineCNVCalls to run on all samples. The point of that tool is to take results from all genomic shards for a single sample and stitch them together, right? Even if we extend this to run on a batch of multiple samples (which would just be moving the loop over samples at the WDL level to some lower level, i.e., Java or python), we still need to see all shards for those samples. Perhaps I'm misunderstanding---can you clarify?. @mwalker174 can we once and for all clearly document the issue with the transpose? Perhaps by pointing to specific WGS runs that have issues with call caching? I think being able to pinpoint the exact issue will help us identify the right solution---whether that be choosing an appropriate bundling scheme, taking advantage of #5781 to reduce the number of shards, batching during the postprocessing step, removing unnecessary outputs, etc. Recall that we'd like to be able to use the same WDL locally (when you have easy access to all GermlineCNVCaller re",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765:2180,extend,extending,2180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765,1,['extend'],['extending']
Modifiability,"o the right processing methods in a single pass over the RDD. Reply by @SHuang-Broad. > I tried to fix it in this PR, but that seems to be a big task,; and probably is impossible to achieve in a single pass,; because currently each class of contig ends up producing a different type of object; (3 general classes: simple -> SimpleNovelAdjacency, complex -> ComplexVariantCanonicalRepresentation, and unknown -> SAM records of the contigs); and a groupBy() operation is necessary in the middle using these objects as keys; due to the fact that different contigs may produce the same variant; So what I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:1548,inherit,inherit,1548,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030,1,['inherit'],['inherit']
Modifiability,"ocker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; de",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:1101,config,configurations,1101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['config'],['configurations']
Modifiability,"ode:. -`BaseRecalibratorSpark` is the standalone BQSR tool, and calls into the `BaseRecalibratorSparkFn` (which is also called from `ReadsPipelineSpark`). -`ApplyBQSRSpark` is the standalone ApplyBQSR tool, and calls into the `ApplyBQSRSparkFn` (also called from `ReadsPipelineSpark`). -Integration tests for the above are in `BaseRecalibratorSparkIntegrationTest` and `ApplyBQSRSparkIntegrationTest`. -Almost all other changes in the branch are related to the BQSR engine refactoring, which I summarize below:; - We pulled out the guts of the walker `BaseRecalibrator` tool, combined it with all of the code from the former `RecalibrationEngine` class (now deleted) to make a new `BaseRecalibrationEngine` class under `utils/recalibration`.; - We stripped out all copies of the code in `BaseRecalibrationEngine` from the walker, dataflow, and spark versions of BQSR, and modified them to call into `BaseRecalibrationEngine`.; - We moved all auxiliary classes needed by the `BaseRecalibrationEngine` (eg., the covariates, etc.) into `utils/recalibration`.; - We refactored the argument collections. Now there is a single shared `RecalibrationArgumentCollection` that contains **only** the parameters for the `BaseRecalibrationEngine` itself, and this argument collection is exposed by all 3 versions of the tool. Input/output arguments have been removed from this argument collection and put into the individual implementations of BQSR, since they vary between the walker, dataflow, and spark versions of the tool. This eliminates awkward problems such as having both a `knownSites` argument AND a `BQSRKnownVariants` exposed at the same time, with only 1 of them usable for a given version of a tool. The dataflow-only `BaseRecalibrationArgumentCollection` has been deleted completely as no longer needed.; - We tweaked the names of some tool arguments to enforce consistency between the 3 versions of the tool as well as the rest of hellbender (eg., output arg for BQSR is now a more standard `-O`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073:1113,refactor,refactored,1113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073,1,['refactor'],['refactored']
Modifiability,odecov.io/gh/broadinstitute/gatk/pull/3447?src=pr&el=tree) | Coverage  | Complexity  | |; |---|---|---|---|; | [.../broadinstitute/hellbender/utils/LoggingUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9Mb2dnaW5nVXRpbHMuamF2YQ==) | `82.222% <> ()` | `11 <0> ()` | :arrow_down: |; | [...ellbender/cmdline/StandardArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL1N0YW5kYXJkQXJndW1lbnREZWZpbml0aW9ucy5qYXZh) | `0% <> ()` | `0 <0> ()` | :arrow_down: |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `65.926% <> ()` | `35 <0> ()` | :arrow_down: |; | [...ellbender/utils/config/CustomBooleanConverter.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb25maWcvQ3VzdG9tQm9vbGVhbkNvbnZlcnRlci5qYXZh) | `100% <100%> ()` | `2 <2> (?)` | |; | [...rg/broadinstitute/hellbender/utils/io/IOUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pby9JT1V0aWxzLmphdmE=) | `60.104% <100%> (+0.418%)` | `50 <2> (+1)` | :arrow_up: |; | [...stitute/hellbender/cmdline/CommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0NvbW1hbmRMaW5lUHJvZ3JhbS5qYXZh) | `86.408% <100%> (+0.408%)` | `29 <0> ()` | :arrow_down: |; | [...oadinstitute/hellbender/engine/FeatureManager.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3447#issuecomment-323474032:1818,config,config,1818,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3447#issuecomment-323474032,1,['config'],['config']
Modifiability,odehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:60); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:235); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:255); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$StreamAction.visitFile(ShadowCopyAction.groovy:185); 	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite$PogoCachedMethodSiteNoUnwrapNoCoerce.invoke(PogoMetaMethodSite.java:210); 	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite.callCurrent(PogoMetaMethodSite.java:59); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:166); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$StreamAction.processFile(ShadowCopyAction.groovy:151); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1$1.processFile(NormalizingCopyActionDecorator.java:66); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator$1$1.processFile(DuplicateHandlingCopyActionDecorator.java:60); 	at org.gradle.api.internal.file.copy.CopyFileVisitorImpl.processFile(CopyFileVisitorImpl.java:62); 	at org.gradle.api.internal.file.copy.CopyFileVisitorImpl.visitFile(CopyFileVisitorImpl.java:46); 	at org.gradle.api.internal.file.collections.jdk7.Jdk7DirectoryWalker$1.visitFile(Jdk7DirectoryWalker.java:86); 	at org.gradle.api.internal.file.collections.jdk7.Jdk7DirectoryWalker$1.visitFile(Jdk7DirectoryWalker.java:59); 	at java.nio.file.Files.walkFileTree(Files.java:2670); 	at org.gradle.api.internal.file.collections.jdk7.Jdk7DirectoryWalker.walkDir(Jdk7DirectoryWalker.java:59); 	at org.gradle.api.internal.file.collections.DirectoryFileTree,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:2008,plugin,plugins,2008,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,"ok, got it. sorry, i missed 'install' in that command. my initial impression is that VariantQC will be able to adapt fine to VariantEvalEngine. I wrote VariantEvalEngine with this is mind, but it's good to formally test it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-759645374:111,adapt,adapt,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-759645374,1,['adapt'],['adapt']
Modifiability,"olders named ""SAMPLE_#""), and just check the sample_name.txt files at the PostprocessGermlineCNVCalls step. I don't think this should require GermlineCNVCaller code changes, right?. 4) We may require additional code at the WDL level if we want to both switch over to primarily using sample names but also get rid of bundling (i.e., by passing only the calls for each sample when needed). Locally, you can always just search all output for directories containing the appropriate sample_name.txt. But on the cloud, you'd want to make sure that the postprocessing step for a particular sample gets only its corresponding directories, which would have to happen at the WDL level; the check against sample_name.txt at the tool level would just be a formality. I can foresee headaches with globbing and funky sample names. I'm not sure I understand your point about extending PostprocessGermlineCNVCalls to run on all samples. The point of that tool is to take results from all genomic shards for a single sample and stitch them together, right? Even if we extend this to run on a batch of multiple samples (which would just be moving the loop over samples at the WDL level to some lower level, i.e., Java or python), we still need to see all shards for those samples. Perhaps I'm misunderstanding---can you clarify?. @mwalker174 can we once and for all clearly document the issue with the transpose? Perhaps by pointing to specific WGS runs that have issues with call caching? I think being able to pinpoint the exact issue will help us identify the right solution---whether that be choosing an appropriate bundling scheme, taking advantage of #5781 to reduce the number of shards, batching during the postprocessing step, removing unnecessary outputs, etc. Recall that we'd like to be able to use the same WDL locally (when you have easy access to all GermlineCNVCaller results from all genomic shards) and in the cloud, with minimal duplication of output from bundling when running locally, if possible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765:2371,extend,extend,2371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765,1,['extend'],['extend']
Modifiability,"olean readHasStarted = false;; boolean addedHardClips = false;. while (!cigarStack.empty()) {; final CigarElement cigarElement = cigarStack.pop();. if (!readHasStarted &&; cigarElement.getOperator() != CigarOperator.DELETION &&; cigarElement.getOperator() != CigarOperator.SKIPPED_REGION &&; cigarElement.getOperator() != CigarOperator.HARD_CLIP) {; readHasStarted = true;; } else if (!readHasStarted && cigarElement.getOperator() == CigarOperator.HARD_CLIP) {; totalHardClip += cigarElement.getLength();; } else if (!readHasStarted && cigarElement.getOperator() == CigarOperator.DELETION) {; totalHardClip += cigarElement.getLength();; } else if (!readHasStarted && cigarElement.getOperator() == CigarOperator.SKIPPED_REGION) {; totalHardClip += cigarElement.getLength();; }. if (readHasStarted) {; if (i == 1) {; if (!addedHardClips) {; if (totalHardClip > 0) {; inverseCigarStack.push(new CigarElement(totalHardClip, CigarOperator.HARD_CLIP));; }; addedHardClips = true;; }; inverseCigarStack.push(cigarElement);; } else {; if (!addedHardClips) {; if (totalHardClip > 0) {; cleanCigar.add(new CigarElement(totalHardClip, CigarOperator.HARD_CLIP));; }; addedHardClips = true;; }; cleanCigar.add(cigarElement);; }; }; }; // first pass (i=1) is from end to start of the cigar elements; if (i == 1) {; shiftFromEnd = shift;; cigarStack = inverseCigarStack;; }; // second pass (i=2) is from start to end with the end already cleaned; else {; shiftFromStart = shift;; }; }; }; `. Notice that the variable _shift_ is initialized, but never assigned to again for the duration of the loop. Thus _shiftFromStart_ and _shiftFromEnd_ are always set to zero upon completion of the loop. These values are used by the _applyHardClipBases_ function, which is called in a number of places to hard clip bases from a read, but because of this error, they will always be zeroed out. The function containing the error is in the file ""src/main/java/org/broadinstitute/hellbender/utils/clipping/ClippingOp.java"" line 523",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6130:1892,variab,variable,1892,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6130,1,['variab'],['variable']
Modifiability,"ollapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, especially given the simplicity of writing a new walker type in GATK4. For all of these reasons we don't want `AssemblyRegionWalker` to inherit from a more abstract traversal type -- it's just going to be its own standalone thing",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:1385,Inherit,Inheriting,1385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,1,['Inherit'],['Inheriting']
Modifiability,"omicsDBImport - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.REFERENCE_FASTA : null; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:4561,Config,ConfigFactory,4561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,on.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:36); 	at StudentAws$delayedInit$body.apply(StudentAws.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at StudentAws$.main(StudentAws.scala:8); 	at StudentAws.main(StudentAws.scala); 23/11/16 12:09:10 INFO SparkContext: Invoking stop() from shutdown hook; 23/11/16 12:09:10 INFO SparkContext: SparkContext is stopping with exitCode 0.; 23/11/16 12:09:10 INFO SparkUI: Stopped Spark web UI at http://SRINIVASiNDRARAVI:4040; 23/11/16 12:09:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 23/11/16 12:09:10 INFO MemoryStore: MemoryStore cleared; 23/11/16 12:09:10 INFO BlockManager: BlockManager stopped; 23/11/16 12:09:10 INFO BlockManagerMaster: BlockManagerMaster stopped; 23/11/16 12:09:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 23/11/16 12:09:10 INFO SparkContext: Successfully stopped SparkContext; 23/11/16 12:09:10 INFO ShutdownHookManager: Shutdown ho,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8587:12992,adapt,adapted,12992,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587,1,['adapt'],['adapted']
Modifiability,"on.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:36); 	at StudentAws$delayedInit$body.apply(StudentAws.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at StudentAws$.main(StudentAws.scala:8); 	at StudentAws.main(StudentAws.scala); Exception in thread ""main"" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z; 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method); 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793); 	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249); 	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454); 	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761); 	at org.apache.hadoop.fs.FileSystem.listS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8587:7349,adapt,adapted,7349,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587,1,['adapt'],['adapted']
Modifiability,"on: class GVCFBlockCombiner; 2022-08-16T00:09:07.3893126Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T00:09:07.3893670Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T00:09:07.3894352Z symbol: class Range; 2022-08-16T00:09:07.3894678Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3897711Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T00:09:07.3902203Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T00:09:07.3902980Z symbol: class RangeMap; 2022-08-16T00:09:07.3903340Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3903864Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T00:09:07.3904505Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T00:09:07.3905250Z symbol: class Range; 2022-08-16T00:09:07.3905751Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3906273Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:101: error: cannot find symbol; 2022-08-16T00:09:07.3906908Z static VCFHeaderLine rangeToVCFHeaderLine(Range<Integer> genotypeQualityBand) ***; 2022-08-16T00:09:07.3907793Z symbol: class Range; 2022-08-16T00:09:07.3908125Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3910592Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPruner.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3914013Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/BaseVertex.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3921838Z src/main/java/org/broadinstitute/hellbender/tools/walkers",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:6678,extend,extends,6678,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['extend'],['extends']
Modifiability,"on: class GVCFBlockCombiner; 2022-08-16T22:45:53.7740892Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T22:45:53.7741707Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T22:45:53.7743523Z symbol: class Range; 2022-08-16T22:45:53.7743866Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7747579Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T22:45:53.7748444Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T22:45:53.7776218Z symbol: class RangeMap; 2022-08-16T22:45:53.7776715Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7777389Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T22:45:53.7778220Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T22:45:53.7779110Z symbol: class Range; 2022-08-16T22:45:53.7779574Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7780209Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:101: error: cannot find symbol; 2022-08-16T22:45:53.7780965Z static VCFHeaderLine rangeToVCFHeaderLine(Range<Integer> genotypeQualityBand) ***; 2022-08-16T22:45:53.7781896Z symbol: class Range; 2022-08-16T22:45:53.7782232Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7785096Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPruner.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7789228Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/BaseVertex.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7798240Z src/main/java/org/broadinstitute/hellbender/tools/walkers",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:8716,extend,extends,8716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,1,['extend'],['extends']
Modifiability,"onActor [968be82cValidateBamsWf.ValidateBAM:0:1]: Unrecognized runtime attribute keys: disks, memory; [2020-07-14 05:09:41,71] [info] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: /gatk/gatk \; ValidateSamFile \; --INPUT /cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/inputs/-1942028726/test.bam \; --OUTPUT test.validation_.txt \; --MODE SUMMARY; [2020-07-14 05:09:41,76] [info] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: executing: # make sure there is no preexisting Docker CID file; rm -f /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/docker_cid; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/docker_cid \; -i \; \; --entrypoint /bin/bash \; -v /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0:/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0:delegated \; broadinstitute/gatk@sha256:18146e79d06787483310e5de666502090a480e10ac0fad06a36a5e7a5c9bb1dc /cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/script. # get the return code (working even if the container was detached); rc=$(docker wait cat /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/docker_cid). # remove the container after waiting; docker rm cat /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/docker_cid. # return exit code; exit $rc; [2020-07-14 05:09:45,29] [info] BackgroundConfigAsyncJobExecutionActor [968be8",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:5347,config,configuration,5347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,1,['config'],['configuration']
Modifiability,once #3480 is in and #3447 is in @magicDGS requested that we should expose DEFAULT_FEATURE_CACHE_LOOKAHEAD as a configurable option,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3489:112,config,configurable,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3489,1,['config'],['configurable']
Modifiability,"onfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls to: zorzan; 18/04/23 20:41:39",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:6038,Config,ConfigFactory,6038,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,"ong. We should make a clear distinction between the old and new tests though. Ideally the GATK3 tests would be in a separate commit that we can just delete at the end, but that can get unwieldy if the files in the commit need to change as we go along. Alternatively you could isolate them into a separate directory. They should either be disabled or made dependent on a test method (see the `@Test` annotation properties `enabled` and `dependsOn`) that is easily toggled so they can be run locally, but don't run on the CI server. Otherwise the CI server build will always fail. In general, its really helpful to have the first commit in the PR contain the completely unmodified GATK3 source files. It makes it much easier for the reviewer to see what changed for the port. I noticed that you have 2 new plugins included in this. I'm not sure if that was suggested by someone on the GATK team (I'm wondering if we want to go down that path...) but I can tell you that the existing plugins required an enormous amount of test development and review iteration. If we do decide to make them plugins, I think it would be a good idea to do so in a separate PR. Also, if we choose to make an AbstractPlugin base class, we may want that to live in the Barclay repo. As @magicdgs points out, master already has your previous commits, so you should start by rebasing on that. Ideally, the branch would have the following commits before we start the first review cycle:. 1. A single commit containing the unmodified GATK3 source (unmodified with the exception that if a file is renamed for GATK4, its helpful to rename the GATK3 version in this commit so it's easy to compare in the next commit). This commit doesn't have to compile or run - its just to make the review process easier for us, and will be deleted at some point. I can help with how to get this into your branch if you like.; 2. Your modified GATK3 tests in a single commit. This will also be removed before merge.; 3. A single commit with all o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352:1781,plugin,plugins,1781,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352,1,['plugin'],['plugins']
Modifiability,onnection.java:966); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredentials.java:176); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.tryGetComputeCredentials(DefaultCredentialsProvider.java:270); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentialsUnsynchronized(DefaultCredentialsProvider.java:194); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentials(DefaultCredentialsProvider.java:112); 	at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:113); 	at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:86); 	at com.google.cloud.ServiceOptions.defaultCredentials(ServiceOptions.java:277); 	at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:252); 	at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:82); 	at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:30); 	at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:77); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtils.setGlobalNIODefaultOptions(BucketUtils.java:361); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); ```; I was able to fix the issue by setting the environment variable `NO_GCE_CHECK=true` in my shell though,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331269235:6606,variab,variable,6606,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331269235,1,['variab'],['variable']
Modifiability,"ool will have the exact same functionality as `CollectAllelicCounts`, to the point where I can re-use the integration tests. However, the integration tests fail. When I dig deeper in `CollectAllelicCountsSpark`, I see that only 8 RDDs (correct amount: 11) are being passed to processAlignments... Consider the following code:. ```; @Override; protected void processAlignments(JavaRDD<LocusWalkerContext> rdd, JavaSparkContext ctx) {; final String sampleName = SampleNameUtils.readSampleName(getHeaderForReads());; final SampleMetadata sampleMetadata = new SimpleSampleMetadata(sampleName);; final Broadcast<SampleMetadata> sampleMetadataBroadcast = ctx.broadcast(sampleMetadata);. final AllelicCountCollector finalAllelicCountCollector =; rdd.mapPartitions(distributedCount(sampleMetadataBroadcast.getValue(), minimumBaseQuality)); .reduce((a1, a2) -> combineAllelicCountCollectors(a1, a2, sampleMetadataBroadcast.getValue()));; final List<LocusWalkerContext> tmp = rdd.collect();; ....snip....; ```. In this case `tmp` will have a size of 8. However, the integration test would indicate a size of 11 is correct, since 11 intervals are being passed in. Note that `emitEmptyLoci()` returns `true`, so 11 is the correct number as seen in `CollectAllelicCountsSparkIntegrationTest` . . Additionally, in (at least) one result, the counts are wrong. `CollectAllelicCounts` (non-spark) passes the integration test. I have tried a couple of tests to gather more information:. - Is `emitEmptyLoci()` causing an issue? ; Does not appear to be causing the issue. I say this because when set to `false`, I get (essentially) the same error.; - The code uses `mapPartition` and not `map`, does this cause the issue? Why are you doing this?; This does not cause the issue. I refactored the code to use `map` and got the exact same issue. I use `mapPartition` in order to instantiate only one instance of `AllelicCountCollector` per partition, instead of per locus. Assigning to @tomwhite by request of @droazen ...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3823:1870,refactor,refactored,1870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3823,1,['refactor'],['refactored']
Modifiability,oppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.grad,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:17128,plugin,plugin,17128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['plugin'],['plugin']
Modifiability,"or users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:21.991 INFO MarkDuplicatesSpark - Executing as cwhelan@gsa6.broadinstitute.org on Linux v2.6.32-696.16.1.el6.x86_64 amd64; 14:40:21.991 INFO MarkDuplicatesSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:40:21.992 INFO MarkDuplicatesSpark - Start Date/Time: May 7, 2018 2:40:21 PM EDT; 14:40:21.992 INFO MarkDuplicatesSpark - -------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746:1396,variab,variables,1396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"ore, the ground truth (mostly/totally in `FuncotatorIntegrationTest`) had to be modified. *Please carefully review the ground truth changes*.; - Introduces the `CompsiteOutputRenderer`, which is composed of multiple output renderers. This is used when output type is `SEG`, so that it can write both output files simultaneously.; - Introduces the `GeneListOutputRenderer`. This does not write anything to disk until the entire input file is processed. The actual writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do the actual writing.; - Introduces the `SimpleTsvOutputRenderer`. This output renderer is very flexible and renders a tab-separated text file based on several output rules. Formats are driven through config files. And developers can limit the output columns to ignore extraneous funcotation fields. Note that excluded fields are honored, regardless. If a configuration + parameter combination would result in this class producing an empty file, an exception is thrown. More notes are in the javadocs of the class.; - Currently, only the `GencodeFuncotationFactory` can actually funcotate segments. ; - Code base currently enforces only small mutations when running `Funcotator` (segs are funcotated as CANNOT_DETERMINE) and only segments when running `FuncotateSegments` (small mutations produce exception). This is enforced with flags in the code. The backend does not disallow a mixture for future use. This may prove important when funcotating CNVs from VCFs produced by tools other than `ModelSegments`.; - Added copy creation method for FuncotationMap based on Kryo. Also, added the necessary Kryo registrations. This induced a new unit test to enforce any concrete implementations of `Fu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5941:2199,config,config,2199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941,1,['config'],['config']
Modifiability,org\springframework\spring-web\5.2.6.RELEASE\spring-web-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-webmvc\5.2.6.RELEASE\spring-webmvc-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-aop\5.2.6.RELEASE\spring-aop-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-context\5.2.6.RELEASE\spring-context-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-expression\5.2.6.RELEASE\spring-expression-5.2.6.RELEASE.jar;E:\repository\org\mybatis\spring\boot\mybatis-spring-boot-starter\2.1.2\mybatis-spring-boot-starter-2.1.2.jar;E:\repository\org\mybatis\spring\boot\mybatis-spring-boot-autoconfigure\2.1.2\mybatis-spring-boot-autoconfigure-2.1.2.jar;E:\repository\org\mybatis\mybatis\3.5.4\mybatis-3.5.4.jar;E:\repository\org\mybatis\mybatis-spring\2.0.4\mybatis-spring-2.0.4.jar;E:\repository\mysql\mysql-connector-java\8.0.20\mysql-connector-java-8.0.20.jar;E:\repository\org\springframework\boot\spring-boot-configuration-processor\2.3.0.RELEASE\spring-boot-configuration-processor-2.3.0.RELEASE.jar;E:\repository\org\springframework\spring-core\5.2.6.RELEASE\spring-core-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-jcl\5.2.6.RELEASE\spring-jcl-5.2.6.RELEASE.jar;E:\repository\com\google\firebase\firebase-admin\6.8.1\firebase-admin-6.8.1.jar;E:\repository\com\google\api-client\google-api-client\1.25.0\google-api-client-1.25.0.jar;E:\repository\com\google\oauth-client\google-oauth-client\1.25.0\google-oauth-client-1.25.0.jar;E:\repository\com\google\http-client\google-http-client-jackson2\1.25.0\google-http-client-jackson2-1.25.0.jar;E:\repository\com\google\api-client\google-api-client-gson\1.25.0\google-api-client-gson-1.25.0.jar;E:\repository\com\google\http-client\google-http-client-gson\1.25.0\google-http-client-gson-1.25.0.jar;E:\repository\com\google\code\gson\gson\2.8.6\gson-2.8.6.jar;E:\repository\com\google\http-client\google-http-client\1.25.0\google-http-client-1.25.0.jar;E:\repository\com\google\code\findbugs\jsr305\3.0,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:5508,config,configuration-processor-,5508,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['config'],['configuration-processor-']
Modifiability,osmic_tissue.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/cosmic_tissue/hg19/cosmic_tissue.tsv; 15:41:49.599 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/dnaRepairGenes.20180524T145835.csv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/dna_repair_genes/hg19/dnaRepairGenes.20180524T145835.csv; 15:41:49.600 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; 15:41:49.604 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/oreganno.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; 15:41:49.604 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.config; 15:41:49.659 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/oreganno.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; 15:41:49.659 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/oreganno.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; WARNING 2020-08-19 15:41:49 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 15:41:49.663 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/hgnc_download_Nov302017.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/hgnc/hg19/hgnc_download_Nov302017.tsv; 15:41:49.851 INFO DataSourceUtils - Resol,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6758:12207,config,config,12207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6758,1,['config'],['config']
Modifiability,"otator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:55:32.063 INFO Funcotator - Deflater: IntelDeflater; 02:55:32.063 INFO Funcotator - Inflater: IntelInflater; 02:55:32.063 INFO Funcotator - GCS max retries/reopens: 20; 02:55:32.063 INFO Funcotator - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 02:55:32.063 WARN Funcotator - . [1m[31m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: Funcotator is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 02:55:32.063 INFO Funcotator - Initializing engine; 02:55:32.318 INFO FeatureManager - Using codec VCFCodec to read file file:///export2/liuhw/wes_test/Mutect2_filter/K001137N_somatic_filtered.vcf.gz; 02:55:32.459 INFO Funcotator - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 02:55:32.466 INFO Funcotator - Shutting down engine; [July 12, 2024 2:55:32 AM EDT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2148532224; ***********************************************************************. A USER ERROR has occurred: Bad input: ERROR in config file: file:///./software/gatk_Funcotator/funcotator_dataSources.v1.8.hg38.20230908s/gnomAD_exome/hg38/gnomAD_exome.config - src_file does not exist: /./software/gatk_Funcotator/funcotator_dataSources.v1.8.hg38.20230908s/gnomAD_exome/hg38/gs:/broad-public-datasets/funcotator/gnomAD_2.1_VCF_INFO_AF_Only/hg38/gnomad.exomes.r2.1.sites.liftoverToHg38.INFO_ANNOTATIONS_FIXED.vcf.gz. ***********************************************************************; ```; How to solved it?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8913:3663,config,config,3663,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8913,2,['config'],['config']
Modifiability,"ould like to address are similar to yours, with some inclussions. * Regarding NIO support, I would go to remove completely `File` support. If API users need to use the `File` abstraction, they should convert to a `java.nio.Path` using the `toPath` method.; * In addition, I would like that HTTP/S and FTP is handled also with NIO. For HTTP/S, I am working in a simple `FileSystemProvider` that should be good enough for using in combination with HTSJDK ([jsr203-http](https://github.com/magicDGS/jsr203-http)), and I can speed up the development there for needs in HTSJDK; for FTP, maybe [ftp-fs](https://github.com/robtimus/ftp-fs) can be used or a simple implementation can be derived from the HTTP/S implementation (without credentials). This will remove the special handling of HTTP/S and FTP paths in HTSJDK in favor of a consistent and pluggable manner.; * Interfaces for the data types are great, and maybe it will be good to have codec interfaces for both encoding and decoding. For example, I am missing encoders in tribble (an attempt in https://github.com/samtools/htsjdk/pull/822 for writing support).; * For VCF, I would like to have a less diploid-centric interface and design, or at least a way of configure the catching of genotype-related attributes. Currently there are methods for homozygotes/heterozygotes that aren't really useful for triploids or even VCFs without variation (for example, in Pool-Seq data).; * Modular design for artifacts: thus, a project with only SAM/BAM requirements will require only `htsjdk-sam`, and if they also want CRAM support, `htsjdk-cram`. See https://github.com/samtools/htsjdk/issues/896 for more info about it.; * Common license for all HTSJDK, or at least for each module. This will be good for taking into account legal concerns when including the library, because now there is a mixture depending on the files that are used. This is what is coming to my mind now. Maybe I added something else in https://github.com/samtools/htsjdk/issues/520",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-363390940:1281,config,configure,1281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-363390940,1,['config'],['configure']
Modifiability,"ouldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, especially given the simplicity of writing a new walker type in GATK4. For all of these reasons we don't want `AssemblyRegionWalker` to inherit from a more abstract traversal type -- it's just going to be its own standalone thing, so that it can evolve freely without affecting anyone else. For `SlidingWindowWalker`, which we still want to merge, I recommend making the traversal do **exactly** what you want for your use case, as clearly and simply as possible, without worrying about serving as a base class for other traversals. Ping me once you're happy with it, and I'll re-review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:2073,inherit,inheritance,2073,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,4,"['evolve', 'inherit']","['evolve', 'inherit', 'inheritance', 'inheriting']"
Modifiability,"output renderers. This is used when output type is `SEG`, so that it can write both output files simultaneously.; - Introduces the `GeneListOutputRenderer`. This does not write anything to disk until the entire input file is processed. The actual writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do the actual writing.; - Introduces the `SimpleTsvOutputRenderer`. This output renderer is very flexible and renders a tab-separated text file based on several output rules. Formats are driven through config files. And developers can limit the output columns to ignore extraneous funcotation fields. Note that excluded fields are honored, regardless. If a configuration + parameter combination would result in this class producing an empty file, an exception is thrown. More notes are in the javadocs of the class.; - Currently, only the `GencodeFuncotationFactory` can actually funcotate segments. ; - Code base currently enforces only small mutations when running `Funcotator` (segs are funcotated as CANNOT_DETERMINE) and only segments when running `FuncotateSegments` (small mutations produce exception). This is enforced with flags in the code. The backend does not disallow a mixture for future use. This may prove important when funcotating CNVs from VCFs produced by tools other than `ModelSegments`.; - Added copy creation method for FuncotationMap based on Kryo. Also, added the necessary Kryo registrations. This induced a new unit test to enforce any concrete implementations of `Funcotation` to be Kryo serializable. The unit test does a recursive search of the funcotator package. For all concrete implementations, it tracks whether this unit test tests the serialization. If not, it fails. Instr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5941:2354,config,configuration,2354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941,1,['config'],['configuration']
Modifiability,overall this looks reasonable to me. Adding an integration test is important - at least lock in the variants that are concordant with gatk3 so that we dont lose those while refactoring and fixing etc.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1567#issuecomment-196949010:173,refactor,refactoring,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1567#issuecomment-196949010,1,['refactor'],['refactoring']
Modifiability,"ozygous genotypes failing two-tailed binomial test (example below); q .. select genotypes using -i/-e options; and the new genotype can be one of:; . .. missing (""."" or ""./."", keeps ploidy); 0 .. reference allele (e.g. 0/0 or 0, keeps ploidy); c:GT .. custom genotype (e.g. 0/0, 0, 0/1, m/M, overrides ploidy); m .. minor (the second most common) allele (e.g. 1/1 or 1, keeps ploidy); M .. major allele (e.g. 1/1 or 1, keeps ploidy); p .. phase genotype (0/1 becomes 0|1); u .. unphase genotype and sort by allele (1|0 becomes 0/1); Usage: bcftools +setGT [General Options] -- [Plugin Options]; Options:; run ""bcftools plugin"" for a list of common options. Plugin options:; -e, --exclude <expr> Exclude a genotype if true (requires -t q); -i, --include <expr> include a genotype if true (requires -t q); -n, --new-gt <type> Genotypes to set, see above; -t, --target-gt <type> Genotypes to change, see above. Example:; # set missing genotypes (""./."") to phased ref genotypes (""0|0""); bcftools +setGT in.vcf -- -t . -n 0p. # set missing genotypes with DP>0 and GQ>20 to ref genotypes (""0/0""); bcftools +setGT in.vcf -- -t q -n 0 -i 'GT=""."" && FMT/DP>0 && GQ>20'. # set partially missing genotypes to completely missing; bcftools +setGT in.vcf -- -t ./x -n . # set heterozygous genotypes to 0/0 if binom.test(nAlt,nRef+nAlt,0.5)<1e-3; bcftools +setGT in.vcf -- -t ""b:AD<1e-3"" -n 0. # force unphased heterozygous genotype if binom.test(nAlt,nRef+nAlt,0.5)>0.1; bcftools +setGT in.vcf -- -t ./x -n c:'m/M'; ```; I was always wondering if GATK will have a plugin interface where people can code their own using groovy, kotlin, javascript or python plugins to extend some of the functionality where developers may not reach immediately. Personally I use htsjdk extensively (and sometimes pysam) to code a new personal tool each time I need something that I cannot find exactly what I look for. But a generic gatk plugin interface would be really useful and may provide means to extend the community support.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1556119501:1923,plugin,plugin,1923,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1556119501,5,"['extend', 'plugin']","['extend', 'plugin', 'plugins']"
Modifiability,packaging gatk-launch in our jar so that it's available to downstream projects; some build.gradle refactoring,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1772:98,refactor,refactoring,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1772,1,['refactor'],['refactoring']
Modifiability,"parameterize TTL with defaults, reduce memory allocation",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7244:0,parameteriz,parameterize,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7244,1,['parameteriz'],['parameterize']
Modifiability,"peGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; > 21:13:04.231 INFO GenotypeGVCFs - Ini",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:4067,Config,ConfigFactory,4067,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,"piped beta variables through to high-level beta workflow.; Also updated the gatk jar so it succeeds, as it didn't before. [Successful run of beta workflow on quickstart data](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history/e9a1af96-8c1a-463a-8063-ae455d0ba6b3)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8200:11,variab,variables,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8200,1,['variab'],['variables']
Modifiability,plBuilder.build(ManagedChannelImplBuilder.java:615); 	at io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:261); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:360); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.access$1800(InstantiatingGrpcChannelProvider.java:81); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider$1.createSingleChannel(InstantiatingGrpcChannelProvider.java:231); 	at com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:72); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:241); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:219); 	at com.google.api.gax.rpc.ClientContext.create(ClientContext.java:199); 	at com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:89); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.<init>(BigQueryReadClient.java:129); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:110); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:102); 	at org.broadinstitute.hellbender.utils.bigquery.StorageAPIAvroReader.<init>(StorageAPIAvroReader.java:60); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.createSortedReferenceRangeCollectionFromBigQuery(ExtractCohortEngine.java:851); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.createVariantsFromUnsortedBigQueryRanges(ExtractCohortEngine.java:891); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.traverse(ExtractCohortEngine.java:224); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohort.traverse(ExtractCohort.java:335); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7583:1949,Enhance,EnhancedBigQueryReadStub,1949,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7583,1,['Enhance'],['EnhancedBigQueryReadStub']
Modifiability,"practice?; - The error message when there is no supporting code does not tell what the problem is; whether the extension of the file (due to the the 1-to-1 name to type quasi-restriction above) or a more complex formatting issue in the file (e.g. required header missing, version not supported ... blah blah). ; - All codecs are tried out even when most won't ever apply. Even if the performance impact should in practice be minimal still may cause several file IO open operations as several Codec do actually peek into the file (e.g. BCF and VCF codecs). ; - Codec developers have to make sure their new codec does not collides with others; it would be better if codec development can be totally independent.; - General file extensions such as .tab , .tsv cannot be used by codecs due to possible collisions constraining users to name their files the way GATK needs them to; ""I don't like people telling what file names a have to use... I'm already placing the correct argument name before the file name. What else you need!"". Proposal:. An annotation to tell what codes to try out, the first one that canDecode returns true is used otherwise a configurable error message saying what the problem could be:. <pre>; @Codecs(BEDCodec.class); FeatureInput&lt;BEDFeature&gt; features;; </pre>. <pre>; @Codecs(value = BEDCodec.class, failureMessage = ""The file provided must be a BED formatted file with extension .bed""); FeatureInput&lt;BEDFeature&gt; features;; </pre> . <pre>; @Codecs(BCFCodec.class, VCFCodec.class); FeatureInput&lt;VariantContext&gt; variants;; </pre>. <pre>; // force = true, means that canDecode won't be called and instead we try to read the content directly,; // the codec's code is responsible to throw an appropriate UserException.BadInput indicating formatting issues; this should be the case already anyway.; @Codecs(value = TargetCodec.class, force = true); FeatureInput&lt;Target&gt; target;; </pre>. If the annotation is not present it can default to the current behavior.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1184:2195,config,configurable,2195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1184,1,['config'],['configurable']
Modifiability,"previously, tools that align reads required you to manually disable sequence dictionary validation; if you didn't, they would fail because the unaligned bam didn't have the required sequence dictionary. extracting out a SequenceDictionaryValidationArgumentCollection and providing a method for GATKSparkTools to configure it; ReadsPipeline couldn't easily make use of this, so instead it overrides the method that does validation. BwaSpark / BwaAndMarkDuplicatesPipelineSpark now do not require or allow dictionary validation; fixes #4131",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4308:312,config,configure,312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4308,1,['config'],['configure']
Modifiability,"provements that were discovered while reviewing the variants ; (https://github.com/SHuang-Broad/GATK-SV-callset-regressionTest/tree/master/Evaluation/Analysis/masterVSfeature/notes.xlsx); The implemented fixes are:; * for removing the hard-coded/explicit mentioning of ""chr"" in non-canonical versions, it is now fixed in 5eff782e4d582d516004fba2cee7535d984b1540; * for contigs whose alignments paint ambiguous picture, i.e. multiple alignment configurations offer equally good explanation:; 	1. if only one configuration has all alignment with MQ above a specified threshold, it is favored; this is implemented in ecc31f5fbec4e524b401fc9474a3a1b7ab08c561; 	2. if one configuration has alignment to non-canonical chromosome that explains the contig better than would-be-event-inducing mappings to canonical chromosomes, the canonical mappings are saved but the better non-canonical mappings are saved as SA tag as in SAM spec, and the VCF record produced is annotated accordingly; this is implemented in 65cdb523a2f9fa2026334713fed45381d76ffc82; * fixed a bug where sometimes an assembly contig as several alignments, only one of which has non-mediocre MQ but at the sametime this alignment contains a large gap, such contigs were previously incorrectly filtered away, they are now salvaged by commit b6b2f197b112981e00efd9d415f010c024d31b36. So, for the FN variants (FN in the sense that they are captured in the stable version of our interpretation tool but now goes missing in the experimental interpretation tool); that were curated in the above-mentioned review, only the following ones are not salvaged, with plans or comments attached. ```; asm012854:tig00000	missing	classified as ""incomplete""; fixable by finishing the last TODO in AssemblyContigAlignmentSignatureClassifier (same problem as face by group represented by asm002398:tig00001); asm014580:tig00018	missing	classified as ""incomplete""; fixable by finishing the last TODO in AssemblyContigAlignmentSignatureClassifier (same problem ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-370923522:863,config,configuration,863,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-370923522,1,['config'],['configuration']
Modifiability,"put); * positive (training with *.annot.hdf5) vs. positive-unlabeled (training with *.annot.hdf5 and *.unlabeled.annot.hdf5); * Java Bayesian Gaussian Mixture Model (BGMM) backend vs. python sklearn IsolationForest backend; (BGMM tests to be added once PR for the backend goes in.); - [x] Tool-level docs. Minor TODOs:. - [x] Parameter-level docs.; - [x] Parameter/mode validation.; - [x] Refactor main code block for model training; it's a bit monolithic and procedural now.; - [x] Decide on behavior for ill-behaved annotations. E.g., all missing, zero variance. Future work:. - [ ] We could allow subsetting of annotations here, which might allow for easier treatment of ill-behaved annotations. However, I'd say enabling workflows where the set of annotations is fixed is the priority.; - [ ] We could do positive-unlabeled training more rigorously or iteratively. Right now, we essentially do a single iteration to determine negative data. This could perhaps be preceded by a round of refactoring to clean up model training and make it less procedural.; - [ ] Automatic threshold tuning could be built into the tool, see #7711. We'd probably have to introduce a ""validation"" label. Perhaps it makes sense to keep this sort of thing at the workflow level?; - [ ] In the positive-negative framework enforced by the Java code in this tool, a ""model"" is anything that assigns a score, we fit two models to different subsets of the data, and then take the difference of the two scores. While the python backend does give some freedom to specify a model, future developers may want to go beyond the framework itself. For example, more traditional classification frameworks, etc. could be explored. As an intermediate step, one could perhaps use the positive/negative scores from the current framework in a more sophisticated way (e.g., using them as features), rather than just taking their difference. This sort of future work could be developed completely independently of the codebase associated wit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948369:1411,refactor,refactoring,1411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948369,1,['refactor'],['refactoring']
Modifiability,"r example?. ```; * <h3>Output</h3>; * <p>; * A combined VCF with combined calls for each pair of samples specified and de-uniquified sample names.; * </p>; *; * <h3>Examples</h3>; * <pre>; * java -jar GenomeAnalysisTK.jar \; * -R ref.fasta \; * -T CombineSampleData \; * --variant vcf1.vcf \; * -o output.vcf; * </pre>; * <pre>; * java -jar GenomeAnalysisTK.jar \; * -R ref.fasta \; * -T CombineSampleData \; * --variant vcf1.vcf \; * --uniquified_sample_name NA12878.variant \; * --uniquified_sample_name NA12878.variant2; * -o output.vcf; * </pre>; ```. I don't get what's the difference between the first and second example. . In any case I'm not going to push this through now in light of all the TODOs:. ```; /*TODO: when this tool is moved into protected the following will have to be addressed:; * Do more robust error checking on sample name de-uniquification -- right now checks for pairs of <sampleName>.variantX and <sampleName>.variantY but should be extended to allow tagged VCF input into GenotypeGVCFs, which will produce names like <sampleName>.RODtagName; * Move sample name uniqufication/de-uniquification to SampleListUtils.java; * Check to make sure all genotype attributes are preserved after merge, e.g. allele phasing and genotype filters; * Generalize for all ploidies?; * Change GenotypeGVCFs --uniquifySamples argument from hidden (maybe still keep @advanced?); *; */; ```. ---. @ldgauthier commented on [Mon Nov 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-159059807). 1) Could be any two sets of data, I just did WGS + WEx for GTEx; 2) I think the first example probably should have two -V entries (that have the same sample names) compared with the second that has one input -V that has already uniquified samples. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260475512). @ldgauthier Will this tool be ported to GATK4? . ---. @ldgauthier commented on [Tue Nov 15 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2485:2013,extend,extended,2013,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485,1,['extend'],['extended']
Modifiability,r.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:31687,adapt,adapted,31687,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,r/ports/biology/gatk/work/gatk-4.0.11.0/build/libs/gatk-package-1.0-SNAPSHOT-local.jar'.; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at org.codehaus.groovy.reflection.CachedConstructor.invoke(CachedConstructor.java:83); 	at org.codehaus.groovy.runtime.callsite.ConstructorSite$ConstructorSiteNoUnwrapNoCoerce.callConstructor(ConstructorSite.java:105); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:60); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:235); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:255); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$StreamAction.visitFile(ShadowCopyAction.groovy:185); 	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite$PogoCachedMethodSiteNoUnwrapNoCoerce.invoke(PogoMetaMethodSite.java:210); 	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite.callCurrent(PogoMetaMethodSite.java:59); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:166); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$StreamAction.processFile(ShadowCopyAction.groovy:151); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1$1.processFile(NormalizingCopyActionDecorator.java:66); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator$1$1.processFile(DuplicateHandlingCopyAction,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:1341,plugin,plugins,1341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,"r_round=2000 --log_emission_sampling_rounds=100 --log_emission_sampling_median_rel_error=5.000000e-04 --max_advi_iter_first_epoch=1000 --max_advi_iter_subsequent_epochs=1000 --min_training_epochs=20 --max_training_epochs=100 --initial_temperature=2.000000e+00 --num_thermal_advi_iters=5000 --convergence_snr_averaging_window=5000 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=1 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=7.500000e-01 --disable_caller=false --disable_sampler=false --disable_annealing=false --interval_list=/tmp/intervals9016836733228000464.tsv --contig_ploidy_prior_table=/home/n.liorni/snakemake_cnv_gatk/resources/contig_ploidy_priors.tsv --output_model_path=/home/n.liorni/snakemake_cnv_gatk/results/cnv/ploidy/ploidy-model; Stdout: 15:09:46.970 INFO cohort_determine_ploidy_and_depth - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 15:09:47.017 INFO gcnvkernel.structs.metadata - Generating intervals metadata...; 15:09:47.024 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the germline contig ploidy determination model...; 15:09:50.320 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy emission sampler...; 15:09:50.321 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy caller...; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Global model variables: {'psi_j_log__', 'mean_bias_j_lowerbound__'}; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'psi_s_log__'}; 15:09:50.957 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 15:09:50.958 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 15:10:03.310 INFO gcnvkern",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:6703,variab,variable,6703,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['variab'],['variable']
Modifiability,"race_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcnvkernel. 23:38:13.336 DEBUG ScriptExecutor - Result: 0; 23:38:13.341 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4943,Config,ConfigFactory,4943,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"ram/md5/%s; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:3155,Config,ConfigFactory,3155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"ranch against 4.5.0.0, as well as this branch against itself (checking for reproducibility). Costs for this branch ($10.92) and 4.5.0.0 ($10.96) were quite comparable. Note that a small portion of these costs derives from Pf7-specific genotyping steps, which I did not bother to remove from the workflow. Runtime for the ploidy modeling and postprocessing steps were comparable. Interestingly, **runtime for the gCNV was ~20-25% longer with this branch than with 4.5.0.0, but memory usage fell by a factor of ~3 (~6GB to ~2GB)!** I am not sure if we could recoup the runtime with some more tweaking of the environment (perhaps double checking that optimized BLAS/MKL/etc. packages are properly used, changing environment variables/flags, etc.), but I think the decrease in memory usage is quite nice. Concordance was checked for the following quantities (4.5.0.0 is on the x-axis and this branch is on the y-axis in all plots below):. 1) Variational posterior means (`mu_*`) and standard deviations (`std_*`) for all analogous variables in the ploidy and gCNV models. There were some slight changes to the gCNV model in this branch (e.g., the functional form of the ARD prior was changed), which means some variables are no longer directly comparable. Furthermore, some variables (such as the bias factors W) are degenerate and cannot be immediately compared. Otherwise, there is good concordance between the remaining variables, e.g.:. ![image](https://github.com/broadinstitute/gatk/assets/11076296/614cf501-ca31-4199-badb-3194b7f78154); ![image](https://github.com/broadinstitute/gatk/assets/11076296/f615084d-d0bf-44e9-bcf5-98abd26ceb06); ![image](https://github.com/broadinstitute/gatk/assets/11076296/48570e53-024c-44b5-8835-3fd40b4c5866); ![image](https://github.com/broadinstitute/gatk/assets/11076296/99100e5d-05e2-4a5c-9d68-57db1b734029); ![image](https://github.com/broadinstitute/gatk/assets/11076296/abae09e1-70a5-4213-95a2-0cb10f9db192); ![image](https://github.com/broadinstitute/gatk/a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268:1391,variab,variables,1391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268,1,['variab'],['variables']
Modifiability,rapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:15070,Plugin,PluginResolutionServiceClient,15070,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['Plugin'],['PluginResolutionServiceClient']
Modifiability,rebase Dgs configure plugin packages,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5573:11,config,configure,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5573,2,"['config', 'plugin']","['configure', 'plugin']"
Modifiability,ree) | Coverage  | Complexity  | |; |---|---|---|---|; | [...bender/tools/spark/pathseq/PathSeqFilterSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BhdGhTZXFGaWx0ZXJTcGFyay5qYXZh) | `70.968% <> ()` | `7 <0> ()` | :arrow_down: |; | [...itute/hellbender/tools/spark/pathseq/PSFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BTRmlsdGVyLmphdmE=) | `92.617% <100%> (+0.531%)` | `33 <1> (+1)` | :arrow_up: |; | [...ools/spark/pathseq/PSFilterArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BTRmlsdGVyQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `80% <100%> (+1.429%)` | `2 <0> ()` | :arrow_down: |; | [...ellbender/transformers/AdapterTrimTransformer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90cmFuc2Zvcm1lcnMvQWRhcHRlclRyaW1UcmFuc2Zvcm1lci5qYXZh) | `92.857% <92.857%> ()` | `12 <12> (?)` | |; | [...nder/transformers/SimpleRepeatMaskTransformer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90cmFuc2Zvcm1lcnMvU2ltcGxlUmVwZWF0TWFza1RyYW5zZm9ybWVyLmphdmE=) | `94.286% <94.286%> ()` | `11 <11> (?)` | |; | [...nstitute/hellbender/utils/clipping/ClippingOp.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jbGlwcGluZy9DbGlwcGluZ09wLmphdmE=) | `84.365% <0%> (+1.629%)` | `91% <0%> (+2%)` | :arrow_up: |; | [...stitute/hellbender/utils/clipping/ReadClipper.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3354#issuecomment-317586310:1867,Adapt,AdapterTrimTransformer,1867,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3354#issuecomment-317586310,1,['Adapt'],['AdapterTrimTransformer']
Modifiability,refactor of VQSR filters,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6508:0,refactor,refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6508,1,['refactor'],['refactor']
Modifiability,refactor print sv evidence tool to use a single output stream,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7045:0,refactor,refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7045,1,['refactor'],['refactor']
Modifiability,refactored table writing in BasicSomaticValidator,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6136:0,refactor,refactored,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6136,1,['refactor'],['refactored']
Modifiability,refactoring code to remove confusion and unnecessary call to calculateChromosomeCounts,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4431:0,refactor,refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4431,1,['refactor'],['refactoring']
Modifiability,refactoring for testablity,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7946:0,refactor,refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7946,1,['refactor'],['refactoring']
Modifiability,refactoring in CalculateGenotypePosteriors to remove confusion,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4431:0,refactor,refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4431,1,['refactor'],['refactoring']
Modifiability,"refactoring our SmithWaterman code to prepare us for using native code optimized aligners. * Adding new interfaces `SmithWatermanAligner` and `SmithWatermanAlignment`.; * Refactoring `SWPairwiseAlignment` to be a `SmithWatermanAligner`, renaming it to SmithWatermanJavaAligner to distinguish it from future native aligners.; * Refactoring and renaming`SWPairwiseAlignmentUnitTest` and abstracting a superclass `SmithWatermanAlignerAbstractUnitTest` ; * Creating `SWNativeAlignerWrapper` which can accept a `SWAlignerNativeBinding` and wrap it into a `SmithWatermanAligner` as well as a test for it; * adding an option to `AssemblyBasedCallerArgumentCollection` which allows the aligner to be specified, currently we only have 1 real option; * adding an aligner as a field to Mutect2 and HaplotypeCaller, updating all library calls that use alignment to accept an aligner as an argument",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3600:0,refactor,refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3600,3,"['Refactor', 'refactor']","['Refactoring', 'refactoring']"
Modifiability,refactoring some stuff for clarity,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8598:0,refactor,refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8598,1,['refactor'],['refactoring']
Modifiability,removed unnecessary inheritance of M2 filtering arguments collection,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5498:20,inherit,inheritance,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5498,1,['inherit'],['inheritance']
Modifiability,"res (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:23047,parameteriz,parameterized,23047,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['parameteriz'],['parameterized']
Modifiability,rg.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initial,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:3251,Config,ConfigureActionsProjectEvaluator,3251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['Config'],['ConfigureActionsProjectEvaluator']
Modifiability,"rg\apache\commons\commons-pool2\2.8.0\commons-pool2-2.8.0.jar;C:\Program Files\JetBrains\IntelliJ IDEA 2020.1\lib\idea_rt.jar"" com.luz.push.PushApplication; Connected to the target VM, address: '127.0.0.1:62530', transport: 'socket'. . ____ _ __ _ _; /\\ / ___'_ __ _ _(_)_ __ __ _ \ \ \ \; ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \; \\/ ___)| |_)| | | | | || (_| | ) ) ) ); ' |____| .__|_| |_|_| |_\__, | / / / /; =========|_|==============|___/=/_/_/_/; :: Spring Boot :: (v2.3.0.RELEASE). 2020-05-29 15:14:30.695 INFO 12904 --- [ main] com.luz.push.PushApplication : Starting PushApplication on DESKTOP-05L3FQL with PID 12904 (C:\project\push\target\classes started by Sweet in C:\project\push); 2020-05-29 15:14:30.712 INFO 12904 --- [ main] com.luz.push.PushApplication : No active profile set, falling back to default profiles: default; 2020-05-29 15:14:32.088 WARN 12904 --- [ main] o.m.s.mapper.ClassPathMapperScanner : No MyBatis mapper was found in '[com.luz.push]' package. Please check your configuration.; 2020-05-29 15:14:32.662 INFO 12904 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8282 (http); 2020-05-29 15:14:32.675 INFO 12904 --- [ main] o.a.coyote.http11.Http11NioProtocol : Initializing ProtocolHandler [""http-nio-8282""]; 2020-05-29 15:14:32.676 INFO 12904 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat]; 2020-05-29 15:14:32.677 INFO 12904 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.35]; 2020-05-29 15:14:32.802 INFO 12904 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext; 2020-05-29 15:14:32.802 INFO 12904 --- [ main] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 1944 ms; 2020-05-29 15:14:32.899 INFO 12904 --- [ main] com.luz.push.utils.GcmUtils : start init gcm server; 2020-05-29 15:14:33.029 WARN 12904 --- [ main] c.g.a.oauth2.ComputeEngineCredenti",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:11692,config,configuration,11692,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['config'],['configuration']
Modifiability,"ribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 20g --executor-cores 4 --executor-memory 8g /gatk/gatk-package-4.0.4.0-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://926a0516ccf6:7077; 11:01:48.445 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:01:48.743 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.4.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:01:49.333 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@926a0516ccf6 on Linux v4.4.0-127-generic amd64; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-1~bpo8+1-b11; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineSpark - Start Date/Time: May 28, 2018 11:01:48 AM UTC; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:14666,variab,variables,14666,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"ries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:4360,Config,ConfigFactory,4360,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"ries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:52.477 INFO GermlineCNVCaller - Initializing engine; 23:43:52.479 DEBUG ScriptExecutor - Executing:; 23:43:52.479 DEBUG ScriptExecutor - python; 23:43:52.479 DEBUG ScriptExecutor - -c; 23:43:52.480 DEBUG ScriptExecutor - impo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:3884,Config,ConfigFactory,3884,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"rite_tribble=false -Dsamjdk.compression_level=1 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 15g --executor-cores 2 --executor-memory 8g /gatk/build/libs/gatk-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 4000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://680776067ebd:7077; 16:33:44.918 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 16:33:45.152 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 16:33:45.587 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 16:33:45.587 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.2.0-4-gb59d863-SNAPSHOT; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@680776067ebd on Linux v4.4.0-127-generic amd64; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-1~bpo8+1-b11; 16:33:45.589 INFO BwaAndMarkDuplicatesPipelineSpark - Start Date/Time: May 26, 2018 4:33:45 PM UTC; 16:33:45.589 INFO BwaAndMarkDupli",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:10926,variab,variables,10926,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"rmlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 08:48:45.928 INFO DetermineGer",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4921,Config,ConfigFactory,4921,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"roadinstitute/hellbender/engine/filters/CountingVariantFilter.java:197: error: cannot find symbol; 2022-08-16T00:09:07.4040311Z @VisibleForTesting; 2022-08-16T00:09:07.4040921Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4041294Z location: class CountingVariantFilter; 2022-08-16T00:09:07.4054361Z src/main/java/org/broadinstitute/hellbender/cmdline/GATKPlugin/GATKReadFilterPluginDescriptor.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4060164Z src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilter.java:75: error: cannot find symbol; 2022-08-16T00:09:07.4060614Z @VisibleForTesting; 2022-08-16T00:09:07.4061233Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4061591Z location: class ReadFilter; 2022-08-16T00:09:07.4083439Z src/main/java/org/broadinstitute/hellbender/utils/config/ConfigFactory.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4092135Z src/main/java/org/broadinstitute/hellbender/utils/config/GATKConfig.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4107682Z src/main/java/org/broadinstitute/hellbender/utils/variant/GATKVariantContextUtils.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4116317Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:3: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.4117746Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:4: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.4124264Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:32: error: cannot find symbol; 2022-08-16T00:09:07.4124816Z private static BiMap<Log.LogLevel, Level> loggingLevelNamespaceMap;; 2022-08-16T00:09:07.4125855Z symbol: class BiMap; 2022-08-16T00:09:07.4126189Z location: class LoggingUtils; 2022-08-16T00:09:07.4126674Z src/main/java/org/broadinstitute/he",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:12376,config,config,12376,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['config'],['config']
Modifiability,"roadinstitute/hellbender/engine/filters/CountingVariantFilter.java:197: error: cannot find symbol; 2022-08-16T22:45:53.8024772Z @VisibleForTesting; 2022-08-16T22:45:53.8025036Z symbol: class VisibleForTesting; 2022-08-16T22:45:53.8025212Z location: class CountingVariantFilter; 2022-08-16T22:45:53.8032154Z src/main/java/org/broadinstitute/hellbender/cmdline/GATKPlugin/GATKReadFilterPluginDescriptor.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8035089Z src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilter.java:75: error: cannot find symbol; 2022-08-16T22:45:53.8035234Z @VisibleForTesting; 2022-08-16T22:45:53.8035505Z symbol: class VisibleForTesting; 2022-08-16T22:45:53.8035658Z location: class ReadFilter; 2022-08-16T22:45:53.8087327Z src/main/java/org/broadinstitute/hellbender/utils/config/ConfigFactory.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8103864Z src/main/java/org/broadinstitute/hellbender/utils/config/GATKConfig.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8113680Z src/main/java/org/broadinstitute/hellbender/utils/variant/GATKVariantContextUtils.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8117654Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:3: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.8118430Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:4: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.8124030Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:32: error: cannot find symbol; 2022-08-16T22:45:53.8124383Z private static BiMap<Log.LogLevel, Level> loggingLevelNamespaceMap;; 2022-08-16T22:45:53.8124657Z symbol: class BiMap; 2022-08-16T22:45:53.8124810Z location: class LoggingUtils; 2022-08-16T22:45:53.8125227Z src/main/java/org/broadinstitute/he",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:14414,config,config,14414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,1,['config'],['config']
Modifiability,"roblem with the following location: '/home/jeremie/GATK/build/classes/java/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/home/jeremie/GATK/build/resources/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/jeremie/GATK/build/tmp/gatkDoc/javadoc.options'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 1 invalid unit(s) of work during this build to ensure correctness.; Please consult deprecation warnings for more details. BUILD FAILED in 33s; 5 actionable tasks: 5 executed; ```; which does not seem related to any changes I made.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500:1832,plugin,plugins,1832,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500,1,['plugin'],['plugins']
Modifiability,"rogram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.net.UnknownHostException: cngb-nas-f17-1: cngb-nas-f17-1: Name or service not known; 	at java.base/java.net.InetAddress.getLocalHost(InetAddress.java:1631); 	at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:891); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress$lzycompute(Utils.scala:884); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress(Utils.scala:884); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.util.Utils$.localHostName(Utils.scala:941); 	at org.apache.spark.internal.config.package$.<init>(package.scala:204); 	at org.apache.spark.internal.config.package$.<clinit>(package.scala); 	... 12 more; Caused by: java.net.UnknownHostException: cngb-nas-f17-1: Name or service not known; 	at java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method); 	at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:924); 	at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1504); 	at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:843); 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1494); 	at java.base/java.net.InetAddress.getLocalHost(InetAddress.java:1626); 	... 21 more; ```. #### Steps to reproduce; On a Linux machine without _Hadoop_, run `java -jar ../gatk-package-4.1.0.0-local.jar CreateReadCountPanelOfNormals --input in.counts.hdf5 --output out.pon.hdf5` locally. #### Expected behavior; Produce *out.pon.hdf5*. #### Actual behavior; Exit with error.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5686:4270,config,config,4270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686,2,['config'],['config']
Modifiability,"root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE"". $min_width; [1] 2. $alpha; [1] 0.01. $nperm; [1] 10000. $pmethod; [1] ""hybrid"". $kmax; [1] 25. $nmin; [1] 200. $eta; [1] 0.05. $trim; [1] 0.025. $undosplits; [1] ""none"". $undoprune; [1] ""0.05"". $undoSD; [1] 3. $help; [1] FALSE. Stderr: Error in sort(abs(diff(genomdat)))[1:n.keep] : ; only 0's may be mixed with negative subscripts; Calls: source ... segment -> inherits -> smooth.CNA -> trimmed.variance; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:163); 	at org.broadinstitute.hellbender.utils.segmenter.RCBSSegmenter.writeSegmentFile(RCBSSegmenter.java:114); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.applySegmentation(PerformSegmentation.java:185); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.doWork(PerformSegmentation.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2944:2704,inherit,inherits,2704,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944,1,['inherit'],['inherits']
Modifiability,"roportional to number of samples, number of intervals, number of bias covariates and max copy number. What the docs don't say is what the default is for the number of bias covariates _and_ how to take these numbers and project an approximate memory usage. 2. It would appear that GermlineCNVCaller will, by default, attempt to use all CPU cores available on the machine. From the WDL I see that setting environment variables `MKL_NUM_THREADS` and `OMP_NUM_THREADS` seems to control the parallelism? It would be nice if `GermlineCNVCaller` took a `--threads` and then set these before spawning the python process. 3. Runtime? This would be really nice to have some guidelines around as I get wildly varying results depending on how I'm running. My experimentation is with a) 20 45X WGS samples, b) bin size = 500bp, c) running on a 96-core general purpose machine at AWS with 384GB of memory. My first attempt a) scattered the genome into 48 shards of approximately 115k bins each, representing ~50mb of genome and b) ran 24 jobs concurrently but failed to set the environment variables to control parallelism. In that attempt the first wave of jobs were still running after 24 hours and getting close to finishing up the initial de-noising epoch, with 3/24 having failed due to memory allocation failures. My second attempt, now running, scattered the genome into 150 shards, and is running 12 jobs at a time with 8 cores each and the environment variables set. On the second attempt it looks like the jobs will finish the first denoising epoch in < 1 hour each. That's far faster than the 6x reduction in runtime you might expect if a) runtime is linear in the number of bins and b) runtime is proportional to 1/cpus used. Without doing a lot more experiments it's hard to tell whether the better runtime is due to less fighting over resources (I can imagine 24 jobs each running 96 threads could degrade performance) or because runtime is super-linear vs. number of bins. I'm not asking for total p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6166:1762,variab,variables,1762,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6166,1,['variab'],['variables']
Modifiability,rpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:615); 	at io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:261); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:360); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.access$1800(InstantiatingGrpcChannelProvider.java:81); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider$1.createSingleChannel(InstantiatingGrpcChannelProvider.java:231); 	at com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:72); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:241); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:219); 	at com.google.api.gax.rpc.ClientContext.create(ClientContext.java:199); 	at com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:89); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.<init>(BigQueryReadClient.java:129); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:110); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:102); 	at org.broadinstitute.hellbender.utils.bigquery.StorageAPIAvroReader.<init>(StorageAPIAvroReader.java:60); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.createSortedReferenceRangeCollectionFromBigQuery(ExtractCohortEngine.java:851); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.createVariantsFromUnsortedBigQueryRanges(ExtractCohortEngine.java:891); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.traverse(ExtractCohortEngine.java:224); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohort.traverse(ExtractCohort.java:335); 	at org.broadinstitute.hellbender.engin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7583:1917,Enhance,EnhancedBigQueryReadStub,1917,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7583,1,['Enhance'],['EnhancedBigQueryReadStub']
Modifiability,"rsions, but also novel adjacencies (BND records whose meanings cannot be fully resolved solely from assembly alignment signatures) as well as complex variants that theoretically could be arbitrarily complex (`<CPX>`, as long as we have assembled across the full event). . ## Planed organization. the `discovery` package could be divided roughly now into. ### interface. `SvDiscoveryDataBundle`, `SvDiscoverFromLocalAssemblyContigAlignmentsSpark`, `SvType`, `AnnotatedVariantProducer`. ### alignment prep (sub package). `AlignmentInterval`, `AlignedContig` (refactor `AssemblyContigWithFineTunedAlignments` into `AlignedContig`), `AlignedContigGenerator`, `AlignedAssembly`, `ContigAlignmentsModifier` (refactor `AlnModType` into it), `GappedAlignmentSplitter`, `StrandSwitch`, `FilterLongReadAlignmentsSAMSpark` (factor out the major methods in the new alignment filter by score into a 1st level class). ### type & location inference (sub package). * imprecise: refactor out methods from to-be-deprecated `DiscoverVariantsFromContigAlignmentsSAMSpark`. * alignment classification: `ChimericAlignment` and `NovelAdjacencyReferenceLocations` (very tricky to decouple the functionalities because both have over 50 uses), `AssemblyContigAlignmentSignatureClassifier`, `VariantDetectorFromLocalAssemblyContigAlignments`. * simple: `SimpleSVType`, `SvTypeInference`, `InsDelVariantDetector`, `BreakpointComplications` (rename to `BreakpointComplicationsForSimpleTypes`). * complex: `BreakEndVariantType`, `SuspectedTransLocDetector`, `SimpleStrandSwitchVariantDetector`. ### deprecated. `DiscoverVariantsFromContigAlignmentsSAMSpark` . It currently provides 3 groups of functionalities:. * novel adjacency detection (for ins, del, small dup, inversion only) by delegating to `ChimericAlignment.parseOneContig` and `NovelAdjacencyReferenceLocations(ChimericAlignment chimericAlignment, byte[] contigSequence, SAMSequenceDictionary)`; this should be deprecated; * exact variant type inference (delegated to `",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111:1322,refactor,refactor,1322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111,1,['refactor'],['refactor']
Modifiability,"rt extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to samp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:1017,refactor,refactor,1017,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['refactor'],['refactor']
Modifiability,"rt tensor, scalar; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/__init__.py"", line 17, in <module>; from theano.tensor import blas; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas.py"", line 155, in <module>; from theano.tensor.blas_headers import blas_header_text; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas_headers.py"", line 987, in <module>; if not config.blas.ldflags:; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 332, in __get__; val_str = self.default(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1451, in default_blas_ldflags; check_mkl_openmp(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1273, in check_mkl_openmp; """"""); RuntimeError: ; Could not import 'mkl'. If you are using conda, update the numpy; packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in; your environment for MKL 2018. If you have MKL 2017 install and are not in a conda environment you; can set the Theano flag blas.check_openmp to False. Be warned that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here is the pip list from my environment:. cached-property 1.5.2+computecanada ; cycler 0.11.0+computecanada ; enum34 1.1.10+computecanada ; gatkpythonpackages 0.1 ; gcnvkernel 0.8 ; h5py 3.1.0+computecanada ; intel-openmp 2021.1.1+computecanada; joblib 0.14.1+computecanada ; kiwisolver 1.3.1+computecanada ; matplotlib 3.3.4+computecanada ; mkl 2021.1.1+computecanada; numpy 1.17.3+computecanada ; pandas 1.0.3+computecanada ; patsy 0.5.3+computecanada ; Pillow 8.1.2+comput",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:4608,config,configdefaults,4608,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['config'],['configdefaults']
Modifiability,"ry - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:5781,Config,ConfigFactory,5781,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,"s VCF file to /staging/wes/1\_sample\_20210615/CNV\_calling/genotyped-intervals-case-A210066-vs-v7cohort.vcf.gz... ; ; 11:04:27.510 INFO PostprocessGermlineCNVCalls - Analyzing shard 1 / 1... ; ; 11:04:30.169 INFO PostprocessGermlineCNVCalls - Generating segments... ; ; 11:04:37.131 INFO PostprocessGermlineCNVCalls - Shutting down engine ; ; \[August 30, 2021 11:04:37 AM HKT\] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.27 minutes. ; ; Runtime.totalMemory()=2463105024 ; ; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; ; python exited with 1 ; ; Command Line: python /tmp/segment\_gcnv\_calls.8152704641395924200.py --ploidy\_calls\_path /staging/wes/healthy\_bams\_for\_CNV/using\_v7\_probe/v7\_case\_ploidy/v7\_cases\_ploidy\_1\_sample\_20210615-calls --model\_shards /staging/wes/healthy\_bams\_for\_C ; ; Stdout: 11:04:36.532 INFO segment\_gcnv\_calls - THEANO\_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast\_run,compute\_test\_value=ignore,openmp=true,blas.ldflags=-lmkl\_rt,openmp\_elemwise\_minsize=10 ; ; 11:04:36.532 INFO segment\_gcnv\_calls - Loading ploidy calls... ; ; 11:04:36.533 INFO gcnvkernel.io.io\_metadata - Loading germline contig ploidy and global read depth metadata... ; ; 11:04:36.543 INFO segment\_gcnv\_calls - Instantiating the Viterbi segmentation engine... Stderr: Traceback (most recent call last): ; ; File ""/tmp/segment\_gcnv\_calls.8152704641395924200.py"", line 92, in <module> ; ; args.intervals\_vcf, args.clustered\_vcf) ; ; TypeError: \_\_init\_\_() takes 6 positional arguments but 8 were given. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75) ; ; at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:112) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:193) ; ;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444:5176,variab,variable,5176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444,1,['variab'],['variable']
Modifiability,"s where results changed:. - For the snpeff test, since the behavior on this branch seems more correct to me than master, I tried running the GATK4 test case inputs with GATK3, and it produces exactly the same results as this branch does. So I think that issue was introduced by the original GATK4 port, and is fixed in this branch.; - The rest of the tests with changed results don't seem to hit your breakpoint, though. So I think we need to figure out why they changed, and maybe also compare them with GATK3 (which can be a pain because the output format is slightly different).; - As you mentioned, you changed the reference for testEvalTrackWithoutGenotypesWithSampleFields, which seems to have only affected the number of loci processed. So I'm unclear why that change was necessary. If the test truly should have been failing without this change, will it still fail if the change is reverted ? If not, can we fix it, and either way there should be a negative test for that case. A few other general comments:. - I changed this PR to `draft` mode for now, which just better categorizes it for our internal workflow purposes. When its ready for a detailed code review we can remove the `draft` status.; - The `HashMap<FeatureInput<VariantContext>, HashMap<String, Collection<VariantContext>>>` can be wrapped in a class with just a couple of methods, so we don't have to manifest that long type all over the place.; - I know this PR still in an interim state, but passing the VariantWalker in as an argument to the comp methods doesn't seem like a step forward to me. If we can't solve that problem completely in this PR (which is fine, I'm all for trying to contain this), are those changes necessary ? Perhaps that part should just wait for the next round.; - Any new classes/methods should use `final` for variables and parameters wherever applicable, and public classes and methods should have javadoc.; - Finally, I'm curious if you've tried any perf testing on this branch ? Is it better ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-744689987:1888,variab,variables,1888,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-744689987,1,['variab'],['variables']
Modifiability,"s(DefaultCredentialsProvider.java:124); at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:127); at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:100); at com.google.cloud.ServiceOptions.defaultCredentials(ServiceOptions.java:304); at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:278); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:83); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:31); at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:78); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.setGlobalNIODefaultOptions(BucketUtils.java:382); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:183); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Produced by pulling the docker image, **shutting off the internet connection**, mounting [helloHaplotypeCaller](https://drive.google.com/file/d/0B7akc6CTmxIHdy11R1M3ZjJJdUU/view), and running:. ```shell; docker run \; --rm \; -v /Users/kshakir/Downloads/helloHaplotypeCaller:/data \; broadinstitute/gatk:4.0.11.0 \; gatk \; HaplotypeCaller \; -R /data/ref/human_g1k_b37_20.fasta \; -I /data/inputs/NA12878_wgs_20.bam \; -O test.vcf; ```. Adding in a `GOOGLE_APPLICATION_CREDENTIALS` environment variable short circuits the above stack trace. ```shell; docker run \; -e GOOGLE_APPLICATION_CREDENTIALS=whatever; --rm \; -v /Users/kshakir/Downloads/helloHaplotypeCaller:/data \; broadinstitute/gatk:4.0.11.0 \; gatk \; HaplotypeCaller \; -R /data/ref/human_g1k_b37_20.fasta \; -I /data/inputs/NA12878_wgs_20.bam \; -O test.vcf; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443830843:4199,variab,variable,4199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443830843,1,['variab'],['variable']
Modifiability,"s.REFERENCE_FASTA : null; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchB",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3692,Config,ConfigFactory,3692,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"s.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.9",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4596,Config,ConfigFactory,4596,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"s/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:192); at org.broadinstitute",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:5007,config,configparser,5007,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['config'],['configparser']
Modifiability,"s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG Conf",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:3710,Config,ConfigFactory,3710,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcnvkernel. 23:38:13.336 DEBUG ScriptExecutor - Result: 0; 23:38:13.341 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:5163,Config,ConfigFactory,5163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; > 21:13:04.231 INFO GenotypeGVCFs - Initializing engine; > 21:13:11.834 INFO GenotypeGVCFs - Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 21:14:17.635 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.858 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:4554,Config,ConfigFactory,4554,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2281); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83); ... 27 more; Caused by,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:32354,adapt,adapted,32354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,"scenario: someone wants to use gatk4 as a framework and add new tools. They need show on the list of tools etc. They package names are the user's, ie not org.broadinstitute.hellbender*. The way to do this is to extend Main but we have no example and not documentation of this (other that in the Main class, which is not the right place - I think it should be in README or some such).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1397:211,extend,extend,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1397,1,['extend'],['extend']
Modifiability,scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:13052,adapt,adapted,13052,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,"se 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2020-07-14 05:09:30,25] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2020-07-14 05:09:30,26] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,26] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,36] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2020-07-14 05:09:30,46] [info] SingleWorkflowRunnerActor: Version 51; [2020-07-14 05:09:30,48] [info] SingleWorkflowRunnerActor: Submitting workflow; [2020-07-14 05:09:30,55] [info] Unspecified type (Unspecified version) workflow 968be82c-eef3-4bdb-a1ab-3d4e2ca70674 submitted; [2020-07-14 05:09:30,66] [info] SingleWorkflowRunnerActor: Workflow submitted 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,67] [info] 1 new workflows fetched by cromid-ca5c695: 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,68] [info] WorkflowManagerActor Starting workflow 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,69] [info] WorkflowManagerActor Successfully started WorkflowActor-968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,69] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2020-07-14 05:09:30,72] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2020-07-14 05:09:30,83] [info] MaterializeWorkflowDescriptorActor [968be82c]: Parsing workflow as WDL 1.0; [2020-07-14 05:09:31,60] [info] MaterializeWorkflowDescriptorActor [968be82c]: Call-to-Backend assignments: ValidateBamsWf.ValidateBAM -> Local; [2020-07-14 05:09:31,82] [warn] Local [968be82c]: Key/s [memory, disks] is/are not supported by backend. Unsupported attributes will not be part of job executions.; [2020-07-14 05:09:35,38] [info] Not triggering log of token queue status. Effective log interval = ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:3664,config,configured,3664,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,1,['config'],['configured']
Modifiability,"se; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; > 21:13:04.231 INFO GenotypeGVCFs - Initializing engine; > 21:13:11.834 INFO GenotypeGVCFs - Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 21:14:17.635 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.858 DEBUG Genoty",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:4491,Config,ConfigFactory,4491,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:2663,refactor,refactor,2663,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['refactor'],['refactor']
Modifiability,see if PerReadAlleleLikelihoodMap can be / should be refactored out,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1467:53,refactor,refactored,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1467,1,['refactor'],['refactored']
Modifiability,seems like a good candidate to be moved into the config files...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3552#issuecomment-327585040:49,config,config,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3552#issuecomment-327585040,1,['config'],['config']
Modifiability,"ser-images.githubusercontent.com/61913000/87845904-eea14f80-c8e0-11ea-90bd-235c9205f72f.png"">. (gatk) root@bc3c6aca6231:/gatk/my_data/tools# java -jar cromwell-51.jar run /gatk/my_data/seq-format-validation/validate-bam.wdl --inputs /gatk/my_data/seq-format-validation/validate-bam.inputs.json; [2020-07-14 05:09:22,78] [info] Running with database db.url = jdbc:hsqldb:mem:f10b64bd-d8ca-4428-917b-311fca24c372;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-07-14 05:09:29,37] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-07-14 05:09:29,47] [info] Running with database db.url = jdbc:hsqldb:mem:e337a356-2f0c-4389-92c5-255465180f24;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,89] [info] Slf4jLogger started; [2020-07-14 05:09:30,10] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-ca5c695"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2020-07-14 05:09:30,23] [info] Metadata summary refreshing every 1 second.; [2020-07-14 05:09:30,23] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2020-07-14 05:09:30,25] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2020-07-14 05:09:30,26] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,26] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,36] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2020-07-14 05:09:30,46] [info] SingleWorkflowRunnerActor: Version 51; [2020-07-14 05:09:30,48] [info] SingleWorkflowRunnerActor: Submitting workflow; [2020-07-14 05:09:30",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:1858,config,configuration,1858,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,1,['config'],['configuration']
Modifiability,"serException.BadArgumentValue(""ERC/gt_mode"",""you cannot request reference confidence output and GENOTYPE_GIVEN_ALLELES at the same time"");; ; SCAC.genotypeArgs.STANDARD_CONFIDENCE_FOR_EMITTING = -0.0;; SCAC.genotypeArgs.STANDARD_CONFIDENCE_FOR_CALLING = -0.0;; ; -; // also, we don't need to output several of the annotations; annotationsToExclude.add(""ChromosomeCounts"");; annotationsToExclude.add(""FisherStrand"");; @@ -651,6 +651,9 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; if (!SCAC.annotateAllSitesWithPLs); logger.info(""All sites annotated with PLs forced to true for reference-model confidence output"");; SCAC.annotateAllSitesWithPLs = true;; + } else if ( ! doNotRunPhysicalPhasing ) {; + doNotRunPhysicalPhasing = true;; + logger.info(""Disabling physical phasing, which is supported only for reference-model confidence output"");; }; ; if ( SCAC.AFmodel == AFCalcFactory.Calculation.EXACT_GENERAL_PLOIDY ); @@ -678,7 +681,7 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; if( SCAC.genotypingOutputMode == GenotypingOutputMode.GENOTYPE_GIVEN_ALLELES && consensusMode ); throw new UserException(""HaplotypeCaller cannot be run in both GENOTYPE_GIVEN_ALLELES mode and in consensus mode. Please choose one or the other."");; ; - genotypingEngine = new HaplotypeCallerGenotypingEngine( getToolkit(), SCAC, tryPhysicalPhasing);; + genotypingEngine = new HaplotypeCallerGenotypingEngine( getToolkit(), SCAC, !doNotRunPhysicalPhasing);; // initialize the output VCF header; final VariantAnnotatorEngine annotationEngine = new VariantAnnotatorEngine(Arrays.asList(annotationClassesToUse), annotationsToUse, annotationsToExclude, this, getToolkit());; ; @@ -699,8 +702,10 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; VCFConstants.DEPTH_KEY,; VCFConstants.GENOTYPE_PL_KEY);; ; - if ( tryPhysicalPhasing ); - headerInfo.add(new VCFFormatHeaderLine(HAPLOTYPE_CALLER_PHASING_KEY, VCFHeaderL",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237:3579,extend,extends,3579,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237,1,['extend'],['extends']
Modifiability,"ses many transcripts to no longer be categorized as protein coding. Therefore, the ground truth (mostly/totally in `FuncotatorIntegrationTest`) had to be modified. *Please carefully review the ground truth changes*.; - Introduces the `CompsiteOutputRenderer`, which is composed of multiple output renderers. This is used when output type is `SEG`, so that it can write both output files simultaneously.; - Introduces the `GeneListOutputRenderer`. This does not write anything to disk until the entire input file is processed. The actual writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do the actual writing.; - Introduces the `SimpleTsvOutputRenderer`. This output renderer is very flexible and renders a tab-separated text file based on several output rules. Formats are driven through config files. And developers can limit the output columns to ignore extraneous funcotation fields. Note that excluded fields are honored, regardless. If a configuration + parameter combination would result in this class producing an empty file, an exception is thrown. More notes are in the javadocs of the class.; - Currently, only the `GencodeFuncotationFactory` can actually funcotate segments. ; - Code base currently enforces only small mutations when running `Funcotator` (segs are funcotated as CANNOT_DETERMINE) and only segments when running `FuncotateSegments` (small mutations produce exception). This is enforced with flags in the code. The backend does not disallow a mixture for future use. This may prove important when funcotating CNVs from VCFs produced by tools other than `ModelSegments`.; - Added copy creation method for FuncotationMap based on Kryo. Also, added the necessary Kryo registrations. T",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5941:2094,flexible,flexible,2094,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941,1,['flexible'],['flexible']
Modifiability,"sh Babadi <mehrtash@broadinstitute.org>; Date: Wed Nov 15 01:50:03 2017 -0500. Polished code, ready for review; ; gCNV computational kernel (initial release); ; renaming gammas_s to psi_s to uniformity (sample-specific unexplained variance); ; renamed determine_ploidy_and_depth.py to cohort_determine_ploidy_and_depth.py; finite-temperature forward-backward algorithm; in the ploidy model, replaced alpha_j (NB over-dispersion) with psi_j (unexplained variance) for uniformity. Also, added the possibility of sample-specific unexplained variance in the germline contig ploidy model; ; updated I/O routines and CLIs according to team discussion; ; updated I/O routines and CLIs according to team discussion; ; changed the output layout of the ploidy determination tool; refactored parts of io.py; upped the version to 0.3 as it is not backwards compatible anymore; ; case ploidy determination tool from a given ploidy model; major code cleanup and refactoring of I/O module; refactoring of common CLI script snippets; ; removed all ""targets""; some code cleanup; ; pad flat class bitmask w/ a given padding value in the hybrid q_c_expectation_mode; option to disable annealing and keep the temperature fixed; ; bugfix in finite-temperature forward-backward; further refactoring of model I/O; ; the option to take a previously trained model as starting point in cohort CLI; the option to take previous calls as a starting point in cohort CLI; ; option to save and load adamax moments; ; import/export adamax bias correction tensor; ; refactoring related to fancy opt I/O; added average ploidy column to read depth; updated docs of hybrid inference; ; modeling intervals can span multiple contigs now; ploidy can change; across contigs with no issue; ; save/load adamax state to .npy instead of .tsv for speed; ; part 1 of doc updates; ; part 2 of doc updates; ; part 3 of doc updates; ; part 4 of doc updates; ; bumped version to 0.5; readme; ; update readme; ; last minute stylistic doc updates.; ````",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:11261,refactor,refactoring,11261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,4,['refactor'],['refactoring']
Modifiability,"share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5686:1652,Config,Config,1652,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686,1,['Config'],['Config']
Modifiability,"sing ) {; + doNotRunPhysicalPhasing = true;; + logger.info(""Disabling physical phasing, which is supported only for reference-model confidence output"");; }; ; if ( SCAC.AFmodel == AFCalcFactory.Calculation.EXACT_GENERAL_PLOIDY ); @@ -678,7 +681,7 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; if( SCAC.genotypingOutputMode == GenotypingOutputMode.GENOTYPE_GIVEN_ALLELES && consensusMode ); throw new UserException(""HaplotypeCaller cannot be run in both GENOTYPE_GIVEN_ALLELES mode and in consensus mode. Please choose one or the other."");; ; - genotypingEngine = new HaplotypeCallerGenotypingEngine( getToolkit(), SCAC, tryPhysicalPhasing);; + genotypingEngine = new HaplotypeCallerGenotypingEngine( getToolkit(), SCAC, !doNotRunPhysicalPhasing);; // initialize the output VCF header; final VariantAnnotatorEngine annotationEngine = new VariantAnnotatorEngine(Arrays.asList(annotationClassesToUse), annotationsToUse, annotationsToExclude, this, getToolkit());; ; @@ -699,8 +702,10 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; VCFConstants.DEPTH_KEY,; VCFConstants.GENOTYPE_PL_KEY);; ; - if ( tryPhysicalPhasing ); - headerInfo.add(new VCFFormatHeaderLine(HAPLOTYPE_CALLER_PHASING_KEY, VCFHeaderLineCount.UNBOUNDED, VCFHeaderLineType.String, ""Physical phasing information, each unique ID within a given sample (but not across samples) connects alternate alleles as occurring on the same haplotype""));; + if ( ! doNotRunPhysicalPhasing ) {; + headerInfo.add(new VCFFormatHeaderLine(HAPLOTYPE_CALLER_PHASING_ID_KEY, 1, VCFHeaderLineType.String, ""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group""));; + headerInfo.add(new VCFFormatHeaderLine(HAPLOTYPE_CALLER_PHASING_GT_KEY, 1, VCFHeaderLineType.String, ""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another""));; + }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237:4352,extend,extends,4352,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237,1,['extend'],['extends']
Modifiability,"st and there's no batch api for it? Multi layer docker builds are pretty standard from what I understand. . It sounds like your suggestions are talking about 2 slightly different issues to me. 1. Too many layers:. We typically have squashed the GATK docker images, but we recently switched to building our release images with google cloud build. Since squash is *STILL* an experimental feature in docker we've had trouble getting it to work there. Since the size reduction was pretty minimal from squashing we figured it would be ok to not prioritize it. It's definitely possible for us to consolidate various layers in the build. Or manually squash the images. We can take a look for our next release. Wide workflows on azure are something we need to support. 2. Docker size reduction:; I've spend a lot of time looking at this in the past. Our docker image is huge, but it's mostly due to the massive size of our python and R dependencies. I've done a bunch of work reducing temporary files in independent layers and using multiple stages to reduce the size. There's not much low hanging fruit left there. Similarly, moving to alpine is tricky an has limited benefit. GATK packages a number of C libraries which do not work out of the box on alpine due to the different C runtime. (At least that was the case the last time I investigated it a few years ago. ) I suspect there's a way to port things so they work on it, but it's not something we can do now. It also wouldn't be much of a help, the base image is completely dwarfed by piles of python and R dependencies which are very difficult to safely trim. Anyway, that's the state of things. We've considered a java only image for a while which would be much smaller than the current one. (although still fat by most docker standards...). We've never released one publicly because it seemed like it might cause confusion, but it's a reasonable possibility. . If you have any secret methods to reduce the size of python or R installations we're ha",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1934859427:1203,layers,layers,1203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1934859427,1,['layers'],['layers']
Modifiability,"stTools$.getAst(AstTools.scala:263); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:170); 	at scala.util.Try$.apply(Try.scala:213); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:630); 	at womtool.graph.GraphPrint$.generateWorkflowDigraph(GraphPrint.scala:19); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:131); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:162); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:167); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:24); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:24); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```. for multi sample:; ```; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: Unrecognized token on line 31, column 50:. <title>gatk/mutect2_multi_sample.wdl at master ? broadinstitute/gatk ? GitHub</title>; ^; 	at wdl.draft2.parser.WdlParser.unrecognized_token(WdlParser.java:6975); 	at wdl.draft2.parser.WdlParser.lex(WdlParser.java:7048); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:263); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:170); 	at scala.util.Try$.apply(Try.scala:213); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6261:1614,adapt,adapted,1614,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6261,1,['adapt'],['adapted']
Modifiability,"stTools$.getAst(AstTools.scala:263); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:170); 	at scala.util.Try$.apply(Try.scala:213); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:630); 	at womtool.graph.GraphPrint$.generateWorkflowDigraph(GraphPrint.scala:19); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:131); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:162); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:167); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:24); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:24); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```. for pon; ```; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: Unrecognized token on line 151, column 69:. gatk GenomicsDBImport --genomicsdb-workspace-path pon_db -R ~{ref_fasta} -V ~{sep=' -V ' input_vcfs} -L ~{intervals}; ^; 	at wdl.draft2.parser.WdlParser.unrecognized_token(WdlParser.java:6975); 	at wdl.draft2.parser.WdlParser.lex(WdlParser.java:7048); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:263); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:170); 	at scala.util.Try$.apply(Try.scala:213); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); 	at wdl.draft2.model.WdlNamespaceWithWorkf",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6261:3254,adapt,adapted,3254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6261,1,['adapt'],['adapted']
Modifiability,"stributions/timeseries.py"", line 1, in <module>; import theano.tensor as tt; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:4873,config,configparser,4873,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['config'],['configparser']
Modifiability,"sync_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; > 21:13:04.231 INFO GenotypeGVCFs - Initializing engine; > 21:13:11.834 INFO GenotypeGVCFs - Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 21:14:17.635 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.858 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 13 > 12",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:4622,Config,ConfigFactory,4622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,"t convergence to 1% was achieved after about 250 iterations. I also did not initialize with PCA. However, upping to T = 10^6 causes out of memory. Not sure if this could be naively alleviated by setting theano flags appropriately, but I think we will probably want to minibatch in T instead. Note also that this model uses the exact Poisson likelihood. Composing with an HMM segmentation step, perhaps alternating for a few iterations, would give the gCNV PoN without the Gaussian approximation we use. ---. @samuelklee commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-302234920). The same run of T = 10^5 and N = 100 took <4 minutes on the gsa5 Tesla K40c GPU---about a 3x speedup over my home CPU. A slightly larger run of T = 1.5 * 10^5 and N = 200 took 10 minutes and 6GB of the GPU's 12GB memory. (I did start running into some weird theano/pymc3 errors when I tried to go bigger, unfortunately.) Moving to the GPU does require a bit of extra configuration but is relatively trivial. The real business goes down in exactly 11 lines of code, which cleanly specify the gCNV probabilistic model for read counts:. ```; with pm.Model() as model:; alpha_u = Uniform(name='alpha_u', lower=alpha_min, upper=alpha_max, shape=D); m_t = Uniform(name='m_t', lower=m_min, upper=m_max, shape=T); psi_t = Uniform(name='psi_t', lower=psi_min, upper=psi_max, shape=T); depth_s = Uniform(name='depth_s', lower=depth_min, upper=depth_max, shape=N); ; z_su = Normal(name='z_us', mu=0., sd=1., shape=(N, D)); W_tu = Normal(name='W_tu', mu=0., sd=1. / sqrt(alpha_u), shape=(T, D)); mu_st = Deterministic(name='mu_st', var=z_su.dot(W_tu.T) + m_t); b_st = Normal(name='b_st', mu=mu_st, sd=sqrt(psi_t), shape=(N, T)); n_ts = Poisson(name='n_ts', mu=depth_s * exp(b_st).T, observed=n_ts_data); ; fit_pm = pm.variational.advi(model=model, n=num_iterations, learning_rate=learning_rate, random_seed=random_seed, eval_elbo=eval_elbo_iterations); ```. @eitanbanks @droa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2984:2178,config,configuration,2178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2984,1,['config'],['configuration']
Modifiability,"t scan_opt; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/scan_module/scan_opt.py"", line 60, in <module>; from theano import tensor, scalar; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/__init__.py"", line 17, in <module>; from theano.tensor import blas; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas.py"", line 155, in <module>; from theano.tensor.blas_headers import blas_header_text; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas_headers.py"", line 987, in <module>; if not config.blas.ldflags:; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 332, in __get__; val_str = self.default(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1451, in default_blas_ldflags; check_mkl_openmp(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1273, in check_mkl_openmp; """"""); RuntimeError: ; Could not import 'mkl'. If you are using conda, update the numpy; packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in; your environment for MKL 2018. If you have MKL 2017 install and are not in a conda environment you; can set the Theano flag blas.check_openmp to False. Be warned that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here is the pip list from my environment:. cached-property 1.5.2+computecanada ; cycler 0.11.0+computecanada ; enum34 1.1.10+computecanada ; gatkpythonpackages 0.1 ; gcnvkernel 0.8 ; h5py 3.1.0+computecanada ; intel-openmp 2021.1.1+computecanada; joblib 0.14.1+computecanada ; kiwisolver 1.3.1+compute",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:4433,config,configdefaults,4433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['config'],['configdefaults']
Modifiability,"t$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:530); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getGermlineAltAlleleFrequencies(SomaticGenotypingEngine.java:354); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getNegativeLogPopulationAFAnnotation(SomaticGenotypingEngine.java:337); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:155); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:259); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:306); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; ```. one happened while working on chr21, the other on chr9",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7494#issuecomment-936771625:2971,variab,variable,2971,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494#issuecomment-936771625,1,['variab'],['variable']
Modifiability,t&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9kcmFnc3RyL0NhbGlicmF0ZURyYWdzdHJNb2RlbC5qYXZh) | `70.345% <> ()` | |; | [...r/utils/fasta/CachingIndexedFastaSequenceFile.java](https://codecov.io/gh/broadinstitute/gatk/pull/7920/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9mYXN0YS9DYWNoaW5nSW5kZXhlZEZhc3RhU2VxdWVuY2VGaWxlLmphdmE=) | `70.330% <> (-1.099%)` | :arrow_down: |; | [...t/java/org/broadinstitute/hellbender/MainTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7920/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9NYWluVGVzdC5qYXZh) | `2.564% <> (-82.182%)` | :arrow_down: |; | [...Plugin/GATKAnnotationPluginDescriptorUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7920/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS0Fubm90YXRpb25QbHVnaW5EZXNjcmlwdG9yVW5pdFRlc3QuamF2YQ==) | `7.219% <> (-81.016%)` | :arrow_down: |; | [...GATKPlugin/GATKReadFilterPluginDescriptorTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7920/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yVGVzdC5qYXZh) | `0.484% <> (-88.136%)` | :arrow_down: |; | [...lbender/engine/AssemblyRegionIteratorUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7920/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7920#issuecomment-1239413884:2777,Plugin,Plugin,2777,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7920#issuecomment-1239413884,1,['Plugin'],['Plugin']
Modifiability,"tCoverage simply counts the number of overlapping reads with each target. Optionally, low quality calls are hard filtered. Here, we propose a probabilistic approach that avoids the usage of hard filters and fits well with the new probabilistic target coverage modeler. By definition, mapping quality MAPQ = -10 \log_10{mapping position is wrong} (see http://samtools.github.io/hts-specs/SAMv1.pdf, pg 5, item 5). It is defined in the range [0, 2^8-1]. The specific value 255 is reserved for when MAPQ is not available. Most MAPQs are well below 255. We consider the following process for assigning reads to each target. Pick a read ""k"" aligned to target ""t"" with a given MAPQ_k. By definition, it maps to the genomic position ""x"" with p_x = 1 - 10^{-MAPQ_k/10}, and to some other position with probability 1 - p_x. We refer to the alignment genomic position of read k as x_k, and the exome target(s) it overlaps with T_k. Let's assume we have T exome targets, and let z_{kt} be a 1-of-#T indicator variable for a read where t is a target and #T is the number of all exome targets. \pi_{kq} = P(z_{kq} = 1) =. p_k x O_{kq} if q \in T_k; (1 - p_k) / (#T - #T_k) if q \notin T_k. Here, O_{kq} is the fractional overlap of the read to an exome target q. Note that since we don't have the information about the next best alignment position, we take a flat prior. Finally, the number of reads belonging to target t, n_t, reads as:. n_t = \sum_k z_{kt}. Since there are many reads, n_t will be approximately Gaussian. It is an elementary calculation to calculate coverage mean E[n_t] and coverage variance var[n_t] in terms of \pi_{kq}. In the probabilistic target coverage model, var[n_t] will be added to the statistical noise. So, the read count collection will have two entries for each target: coverage mean, and coverage variance. ---. @mbabadi commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/748#issuecomment-254812286). @samuelklee @asmirnov239 ; The outcome of",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2908:1129,variab,variable,1129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2908,1,['variab'],['variable']
Modifiability,taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; Using GATK jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; 20:41:36.853 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 20:41:37.246 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:41:37.277 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/zorzan/libgkl_compression6179723182683465083.so; 20:41:37.613 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 20:41:37.613 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 20:41:37.613 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 20:41:37.614 INFO PathSeqPipelineSpark - Executing as zorzan@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 20:41:37.614 INFO PathSeqPipelineSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11; 20:41:37.614 INFO PathSeqPipelineSpark - Start ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:2074,variab,variables,2074,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"te). Strands of the intervals indicate whether the distal target intervals are; * upstream or downstream of their proposed breakpoints: true indicates that the breakpoint is upstream of the interval; * start position; false indicates that the breakpoint is downstream of the interval end position; */; ```. What else would you like to see documented there? . - The use of the word strand in this case is largely driven by a mapping of these data structures to the BEDPE format, which is the older format for representing breakpoints implied by paired-end mapping data without assembly. If you only consider read pair mappings, strand has the natural interpretation of being the strand to which reads aligned. For example, a deletion's two intervals have strands `+` and `-` because the `+` reads align at left breakpoint and `-` reads align near the right breakpoint. Extending the concept to supplementary mappings of split reads muddies the concept a bit, which made me change the definition of strand to the existing one: whether the evidence suggests a breakpoint upstream of the interval start or downstream of the interval end. . - I created `StrandedInterval` mostly just as a data container since I was often passing around an interval and an associated strand, and using them in conjunction with the `PairedStrandedIntervalTree` data structure. My goal with those was to have them be utility classes that could be used by anyone without regards to the particular mechanics of imprecise evidence clustering I've implemented here. I'd prefer to put the definition of how we're interpreting the interval and strand in our logic classes (`BreakpointEvidence`, `EvidenceTargetLink`, and EvidenceTargetLinkClusterer`). Does that make sense?. - A ""distal target region"" can be represented by a `StrandedInterval`. So can the original, proximal (non-distal) location of the breakpoint evidence. An `EvidenceTargetLink` has the two `StrandedInterval` objects representing the proximal and distal loca",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3628#issuecomment-333857471:1758,Extend,Extending,1758,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3628#issuecomment-333857471,1,['Extend'],['Extending']
Modifiability,"team would potentially be willing to accept a PR to add a feature to GenotypeGVCFs. The general problem is this:. 1) When running GenotypeGVCFs, the default is to output variant sites, and this will therefore vary based on the set of samples. While there is an argument to include every site, calling against every position of the genome takes a very long time.; 2) As you know, a VCF file generally only includes variable sites in the current samples. Therefore, this doesnt differentiate between the situation where all samples have no data and when all samples are wild-type.; 3) We want to merge VCFs with data from different cohorts, including WGS and WES. It's just not practical to call 1000s of samples as one unit through GenotypeGVCFs (we're constantly adding new data and would need to keep re-calling). When merging these VCFs, we see a problem that is especially acute at sites with relatively rare variants. If the variant is only present in one or a few input VCFs, the other VCFs frequently lack that site (they are all wild-type). On merge, this is interpreted as no-data, which can be misleading. . The best solution I can devise is to force the input VCFs to output at a whitelist of sites, including if all samples are non-variant. While GenotypeGVCFs can be made to output at every genomic position, outputting everything is a huge leap in computational time. . I would propose to make a PR to augment GenotypeGVCFs to support an ""--always-output-calls-whitelist"" argument. The user can provide a FeatureInput. If provided, GenotypeGVCFs would output all variable sites (existing behavior), and also output any position spanning the intervals of this file, even if all samples at wild-type. . I only just started to look at how to implement this - i can some back with a more specific proposal. However, my initial thought is that we need to hook into drivingVariants or LocusWalker.traverse. Does your team have any thoughts on this, before we spend too much time on it? Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6239:1608,variab,variable,1608,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6239,1,['variab'],['variable']
Modifiability,tentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.Composit,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:16967,plugin,plugin,16967,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['plugin'],['plugin']
Modifiability,ter yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 2019-01-07 11:33:18 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:19 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 11:33:24.377 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:33:24.549 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:33:26.271 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.272 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 11:33:26.272 INFO CountReadsSpark - For support and documentat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:1963,config,configuration,1963,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['config'],['configuration']
Modifiability,"termineGermlineContigPloidy - HTSJDK Defaults.REFERENCE_FASTA : null; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.92",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4405,Config,ConfigFactory,4405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,ternal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(Def,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:17284,plugin,plugin,17284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['plugin'],['plugin']
Modifiability,tests for CigarUtils + refactored methods in CigarUtils. ; Did not add tests to isValid because pull req #380 is addressing this method. There's a potential issue in countRefBasesBasedOnCigar - it's not clear why the implementation does what it does. @amilev can you comment on the intended semantics of this method and whether it can/should use `CigarOperator.consumesReferenceBases`?. addresses #153 and #450 ; @vruano please review.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/455:23,refactor,refactored,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/455,1,['refactor'],['refactored']
Modifiability,thanks for the review @kcibul. I made some changes accordingly. re: PrepareCallset file of sample names. That would be nice! It would make this workflow simpler and it also simplifies the access requirements for PrepareCallset. re: Dockstore. We actually ruled this out because Terra says that the definition of a method configuration can change automatically if its updated in dockstore. Which can be useful but it adds a security risk since a compromised Dockstore can change the definition of the production AoU extraction WDL which runs with highly elevated permissions. We already have a script that creates method configurations from github so I can probably add something a little hacky to resolve relative imports to the raw github file that it refers to.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7242#issuecomment-846494686:321,config,configuration,321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7242#issuecomment-846494686,2,['config'],"['configuration', 'configurations']"
Modifiability,"that worked, thanks. I had a typo in the variable ""recaFile"" vs ""recalFile"". . how does this group want to proceed with alerting on failures? email? hipchat? slack? carrier pigeon? and who should be on the list?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-227783654:41,variab,variable,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-227783654,1,['variab'],['variable']
Modifiability,"the Main class is nominally extensible and our doc claims "" If you want your own single command line program, extend this class and give instanceMain a new list of java packages in which to search for classes that extend CommandLineProgram."". we need a test + example of this (test can serve as and example tool)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1396:110,extend,extend,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1396,2,['extend'],['extend']
Modifiability,"the deletion more or less the same and call it (assigning B to the variant and A/C to reference) ; - At the second position:; -- DRAGEN (and GATK with the `--disable-spanning-event-genotyping` argument enabled) follow the GATK3 approach of assigning haplotype C to the variant and the A and B haplotypes to the reference. The B haplotype is assigned as such because the deletion does not START at position 224905964 thus its reference according to the old way of assigning likelihoods. This means that all of the likelihoods from the true deletion at this site are weighted towards the reference which will end up drowning out the SNP call resulting in no SNP being called at this site.; -- GATK assigns C to the variant, A to to the reference, and B to a third option spanning deletion which prevents the deletion from outweighing the likelihoods assigned to the SNP resulting in better performance at many sites. This pattern even extends to SNP sites where a deletion was not called, since we still assign the haplotype to ""spanning deletion"" if there was a deletion at that site. . For indels however this can cause some extra false positives at sites like this one (the left variant under the deletion in the gatk track):; <img width=""1616"" alt=""Screen Shot 2020-07-14 at 4 09 47 PM"" src=""https://user-images.githubusercontent.com/16102845/87471543-86a2ee80-c5ec-11ea-9cdd-8acf1beb8c14.png"">; <img width=""178"" alt=""Screen Shot 2020-07-14 at 4 10 45 PM"" src=""https://user-images.githubusercontent.com/16102845/87471596-9de1dc00-c5ec-11ea-9d4c-786e114d57d3.png"">; This is a messy site that is perhaps complicated by representation issues but we can see that GATK emitted an extra insertion underlying the longer event (which was marked as homozygous in the truth set). Following the same logic as above we can see DRAGEN did not make the call because it assigned all of the likelihoods for the longer deletion to the reference when compared against the shorter insertion underlying it which outwe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6707:1993,extend,extends,1993,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6707,1,['extend'],['extends']
Modifiability,"the htsget protocol ; > ; > This year, the GA4GH team introduced the htsget protocol to allow users to download read data for subsections of the genome in which they are interested. This is a richer and more flexible approach to working with reads data. It allows you to keep your genomics data in a common BAM file format on Google Cloud Storage and work with it efficiently from your computation pipelines, using standard bioinformatics tools. We have already launched our own open source implementation of this protocol, which you can use to access your reads data. Many popular tools such as samtools and htslib have been updated by the community to support htsget. Documentation is provided here. The Reads API is now deprecated, and will be decommissioned after one year, or after there has been no API activity for one month by those receiving this notice, whichever comes first. ; > ; > Variants API is now replaced by htsget and Variant Transforms ; > ; > The GA4GH team also plans to extend the htsget protocol to cover variant data, and we will extend our implementation of htsget to cover this use case. ; > ; > After analyzing usage of the Variants API, we found that users primarily used it to import variant data and then export it to BigQuery. To save time and effort, we created Variant Transforms, an open source tool for directly importing VCF data into BigQuery. Variant Transforms and its documentation are published here. Variant Transforms is more scalable than the legacy Variants API, and it has a robust roadmap with a dedicated team. We also welcome collaborators on this project as it advances. ; > ; > The Variants API is now deprecated, and will be decommissioned after one year, or after there has been no API activity for one month, whichever comes first. ; > ; > We are excited to move in step with the global genomics community and provide you with the latest technology for managing your genomic data. We have lots of other projects on the way, and look forward to s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4166:1150,extend,extend,1150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4166,2,['extend'],['extend']
Modifiability,"the implementation of posterior sampling, 3) some shape/dimshuffle operations, and other things along these lines. Using a single test shard of 20 1kGP WES samples x 1000 intervals, I have verified determinism/reproducibility for DetermineGermlineContigPloidy COHORT/CASE modes, GermlineCNVCaller COHORT/CASE modes, and PostprocessGermlineCNVCalls. Numerical results are also relatively close to those from 4.4.0.0 for all identifiable call and model quantities (albeit far outside any reasonable exact-match thresholds, most likely due to differences in RNG, sampling, and the aforementioned priors). Some remaining TODOs:. - [x] Rebuild and push the base Docker. EDIT: Mostly covered by #8610, but this also includes an addition of `libblas-dev`.; - [x] Update expected results for integration tests, perhaps add any that might be missing. EDIT: These were generated on WSL Ubuntu 20.04.2, we'll see if things pass on 22.04. Note that changing the ARD priors does change the *names* of the expected files, since the transform is appended to the corresponding variable name. DetermineGermlineContigPloidy and PostprocessGermlineCNVCalls are missing exact-match tests and should probably have some, but I'll leave that to someone else.; - [x] Update other python integration tests.; - [x] Clean up some of the changes to the priors.; - [x] Clean up some TODO comments that I left to track code changes that might result in changed numerics. I'll try to go through and convert these to PR comments in an initial review pass.; - [x] Test over multiple shards on WGS and WES. Probably some scientific tests on ~100 samples in both cohort and case mode would do the trick. We should also double check runtime/memory performance (I noted ~1.5x speedups, but didn't measure carefully; I also want to make sure the changes to posterior sampling didn't introduce any memory issues). @mwalker174 will ping you when a Docker is ready! Might be good to loop in Isaac and/or Jack as well.; - [x] Perhaps add back ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1847549285:2232,variab,variable,2232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1847549285,1,['variab'],['variable']
Modifiability,"the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum values permitted by machine precision in order to avoid NaNs and overflows. - Took a first step toward tracking and logging parameters during inference, starting with the ELBO history. In the future, it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4720:1436,variab,variables,1436,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720,1,['variab'],['variables']
Modifiability,the requirement is to make MD fully work in a tested way (all Picard integration tests must work - perhaps by comparing the sets of reads that got marked as 'duplicate'). Note: we'll migrate this code from genomics-pipeline and adapt it to our needs and style.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/488:228,adapt,adapt,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/488,1,['adapt'],['adapt']
Modifiability,this is a bigger project - tribble async reading is very broken. we need to rewrite that part and maybe expose sync/async via factory methods.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1597#issuecomment-198510701:76,rewrite,rewrite,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1597#issuecomment-198510701,1,['rewrite'],['rewrite']
Modifiability,"this is a script which can be used after running gradle installDist to run spark jobs; it can be used identically to ths build/install/bin/gatk script, but has extra features for dealing with spark. running a spark tool and supplying the option --sparkTarget with LOCAL, CLUSTER, or GCS has special behavior; LOCAL will run the tool in the in memory spark runner; CLUSTER along with an appropriate --sparkMaster will run on an accessible spark cluster using spark-submit; arguments to spark-submit may be specified before the arguments to GATK by separating them with a --; GCS will submit jobs to google dataproc using gcloud; common arguments for spark submit will be adapted to match the gcloud formating; this will fail if gcloud isn't installed. if GATK_GCS_STAGING is specified, the jar will be uploaded and cached in the specified bucket for rapid re-use. input files will not be autouploaded to the cloud. --dry-run may be specified before the --, this will only print the commands that will be run instead of actually running them. Adding DataProcArgumentReplace simple tool to convert spark-submit args into gcloud args.; This conversion is not guarenteed to translate all spark command line options to matching gcloud ones.; If you find options that are not translated or are miss-translated please file an issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1211:670,adapt,adapted,670,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1211,1,['adapt'],['adapted']
Modifiability,this is fine - all those used a variable as a second argument to the semantics is equivalent and this change improves readability (less surpirse to the reader),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1616#issuecomment-200638990:32,variab,variable,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1616#issuecomment-200638990,1,['variab'],['variable']
Modifiability,this is our chance to rewrite gatk-protected history and un-commit the large files that were added by mistake,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2777:22,rewrite,rewrite,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2777,1,['rewrite'],['rewrite']
Modifiability,this is pretty scary looking code and it's not tied with the dataflow 'walker' interface and so i'm assuming it's just a temporary step to putting all of this in and start refactoring. fine to live in the dev package for now. It will show up on the commandline though - maybe we need a way to hide those dev tools. ; @droazen your turn,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/523#issuecomment-103708285:172,refactor,refactoring,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/523#issuecomment-103708285,1,['refactor'],['refactoring']
Modifiability,"tionally, this PR adds branch filters to the dockstore.yml file that will help with development. The filter for each workflow indicates which branch(es) will show up for that workflow in dockstore. If we don't include these filters, dockstore will run checks of ALL workflows on ALL branches, which causes timeouts. We could remove these filters later (before merging to master) or not, but for now this could help us develop on ah_var_store. Note that we'll need to add feature branches to that file as we work on them. This workflow was tested in Terra and the upload succeeded. Also confirmed that if one file fails, the entire process throws an error code (i.e. -m flag will not cause failures to silently pass) - in example below, `test_file_list.txt` was a list of 6 files, including 1 file that did not exist.; ```;  cat test_file_list.txt | gsutil cp -I gs://dsp-fieldeng-dev/test_cp/; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; CommandException: No URLs matched: test4.txt;  cat test_file_list.txt | gsutil -m cp -I gs://dsp-fieldeng-dev/test_cp/; If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. CommandException: No URLs matched: test4.txt; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test5.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; Copying file://test6.txt [Content-Type=text/plain]...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Operation completed over 5 objects/37.0 B.; CommandException: 1 file/object could not be transferred.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7104:1458,config,config,1458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7104,1,['config'],['config']
Modifiability,"titute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce. This is test script (test.sh) that is used.; ```; module load gatk; CRAM=$1; SAMPLE=$(basename $CRAM); SAMPLE=${SAMPLE/\.cram/}; mkdir -p gvcf.STR/$SAMPLE; mkdir -p gvcf.STR/$SAMPLE/tmp; gatk --java-options ""-Xmx16G"" ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/$SAMPLE/$SAMPLE.STR.table -I $CRAM; gatk --java-options ""-Xmx16G"" CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/$SAMPLE/$SAMPLE.STR.table -O gvcf.STR/$SAMPLE/$SAMPLE.Dragstr.model -I $CRAM. ```; The script runs the ComposeSTRTableFile to produce the table that is then read by CalibrateDragstrModel. ; ```; ./test.sh /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:44:55.228 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:44:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:44:55.456 INFO ComposeSTRTableFile - ----------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182:3549,variab,variable,3549,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182,1,['variab'],['variable']
Modifiability,"toHDFSSpark - Defaults.REFERENCE_FASTA : null; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Deflater IntelDeflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Inflater IntelInflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Initializing engine; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase).; log4j:WARN Please initia",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363:1834,variab,variable,1834,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363,1,['variab'],['variable']
Modifiability,"tools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600 --jar gs://hellbender-test-logs/test/staging/lb_staging/gatk-package-4.beta.6-37-g0a135f8-SNAPSHOT-spark_7002d0551e84ddef0d74adf95dfee104.jar -- PrintVariantsSpark --V gs://hellbender/test/resources/large/gvcfs/gatk3.7_30_ga4f720357.24_sample.21.expected.vcf --output gs://hellbender-test-logs/test/staging/lb_staging/756f43e6-4663-49ce-8a8c-bf717b07a8c7.vcf --sparkMaster yarn; Job [dfac787d-19aa-4296-8078-c033cd9f440d] submitted.; Waiting for job output...; 19:43:09.678 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:43:09.837 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/dfac787d-19aa-4296-8078-c033cd9f440d/gatk-package-4.beta.6-37-g0a135f8-SNAPSHOT-spark_7002d0551e84ddef0d74adf95dfee104.jar!/com/intel/gkl/native/libgkl_compression.so; [November 15, 2017 7:43:09 PM UTC] PrintVariantsSpark --output gs://hellbender-test-logs/test/staging/lb_staging/756f43e6-4663-49ce-8a8c-bf717b07a8c7.vcf --variant gs://hellbender/test/resources/large/gvcfs/gatk3.7_30_ga4f720357.24_sample.21.expected.vcf --sparkMaster yarn --variantShardSize 10000 --variantShardPadding 1000 --shuffle false --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --Q",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840:1336,variab,variables,1336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,tools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; 17:39:18.382 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 17:39:18.825 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 17:39:18.857 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/username/libgkl_compression3681606702485397808.so; 17:39:19.218 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 17:39:19.218 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 17:39:19.218 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:39:19.219 INFO PathSeqPipelineSpark - Executing as username@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 17:39:19.220 INFO PathSeqPipelineSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11; 17:39:19.220 INFO PathSeqPipelineSpark - ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:2348,variab,variables,2348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"tor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 2019-01-07 11:33:18 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:19 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 11:33:24.377 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:33:24.549 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:33:26.271 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.272 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 11:33:26.272 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:33:26.272 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 11:33:26.273 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 11:33:26.273 INFO CountReadsSpark - Start Date/Time: January 7, 2019 11:33:24 AM EST; 11:33:26.273 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.273 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:2355,variab,variables,2355,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"tory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engine; 11:35:41.748 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:35",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:4775,Config,ConfigFactory,4775,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"tory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:52.477 INFO GermlineCNVCaller - Initializing engine; 23:43:52.479 DEBUG ScriptExecutor - Executing:; 23:43:52.479 DEBUG ScriptExecutor - python; 23:43:52.479 DEBUG ScriptExecutor - -c; 23:43:52.480 DEBUG ScriptExecutor - import gcnvkernel. INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (thea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:4299,Config,ConfigFactory,4299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"tory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engine; 11:35:41.748 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:35:41.775 DEBUG NativeLibraryLoader - Extracting libgkl_utils.so to ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:4842,Config,ConfigFactory,4842,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"tory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:52.477 INFO GermlineCNVCaller - Initializing engine; 23:43:52.479 DEBUG ScriptExecutor - Executing:; 23:43:52.479 DEBUG ScriptExecutor - python; 23:43:52.479 DEBUG ScriptExecutor - -c; 23:43:52.480 DEBUG ScriptExecutor - import gcnvkernel. INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:4366,Config,ConfigFactory,4366,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"tory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:16:36.298 INFO GenomicsDBImport - Inflater: IntelInflater; 16:16:36.298 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:16:36.298 INFO GenomicsDBImport - Requester pays: disabled; 16:16:36.298 INFO GenomicsDBImport - Initializing engine; 16:16:36.523 WARN GenomicsDBImport - genomicsdb-update-workspace-path was set, so ignoring specified intervals.The tool will use the intervals specified by the initial import; 16:16:37.372 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 16:16:37.372 DEBUG GenomeLocParser - chr1 (248956422 bp); 16:16:37.373 DEBUG GenomeLocParser - chr2 (242193529 bp); 16:16:37.373 DEBUG GenomeLocParser - chr3 (198295559 bp); 16:16:37.373 DEBUG GenomeLocParser - chr4 (190214555 bp); 16:16:37.373 DEBUG GenomeLocParser - chr5 (181",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:5706,Config,ConfigFactory,5706,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"tory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:05:38.396 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:05:38.396 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:05:38.396 INFO GermlineCNVCaller - Requester pays: disabled; 21:05:38.396 INFO GermlineCNVCaller - Initializing engine; 21:05:38.399 DEBUG ScriptExecutor - Executing:; 21:05:38.399 DEBUG ScriptExecutor - python; 21:05:38.399 DEBUG ScriptExecutor - -c; 21:05:38.399 DEBUG ScriptExecutor - import gcnvkernel; 21:06:10.792 DEBUG ScriptExecutor - Result: 0; 21:06:10.792 INFO GermlineCNVCaller - Done initializing engine; 21:06:10.826 INFO GermlineCNVCaller - Intervals specified...; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logg",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:4603,Config,ConfigFactory,4603,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,tractPipeline.java:472); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485); at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:102); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx3500m -jar /root/gatk.jar Funcotator --data-sources-path /cromwell_root/datasources_dir --ref-version hg38 --output-file-format VCF -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -V gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/94e769a1-28e1-4bd7-b09f-9e47fb7d8352/omics_mutect2/14fe5685-740c-4e09-9d1a-8c8d14c0ae5b/call-mutect2/Mutect2/2de52f4f-eea0-4ec7-acc1-f47b1a2d1e6c/call-Filter/attempt-2/CDS-2jucw0.hg38-filtered.vcf.gz -O CDS-2jucw0.hg38-filtered.vcf.gz.annotated.vcf.gz -L /cromwell_root/ccleparams/region_file_wgs.list --annotation-default normal_barcode: --annotation-default tumor_barcode:NP5 --annotation-default Center:DEPMAP --annotation-default source:Unknown; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1182102653:7041,variab,variable,7041,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1182102653,1,['variab'],['variable']
Modifiability,"tre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/scan_module/__init__.py"", line 41, in <module>; from theano.scan_module import scan_opt; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/scan_module/scan_opt.py"", line 60, in <module>; from theano import tensor, scalar; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/__init__.py"", line 17, in <module>; from theano.tensor import blas; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas.py"", line 155, in <module>; from theano.tensor.blas_headers import blas_header_text; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas_headers.py"", line 987, in <module>; if not config.blas.ldflags:; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 332, in __get__; val_str = self.default(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1451, in default_blas_ldflags; check_mkl_openmp(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1273, in check_mkl_openmp; """"""); RuntimeError: ; Could not import 'mkl'. If you are using conda, update the numpy; packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in; your environment for MKL 2018. If you have MKL 2017 install and are not in a conda environment you; can set the Theano flag blas.check_openmp to False. Be warned that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here is the pip list from my environment:. cached-property 1.5.2+computecanada ; cycler 0.11.0+computecanada ; enum34 1.1.10+computecan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:4268,config,configparser,4268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['config'],['configparser']
Modifiability,"ts header (either proactively or on demand), but transparently to ; application code that is running against the SAMRecord API.; This would allow SAM headers to be transmitted out-of-band in a way that ; depends on the execution environment. Depending on the environment, ; this might be done by proactive broadcast, or you could think of the ; header tag as a promise to retrieve the header if/when it is needed. ; The size and complexity of the header tag might also depend on the ; execution environment. If the execution environment only supports a ; small finite number of headers, the header tag could be a small integer, ; or in a different execution environment it could be; a unique hash of the header or something like that. Memory footprint in ; the receiver is minimized because many SAMRecords can all share the same ; header object.; This requires more work to support in each execution environment, but it ; seems like it could be efficient and allows application code written to ; operate on SAMRecords to be portable; across different execution environments without having to contend with ; the possible presence of headerless SAMRecords. -Bob. On 9/17/15 4:28 PM, droazen wrote:. > @davidadamsphd https://github.com/davidadamsphd, @lbergelson ; > https://github.com/lbergelson, and myself met for an hour or two ; > just now to discuss this issue, and after reviewing all the options I ; > think we were convinced by the following argument:; > ; > The |SAMRecord| class currently allows its header to be set to null, ; > so if there are cases where the class won't function properly or can ; > enter into an inconsistent state when a header is not present these ; > should be treated as bugs and patched, and we should add unit tests to ; > htsjdk to prove that headerless |SAMRecords| function properly. Then ; > in hellbender we can freely use headerless |SAMRecords| everywhere, ; > only restoring the header to the record when writing out the final bam ; > (since our bam writers",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:2594,portab,portable,2594,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518,1,['portab'],['portable']
Modifiability,tureManager - Using codec VCFCodec to read file gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-Filter/22.hg38-filtered.vcf; 01:39:08.399 INFO FilterAlignmentArtifacts - Done initializing engine; 01:39:09.523 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 01:39:09.565 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 01:39:09.566 INFO IntelPairHmm - Available threads: 4; 01:39:09.566 INFO IntelPairHmm - Requested threads: 4; 01:39:09.566 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 01:39:09.567 INFO ProgressMeter - Starting traversal; 01:39:09.567 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; munmap_chunk(): invalid pointer; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx11500m -jar /root/gatk.jar FilterAlignmentArtifacts -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -V gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-Filter/22.hg38-filtered.vcf -I gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/209d1183-ed9a-4755-a4b3-d595797640ea/PreProcessingForVariantDiscovery_GATK4/9f7c0ab6-b61b-4797-92f1-7929bbf677d8/call-GatherBamFiles/22.hg38.bam --bwa-mem-index-image /cromwell_root/gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle -O 22.hg38-filtered.vcf; 2020/07/25 01:46:01 Starting delocalization.; 2020/07/25 01:46:02 Delocalization script execution started...; 2020/07/25 01:46:02 Delocalizing output /cromwell_root/memory_retry_rc -> gs://fc-ac4624cb-a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:5507,variab,variable,5507,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['variab'],['variable']
Modifiability,"ty to actually address this issue is to dynamically reduce the number of alt alleles loosing the less likely ones base on a maximum number of possible genotypes. So the user does not indicate the maximum number of alternative but the maximum number of genotypes. Which alt. alleles make it could be decided by taking a look in the corresponding hom. alt genotype likelihood dropping those alternatives with the worst hom. PLs. ---. @vdauwera commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-78122186). @vruano What you propose sounds great. How much work would it take to implement this? . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-85066881). Looking into that particular use case... the problem seem to be in position:. 45SrDNA_Jacobsen 9283. That seems to be very polymorphic or noisy even within individual samples, to the point that many lack PLs so perhaps merging would not work or at least the exact model depending annotations (QUAL column and MLEAC/F format field) cannot be updated based on them... I think that best way to move forward here is:; 1. Lift up that maximum number of Genotypes to output PLs based on the ploidy parameter (I think the limit was quite modest perhaps as low as 20).; 2. Implement the alt. allele `culling` or `collapsing` that I mention above in HaplotypeCaller already. ; 3. Implement the alt. allele `re-culling` or `re-collapsing` in GVCF (VCF as well?) merging tools such as CombineGVCFs/GenotypeGVCFs.; 4. Regenotyping and QUAL recalculating tools would need to make sure that PLs less input are handled appropriately, not sure what would happen now if some of the inputs lack PLs... (an Exception?) ; - For example QUAL could be approximated as the max of the input Quals, and QD as the average? ; - Or simple lift them blank?. So it would a bit of work I would say... 3 of the old PTs worth. ---. @vdauwera commented on [Thu May 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2955:2187,polymorphi,polymorphic,2187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955,1,['polymorphi'],['polymorphic']
Modifiability,typingEngineUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9Bc3NlbWJseUJhc2VkQ2FsbGVyR2Vub3R5cGluZ0VuZ2luZVVuaXRUZXN0LmphdmE=) | `100% <100%> ()` | `31 <0> ()` | :arrow_down: |; | [...ypecaller/AssemblyBasedCallerGenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9Bc3NlbWJseUJhc2VkQ2FsbGVyR2Vub3R5cGluZ0VuZ2luZS5qYXZh) | `89.45% <100%> (+0.049%)` | `89 <0> (+1)` | :arrow_up: |; | [...stitute/hellbender/cmdline/CommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0NvbW1hbmRMaW5lUHJvZ3JhbS5qYXZh) | `83.117% <0%> (+1.948%)` | `43% <0%> (+1%)` | :arrow_up: |; | [...stitute/hellbender/utils/config/ConfigFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb25maWcvQ29uZmlnRmFjdG9yeS5qYXZh) | `76.398% <0%> (+3.727%)` | `45% <0%> (+2%)` | :arrow_up: |; | [...r/arguments/CopyNumberArgumentValidationUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2FyZ3VtZW50cy9Db3B5TnVtYmVyQXJndW1lbnRWYWxpZGF0aW9uVXRpbHMuamF2YQ==) | `77.778% <0%> (+6.173%)` | `20% <0%> (+1%)` | :arrow_up: |; | [...tute/hellbender/utils/runtime/ProcessSettings.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9ydW50aW1lL1Byb2Nlc3NTZXR0aW5ncy5qYXZh) | `93.75% <0%> (+6.25%)` | `18% <0%> (+2%)` | :arrow_up: |; | [...te/hellbender/utils/python/PythonExecutorBase.java](https://codecov.io/gh/br,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5365#issuecomment-433471265:1961,config,config,1961,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5365#issuecomment-433471265,2,"['Config', 'config']","['ConfigFactory', 'config']"
Modifiability,"ub.com> wrote:; > ; > @SHuang-Broad commented on this pull request.; > ; > In src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/prototype/CpxVariantDetector.java:; > ; > > + this.tigWithInsMappings = new AssemblyContigWithFineTunedAlignments(contig, tigWithInsMappings.insertionMappings);; > +; > + this.basicInfo = new BasicInfo(contig);; > +; > + annotate(refSequenceDictionary);; > + }; > +; > + private static List<AlignmentInterval> deOverlapAlignments(final List<AlignmentInterval> originalAlignments,; > + final SAMSequenceDictionary refSequenceDictionary) {; > + final List<AlignmentInterval> result = new ArrayList<>(originalAlignments.size());; > + final Iterator<AlignmentInterval> iterator = originalAlignments.iterator();; > + AlignmentInterval one = iterator.next();; > + while (iterator.hasNext()) {; > + final AlignmentInterval two = iterator.next();; > + // TODO: 11/5/17 an edge case is possible where the best configuration contains two alignments,; > + // one of which contains a large gap, and since the gap split happens after the configuration scoring,; > I agree it is backwards. But...; > ; > The reason was that the (naive) alignment configuration scoring module rightnow uses MQ and AS (aligner score) for picking the ""best"" configuration (i.e. sub-list of the alignments given by aligner), which would be technically wrong if we were to split the gap and to simply grab the originating alignment's values.; > ; > This is especially true for AS, whose recomputing takes more time, and code, and forces us to know how AS are computed in the aligner so that there's no bias in computing the scores of naive alignments vs gap-split alignments (may not matter in practice, but still takes more code to compute).; > ; > Lots of the code in the discovery stage was devoted actually to alignment related acrobatics and edge cases so that the breakpoints we could resolve are as accurate as possible.; > I've kept in mind your wisdom that different aligners may ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-350618009:1509,config,configuration,1509,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-350618009,2,['config'],['configuration']
Modifiability,"ugh, made some progress but got stuck because that code requires that penalties are less than 256 and match/mismatch are below 128. Or default values are match 200, mismatch -150, open -260, extend -11. work in progress is in branch ak_native_smithwaterman_sse . @gspowley FYI",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1812#issuecomment-218537669:191,extend,extend,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1812#issuecomment-218537669,1,['extend'],['extend']
Modifiability,"ults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:16:36.298 INFO GenomicsDBImport - Inflater: IntelInflater; 16:16:36.298 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:16:36.298 INFO GenomicsDBImport - Requester pays: disabled; 16:16:36.298 INFO Gen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:5146,Config,ConfigFactory,5146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,umReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at chrM:15445-15744 isActive: false numReads: 0; 12:13:57.027 INFO ProgressMeter - chrM:15445 38.3 60 1.6; 12:13:57.035 DEBUG Mutect2 - Processing assembly region at chrM:15745-15960 isActive: false numReads: 14; 12:13:57.047 DEBUG Mutect2 - Processing assembly region at chrM:15961-16230 isActive: true numReads: 30; 12:13:57.055 DEBUG ReadThreadingGraph - Recovered 1 of 1 dangling tails; 12:13:57.063 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 12:13:57.096 DEBUG ReadThreadingGraph - Recovered 3 of 3 dangling tails; 12:13:57.106 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling heads; 12:13:57.464 DEBUG Mutect2Engine - Active Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonChimericOriginalAlignmentReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filtered by: WellformedReadFilter ; 0 total reads filtered; 12:13:58.943 INFO ProgressMeter - chrM:15445 38.3 63 1.6; 12:13:58.946 INFO ProgressM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:22555,Extend,Extended,22555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Extend'],['Extended']
Modifiability,update the artifactory url to point to the new artifactory; update the travis build with another environment variable called UPLOAD which determines if that build should upload a snapshot or not; fixes #3068,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3075:109,variab,variable,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3075,1,['variab'],['variable']
Modifiability,"updated shadowJar to 1.2.3 since version 1.2.2 of the shadowJar plugin had some issues with gradle ; 2.11 which just released. some `build.gradle` cleanup; - removed dependency on `lib/tools.java` since it doesn't seem to be used and should be provided by the system anyway; - removed individual excludes of `guava-jdk5` since we exclude them globally; - changed our plugin application to use the newer style; - updated jacoco, coverals, and versions plugin versions; - added group and description to sparkJar task so it shows up in `gradle tasks`; - updated gradle wrapper version to 2.11; - readme now states 2.11 as minimum version",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1478:64,plugin,plugin,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1478,3,['plugin'],['plugin']
Modifiability,updating dataflow and htsjdk to newest versions; adding gradle versions plugin to help with identifying dependencies that need updates. This broke one of our spark related tests so I've excluded it for now. See #581. It should be reeneabled when https://github.com/cloudera/spark-dataflow/issues/49 is complete.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/582:72,plugin,plugin,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/582,1,['plugin'],['plugin']
Modifiability,urces.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.997 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > WARNING 2020-07-21 12:28:17 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 12:28:18.002 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; > 12:28:18.009 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > 12:28:18.020 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.config; > 12:28:18.120 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > 12:28:18.121 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > WARNING 2020-07-21 12:28:18 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 12:28:18.125 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/simple_uniprot_Dec012014.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/simple_uniprot/hg38/simple_uniprot_Dec012014.tsv; > 12:28:18.424 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:10036,config,config,10036,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['config'],['config']
Modifiability,"ureInput;. public CodecWrapper(FeatureCodec<FEATURE_TYPE, SOURCE> childCodec, FeatureInput<FEATURE_TYPE> featureInput); {; this.childCodec = childCodec;; this.featureInput = featureInput;; }. @Override; public Feature decodeLoc(SOURCE source) throws IOException {; return childCodec.decodeLoc(source);; }. @Override; public FEATURE_TYPE decode(SOURCE source) throws IOException {; FEATURE_TYPE feature = childCodec.decode(source);. //Either look for marker class or otherwise poke in FeatureInput here:; if (feature instanceof VariantContext); {; feature = new FeatureInputAwareVariantContext(feature, featureInput);; }. return feature;; }. @Override; public FeatureCodecHeader readHeader(SOURCE source) throws IOException {; return childCodec.readHeader(source);; }. @Override; public Class<FEATURE_TYPE> getFeatureType() {; return childCodec.getFeatureType();; }. @Override; public SOURCE makeSourceFromStream(InputStream bufferedInputStream) {; return childCodec.makeSourceFromStream(bufferedInputStream);; }. @Override; public LocationAware makeIndexableSourceFromStream(InputStream inputStream) {; return childCodec.makeIndexableSourceFromStream(inputStream);; }. @Override; public boolean isDone(SOURCE source) {; return childCodec.isDone(source);; }. @Override; public void close(SOURCE source) {; childCodec.close(source);; }. @Override; public boolean canDecode(String path) {; return childCodec.canDecode(path);; }; }. public static interface FeatureInputAware<FEATURE_TYPE extends Feature>; {; public FeatureInput<FEATURE_TYPE> getFeatureInput();; }. public static class FeatureInputAwareVariantContext extends VariantContext implements FeatureInputAware<VariantContext>; {; private FeatureInput<VariantContext> featureInput;. public FeatureInputAwareVariantContext(VariantContext parent, FeatureInput<VariantContext> featureInput); {; super(parent);; this.featureInput = featureInput;; }. @Override; public FeatureInput<VariantContext> getFeatureInput() {; return featureInput;; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823546766:2219,extend,extends,2219,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823546766,2,['extend'],['extends']
Modifiability,"use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engine; 11:35:41.748 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-packa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:4713,Config,ConfigFactory,4713,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:52.477 INFO GermlineCNVCaller - Initializing engine; 23:43:52.479 DEBUG ScriptExecutor - Executing:; 23:43:52.479 DEBUG ScriptExecutor - python; 23:43:52.479 DEBUG ScriptExecutor - -c; 23:43:52.480 DEBUG ScriptExecutor - import gcnvkernel. INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:4237,Config,ConfigFactory,4237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"using --conf passes args into the spark configuration. these args will take precendence over spark args specified in any other way. moved --sparkMaster and --conf to their own SparkCommandLineArgumentCollection. passing spark options through to gatk with DIRECT, this will only work with --sparkMaster and --conf. fixes #1339",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1356:40,config,configuration,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1356,1,['config'],['configuration']
Modifiability,"ut allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case in output (#7252); - Support for FORMAT/FT VQSLod Filtering and cohort-wide LowQual filter (#7248); - removed arrays code, renamed packages (#7260); - 279 labels (#7233); - add conda commands to GIAB readme (#7268); - remove gvs branch (#7263); - remove gvs branch (#7263); - upgrade bq libraries (#7264); - #299 - Sample list ease of use for cohort extracts (#7272); - check for duplicate ids (#7273); - Rc 274 passing sites only (#7275); - added default value to drop_state; broadinstitute/dsp-spec-ops#310 (#7278); - version bump for reliability (#7284); - add timestamp check to ExtractTask call https://github.com/broadinstitute/dsp-spec-ops/issues/320; - serial inserts for scaling prepare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage versi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:14093,parameteriz,parameterize,14093,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['parameteriz'],['parameterize']
Modifiability,"uted data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:2056,refactor,refactor,2056,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['refactor'],['refactor']
Modifiability,"utputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). 11:00:53.977 INFO DAGScheduler - Job 1 failed: runJob at SparkHadoopWriter.scala:83, took 3.799268 s; 11:00:53.979 ERROR SparkHadoopWriter - Aborting job job_202408111100502620487673658411251_0021.; org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.OutOfMemoryError: Required array length 2147483639 + 798 is too large; java.la",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:7773,adapt,adapted,7773,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,utputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607) ~[gatk-pack,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:12097,adapt,adapted,12097,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,utputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collecti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:30816,adapt,adapted,30816,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,utputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 11:00:54.334 INFO ShutdownHookManager - Shutdown hook called; 11:00:54.335 INFO ShutdownHookManager - Deleting directory /raid/tmp/d6/c66ba827e22dbc38625af1cbc85adc/tmp/spark-f9c7c336-4e98-4fcc-855b-ba8a5a29e074; ```. The first lines of the log file:; ```; vm.max_map_count = 2147483642; Using GATK jar /Public/Everythings/misc/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:36687,adapt,adapted,36687,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,va:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsSequencial(CalibrateDragstrModel.java:459); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:159); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar CalibrateDragstrModel --tmp-dir tmp -R /restricte; d/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/HGDP00001.alt_bwamem_GRCh38DH.20181023.Brahui/HGDP00001.alt_bwamem_GRCh38DH.20181023.Brahui.STR.table -O gvcf.STR/HGDP00001.alt_bwamem_GRCh38DH.20181023.Brahui/HGDP00001.alt_bwamem_GRCh38DH.20181023.Brahui; .Dragstr.model -I ../pop/Brahui/HGDP00001/alignment/HGDP00001.alt_bwamem_GRCh38DH.20181023.Brahui.cram; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182#issuecomment-821876394:6783,variab,variable,6783,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182#issuecomment-821876394,1,['variab'],['variable']
Modifiability,"ver VM v1.8.0_181-b13; 14:35:47.080 INFO SelectVariants - Start Date/Time: September 24, 2018 2:35:45 PM EET; 14:35:47.080 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.081 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.082 INFO SelectVariants - HTSJDK Version: 2.16.1; 14:35:47.082 INFO SelectVariants - Picard Version: 2.18.13; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:47.082 INFO SelectVariants - Deflater: IntelDeflater; 14:35:47.082 INFO SelectVariants - Inflater: IntelInflater; ```. From @jean-philippe-martin . > This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM).; > ; > As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via gcloud auth, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on.; > ; > The message is useful, for if we were running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.; > . We should tone down the error message if possible.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5220:5855,variab,variable,5855,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220,1,['variab'],['variable']
Modifiability,"versed.selfRef.shifted.homoplasmies.vcf.bgz \\ ; ; \--annotation StrandBiasBySample \\ ; ; \--mitochondria-mode \\ ; ; \--max-reads-per-alignment-start 75 \\ ; ; \--max-mnp-distance 0 \\ ; ; \-L chrM:8023-9140 \\ ; ; \--genotype-filtered-alleles \\ ; ; \--debug-assembly-variants-out /rej.vcf \\ ; ; \--bam-output bamout.bam. In this instance the variant in question is listed in the rej.vcf file obtained via `--debug-assembly-variants-out`. I have examined `bamout.bam` as well as the input bam and there appears to be ample coverage at the site of interest (the T at position 8316 is the position of interest, highlighted):. ![](https://gatk.broadinstitute.org/hc/user_images/aGbHKebG7Tb8Lgu33gGzXw.png). I have tried running this with some of the additional parameters in \[[https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)](https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)) (namely `--linked-de-bruijn-graph` and `--recover-all-dangling-branches`) to no avail. Coverage is very deep at this position (>2000x). Notably if I edit the input to `--alleles` and change the allele of interest (8316:T>A) to anything else (8316:T>C or T>G) it appropriately shows up in the output VCF. What am I missing here? Let me know if you have any solutions or if you need any additional files. UPDATE: Adding `--disable-adaptive-pruning` now produces the variant of interest specified in --alleles, but also adds several other new calls, in case that is helpful in isolating where this force-call variant is being lost.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/270138'>Zendesk ticket #270138</a>)<br> gz#270138</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7672:2858,adapt,adaptive-pruning,2858,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7672,1,['adapt'],['adaptive-pruning']
Modifiability,version: gatk 4.0.2.1; I use the pipeline :BwaAndMarkDuplicatesPipelineSpark-BQSRPipelineSpark-HaplotypeCallerSparkand I get the bad resultby testingHaplotypeCallerSpark lose a lot of variable sites and HaplotypeCallerSpark 'result jitter to the same input bam,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4488:189,variab,variable,189,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4488,1,['variab'],['variable']
Modifiability,"we need a canonical set of tests that we run when we upgrade the cluster. We've been running terasort but it's not enough: 1) it does not run our code and 2) it does not even run java8 (recent config error when 2 nodes were running java7 was undetected). The task here is to write, in readme or in scripts directory, a script or set of scripts that must be run after every change to the cluster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1392:193,config,config,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1392,1,['config'],['config']
Modifiability,we need to keep track of what `GATKReads` correspond to which `ShortReads` and the `Strings` returned from jbwa. One way may be to extend `ShortRead` with a data slot to keep the back pointer to the original `GATKRead` (the jni layer would ignore it and copying 64 bits per read is not a big deal) and then copy the auxiliary data (like read groups etc) back from the `GATKRead` to the newly created `SAMRecord`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1904#issuecomment-230863254:131,extend,extend,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1904#issuecomment-230863254,1,['extend'],['extend']
Modifiability,"we've run all the relevant tools recently. What else is needed here? Can we close this? If not, please make an explicit list of things to check (tools, configuration, etc). Back to @droazen for clarifications.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/968#issuecomment-152306914:152,config,configuration,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/968#issuecomment-152306914,1,['config'],['configuration']
Modifiability,"willing to give it a try but I need minimal info to do so. * can I add `library(""ggplot2"")` to the code or will this new library not be supported by the package?. * what value (are these defaults?) do. ```; targetTITV = as.numeric(args[2]); targetSensitivity = as.numeric(args[3]) ; ```. take for a command like this. > cmd=""java ${javaopts} -jar $GATK/gatk.jar \; > 	VariantRecalibrator \; > 	-R ${reference_fa} \; > 	-V ${outfolder}/gatk_variants_excesshet_sitesonly.vcf.gz \; > 	-O ${outfolder}/gatk_variants_recalibrate_SNP.recal.vcf.gz \; > 	${intervals} \; > 	--resource:1001Gsnp,known=true,training=true,truth=true,prior=12.0 ${knownsnps} \; > 	--trust-all-polymorphic \; > 	--use-annotation DP \; > 	--use-annotation QD \; > 	--use-annotation FS \; > 	--use-annotation SOR \; > 	--use-annotation MQ \; > 	--use-annotation MQRankSum \; > 	--use-annotation ReadPosRankSum \; > 	--mode SNP \; > 	--max-gaussians ${maxSNPgaussians} \; > 	--tranches-file ${outfolder}/gatk_variants_recalibrate_snp.tranches \; > 	--tranche 100.0 \; > 	--tranche 99.95 \; > 	--tranche 99.9 \; > 	--tranche 99.8 \; > 	--tranche 99.6 \; > 	--tranche 99.5 \; > 	--tranche 99.4 \; > 	--tranche 99.3 \; > 	--tranche 99.0 \; > 	--tranche 98.0 \; > 	--tranche 97.0 \; > 	--tranche 90.0 \; > 	--rscript-file ${outfolder}/gatk_variantsgatk_variants_recalibrate_snp_plots.R \; > 	--tmp-dir ${basedir}/tmpfiles/"". Sorry but I do not know where to look in the java code for this. Thanks in advance",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6585#issuecomment-624680466:664,polymorphi,polymorphic,664,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6585#issuecomment-624680466,1,['polymorphi'],['polymorphic']
Modifiability,"windows in my implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract methods for the three parameters (`getWindowSize()`, `getWindowStep()` and `getWindowPadding()`, and implement `ReadWindowWalker` as a extension of this interface setting `getWindowStep()` to return `getWindowSize()` and `requiresReads()` to true. I can do this once the PR #1567 is accepted and generate the two interfaces (to be sure that the integration with the HC engine is working as expected with the changes), or just implement the `SlidingWindowWalker` and you can include it in the HC PR, or update afterwards to avoid redundancy in the code. What do you thi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:1629,extend,extend,1629,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775,1,['extend'],['extend']
Modifiability,"y 7, 2018 3:41:34 PM EDT] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 61.21 minutes.; Runtime.totalMemory()=13635682304; ```. With native libraries (note the lack of the usual warning):. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark --java-options ""-Djava.library.path=${HADOOP_DIR}/hadoop-2.6.5-src/hadoop-common-project/hadoop-common/target/hadoop-common-2.6.5/lib/native"" -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam --spark-master local[8]; 21:47:47.494 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:47:47.827 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 21:47:48.268 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.268 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 21:47:48.268 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:47:48.270 INFO MarkDuplicatesSpark - Executing as cwhelan@gsa6.broadinstitute.org on Linux v2.6.32-696.16.1.el6.x86_64 amd64; 21:47:48.270 INFO MarkDuplicatesSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 21:47:48.270 INFO MarkDuplicatesSpark - Start Date/Time: May 7, 2018 9:47:47 PM EDT; 21:47:48.270 INFO MarkDuplicatesSpark - -------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746:5242,variab,variables,5242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"y and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is manda",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:6772,refactor,refactored,6772,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['refactor'],['refactored']
Modifiability,"y increasing TILEDB_UPLOAD_BUFFER_SIZE to at least 5MB ; ```. #### Steps to reproduce. Can't produce a small reproducible examples because it only happens with the full dataset. However, below is the command that I ran. . ```sh; gatk --java-options -Xms16g GenomicsDBImport \; --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 \; --batch-size 50 -L 0000-scattered.interval_list \; --sample-name-map sample_map.csv \; --reader-threads 16 \; --merge-input-intervals \; --consolidate; ```. * `sample_map.csv` contains GCS paths to the GVCFs.; * `0000-scattered.interval_list` is one interval generated by calling SplitIntervals to make 50 intervals. #### Expected behavior. Finish without an error, write DB to the specified bucket. #### Actual behavior. Throws a TileDB error. . Does it have to do with the `--consolidate` flag? I couldn't find what `TILEDB_UPLOAD_BUFFER_SIZE` means, but the [TileDB docs](https://docs.tiledb.com/main/how-to/configuration) reference ""sm.consolidation.buffer_size"" with the default size of 50000000 (50MB?). I'll try rerunning without consolidation. Full log:. ```sh; Using GATK jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms16g -jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar GenomicsDBImport --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 --batch-size 50 -L /io/batch/8900b8/inputs/kownK/0000-scattered.interval_list --sample-name-map /io/batch/8900b8/inputs/ZHdri/sample_map.csv --reader-threads 16 --merge-input-intervals --consolidate; 14:26:51.130 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7653:1623,config,configuration,1623,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653,1,['config'],['configuration']
Modifiability,"y we are making it through much of the pipeline, but failing on `GetPileupSummaries`. There's a thread about it on the discussion board [here] (https://gatk.broadinstitute.org/hc/en-us/community/posts/6179012337819-No-Pileup-Tables). . We are specifying a file for `variants_for_contamination`, and a file for `variants_for_contamination_idx` in the workflow, but the index is never passed to `GetPileupSummaries`, and it fails with this enigmatic error message:. ```; A USER ERROR has occurred: An index is required but was not found for file gs://bruce-processed-data/Prins_Cloughesy_Neoadjuvant/terra_reference_files/small_exac_common_3.hg38.vcf.gz. Support for unindexed block-compressed files has been temporarily disabled. Try running IndexFeatureFile on the input.; ```. If you check out the source code in [mutect2.wdl](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl), you can see that that input variable `variants_for_contamination_idx`, which we have thoughtfully set and passed into the workflow, is never actually used in `GetPileupSummaries`. I'm not even sure there is an option to pass the index, from reading the [docs](https://gatk.broadinstitute.org/hc/en-us/articles/360037593451-GetPileupSummaries). Here is an example of how the command is being called within our workflow:. ```; gatk --java-options ""-Xmx149500m"" GetPileupSummaries -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://fc-d31bc4e7-6d10-4dc4-a585-5895ab2346f3/cfce2061-efd6-449e-bdc9-a7ff2b633644/PreProcessingForVariantDiscovery_GATK4/b4adf777-4f97-425c-b3e2-b37c9d927667/call-GatherBamFiles/SRR7588418.hg38.bam --interval-set-rule INTERSECTION -L gs://fc-d31bc4e7-6d10-4dc4-a585-5895ab2346f3/81583498-648e-4e70-8452-80509b626927/Mutect2/dbb6ef96-ea07-4cfe-9e85-3b133c6d89ea/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0030-scattered.interval_list \; -V gs://bruce-processed-data/Prins_Cloughesy_Neoadjuvant/terra_re",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7935:1237,variab,variable,1237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7935,1,['variab'],['variable']
Modifiability,"yf_documentation_update we; can use that for initial testing. On Tue, Dec 5, 2017 at 1:56 PM, sooheelee <notifications@github.com> wrote:. > @samuelklee <https://github.com/samuelklee>, thanks for the update and; > suggestion. I moved CollectAllelicCounts to the Coverage Analysis; > category. CollectFragmentCounts isn't on the list currently so I added it; > to the same. I hope I'm not missing a bunch of other new tools given I; > missed this one.; >; > @yfarjoun <https://github.com/yfarjoun>; >; > - You are now in charge of deciding whether we should include; > authorship in code. What the Comms team wants is for authorship to NOT show; > up in the gatkDoc/javaDoc. If you want to keep them, author lines should be; > at the bottom and formatted so they do not show up in the documentation.; > Geraldine is fine with completely removing them if you prefer that. There; > is a format trick that has javaDoc skip the author line and I can get that; > to you if you decide to keep some of these and @vdauwera; > <https://github.com/vdauwera> would know this or I can get you what I; > see in other docs. Let either of us know.; > - I can help you test your changes. I think the categories are good to; > go now so I will need to put these into both Picard and GATK; > HelpConstants.java, with the latter being a placeholder until the new; > Picard release is incorporated into the next GATK release, with variables; > that then must be included in each tool doc. I will find an example in a; > bit. Which tool do you want to test? @cmnbroad; > <https://github.com/cmnbroad> can explain the engineering details in; > engineering lingo if you need more information.; >; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349404645>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0jIdprE580XBgq1jL-EIV1hFOcDyks5s9ZHAgaJpZM4QitCF>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349407253:1482,variab,variables,1482,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349407253,1,['variab'],['variables']
Modifiability,"bly to be activated if a mininum number of pieces of evidence agree on the distal target. Also:. - Some refactoring of the SATagAlignment and builder classes to support better treatment of SA tags.; - Increased the spark network timeout values for the SV pipeline to prevent nodes from losing heartbeats and being orphaned with running tasks. Since I made this change I have not had the issue. On the performance of this change on our calls:. I compared this branch with master. Master's results on the CHM1/13 mix:. ```; 16:57:37.270 INFO StructuralVariationDiscoveryPipelineSpark - Metadata retrieved.; 16:58:20.436 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 25977 intervals.; 16:58:20.517 INFO StructuralVariationDiscoveryPipelineSpark - Killed 377 intervals that were near reference gaps.; 16:58:49.939 INFO StructuralVariationDiscoveryPipelineSpark - Killed 175 intervals that had >1000x coverage.; 16:59:33.036 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 8773016 mapped template names.; 17:00:07.058 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 17:05:25.896 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 34752266 kmers.; 17:10:46.253 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 31945322 unique template names for assembly.; 17:45:06.748 INFO StructuralVariationDiscoveryPipelineSpark - Wrote SAM file of aligned contigs.; 17:45:26.199 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 5716 variants.; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INV: 231; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 3262; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1065; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1158; 17:45:26.397 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 8, 2017 5:45:26 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDis",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2684:105,refactor,refactoring,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684,1,['refactor'],['refactoring']
Performance,"	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:24); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```. for multi sample:; ```; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: Unrecognized token on line 31, column 50:. <title>gatk/mutect2_multi_sample.wdl at master ? broadinstitute/gatk ? GitHub</title>; ^; 	at wdl.draft2.parser.WdlParser.unrecognized_token(WdlParser.java:6975); 	at wdl.draft2.parser.WdlParser.lex(WdlParser.java:7048); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:263); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:170); 	at scala.util.Try$.apply(Try.scala:213); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:630); 	at womtool.graph.GraphPrint$.generateWorkflowDigraph(GraphPrint.scala:19); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:131); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:162); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:167); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:24); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6261:2442,load,load,2442,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6261,1,['load'],['load']
Performance, 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); 	at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:330); 	at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:88); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:72); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:62); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.ClassNotFoundException: htsjdk.variant.variantcontext.LazyGenotypesContext; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); 	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826); 	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000); 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840:6486,concurren,concurrent,6486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840,1,['concurren'],['concurrent']
Performance, 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); 	at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:362); 	at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:88); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:72); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:62); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; 	at htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158); 	at htsjdk.variant.variantcontext.LazyGenotypesContext.invalidateSampleOrdering(LazyGenotypesContext.java:205); 	at htsjdk.variant.variantcontext.GenotypesContext.add(GenotypesContext.java:353); 	at htsjdk.variant.variantcontext.GenotypesContext.add(GenotypesContext.java:46); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:134); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:40); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 18 more; 19/02/18 16:58:29 INFO org.spark_pro,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:7675,concurren,concurrent,7675,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['concurren'],['concurrent']
Performance, 	at com.github.discvrseq.walkers.BackportLiftedVcf.apply(BackportLiftedVcf.java:156); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at com.github.discvrseq.Main.main(Main.java:51); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 26 more,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-333579182:2635,Load,LoadSnappy,2635,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-333579182,4,"['Load', 'load']","['LoadSnappy', 'loadClass']"
Performance, 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 1. 18/01/09 18:31:15 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1515493209401_0001_01_000006 on host: tele-6. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_1515493209401_0001_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:262); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 1. 18/01/09 18:31:15 INFO storage.BlockManagerMaster: Removal of executor 4 requested; 18/01/09 18:31:15 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 4; 18/01/09 18:31:15 INFO storage.BlockManagerMaster: Removal of executor 5 requested; 18/01/09 18:31:15 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 5; 18/01/09 18:31:15 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 4 from BlockManagerMaster.; 18/01/09 18:31:15 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 5 from BlockManagerMaster.; 18/01/09 18:31:18 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Cont,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:22620,concurren,concurrent,22620,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['concurren'],['concurrent']
Performance, 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 ERROR Executor: Exception in task 518.0 in stage 0.0 (TID 518); java.io.FileNotFoundException: /home/data/WGS/F002/F002.sort.bam (Too many open files); 	at java.io.FileInputStream.open0(Native Method); 	at java.io.FileInputStream.open(FileInputStream.java:195); 	at java.io.FileInputStream.<init>(FileInputStream.java:138); 	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.java:106); 	at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:202); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:349); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.seqdoop.hadoop_bam.util.WrapSeekable.openPath(WrapSeekable.java:60); 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:147); 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316:4635,concurren,concurrent,4635,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316,1,['concurren'],['concurrent']
Performance," 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 WARN TaskSetManager: Lost task 517.0 in stage 0.0 (TID 517, localhost, executor driver): org.broadinstitute.hellbender.exceptions.UserException$NoSuitableCodecs: Cannot read /dev/shm/gatktmp/spark-30e238e4-b1b7-41f9-b31e-844f16879051/userFiles-4621c82d-5f86-4b51-9321-ccc84ab49979/dbsnp_138.hg19.vcf because no suitable codecs found; 	at org.broadinstitute.hellbender.engine.FeatureManager.getCodecForFile(FeatureManager.java:462); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getCodecForFeatureInput(FeatureDataSource.java:320); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:300); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:230); 	at org.broadinstitute.hellbender.e",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316:7979,concurren,concurrent,7979,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316,1,['concurren'],['concurrent']
Performance, 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1798951329-10.128.1.77-1564169124618:blk_1073741844_1020 file=/reference/Homo_sapiens_assembly38.fasta; 	at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1085); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1068); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1047); 	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655); 	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:949); 	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1004); 	at java.io.DataInputStream.read(DataInputStream.java:149); 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385); 	at hdfs.jsr203.HadoopFileSystem$3.read(HadoopFileSystem.java:478); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6064:3669,concurren,concurrent,3669,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6064,1,['concurren'],['concurrent']
Performance," ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-NISTSampleHeadToHead/BenchmarkComparison/a39481f5-0969-4891-a843-f3c3fd7437d1/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-NISTSampleHeadToHead/BenchmarkComparison/a39481f5-0969-4891-a843-f3c3fd7437d1/call-BenchmarkVCFControlSample/Benchmark/0c99102a-bca1-4426-97c6-5a311ace93c1/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""95.62183055555556"",; ""NIST evalHCsystemhours"": ""0.18361111111111117"",; ""NIST evalHCwallclockhours"": ""64.22846111111112"",; ""NIST evalHCwallclockmax"": ""3.3683277777777776"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-NISTSampleHeadToHead/BenchmarkComparison/a39481f5-0969-4891-a843-f3c3fd7437d1/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-NISTSampleHeadToHead/BenchmarkComparison/a39481f5-0969-4891-a843-f3c3fd7437d1/call-BenchmarkVCFTestSample/Benchmark/a3925c8a-7e0a-4fec-8507-f885061b69c3/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CreateHTMLReport/cacheCopy/report.html""; }; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207:14465,cache,cacheCopy,14465,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207,2,['cache'],['cacheCopy']
Performance," ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); 4.1.8.1. ### Description ; A user is getting a java.lang.NullPointerException when running Mutect2. As discussed at GATK Office Hours 09/28/20, it seems to be an issue where the BAM contigs are not present in the reference sequence dictionary. We discussed an improvement with either the filter for this problem or the error message. Complete stack trace:; ```; Using GATK jar GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16g -jar /GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8/gatk-package-4.1.8.1-local.jar Mutect2 -R ref/Homo_sapiens_assembly38.fasta -I SRR_MM10_2pass_recal.bam --germline-resource /af-only-gnomad.hg38.vcf.gz --panel-of-normals ref/1000g_pon.hg38.vcf.gz -O SRR_somatic_mutect.vcf.gz; 13:24:08.400 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Sep 25, 2020 1:24:08 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:24:08.556 INFO Mutect2 - ------------------------------------------------------------; 13:24:08.557 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.1; 13:24:08.557 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:24:08.557 INFO Mutect2 - Executing as xxx on Linux v3.10.0-1127.18.2.el7.x86_64 amd64; 13:24:08.557 INFO Mutect2 - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_192-b12; 13:24:08.557 INFO Mutect2 - Start Date/Time: September 25, 2020 1:24:08 PM BST; 13:24:08.557 INFO Mutect2 - ------------------------------------------------------------; 13:24:08.557 INFO Mutect2 - -----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6851:1016,Load,Loading,1016,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6851,1,['Load'],['Loading']
Performance," (Fvrier in french) August (Aot) or December (Dcembre) have ISO-8859-1 encoding instead of UTF-8 encoding. Indeed, the output files have this line:. `##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output /volumes/vol002/COVID/GenomicDB/vcf/COVID.05022021.int00.vcf.gz --variant gendb://dbtot/int00 --reference /volumes/vol002/reference/human_g1k_v37.fasta --tmp-dir /volumes/vol002/COVID/GenomicDB/tmp/tmpint00 --include-non-variant-sites false --merge-input-intervals false --input-is-somatic false --tumor-lod-to-emit 3.5 --allele-fraction-error 0.001 --keep-combined-raw-annotations false --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 30.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --num-reference-samples-if-no-call 0 --genomicsdb-use-bcf-codec false --genomicsdb-shared-posixfs-optimizations false --only-output-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false --disable-tool-default-annotations false --enable-all-annotations false --allow-old-rms-mapping-quality-annotation-data false"",Version=""4.1.9.0"",Date=""5 f<E9>vrier 2021 10:42:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7081:1246,optimiz,optimizations,1246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7081,1,['optimiz'],['optimizations']
Performance," (PathSeqBuildKmers) and filtering reads that are low-quality, low-complexity, or come from the host (PathSeqFilterSpark). Sorry for the especially large size on this PR. **PathSeqBuildKmers tool**. Note this has been renamed from PathSeqKmerSpark. Input:; 1) Host reference FASTA; 2) False positive probability (0 create a hash set, >0 to create a Bloom filter); 3) Kmer length (1-31); 4) Kmer base indices to mask (optional). Output:; 1) Serialized kmer Hopscotch set (.hss) or Bloom filter (.bfi) file. For each reference record, the tool generates a list of long's containing the canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned to the host reference and filtered if it maps sufficiently ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3115:1098,load,loading,1098,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115,1,['load'],['loading']
Performance," - 1:210675831 4.2 5438000 1282169.2; 15:39:25.463 INFO ProgressMeter - 10:119579965 4.4 5479000 1242549.2; 15:39:35.700 INFO ProgressMeter - 11:118752077 4.6 5530000 1207397.2; 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; [March 2, 2023 3:40:16 PM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 5.27 minutes.; Runtime.totalMemory()=3432513536; java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to java.lang.Comparable; 	at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.util.TimSort.sort(TimSort.java:234); 	at java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); 	at java.util.Arrays.parallelSort(Arrays.java:1180); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.lang.Thread.run(Thread.java:750); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAs",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:6103,concurren,concurrent,6103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['concurren'],['concurrent']
Performance," - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 04:37:33.253 INFO GenomicsDBImport - Starting batch input file preload; 04:37:35.079 INFO GenomicsDBImport - Finished batch preload; 04:37:35.079 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:37.079 INFO GenomicsDBImport - Starting batch input file preload; 04:37:38.712 INFO GenomicsDBImport - Finished batch preload; 04:37:38.712 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:39.162 INFO GenomicsDBImport - Shutting down engine; [October 8, 2018 4:37:39 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=4116185088; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:769); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:165); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:604); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; 04:37:39.167 INFO GenomicsDBImport - Starting batch input fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5300:4438,concurren,concurrent,4438,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300,1,['concurren'],['concurrent']
Performance," - Picard Version: 2.21.9; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:40:45.792 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:40:45.792 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:40:45.792 INFO HaplotypeCaller - Inflater: IntelInflater; 14:40:45.792 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:40:45.792 INFO HaplotypeCaller - Requester pays: disabled; 14:40:45.792 INFO HaplotypeCaller - Initializing engine; 14:40:47.694 INFO IntervalArgumentCollection - Processing 50818468 bp from intervals; 14:40:47.714 INFO HaplotypeCaller - Done initializing engine; 14:40:47.826 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:40:47.864 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:40:47.868 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:40:47.921 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:40:47.922 INFO IntelPairHmm - Available threads: 1; 14:40:47.922 INFO IntelPairHmm - Requested threads: 4; 14:40:47.922 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:40:47.922 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:40:48.005 INFO ProgressMeter - Starting traversal; 14:40:48.006 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:40:51.792 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7076:8200,Load,Loading,8200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076,1,['Load'],['Loading']
Performance," - Requester pays: disabled; 02:07:51.775 INFO HaplotypeCaller - Initializing engine; 02:07:52.246 INFO HaplotypeCaller - Done initializing engine; 02:07:52.303 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 02:07:52.312 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 02:07:52.314 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 02:07:52.355 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 02:07:52.355 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 02:07:52.356 INFO IntelPairHmm - Available threads: 104; 02:07:52.356 INFO IntelPairHmm - Requested threads: 4; 02:07:52.356 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 02:07:52.408 INFO ProgressMeter - Starting traversal; 02:07:52.408 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 02:07:53.316 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes; 02:07:53.598 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 1.49244E-4; 02:07:53.598 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.007888748000000001; 02:07:53.598 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.00 sec; 02:07:53.598 INFO HaplotypeCaller - Shutting down engine; [28 November 2019 at 2:07:53 AM IST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=8220835840; java.lang.NullPointerException; at org.broadinstitute.hellbender.utils.read.AlignmentUtils.needsConsolidation(AlignmentUtils.java:758); at org.broadins",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6292:3602,multi-thread,multi-threaded,3602,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6292,1,['multi-thread'],['multi-threaded']
Performance, - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/oreganno.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; WARNING 2020-08-19 15:41:49 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 15:41:49.663 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/hgnc_download_Nov302017.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/hgnc/hg19/hgnc_download_Nov302017.tsv; 15:41:49.851 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_20180401.vcf -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:49.851 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar_VCF : 100000; 15:41:49.852 INFO FeatureManager - Using codec VCFCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:49.938 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_20180401.vcf -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:50.021 INFO FeatureManager - Using codec VCFCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:50.092 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar : 100000; 15:41:50.093 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6758:13492,cache,cache,13492,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6758,1,['cache'],['cache']
Performance," - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC curve knobs: 20; 10:20:12.555 INFO root - - GC curve prior standard deviation: 1.0; 10:20:12.954 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the denoising model...; 10:20:15.806 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the sampler...; 10:20:15.807 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the copy number caller...; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Global model variables: {'log_mean_bias_t', 'psi_t_log__', 'W_tu', 'ard_u_log__'}; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'read_depth_s_log__', 'psi_s_log__', 'z_sg', 'z_su'}; 10:20:18.549 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 10:20:18.549 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 10:20:24.995 INFO gcnvkernel.tasks.task_case_denoising_calling - Loading the model and updating the instantiated model and workspace...; 10:20:25.005 INFO gcnvkernel.io.io_commons - Reading model parameter values for ""log_mean_bias_t""... Stderr: Traceback (most recent call last):; File ""/media/Data/tmp/case_denoising_calling.3564509013495540802.py"", line 201, in <module>; shared_workspace, initial_params_supplier, args.input_model_path); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/tasks/task_case_denoising_calling.py"", line 128, in __init__; self.continuous_model_approx, input_model_path)(); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_denoising_calling.py"", line 93, in __call__; self.input_path, self.denoising_model_approx, self.denoising_model); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_commons.py"", line 471, in read_mean_field_global_params; ""expected: {2}"".format(var_name, var_mu.sha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8740:7758,Load,Loading,7758,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740,1,['Load'],['Loading']
Performance," --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.3.0/install/bin/gatk-package-4.0.3.0-spark.jar CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 13:48:31.261 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:48:31.426 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.3.0/install/bin/gatk-package-4.0.3.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:48:31.693 INFO CountReadsSpark - ------------------------------------------------------------; 13:48:31.693 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 13:48:31.693 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:48:31.694 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:48:31.694 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:48:31.694 INFO CountReadsSpark - Start Date/Time: December 21, 2018 1:48:31 PM EST; 13:48:31.694 INFO CountReadsSpark - ------------------------------------------------------------; 13:48:31.694 INFO CountReadsSpark - ------------------------------------------------------------; 13:48:31.695 INFO CountReadsSpark - HTSJDK Vers",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-449510725:1878,Load,Loading,1878,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-449510725,1,['Load'],['Loading']
Performance," -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx130g -jar /gatk/gatk-package-4.1.8.1-local.jar Mutect2 -R /ucsc.hg19.fasta -I my.bam -L /test.bed --f1r2-tar-gz DD.f1r2.tar.gz --force-active --genotype-germline-sites --kmer-size 10 --kmer-size 20 --recover-all-dangling-branches --max-reads-per-alignment-start 0 --native-pair-hmm-threads 33 -O DD.vcf.gz; `. ### Affected version(s); Using GATK jar /gatk/gatk-package-4.1.8.1-local.jar. ### Description ; When bed is created with a reference genome that is not the same as the bam file, an null pointer can occurs. The error is not catched by GATK, and the error is difficult to understand. Here a discussion about it.; https://gatk.broadinstitute.org/hc/en-us/community/posts/360077477391-Haplotype-caller-fails-to-run-GATK-4-1-8-0-and-GATK-4-2-0-0-. The case below occurs when provided bed has been made with the wrong genome reference.; `; 14:25:55.254 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 07, 2021 2:25:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:25:55.525 INFO Mutect2 - ------------------------------------------------------------; 14:25:55.525 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.1; 14:25:55.525 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:25:55.525 INFO Mutect2 - Executing as toto on Linux v5.4.123-1.el7.elrepo.x86_64 amd64; 14:25:55.525 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:25:55.526 INFO Mutect2 - Start Date/Time: October 7, 2021 2:25:55 PM GMT; 14:25:55.526 INFO Mutect2 - ------------------------------------------------------------; 14:25:55.526 INFO Mutect2 - ----------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7496:1110,Load,Loading,1110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7496,1,['Load'],['Loading']
Performance, 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 21:14:17.635 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.858 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 13 > 12 expanding to 26; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 27 > 26 expanding to 54; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 55 > 54 expanding to 110; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 111 > 110 expanding to 222; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 223 > 222 expanding to 446; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 447 > 446 expanding to 894; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 895 > 894 expanding to 1790; > 21:14:17.874 DEBUG MathUtils$Log10Cache - cache miss 1791 > 1790 expanding to 3582; > 21:14:17.894 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.930 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.937 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:18.507 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:3->4; > 21:14:18.510 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:27:38.720 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:28:26.332 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:30:24.296 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:4->5; > 21:30:24.299 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:3->4; > . Here's standard error:. > WARNING: No valid combination operation found for INFO field DS -,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:6201,cache,cache,6201,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['cache'],['cache']
Performance," 0 iteration; Apply node that caused the error: forall_inplace,cpu,scan_fn}(Elemwise{minimum,no_inplace}.0, InplaceDimShuffle{0,2,1}.0, Subtensor{int64:int64:int64}.0, IncSubtensor{InplaceSet;:int64:}.0, Shape_i{0}.0); Toposort index: 97; Inputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(int64, scalar)]; Inputs shapes: [(), (0, 6, 6), (0, 6), (2, 6), ()]; Inputs strides: [(), (288, 8, 48), (48, 8), (48, 8), ()]; Inputs values: [array(0), array([], shape=(0, 6, 6), dtype=float64), array([], shape=(0, 6), dtype=float64), 'not shown', array(6)]; Outputs clients: [[Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})]]. HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be d; isabled with 'optimizer=None'.; HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.executeSegmentGermlineCNVCallsPythonScript(PostprocessGermlineCNVCalls.java:500); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.generateSegmentsVCFFileFromAllShards(Postproc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4840:12415,optimiz,optimizations,12415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840,2,['optimiz'],"['optimizations', 'optimizer']"
Performance," 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/24 14:34:27 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 4.816635 s; ```; Our system is an HPC, where all the nodes share the same file system. I run my SPARK on only one node to test the software. I red elesewhere that this might be aproblem of missing jars, so I tried to inlcude these libraries in the SPARK jar folder and added the option:; `; --conf [--jars=""~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-common-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-hadoop2-compat-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hive-hbase-handler-1.2.1.spark2.jar"" ]`. But I still get the error. Is GATK using hbase? If yes shall some jars be included to a local SPARK system to enable it to run GATK tools? Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:1991,concurren,concurrent,1991,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494,1,['concurren'],['concurrent']
Performance," 02:53:12,34] [info] Aborting all running workflows.; [2019-10-01 02:53:12,34] [info] JobExecutionTokenDispenser stopped; [2019-10-01 02:53:12,35] [info] WorkflowStoreActor stopped; [2019-10-01 02:53:12,35] [info] WorkflowLogCopyRouter stopped; [2019-10-01 02:53:12,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-10-01 02:53:12,35] [info] WorkflowManagerActor All workflows finished; [2019-10-01 02:53:12,35] [info] WorkflowManagerActor stopped; [2019-10-01 02:53:12,65] [info] Connection pools shut down; [2019-10-01 02:53:12,65] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-10-01 02:53:12,65] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-10-01 02:53:12,65] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-10-01 02:53:12,65] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-10-01 02:53:12,65] [info] SubWorkflowStoreActor stopped; [2019-10-01 02:53:12,65] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-10-01 02:53:12,65] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-10-01 02:53:12,66] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-10-01 02:53:12,66] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-10-01 02:53:12,66] [info] JobStoreActor stopped; [2019-10-01 02:53:12,66] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-10-01 02:53:12,66] [info] CallCacheWriteActor stopped; [2019-10-01 02:53:12,66] [info] IoProxy stopped; [2019-10-01 02:53:12,66] [info] ServiceRegistryActor stopped; [2019-10-01 02:53:12,67] [info] DockerHashActor stopped; [2019-10-01 02:53:12,69] [info] Database closed; [2019-10-01 02:53:12,69] [info] Stream materializer shut down; [2019-10-01 02:53:12,69] [info] WDL HTTP import resolver closed; Workflow c55a06f3-abc1-4db1-8e0f-ea0303caab2c transitioned to state Failed; ```. Any help will be much appreciated. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6189:10800,queue,queued,10800,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6189,3,['queue'],['queued']
Performance," 07:32:25 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 107) in 279 ms on localhost (executor driver) (2/3); 21/04/13 07:32:25 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:9236,concurren,concurrent,9236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['concurren'],['concurrent']
Performance, 0x00005648765c2000 Exception <a 'java/lang/NoSuchMethodError': java.lang.Object.lambda$comparing$ea9a8b3a$1(Ljava/util/Comparator;Ljava/util/function/Function;Ljava/lang/Object;Ljava/lang/Object;)I> (0x000000067b20f3e0) thrown at [/home/buildozer/aports/community/openjdk8/s; Event: 3.490 Thread 0x00005648765c2000 Exception <a 'java/lang/NoSuchMethodError': java.lang.Object.lambda$comparingInt$7b0bb60$1(Ljava/util/function/ToIntFunction;Ljava/lang/Object;Ljava/lang/Object;)I> (0x000000067b219168) thrown at [/home/buildozer/aports/community/openjdk8/src/icedtea-3.6.; Event: 3.491 Thread 0x00005648765c2000 Exception <a 'java/lang/NoSuchMethodError': java.lang.Object.lambda$thenComparing$36697e65$1(Ljava/util/Comparator;Ljava/lang/Object;Ljava/lang/Object;)I> (0x000000067b220588) thrown at [/home/buildozer/aports/community/openjdk8/src/icedtea-3.6.0/openjdk/. Events (10 events):; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmmOMP; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmmOMP done; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmm; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmm done; Event: 4.326 loading class com/intel/gkl/IntelGKLUtils; Event: 4.326 loading class com/intel/gkl/IntelGKLUtils done; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder done; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder done. Dynamic libraries:; 3c0000000-41b600000 rw-p 00000000 00:00 0 ; 41b600000-66ab00000 ---p 00000000 00:00 0 ; 66ab00000-6aef00000 rw-p 00000000 00:00 0 ; 6aef00000-7c0000000 ---p 00000000 00:00 0 ; 7c0000000-7c0520000 rw-p 00000000 00:00 0 ; 7c0520000-800000000 ---p 00000000 00:00 0 ; 2b5f56cd5000-2b5f56d5e000 r-xp 00000000 07:00 565 /lib/ld-musl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:29085,load,loading,29085,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['load'],['loading']
Performance," 1.6.1-hadoop2; 17/10/11 14:19:12 INFO yarn.Client: Uploading resource file:/tmp/hdfs/spark-8c88439f-dcb0-48b2-86f3-fc82cef4c438/__spark_conf__8945422067005652415.zip -> hdfs://mg:8020/user/hdfs/.sparkStaging/application_1507683879816_0006/__spark_conf__8945422067005652415.zip; 17/10/11 14:19:13 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/11 14:19:13 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/11 14:19:13 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); users with modify permissions: Set(hdfs); 17/10/11 14:19:13 INFO yarn.Client: Submitting application 6 to ResourceManager; 17/10/11 14:19:13 INFO impl.YarnClientImpl: Submitted application application_1507683879816_0006; 17/10/11 14:19:14 INFO yarn.Client: Application report for application_1507683879816_0006 (state: ACCEPTED); 17/10/11 14:19:14 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: root.users.hdfs; 	 start time: 1507702753100; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088/proxy/application_1507683879816_0006/; 	 user: hdfs; 17/10/11 14:19:15 INFO yarn.Client: Application report for application_1507683879816_0006 (state: ACCEPTED); 17/10/11 14:19:15 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(null); 17/10/11 14:19:15 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> mg, PROXY_URI_BASES -> http://mg:8088/proxy/application_1507683879816_0006), /proxy/application_1507683879816_0006; 17/10/11 14:19:15 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 17/10/11 14:19:16 INFO yarn.Client: Application report for application_1507683879816_0006 (state: ACCEPTED); 17/10/11 14:19:17 INFO yarn.Client: Applicatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:6898,queue,queue,6898,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['queue'],['queue']
Performance," 11 hours to get to the point where the error occurs it has been difficult to trouble shoot, I am hoping that I can fix this without rebuilding everything which is why I decided to write. Thanks for any information or suggestions you may have. . Dan; ; Using GATK jar /home/dan_vanderpool/src/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx1600g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /home/dan_vanderpool/src/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar GenotypeGVCFs -R /home/dan_vanderpool/Wolf_raw_reads/Wolf_genome/GCA_905319855.2_mCanLor1.2_genomic.fa -V gendb://Wolf_Genome_Variantsdb -O All_Wolf_Samples_Joint_Genotypes_Raw.vcf.gz -L /scratch/dan/Wolf_reads_raw/Wolf_GenCov300_Q20_Merged.interval_list -imr ALL --genomicsdb-max-alternate-alleles 10 --max-alternate-alleles 6; 17:49:29.781 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dan_vanderpool/src/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 23, 2022 5:49:30 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:49:30.164 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:30.165 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 17:49:30.165 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:49:30.165 INFO GenotypeGVCFs - Executing as dan_vanderpool@0e07622619ad on Linux v4.4.0-210-generic amd64; 17:49:30.165 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 17:49:30.166 INFO GenotypeGVCFs - Start Date/Time: February 23, 2022 at 5:49:29 PM UTC; 17:49:30.166 INFO GenotypeGVCFs - -------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1049112454:4188,Load,Loading,4188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1049112454,1,['Load'],['Loading']
Performance," 15:39:35.700 INFO ProgressMeter - 11:118752077 4.6 5530000 1207397.2; > 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; > 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; > 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; > 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; > [March 2, 2023 3:40:16 PM EST]; > org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done.; > Elapsed time: 5.27 minutes.; > Runtime.totalMemory()=3432513536; > java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to; > java.lang.Comparable; > at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); > at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); > at java.util.TimSort.sort(TimSort.java:234); > at; > java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); > at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); > at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); > at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); > at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); > at java.util.Arrays.parallelSort(Arrays.java:1180); > at; > htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); > at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); > at; > htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); > at; > htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); > at java.lang.Thread.run(Thread.java:750); > Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record; > to closed writer.; > at; > htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:6619,concurren,concurrent,6619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['concurren'],['concurrent']
Performance, 2017 16:51:57 BST] Executing as ameyner2@node2c15.ecdf.ed.ac.uk on Linux 3.10.0-327.36.3.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_121-b13; Version: 4.beta.2; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:51:57.770 INFO SparkGenomeReadCounts - Deflater: IntelDeflater; 16:51:57.770 INFO SparkGenomeReadCounts - Inflater: IntelInflater; 16:51:57.770 INFO SparkGenomeReadCounts - Initializing engine; 16:51:57.770 INFO SparkGenomeReadCounts - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/07/21 16:51:58 INFO SparkContext: Running Spark version 2.0.2; 17/07/21 16:51:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(ameyner2); groups with view permissions: Set(); users with modify permissions: Set(ameyner2); groups with modify permissions: Set(); 17/07/21 16:51:58 INFO Utils: Successfully started service 'sparkDriver' on port 43815.; 17/07/21 16:51:58 INFO SparkEnv: Registering MapOutputTracker; 17/07/21 16:51:58 INFO SparkEnv: Registering BlockManagerMaster; 17/07/21 16:51:58 INFO DiskBlockManager: Created local directory at /tmp/ameyner2/blockmgr-d8bbd2bc-8366-4b98-a238-2d51da1689d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3360:3010,load,load,3010,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360,1,['load'],['load']
Performance," 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3688baab{/,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4fe2dd02{/api,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@726a8729{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1a2724d3{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://scc-hadoop.bu.edu:4041; 2019-01-07 11:33:27 INFO SparkContext:54 - Added JAR file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar at spark://scc-hadoop.bu.edu:46828/jars/gatk-package-4.0.12.0-spark.jar with timestamp 1546878807984; 2019-01-07 11:33:28 INFO GoogleHadoopFileSystemBase:607 - GHFS version: 1.6.3-hadoop2; 2019-01-07 11:33:29 WARN DomainSocketFactory:117 - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-07 11:33:30 INFO Client:54 - Requesting a new application from cluster with 21 NodeManagers; 2019-01-07 11:33:30 INFO Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-07 11:33:30 INFO Client:54 - Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-07 11:33:30 INFO Client:54 - Setting up container launch context for our AM; 2019-01-07 11:33:30 INFO Client:54 - Setting up the launch environment for our AM container; 2019-01-07 11:33:30 INFO Client:54 - Preparing resources for our AM container; 2019-01-07 11:33:30 INFO HadoopFSDelegationTokenProvider:54 - getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1883879239_1, ugi=farrell@AD.BU.EDU (auth:KERBEROS)]]; 2019-01-07 11:33:30 INFO DFSClient:1023 - Created HDFS_DELEGATION_TOKEN token 11334 for farrell on ha-hdfs",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:10619,load,loaded,10619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['load'],['loaded']
Performance," 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@16b64a03{/,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1584c019{/api,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5817f1ca{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2b395581{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://scc-hadoop.bu.edu:4041; 2019-01-09 13:35:12 INFO SparkContext:54 - Added JAR file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar at spark://scc-hadoop.bu.edu:42689/jars/gatk-package-4.0.12.0-spark.jar with timestamp 1547058912934; 2019-01-09 13:35:13 INFO GoogleHadoopFileSystemBase:607 - GHFS version: 1.6.3-hadoop2; 2019-01-09 13:35:13 WARN DomainSocketFactory:117 - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-09 13:35:14 INFO Client:54 - Requesting a new application from cluster with 21 NodeManagers; 2019-01-09 13:35:14 INFO Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-09 13:35:14 INFO Client:54 - Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-09 13:35:14 INFO Client:54 - Setting up container launch context for our AM; 2019-01-09 13:35:14 INFO Client:54 - Setting up the launch environment for our AM container; 2019-01-09 13:35:14 INFO Client:54 - Preparing resources for our AM container; 2019-01-09 13:35:14 INFO HadoopFSDelegationTokenProvider:54 - getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-682487019_1, ugi=farrell@AD.BU.EDU (auth:KERBEROS)]]; 2019-01-09 13:35:14 INFO DFSClient:1023 - Created HDFS_DELEGATION_TOKEN token 11353 for farrell on ha-hdfs:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:10359,load,loaded,10359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['load'],['loaded']
Performance," 2021] Executing on Linux 2.6.32-696.el6.x86_64 amd64; INFO 14:49:42,884 HelpFormatter - Java HotSpot(TM) 64-Bit Server VM 1.8.0_11-b12; INFO 14:49:42,889 HelpFormatter - Program Args: -T HaplotypeCaller -R /share/Onc_Soft_DB/database/capsmart/hg19/hg19_20210805/hg19.rm_CRLF2_P2RY8.fix_PRSS1_MUC16.fasta -I /share/Onc_RD_Pipeline/OncDir/zhuangll/210927-commercial-tissue-zhangaiyuan/germline/Z19W06700-F1WA/2.Realign/Z19W06700-F1WA.bam -L /share/Onc_Soft_DB/database/capsmart/bed/gene102.snpindel.capsmart.bed -U -o /share/Onc_RD_Pipeline/OncDir/yanhs/test/GATK/Z19W06700-F1WA.HaplotypeCaller.raw.vcf -stand_call_conf 50 -A RMSMappingQuality -A BaseCounts; INFO 14:49:42,892 HelpFormatter - Executing as yanhs3941@compute-0-76 on Linux 2.6.32-696.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_11-b12.; INFO 14:49:42,892 HelpFormatter - Date/Time: 2021/10/09 14:49:42; INFO 14:49:42,892 HelpFormatter - ------------------------------------------------------------------------------------; INFO 14:49:42,892 HelpFormatter - ------------------------------------------------------------------------------------; INFO 14:49:42,922 NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/Onc_Soft_DB/software/GATK3.8/GenomeAnalysisTK.jar!/com/intel/gkl/native/libgkl_compression.so; INFO 14:49:42,957 GenomeAnalysisEngine - Deflater: IntelDeflater; INFO 14:49:42,958 GenomeAnalysisEngine - Inflater: IntelInflater; INFO 14:49:42,958 GenomeAnalysisEngine - Strictness is SILENT; INFO 14:49:43,125 GenomeAnalysisEngine - Downsampling Se. the error is :; maxAltAlleles (6), the following will be dropped: TAAC.; WARN 14:59:10,944 HaplotypeCallerGenotypingEngine - location chr12:21623284-21623286: too many alternative alleles found (8) larger than the maximum requested with -maxAltAlleles (6), the following will be dropped: C, CA.; INFO 14:59:13,453 ProgressMeter - chr12:21624342 1.0358131E7 9.5 m 55.0 s 49.5% 19.2 m 9.7 m; WARN 14:59:37,613 HaplotypeCallerGenotypingEngine -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7499:1683,Load,Loading,1683,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7499,1,['Load'],['Loading']
Performance," 26|2, 2], DP=94, ECNT=1, GERMQ=93, MBQ=[31, 20], MFRL=[288, 110], MMQ=[60, 60], MPOS=56, NALOD=1.37, NLOD=6.17, POPAF=4.6, ROQ=93, TLOD=10.97} GT=GT:AD:AF:DP:F1R2:F2R1:SB 0/1:46,4:0.07:50:14,3:10,0:28,18,2,2 0/0:23,0:0.041:23:8,0:5,0:15,8,0,0 filters=; 11:43:25.661 WARN GencodeFuncotationFactory - Creating default GencodeFuncotation on transcript ENST00000441716.2 for problem variant: chr6:167976552-167976594(ACAGTGGGGGTCATTCCCCCTGCAGTGTGTTGGGAGGAGGAGG* -> A); 11:44:04.904 INFO ProgressMeter - chr8:677091 4.5 3000 666.0; 11:45:35.226 INFO ProgressMeter - chr11:62279639 6.0 4000 665.6; 11:46:54.284 INFO ProgressMeter - chr15:19905537 7.3 5000 682.4; 11:48:12.767 WARN FuncotatorUtils - createAminoAcidSequence given a coding sequence of length not divisible by 3. Dropping bases from the end: 2 (size=293, ref allele: G); 11:48:16.949 ERROR GencodeFuncotationFactory - Problem creating a GencodeFuncotation on transcript ENST00000379751.5 for variant: chr20:3786474-3786537(TGGGGCCCATCCCGGCGCGCCCCCCGCCCCGGGGCCCGGCGCCGCCGCCGCCGCCCCGGGGCGG* -> T): Cannot yet handle indels starting outside an exon and ending within an exon.; 11:48:16.949 WARN GencodeFuncotationFactory - Creating default GencodeFuncotation on transcript ENST00000379751.5 for problem variant: chr20:3786474-3786537(TGGGGCCCATCCCGGCGCGCCCCCCGCCCCGGGGCCCGGCGCCGCCGCCGCCGCCCCGGGGCGG* -> T); 11:48:31.506 INFO ProgressMeter - chr21:18282114 8.9 6000 670.6; 11:49:08.210 INFO ProgressMeter - chr21:18282114 9.6 6888 720.6; 11:49:08.210 INFO ProgressMeter - Traversal complete. Processed 6888 total variants in 9.6 minutes.; 11:49:08.210 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/2; 11:49:08.211 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/4781; 11:49:08.230 INFO Funcotator - Shutting down engine; [July 7, 2021 11:49:08 AM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 9.72 minutes.; Runtime.totalMemory()=4879548416; Tool returned:; true",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-887961422:2306,cache,cache,2306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-887961422,2,['cache'],['cache']
Performance," 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 5; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 5; cpu cores	: 14; apicid		: 10; initial apicid	: 10; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 6; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 6; cpu cores	: 14; apicid		: 12; initial apicid	: 12; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:48865,cache,cache,48865,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance," 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 6; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 6; cpu cores	: 14; apicid		: 12; initial apicid	: 12; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 7; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2900.062; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 8; cpu cores	: 14; apicid		: 16; initial apicid	: 16; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:50038,cache,cache,50038,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance," 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 7; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2900.062; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 8; cpu cores	: 14; apicid		: 16; initial apicid	: 16; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 8; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 9; cpu cores	: 14; apicid		: 18; initial apicid	: 18; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:51211,cache,cache,51211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance," 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 8; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 9; cpu cores	: 14; apicid		: 18; initial apicid	: 18; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 9; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 10; cpu cores	: 14; apicid		: 20; initial apicid	: 20; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:52384,cache,cache,52384,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance," 4 --bam-output tumor.recalibrated.realigned.bam --add-output-sam-program-record false -bam-output. The log of the command that generated the error was :. Using GATK jar /data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar. Running:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar FilterAlignmentArtifacts --variant tumor.recalibrated.filtered.vcf --input tumor.recalibrated.realigned.bam --reference /data/genepattern/users/.cache/uploads/cache/data.gp.vib.be/pub/genome/Homo_sapiens.UCSC.hg38.fa --bwa-mem-index-image /data/genepattern/users/.cache/uploads/cache/data.gp.vib.be/pub/bwa_index_img/Homo_sapiens.UCSC.hg38.img --output tumor.recalibrated.filtered2.vcf --bam-output tumor.recalibrated.realigned2.bam --verbosity ERROR --tmp-dir TMP --QUIET true. 14:38:44.077 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so. 14:38:44.103 INFO SmithWatermanAligner - AVX accelerated SmithWaterman implementation is not supported, falling back to the Java implementation. java.lang.IllegalArgumentException: Program record with group id HalpotypeBAMWriter already exists in SAMFileHeader!. at htsjdk.samtools.SAMFileHeader.addProgramRecord(SAMFileHeader.java:202). at htsjdk.samtools.SAMTextHeaderCodec.parsePGLine(SAMTextHeaderCodec.java:158). at htsjdk.samtools.SAMTextHeaderCodec.decode(SAMTextHeaderCodec.java:107). at htsjdk.samtools.SAMFileHeader.clone(SAMFileHeader.java:398). at org.broadinstitute.hellbender.utils.read.ReadUtils.createCommonSAMWriterFromFactory(ReadUtils.java:1215). at org.broadinstitute.hellbender.utils.read.ReadUtils.createCommonSAMWriter(ReadUtils.java:1163). at org.broadinstitute.hellbender.utils.haplotype.SAMFileDestination.(SAMFileDestinat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6287:1698,Load,Loading,1698,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6287,1,['Load'],['Loading']
Performance, 7	CombineReadCounts		19-May	https://github.com/broadinstitute/gatk-protected/blob/3e6142ad4eb23d4d9227fafd8e52b498263b4369/src/main/java/org/broadinstitute/hellbender/tools/exome/CombineReadCounts.java	scripts/cnv_wdl/somatic/cnv_somatic_panel_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1072	yes	; 13	CreatePanelOfNormals		19-May	https://github.com/broadinstitute/gatk-protected/blob/e1ffbff498db40c894105c06a41b443859b58a04/src/main/java/org/broadinstitute/hellbender/tools/exome/CreatePanelOfNormals.java	scripts/cnv_wdl/somatic/cnv_somatic_panel_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1073	yes	; 20	NormalizeSomaticReadCounts		19-May	https://github.com/broadinstitute/gatk-protected/blob/e1ffbff498db40c894105c06a41b443859b58a04/src/main/java/org/broadinstitute/hellbender/tools/exome/NormalizeSomaticReadCounts.java	scripts/cnv_wdl/somatic/cnv_somatic_copy_ratio_bam_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1074	yes	; 25	PerformSegmentation		19-May	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/PerformSegmentation.java	scripts/cnv_wdl/somatic/cnv_somatic_copy_ratio_bam_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1075	yes	; 27	PlotSegmentedCopyRatio		21-May	https://github.com/broadinstitute/gatk-protected/blob/e1ffbff498db40c894105c06a41b443859b58a04/src/main/java/org/broadinstitute/hellbender/tools/exome/plotting/PlotSegmentedCopyRatio.java	scripts/cnv_wdl/somatic/cnv_somatic_copy_ratio_bam_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1077	yes	; 5	CallSegments		21-May	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/CallSegments.java	scripts/cnv_wdl/somatic/cnv_somatic_copy_ratio_bam_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3055:2030,Perform,PerformSegmentation,2030,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055,1,['Perform'],['PerformSegmentation']
Performance," 7, 2017 12:48:13 AM UTC] Executing as tianj@ip-xxx-xx-xx-xxx on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11; Version: 4.alpha.2-1100-g04dbeb2-SNAPSHOT; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:48:13.680 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:3903,load,loaded,3903,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['load'],['loaded']
Performance, : false; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:18:11.388 INFO Mutect2 - Deflater: IntelDeflater; 12:18:11.388 INFO Mutect2 - Inflater: IntelInflater; 12:18:11.389 INFO Mutect2 - GCS max retries/reopens: 20; 12:18:11.389 INFO Mutect2 - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:18:11.389 INFO Mutect2 - Initializing engine; 12:18:11.724 INFO Mutect2 - Done initializing engine; 12:18:12.288 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 12:18:12.290 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 12:18:12.290 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 12:18:12.290 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 12:18:12.368 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:18:12.368 WARN IntelPairHmm - Ignoring request for 4 threads; not using OpenMP implementation; 12:18:12.369 INFO PairHMM - Using the AVX-accelerated native PairHMM implementation; 12:18:12.403 INFO ProgressMeter - Starting traversal; 12:18:12.403 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:18:22.403 INFO ProgressMeter - chr1:75065650 0.2 250240 1501440.0; 12:18:29.713 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.009098343; 12:18:29.713 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.121747383; 12:18:29.713 INFO SmithWatermanAligner - Total compute ti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665:2963,multi-thread,multi-threaded,2963,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665,1,['multi-thread'],['multi-threaded']
Performance, ; at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:570) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:119) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:156) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29) ; ; at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:27) ; ; at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:13) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadFilteringIterator.loadNextRead(ReadFilteringIterator.java:53) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadFilteringIterator.next(ReadFilteringIterator.java:47) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadFilteringIterator.next(ReadFilteringIterator.java:13) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14) ; ; at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.fillCache(PushToPullIterator.java:72) ; ; at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.advanceToNextElement(PushToPullIterator.java:58) ; ; at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.next(PushToPullIterator.java:52) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadCachingIterator.next(ReadCachingIterator.java:42) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadCachingIterator.ne,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582:11124,load,loadNextRead,11124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582,1,['load'],['loadNextRead']
Performance," > 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; > 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; > 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; > 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; > [March 2, 2023 3:40:16 PM EST]; > org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done.; > Elapsed time: 5.27 minutes.; > Runtime.totalMemory()=3432513536; > java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to; > java.lang.Comparable; > at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); > at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); > at java.util.TimSort.sort(TimSort.java:234); > at; > java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); > at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); > at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); > at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); > at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); > at java.util.Arrays.parallelSort(Arrays.java:1180); > at; > htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); > at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); > at; > htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); > at; > htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); > at java.lang.Thread.run(Thread.java:750); > Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record; > to closed writer.; > at; > htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); > at; > htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:6689,concurren,concurrent,6689,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['concurren'],['concurrent']
Performance, Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:12354,cache,cache,12354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance," BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam ; ; 00:11:11.683 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.697 WARN NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.700 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.700 WARN NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.812 INFO BaseRecalibrator - ------------------------------------------------------------ ; ; 00:11:11.813 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:11:11.813 INFO BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:11:11.813 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86\_64 amd64 ; ; 00:11:11.813 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087 ; ; 00:11:11.813 INFO BaseRecalibrator - Start Date/Time: August 21, 2022 at 12:11:11 AM CST ; ; 00:11:11.8",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:9463,Load,Loading,9463,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['Load'],['Loading']
Performance, Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 21:14:17.635 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.858 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 13 > 12 expanding to 26; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 27 > 26 expanding to 54; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 55 > 54 expanding to 110; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 111 > 110 expanding to 222; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 223 > 222 expanding to 446; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 447 > 446 expanding to 894; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 895 > 894 expanding to 1790; > 21:14:17.874 DEBUG MathUtils$Log10Cache - cache miss 1791 > 1790 expanding to 3582; > 21:14:17.894 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.930 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.937 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:18.507 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:3->4; > 21:14:18.510 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:27:38.720 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:28:26.332 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:30:24.296 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:4->5; > 21:30:24.299 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:3->4; > . Here's,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:6117,cache,cache,6117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['cache'],['cache']
Performance, Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 - Processing assembly region at chrM:2544-2841 isActive: true numReads: 5108; 11:35:48.094 DEBUG ReadThreadingGraph - Recovered 17 of 20 dangling tails; 11:35:48.198 DEBUG ReadThreadingGraph - Recovered 16 of 50 dangling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 > 10 expanding to 2399; 11:35:48.874 DEBUG Mutect2Engine - Active Region chrM:2544-2841; 11:35:48.874 DEBUG Mutect2Engine - Extended Act Region chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Ref haplotype coords chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Haplotype count 128; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:08.907 INFO ProgressMeter - chrM:2544 0.4 10 22.3; 11:36:08.954 DEBUG Mutect2 - Processing assembly region at chrM:2842-2920 isActive: false numReads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:9348,cache,cache,9348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance," FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../..",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4686:2048,Queue,Queue,2048,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686,1,['Queue'],['Queue']
Performance," HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:19:40.101 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:19:40.101 INFO GenomicsDBImport - Inflater: IntelInflater; 11:19:40.101 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:19:40.102 INFO GenomicsDBImport - Requester pays: disabled; 11:19:40.102 INFO GenomicsDBImport - Initializing engine; 11:19:40.385 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/data/project/reseq/KPSNY042021067K/result/03.bwa_dup_gvcf/geno/chr33.bed; 11:19:40.390 INFO IntervalArgumentCollection - Processing 10664 bp from intervals; 11:19:40.391 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 11:19:40.429 INFO GenomicsDBImport - Done initializing engine; 11:19:40.624 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; 11:19:40.625 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/data/chr33.db/vidmap.json; 11:19:40.625 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/data/chr33.db/callset.json; 11:19:40.625 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/data/chr33.db/vcfheader.vcf; 11:19:40.625 INFO GenomicsDBImport - Importing to workspace - /mnt/data/chr33.db; 11:19:40.625 INFO ProgressMeter - Starting traversal; 11:19:40.625 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:19:49.073 INFO GenomicsDBImport - Importing batch 1 with 1115 samples; 11:20:12.073 I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460:3311,perform,performance,3311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460,1,['perform'],['performance']
Performance," HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false. 16:17:05.844 INFO HaplotypeCaller - Deflater: JdkDeflater. 16:17:05.844 INFO HaplotypeCaller - Inflater: JdkInflater. 16:17:05.844 INFO HaplotypeCaller - GCS max retries/reopens: 20. 16:17:05.844 INFO HaplotypeCaller - Requester pays: disabled. 16:17:05.845 INFO HaplotypeCaller - Initializing engine. 16:17:05.928 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater. 16:17:05.932 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater. 16:17:06.503 INFO FeatureManager - Using codec VCFCodec to read file file:///home/robert/test/snps.vcf. 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals. 16:17:06.551 INFO HaplotypeCaller - Done initializing engine. 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output. 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so. 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded. 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported. 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so. 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794:4051,Load,Loading,4051,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794,1,['Load'],['Loading']
Performance," I am working on GATK VariantsToTable tool and my VCF file consists of 12 chromosomes but the output shows only one chromosome. Could you please help me out.; OS:-Ubuntu 20.04; GATK version:-4.1.9.0; Java:-open jdk version 11.0.8; Command:-gatk VariantsToTable -R '/home/india/Downloads/Reference.fasta' -V '/home/india/Downloads/Galaxy57-[Merged_file.vcf].vcf' -F CHROM -F POS -F REF -F ALT -GF AD -GF DP -GF GQ -GF PL -O bothbulks_new.table. Using GATK jar /home/india/Downloads/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/india/Downloads/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar VariantsToTable -R /home/india/Downloads/Reference.fasta -V /home/india/Downloads/Galaxy57-[Merged_file.vcf].vcf -F CHROM -F POS -F REF -F ALT -GF AD -GF DP -GF GQ -GF PL -O bothbulks_new.table; 16:46:03.294 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/india/Downloads/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 16, 2020 4:46:04 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:46:04.315 INFO VariantsToTable - ------------------------------------------------------------; 16:46:04.316 INFO VariantsToTable - The Genome Analysis Toolkit (GATK) v4.1.9.0; 16:46:04.316 INFO VariantsToTable - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:46:04.317 INFO VariantsToTable - Executing as india@india-HP-ProBook-445-G1 on Linux v5.4.0-26-generic amd64; 16:46:04.317 INFO VariantsToTable - Java runtime: OpenJDK 64-Bit Server VM v11.0.8+10-post-Ubuntu-0ubuntu120.04; 16:46:04.317 INFO VariantsToTable - Start Date/Time: 16 October 2020 at 4:46:02 PM IST; 16:46:04.318 INFO VariantsToTable - -------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6897:1006,Load,Loading,1006,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6897,1,['Load'],['Loading']
Performance," I don't know how much you cleaned up the GATK4 version. We refactored all the engine stuff shared with `HaplotypeCaller` to be very distinct from the somatic genotyping logic, so the only complexity would be in local assembly and PairHMM. Which could be significant, of course. > We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. . .Although we've seen that the graph traversal frequently breaks phasing. The specific case I had in mind is when you have a bubble or something more complex in the graph, followed by a stretch of reference (i.e. all haplotypes have nothing going on here), followed (or not) by more activity. It seems reasonable in that case to chop each active area into its own haplotype(s), which is equivalent to pinning the ref-only area to be ref-only in PairHMM. I believe but could be wrong that in a case like this our assembly would not respect phasing between the two active areas anyway, so we lose nothing. By the way, I should clarify that the idea is not to truncate the `ActiveRegion`, but rather to break it into a few small haplotypes semi-intelligently *after* building the whole deBruijn graph. It could well be that my optimism is ill-founded. Nonetheless, having a quick-and-dirty mode would be very useful for the following purposes where you need to run M2 a lot and don't need perfection:. * making an M2 panel of normals; * making true positives + false positives training data sets for artifact classifiers; * testing changes. ---. @ldgauthier commented on [Mon Mar 06 2017](https://github.com/broadinstitute/gatk-protected/issues/909#issuecomment-284502170). ""Quick and dirty"" would be useful for testing changes, but the PoN and training sets shouldn't be recreated very often so there's less savings. I hate to leverage the fact that we break phasing to optimize things because I dream of a future where HaplotypeCaller actually calls haplotypes (as you've already added an issue for).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2945:3348,optimiz,optimize,3348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945,1,['optimiz'],['optimize']
Performance," INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 3, 2017 5:27:51 AM UTC] Executing as centos@master.novalocal on Linux 3.10.0-514.10.2.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11; Version: 4.beta.5; 05:27:52.642 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 05:27:52.642 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 05:27:52.642 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 05:27:52.642 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 05:27:52.642 INFO PrintReadsSpark - Deflater: IntelDeflater; 05:27:52.642 INFO PrintReadsSpark - Inflater: IntelInflater; 05:27:52.643 INFO PrintReadsSpark - GCS max retries/reopens: 20; 05:27:52.643 INFO PrintReadsSpark - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 05:27:52.643 INFO PrintReadsSpark - Initializing engine; 05:27:52.643 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@dcf3e99] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@61df66b6].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```; I can run command using the spark-shell but somehow GATK4 fails. Any idea?. thank you very much",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3651:3601,load,loaded,3601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3651,2,['load'],['loaded']
Performance, INFO Funcotator - Finalizing data sources (this step can be long if data sources are cloud-based)...; > 15:16:41.066 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/CancerGeneCensus_Table_1_full_2012-03-15.txt -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/cancer_gene_census/hg38/CancerGeneCensus_Table_1_full_2012-03-15.txt; > 15:16:41.083 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/simple_uniprot_Dec012014.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/simple_uniprot/hg38/simple_uniprot_Dec012014.tsv; > 15:16:41.540 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/Familial_Cancer_Genes.no_dupes.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/familial/hg38/Familial_Cancer_Genes.no_dupes.tsv; > 15:16:41.545 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; > 15:16:41.556 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/oreganno/hg38/oreganno.tsv; > 15:16:41.575 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/oreganno/hg38/oreganno.config; > 15:16:41.707 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/oreganno/hg38/oreganno.tsv; > 15:16:41.709 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/oreganno/hg38/oreganno.tsv; > WARNING 2020-07-17 15:16:41 AsciiLineReader Creating an indexable source for an ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708:9824,cache,cache,9824,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708,1,['cache'],['cache']
Performance," INFO HaplotypeCaller - Inflater: IntelInflater; 12:18:42.093 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:18:42.093 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:18:42.093 INFO HaplotypeCaller - Initializing engine; 12:18:42.597 INFO FeatureManager - Using codec VCFCodec to read file file:///beegfs/work/zxmai83/Reference/dbs/b37/dbsnp_138.b37.vcf; 12:18:42.723 INFO HaplotypeCaller - Done initializing engine; 12:18:42.732 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 12:18:42.732 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 12:18:43.546 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:18:43.549 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:18:43.599 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002b5f92e39fab, pid=85482, tid=0x00002b5f56e60ae8; #; # JRE version: OpenJDK Runtime Environment (8.0_151-b12) (build 1.8.0_151-b12); # Java VM: OpenJDK 64-Bit Server VM (25.151-b12 mixed mode linux-amd64 compressed oops); # Derivative: IcedTea 3.6.0; # Distribution: Custom build (Tue Nov 21 11:22:36 GMT 2017); # Problematic frame:; # C [libgomp.so.1+0x7fab] omp_get_max_threads+0xb; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /beegfs/work/iiipe01/Exome-Test/work/1e/fc972c6b14c8006857230849630a49/hs_err_pid85482.log; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:3597,Load,Loading,3597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['Load'],['Loading']
Performance," INFO Mutect2 - -----------------------------------------------------------; 08:27:10.887 INFO Mutect2 - HTSJDK Version: 2.19.; 08:27:10.887 INFO Mutect2 - Picard Version: 2.19.; 08:27:10.887 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : fals; 08:27:10.888 INFO Mutect2 - Deflater: IntelDeflate; 08:27:10.889 INFO Mutect2 - Inflater: IntelInflate; 08:27:10.889 INFO Mutect2 - GCS max retries/reopens: 2; 08:27:10.889 INFO Mutect2 - Requester pays: disable; 08:27:10.889 INFO Mutect2 - Initializing engin; 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.<init>(ReferenceConfidenceModel.java:116); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticReferenceConfidenceModel.<init>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:2568,Load,Loading,2568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['Load'],['Loading']
Performance," IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the library without assuming that the final user will have support for the native code or not. Could this be done? I prefer not to remove the faster code by intel because I know that some users will benefit from it. Just in case it is needed, my system is a Mac OS X (10.11.5) with Darwin Kernel Version 15.5.0 (root:xnu-3248.50.21~8/RELEASE_X86_64 x86_64). Thank you very much in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1985:1613,load,load,1613,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985,1,['load'],['load']
Performance," JdkInflater. 16:17:05.844 INFO HaplotypeCaller - GCS max retries/reopens: 20. 16:17:05.844 INFO HaplotypeCaller - Requester pays: disabled. 16:17:05.845 INFO HaplotypeCaller - Initializing engine. 16:17:05.928 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater. 16:17:05.932 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater. 16:17:06.503 INFO FeatureManager - Using codec VCFCodec to read file file:///home/robert/test/snps.vcf. 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals. 16:17:06.551 INFO HaplotypeCaller - Done initializing engine. 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output. 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so. 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded. 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported. 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so. 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). 16:17:06.590 **WARN** IntelPairHmm - Intel G",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794:4239,load,load,4239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794,1,['load'],['load']
Performance," Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.389 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils5391341743604217103.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.389 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.389 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; > 19:43:36.389 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.390 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils3484179251394006588.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.390 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.390 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING imple",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6012:2288,load,load,2288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012,1,['load'],['load']
Performance," ProgressMeter - 3:1 2531.4 1 0.0; 05:39:42.051 INFO GenomicsDBImport - Done importing batch 1/1; 05:39:42.060 INFO ProgressMeter - 3:1 2531.4 1 0.0; 05:39:42.061 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 2531.4 minutes.; 05:39:42.061 INFO GenomicsDBImport - Import completed!; 05:39:42.061 INFO GenomicsDBImport - Shutting down engine; [January 16, 2021 5:39:42 AM CST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 2,531.64 minutes.; Runtime.totalMemory()=9711910912; Tool returned:; true; **Calling Variants Attempt**; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx32g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 21:16:35.251 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 17, 2021 9:16:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:16:35.496 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:16:35.497 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 21:16:35.497 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:16:35.497 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 21:16:35.497 INFO GenotypeGVCFs - Java runtime: OpenJ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839:2237,optimiz,optimizations,2237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839,1,['optimiz'],['optimizations']
Performance," ProgressMeter - chr3:54999378 1.7 1197525 698570.8; 10:26:09.642 INFO CalibrateDragstrModel - Shutting down engine; [January 2, 2023 at 10:26:09 AM GMT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 1.81 minutes.; Runtime.totalMemory()=47647293440; java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006); at org.broadinstitute.hellbender.utils.Utils.runInParallel(Utils.java:1479); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsParallel(CalibrateDragstrModel.java:551); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:202); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139:6172,concurren,concurrent,6172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139,1,['concurren'],['concurrent']
Performance," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:12928,optimiz,optimizations,12928,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['optimiz'],['optimizations']
Performance, Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 D,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:11212,cache,cache,11212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance, [...ragemodel/cachemanager/ComputableNodeFunction.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlTm9kZUZ1bmN0aW9uLmphdmE=) | `100% <100%> (+66.667%)` | `4 <1> (+2)` | :arrow_up: |; | [.../coveragemodel/cachemanager/DuplicableNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTkRBcnJheS5qYXZh) | `81.818% <100%> (+38.068%)` | `6 <2> (+2)` | :arrow_up: |; | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9QcmltaXRpdmVDYWNoZU5vZGUuamF2YQ==) | `83.333% <71.429%> (+30.702%)` | `10 <7> (+3)` | :arrow_up: |; | [...er/tools/coveragemodel/cachemanager/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9DYWNoZU5vZGUuamF2YQ==) | `80.645% <76.923%> (+30.645%)` | `9 <8> (+4)` | :arrow_up: |; | [...overagemodel/cachemanager/ComputableCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlQ2FjaGVOb2RlLmphdmE=) | `89.189% <80%> (+32.779%)` | `18 <17> (+2)` | :arrow_up: |; | [...ols/coveragemodel/CoverageModelEMCompute,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:2568,cache,cachemanager,2568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,1,['cache'],['cachemanager']
Performance," \ \ / /_ _ _ __ _ __ (_)_ __ __ _ | || || | ; 12:11:32.828 WARN Funcotator - | || || | \ \ /\ / / _` | '__| '_ \| | '_ \ / _` | | || || | ; 12:11:32.828 WARN Funcotator - |_||_||_| \ \V V / (_| | | | | | | | | | | (_| | |_||_||_| ; 12:11:32.828 WARN Funcotator - (_)(_)(_) \_/\_/ \__,_|_| |_| |_|_|_| |_|\__, | (_)(_)(_) ; 12:11:32.828 WARN Funcotator - |___/ ; 12:11:32.828 WARN Funcotator - --------------------------------------------------------------------------------; 12:11:32.828 WARN Funcotator - Only IGRs were produced for this dataset. This STRONGLY indicates that this ; 12:11:32.828 WARN Funcotator - run was misconfigured. ; 12:11:32.828 WARN Funcotator - You MUST check your data sources to make sure they are correct for these data.; 12:11:32.828 WARN Funcotator - ================================================================================; 12:11:32.829 INFO VcfFuncotationFactory - ClinVar_VCF 20180401 cache hits/total: 0/0; 12:11:32.829 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/0; 12:11:32.830 INFO Funcotator - Shutting down engine; [March 24, 2021 12:11:32 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.22 minutes.; Runtime.totalMemory()=1793064960; Tool returned:; true; (gatk) root@75181703d894:/gatk# . ----------------------------------------------------------------------------------------------------------------------------------. the variants.funcotated.maf:. #version 2.4; ##; ## fileformat=VCFv4.2; ## FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ## FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ## FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ## source=Funcotator; ## GATKCommandLine=<ID=Funcotator,CommandLine=""Funcotator --output ./my_data/variants.funcotated.maf --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output-file-format MAF --variant ./my_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:17932,cache,cache,17932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,1,['cache'],['cache']
Performance," `HaplotypeCaller` and `Mutect2`. Probably HTSJDK level issue, but popped up exception is kind of misleading. #### Stacktrace:; ```; java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:863); at htsjdk.samtools.MemoryMappedFileBuffer.<init>(MemoryMappedFileBuffer.java:23); at htsjdk.samtools.AbstractBAMFileIndex.<init>(AbstractBAMFileIndex.java:64); at htsjdk.samtools.CachingBAMFileIndex.<init>(CachingBAMFileIndex.java:56); at htsjdk.samtools.BAMFileReader.getIndex(BAMFileReader.java:418); at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:931); at htsjdk.samtools.BAMFileReader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:550); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.iterator(ReadsPathDataSource.java:336); at java.lang.Iterable.spliterator(Iterable.java:101); at org.broadinstitute.hellbender.utils.Utils.stream(Utils.java:1176); at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:384); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:97); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7579:1437,load,loadNextIterator,1437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7579,1,['load'],['loadNextIterator']
Performance, a7bb49eaece47a172e2d/TMP/jeter.vcf.gz ; 18:15:23.077 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.361 INFO FeatureManager - Using codec VCFCodec to read file file:///SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/34/f410396038; 18:15:23.361 INFO FeatureManager - Using codec VCFCodec to read file file:///SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/34/f4[0/1667]a7bb49eaece47a172e2d/TMP/jeter.vcf.gz ; 18:15:23.374 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.385 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.403 INFO IntervalArgumentCollection - Processing 1028 bp from intervals ; 18:15:23.411 INFO HaplotypeCaller - Done initializing engine ; 18:15:23.430 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so ; 18:15:23.475 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so ; 18:15:23.651 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; 18:15:23.651 INFO IntelPairHmm - Available threads: 4 ; 18:15:23.651 INFO IntelPairHmm - Requested threads: 4 ; 18:15:23.651 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; 18:15:23.671 INFO ProgressMeter - Starting traversal ; 18:15:23.671 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute ; 18:15:26.788 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr1:30191420 and possibly subsequent; at least 10 samples must have called genotypes ; 18:15:27.190 WARN DepthPerSampleHC - Annotation will not be calculated at position chr1:30477350 and possibly subsequent; genotype for sample B00I9EL is not called; 18:15:35.547 IN,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8106:4647,Load,Loading,4647,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8106,1,['Load'],['Loading']
Performance," accounting for 1) sample-specific depth (which determines the means of the negative-binomial distributions), 2) multiplicative contig-specific bias (which is mild, at least for WGS), and 3) additive sample-contig-specific mosaicism or bias (note that the above genotype priors imply that mosaicism/bias on top of a baseline of CN = 2 is the only deviation allowed for the autosomes, which is somewhat restrictive but greatly aids convergence). I put together a pure PyMC3 prototype that seems to work relatively well. Here are the per-contig coverage histograms (unfiltered bins in blue, bins retained after filtering in red, and negative-binomial fit in green) and a heatmap of per-contig ploidy probabilities. Both the panel (first 20) and case (remaining) samples are shown:. ![prototype-result](https://user-images.githubusercontent.com/11076296/37938642-e9fbd804-312c-11e8-8a6c-02ea4e4fa704.png). Although the prototype model is clearly a good fit to the filtered data, some care in choosing the optimizer and its learning parameters is required to achieve convergence to the correct solution. This is because the problem is inherently multimodal and thus there are many local minima. I found that using AdaMax with a naive strategy of warm restarts (to help kick us out of local minima) worked decently; we can achieve convergence in <10 minutes for 60 samples x 24 contigs x 250 count bins:. ![elbo](https://user-images.githubusercontent.com/11076296/37938658-fc176f12-312c-11e8-89e2-40c68e0f9953.png). I expect that @mbabadi's annealing implementation in the gcnvkernel package will handle the local minima much better. The course of action needed to implement this model should be as follows:. 1) Alter Java code to emit per-contig histograms. Change python code to consume histograms, perform filtering, and fit using the above model (or some variation).; 2) Choose learning parameters appropriate with annealing and check that results are still good.; 3) Update gCNV model to consume the d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271:2495,optimiz,optimizer,2495,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271,1,['optimiz'],['optimizer']
Performance," acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:37 INFO yarn.Client: Submitting application application_1507856833944_0003 to ResourceManager; 17/10/13 18:11:37 INFO impl.YarnClientImpl: Submitted application application_1507856833944_0003; 17/10/13 18:11:37 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1507856833944_0003 and attemptId None; 17/10/13 18:11:38 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:38 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: root.users.hdfs; 	 start time: 1507889497661; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088/proxy/application_1507856833944_0003/; 	 user: hdfs; 17/10/13 18:11:39 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:40 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:41 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 17/10/13 18:11:41 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> mg, PROXY_URI_BASES -> http://mg:8088/proxy/application_1507856833944_0003), /proxy/application_1507856833944_0003; 17/10/13 18:11:41 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 17/10/13 18:11:41 INFO yarn.C",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:11718,queue,queue,11718,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['queue'],['queue']
Performance," advance. ```. Using GATK jar /home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar CNNScoreVariants --version; Using GATK jar /home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar CNNScoreVariants -R /home/fmbuga/tools/hg38/hg38.fa -V /home/fmbuga/gatk4_gcp_wgs/06_vcf_raw/SRR16299720_dedup_AORRG_recal_raw.vcf -O ./08_vcf_1dCNN/SRR16299720_dedup_AORRG_recal_raw_1dCNN_scored.vcf; 05:39:39.149 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 05:39:39.304 INFO CNNScoreVariants - ------------------------------------------------------------; 05:39:39.305 INFO CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.2.6.1; 05:39:39.305 INFO CNNScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 05:39:39.305 INFO CNNScoreVariants - Executing as fmbuga@node05.cluster on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 05:39:39.305 INFO CNNScoreVariants - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_332-b09; 05:39:39.305 INFO CNNScoreVariants - Start Date/Time: October 9, 2022 5:39:39 AM PDT; 05:39:39.305 INFO CNNScoreVariants - ------------------------------------------------------------; 05:39:39.306 INFO CNNScoreVariants - ------------------------------------------------------------; 05:39:39.306 INFO CNNScoreVariants ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1274925490:1114,Load,Loading,1114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1274925490,1,['Load'],['Loading']
Performance," array, etc.). This is (we think) done in `VariantContext.fullyDecode().`. This turned out not to be possible for the following reasons. First, there are roughly four types of genotype subsetting you could do:. a) By the sample names (`--sample-name NA12878`); b) JEXL (`--select GQ > 0`); c) JEXL by accessing the variant context object (`--select vc.getGenotype('NA12878').getGQ() > 1`); d) Others (e.g. `--remove-fraction-genotype`). a) does not need ""fully-decode."" It turns out b) was never supported (GATK currently removes all variants and succeed.) And from my experiments, c) does not seem to ever trigger calling `VariantContext.fullyDecode().` In fact the only code path I can see that calls fullyDecode() is by setting the `fully-decode` SelectVariants argument, which seems to just call fullyDecode at the beginning just for the sake of calling it (or so it appears to me. The utility of this command line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype fields like 'GQ > 0', where the behavior in the multi-sample case is 'GQ > 0' in at least one sample. I have not added the ability to do 'GQ > 0 for all samples' but it should be a simple (but not easy) exercise in boolean operations.; * `applyJexlFiltersBeforeFilteringGenotypes`: if set to true, we do the JEXL checking before we subset by samples. In my tests, performance improvement from this option was very modest. Subsetting a ~3k 1kg SV vcf to a single sample was about 30 seconds faster (out of ~20 min total run t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8092:1564,perform,performance,1564,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092,1,['perform'],['performance']
Performance," as the reference genotype for any site not present in the source workspace. In contrast, if you run a similar command with GenotypeGVCFs + --force-output + gVCF, the resulting VCF reports the correct reference base. In both cases we are passing the FASTA as -R to the tool. Here are two example outputs:. This is from GenotypeGVCFs with a GenomicsDB workspace as input. Note: 1565827 and 1565828 are wild-type in all samples, but are included b/c of --force-output. It reports N as REF:; #CHROM	POS	ID	REF	ALT; 1	1565827	.	N	.; 1	1565828	.	N	.; 1	1565829	.	T	TGATGGTGGC. This is from a similar command with a gVCF as input. The REF base is correct. Note, the ALT is different b/c of different samples in the inputs:; #CHROM	POS	ID	REF	ALT; 1	1565827	.	G	.; 1	1565828	.	A	.; 1	1565829	.	T	. But the key behavior difference is that if this is a site that would only get output b/c of --force-output, then when a gVCF is the source it gets the REF right, and when GenomicsDB is the source it reports N. I am happy to do some legwork digging into the code and proposing a fix, but it would be helpful to know if there's a known issue around this, and/or get any pointers into where in the GATK/HTSJDK layer I might start looking. An example command is something like:. ```; java8 -jar <JAR> GenotypeGVCFs \; 	-R $REF \; 	-O $OUTPUT \; 	--variant $GENOMICS_DB_WORKSPACE \; 	-L 1:1565827-1565829 \; 	-L 1:1699262-1699298 \; 	-L 6:14972856-14972872 \; 	-L X:135349386-135349395 \; 	-L Y:3491100-3491102 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations \; 	--force-output-intervals <BED_FILE>. ```. In this example, those -L positions were selected b/c they elicit the behavior. Those sites are present in the BED file used for --force-output-intervals. At least in our hands, the combination of calling from a GenomicsDB workspace and using --force-output-intervals to output some site that would not normally get output will result in the 'N' REFs.; Thanks,; Ben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7005:1900,optimiz,optimizations,1900,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7005,1,['optimiz'],['optimizations']
Performance, at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24); at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96); at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:38); at org.gradle.inter,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:9165,Cache,CacheStep,9165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Cache'],['CacheStep']
Performance," cases where the haplotype has multiple SNPs and the phasing is poor, this could artificially inflate the likelihoods. Although we've seen that the graph traversal frequently breaks phasing then generating haplotypes anyway, so maybe I overestimate our current likelihood accuracy. Anyway, take my advice with a grain of salt. It's just some musings from a bored and somewhat sleep-deprived mom with a sleeping baby on her lap. ---. @davidbenjamin commented on [Fri Mar 03 2017](https://github.com/broadinstitute/gatk-protected/issues/909#issuecomment-284024760). > I think the main blocker in implementing it would be the complexity of the existing code, though I don't know how much you cleaned up the GATK4 version. We refactored all the engine stuff shared with `HaplotypeCaller` to be very distinct from the somatic genotyping logic, so the only complexity would be in local assembly and PairHMM. Which could be significant, of course. > We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. . .Although we've seen that the graph traversal frequently breaks phasing. The specific case I had in mind is when you have a bubble or something more complex in the graph, followed by a stretch of reference (i.e. all haplotypes have nothing going on here), followed (or not) by more activity. It seems reasonable in that case to chop each active area into its own haplotype(s), which is equivalent to pinning the ref-only area to be ref-only in PairHMM. I believe but could be wrong that in a case like this our assembly would not respect phasing between the two active areas anyway, so we lose nothing. By the way, I should clarify that the idea is not to truncate the `ActiveRegion`, but rather to break it into a few small haplotypes semi-intelligently *after* building the whole deBruijn graph. It could well be that my optimism is ill-founded. Nonetheless, having a quick-and-dirty mode would be very useful for the following purpose",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2945:1820,optimiz,optimization,1820,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945,1,['optimiz'],['optimization']
Performance, chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotyp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:15701,cache,cache,15701,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance," class(es). All Spark tools that takes parameter `-L`. ### Affected version(s); - [x] Latest public release version [4.0.4.0]; - [x] Latest master branch as of [2018-06-30]. ### Description . When running a Spark tool and passing in interval arguments via the standard `-L` argument, if the interval file (only BED file is tested) is stored in HDFS, we see errors like below. ```; org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: Badly formed genome unclippedLoc: Query interval ""hdfs://shuang-g94794-chmi-chmi3-wgs1-cram-bam-feature-m:8020/data/merged_commonFPDel.bed"" is not valid for this input.; 	at org.broadinstitute.hellbender.utils.GenomeLocParser.getUnambiguousInterval(GenomeLocParser.java:350); 	at org.broadinstitute.hellbender.utils.GenomeLocParser.parseGenomeLoc(GenomeLocParser.java:309); 	at org.broadinstitute.hellbender.utils.IntervalUtils.parseIntervalArguments(IntervalUtils.java:300); 	at org.broadinstitute.hellbender.utils.IntervalUtils.loadIntervals(IntervalUtils.java:226); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.parseIntervals(IntervalArgumentCollection.java:174); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getTraversalParameters(IntervalArgumentCollection.java:155); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getIntervals(IntervalArgumentCollection.java:111); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeIntervals(GATKSparkTool.java:514); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4852:1024,load,loadIntervals,1024,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852,1,['load'],['loadIntervals']
Performance," compare the result of MarkDuplicates and MarkDuplicatesSpark.; the same input SAM file and the default parameter, the MarkDuplicatesSpark have more data marked as duplicated.; Can you give me any suggest how to debug it, why the Spark version have more data marked?. READ_PAIR_DUPLICATES; **11933661 (MarkDuplicates); 11974162 (MarkDuplicatesSpark)**. Here is the metric file; ```. MarkDuplicatesSpark --output hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates.bam --metrics-file hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates-metrics.txt --input hdfs://wolfpass-aep:9000/user/test/spark_412.bowtie2.bam --spark-master yarn --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --read-validation-stringency SILENT --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:759,optimiz,optimized,759,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905,1,['optimiz'],['optimized']
Performance, configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirector,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:1733,cache,cache,1733,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['cache']
Performance," do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.389 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils5391341743604217103.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.389 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.389 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; > 19:43:36.389 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.390 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils348417925139",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6012:1926,multi-thread,multi-threaded,1926,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012,1,['multi-thread'],['multi-threaded']
Performance," for any reference. This allows us (i) to call on ALT-aware mappings if data is such and (ii) call on SNPs and indels generated by putative structural variants that go _across contigs_. I know that this filter is active in the GATK4.beta.3-Mutect2 (see last line):; ```; WMCF9-CB5:align shlee$ gatk-launch Mutect2 -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -I hcc1143_N_subset500.bam -tumor HCC1143_normal -O 1_normalforpon.vcf.gz; Using GATK jar /Applications/genomicstools/gatk/gatk-4.latest/gatk-package-4.beta.3-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /Applications/genomicstools/gatk/gatk-4.latest/gatk-package-4.beta.3-local.jar Mutect2 -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -I hcc1143_N_subset500.bam -tumor HCC1143_normal -O 1_normalforpon.vcf.gz; 19:26:43.105 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Applications/genomicstools/gatk/gatk-4.latest/gatk-package-4.beta.3-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; [August 24, 2017 7:26:43 PM EDT] Mutect2 --tumorSampleName HCC1143_normal --output 1_normalforpon.vcf.gz --input hcc1143_N_subset500.bam --reference /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta --genotypePonSites false --af_of_alleles_not_in_resource 0.001 --log_somatic_prior -6.0 --tumor_lod_to_emit 3.0 --initial_tumor_lod 2.0 --max_population_af 0.01 --normal_lod 2.2 --annotation Coverage --annotation DepthPerAlleleBySample --annotation TandemRepeat --annotation OxoGReadCounts --annotation ClippedBases --annotation ReadPosition --annotation BaseQuality --annotation MappingQuality --annotation FragmentLength --annotation StrandArtifact --dontTrimActiveRegions false --maxDiscARExtension 25 --maxGGAARExtension 300 --paddingAroundIndels 150 --paddingAroundSNPs 20 --kmerSize 10 --kmerSize 25 --dontI",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3514:1770,Load,Loading,1770,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3514,1,['Load'],['Loading']
Performance," from TumorNormalizeSomaticReadCounts contained NaNs; 2. TumorPerformSeg threw the following error:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; [February 16, 2017 3:23:02 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation --tangentNormalized /cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output small_NA12878.seg --log2Input true --alpha 0.01 --nperm 10000 --pmethod hybrid --minWidth 2 --kmax 25 --nmin 200 --eta 0.05 --trim 0.025 --undoSplits none --undoPrune 0.05 --undoSD 3 --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [February 16, 2017 3:23:02 PM UTC] Executing as root@3addd2d7b373 on Linux 3.16.0-0.bpo.4-amd64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; [February 16, 2017 3:23:04 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=185597952; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/cromwell_root/tmp/root/Rlib.5210694187065743072';source('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2944:1326,Perform,PerformSegmentation,1326,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944,1,['Perform'],['PerformSegmentation']
Performance," go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I think we should start; > thinking of ""Best Practices Recommendations"" less as ""here is the best set; > of parameters to use with your data"" and more as ""here is *how to find*; > the best set of parameters to use with your data (for a given truth set,; > sensitivity requirement, etc.)"". After all, if we are putting together; > pipelines to do hyperparameter optimization, there is no reason not to; > share them with the community.; >; > This would also relax the requirement that the defaults in the WDL (which; > have to be kept in sync with those in the GATK jar) represent some sort of; > Best Practices Recommendation, which is awkward in exactly scenarios like; > the one you highlight.; >; > @vdauwera <https://github.com/vdauwera> @LeeTL1220; > <https://github.com/LeeTL1220> @sooheelee <https://github.com/sooheelee>; > might have some thoughts.; >; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2h5MhZ7nXrNgo6MrFpMD-TGiAE8ks5tt8gjgaJpZM4TtVZZ>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:2359,optimiz,optimization,2359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379,1,['optimiz'],['optimization']
Performance," implemented in sl_purity_ploidy_mcmc branch. Could stand some refactoring and code cleanup before it is PR ready and needs tests.; - [x] Algorithm improvements; - Currently, the model is initialized assuming a 50-50 normal-tumor split and only a clonal population. This is run for ~100 MCMC iterations, and the result is used to initialize a second run that expands the number of populations. This tends to work reasonably well, but there are situations where the model can get stuck in incorrect, degenerate solutions. Going to try adding some MH steps that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2909:1618,Queue,Queue,1618,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909,1,['Queue'],['Queue']
Performance," in stage 0.0 (TID 0, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 1]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 2]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on xx.xx.xx.xx, executor 0: java.lang.IllegalStat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:13807,concurren,concurrent,13807,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['concurren'],['concurrent']
Performance," in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/23 20:42:02 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 11.814317 s; 18/04/23 20:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/23 20:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/23 20:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/23 20:42:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/23 20:42:03 INFO MemoryStore: MemoryStore cleared; 18/04/23 20:42:03 INFO BlockManager: BlockManager stopped; 18/04/23 20:42:03 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/23 20:42:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/23 20:42:03 INFO SparkContext: Successfully stopped SparkContext; 20:42:03.045 INFO PathSeqPipelineS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:16330,concurren,concurrent,16330,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['concurren'],['concurrent']
Performance," in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:18669,concurren,concurrent,18669,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['concurren'],['concurrent']
Performance," in the sampling of denoised copy ratios, fixes a memory leak by updating theano, and also adds some theano flags that typically yield a factor of ~2 speedup (notably, the OpenMP elemwise flag, although we also get a slight boost from using numpy MKL). This allows us to run, e.g.: . 2 shards of 50 samples by 100000 intervals on n1-standard-8s (8 CPU, 30GB memory, $0.08 / hr) each taking ~5 hours = ~1.6 cents / sample; 4 shards of 50 samples by 50000 intervals on n1-highmem-4s (4 CPU, 26GB memory, $0.05 / hr) each taking ~3.25 hours = ~1.3 cents / sample; 45 shards of 50 samples by 5000 intervals on *n1-standard-1s* (1CPU, 3.75GB memory, $0.01 / hr) each taking ~0.5 hours = ~0.5 cents / sample. For these runs, we used a slightly larger interval list and 1/4 the number of samples than in the first example, but because everything scales linearly, it's probably fair to compare the per-sample-and-interval costs. So we get a factor of ~8 savings if we keep the shard size the same. The cost was already satisfactory, but fixing the leak allows us to more easily run scatters that are not so wide, which may be crucial for running the megaWDL. Adding the OpenMP flag also lets CPU scalability work as intended. We can do a more systematic optimization for cost if desired, and we should also revalidate to make sure performance doesn't vary too much with shard size (from spot checking, it looks like marginal and/or single-bin calls may flicker on and off). Note that we have still not optimized inference for WES, although I believe @vruano has done some optimizations for WGS. @mwalker174 @vruano for WGS with 2kb bins, I would expect the cost of the gCNV step to be ~10 cents in cohort mode before inference optimizations, assuming we address #5716 to minimize disk costs. @asmirnov239 can you review? And maybe you can address dCR output in PostprocessGermlineCNVCalls and expose the number of samples in a separate PR? We can make some further changes to the dCR format there if we need.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5781#issuecomment-471570697:1475,scalab,scalability,1475,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5781#issuecomment-471570697,6,"['optimiz', 'perform', 'scalab']","['optimization', 'optimizations', 'optimized', 'performance', 'scalability']"
Performance," increased dictionary size, because they are more repetitive than the DNA data. And shorter BAMs would be different because they are less repetitive (usually less coverage), so their compression relies more on CPU-expensive crunching of the ""2bit nature"" of the DNA.; So they might logically suffer more from a lower compression level.; It might be instructive to compare compression sizes of raw sequence data of two BAM files with output of faToTwoBit / 2.bit files.; 2bit files are compressed by a factor of around four, which gzip often does not reach (because it doesn't know ahead of time that DNA has only four letters).; Use reference genome fasta as proxy for nearly no repetition at all. It doesn't compress much beyond 2bit. Tweaking of the Huffmann coding etc. might have influenced the compression level much in this case, by ""giving the compressor a subtle hint about the four letters"".; Paradoxically, Intel might have optimized for average data and thus brought a disadvantage for the four letter nature of DNA (and also the few letters used in quality data encoding compared to text). 3. BQSR:; When I did interleaving compression experiments, I noticed that the BQSR step decreases compressiblity considerably.; In this example I had the same BAM file in different versions that were aligned to hs38DH, hs38, hs37d5 and could compress them to nearly the size of one, by putting similar pieces of the files after one another.; Adding the same BAM with BQSR increased final file size more than several pre-BQSR versions together.; Note: This piece-meal packing might be useful for different BAMs mostly only with many BAMs where similar regions accumulate. 4. Even faster:; In my experience, level 0 (no compression) (with samtools view -u) increases speed even more, if files are on a lz4 encrypted disk (such as with ZFS).; The speed-up of lz4 over even level 1 of any gzip-like compression is substantial.; With data on SSDs or similarly fast storage, that can make a huge differenc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-360179673:2700,optimiz,optimized,2700,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-360179673,1,['optimiz'],['optimized']
Performance," is specific to human data. Funcotator could be useful for users with non-human data if there is a workaround for these errors. ### GATK Information; GATK 4.1.9.0; gatk IndexFeatureFile -I gencode.vM25.annotation.gtf; This request was created from a contribution made by T. Li on January 25, 2021 04:37 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360076815852-Error-Running-IndexFeatureFile-on-Ensembl-Mouse-GTF-file-](https://gatk.broadinstitute.org/hc/en-us/community/posts/360076815852-Error-Running-IndexFeatureFile-on-Ensembl-Mouse-GTF-file-). #### Error Log. ```; Using GATK jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar IndexFeatureFile -I gencode/mm10/gencode.vM25.annotation.gtf ; ; 04:33:13.081 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jan 25, 2021 4:33:13 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 04:33:13.195 INFO IndexFeatureFile - ------------------------------------------------------------ ; ; 04:33:13.195 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.1.9.0-SNAPSHOT ; ; 04:33:13.195 INFO IndexFeatureFile - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 04:33:13.195 INFO IndexFeatureFile - Executing as root@b4c480938d0d on Linux v5.4.0-1029-aws amd64 ; ; 04:33:13.195 INFO IndexFeatureFile - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_242-8u242-b08-0ubuntu3~18.04-b08 ; ; 04:33:13.195 INFO IndexFeatureFile - Start Date/Time: January 25, 2021 4:33:13 AM ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7054:1133,Load,Loading,1133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7054,1,['Load'],['Loading']
Performance," is that it is a ppc64le system. When I use HaplotypeCaller, I see the following messages on the screen:. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100. 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so. 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so. 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine. INFO: Failed to detect whether we are running on Google Compute Engine. 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------. 16:17:05.843 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1. 16:17:05.843 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/. 16:17:05.843 INFO Haplotyp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794:1182,Load,Loading,1182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794,1,['Load'],['Loading']
Performance, java.io.InputStreamReader.read(InputStreamReader.java:184); 	at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBufferedReader.java:140); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:298); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:354); 	at htsjdk.tribble.readers.SynchronousLineReader.readLine(SynchronousLineReader.java:51); 	at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:24); 	at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:11); 	at htsjdk.samtools.util.AbstractIterator.hasNext(AbstractIterator.java:44); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:89); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:36); 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:100); 	... 12 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 41 more; Caused by: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.jav,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:4625,concurren,concurrent,4625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance," localhost (executor driver) (2/3); 21/04/13 07:32:25 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:9320,concurren,concurrent,9320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['concurren'],['concurrent']
Performance," long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKSparkTool that would run `modifyProviders` once on each executor which may be a bit of a trick on it's own. . If we decided to do this it would make sense to make `modifyProviders` use the same synchronization conditions as the actual `FileSystemProvider` loading, in order to not have any race condition",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2312:2185,load,loadInstalledProviders,2185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312,1,['load'],['loadInstalledProviders']
Performance," max retries/reopens: 20; 09:01:25.951 INFO HaplotypeCaller - Requester pays: disabled; 09:01:25.952 INFO HaplotypeCaller - Initializing engine; 09:01:26.059 INFO HaplotypeCaller - Done initializing engine; 09:01:26.060 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 09:01:26.067 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output; 09:01:26.067 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 09:01:26.077 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:01:26.078 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 09:01:26.089 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 09:01:26.089 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 09:01:26.090 INFO IntelPairHmm - Available threads: 36; 09:01:26.090 INFO IntelPairHmm - Requested threads: 4; 09:01:26.090 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 09:01:26.121 INFO ProgressMeter - Starting traversal; 09:01:26.121 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:01:26.406 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position 1A:145 and possibly subsequent; at least 10 samples must have called genotypes; 09:01:33.373 WARN DepthPerSampleHC - Annotation will not be calculated at position 1A:1702502 and possibly subsequent; genotype for sample",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8192:3658,Load,Loading,3658,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192,1,['Load'],['Loading']
Performance," may occur.; 06:42:41.665 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; 06:42:41.666 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; 06:42:41.691 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.pc_transcripts.fa -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.pc_transcripts.fa; 06:42:46.805 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/clinvar_20180429_hg38.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:46.805 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar_VCF : 100000; 06:42:46.807 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:46.951 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/clinvar_20180429_hg38.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:47.023 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:47.098 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg_lof.tsv -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/acmg_lof/hg38/acmg_lof.tsv; 06:42:47.107 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg59_test_cleaned.txt -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7090:7685,cache,cache,7685,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090,1,['cache'],['cache']
Performance, more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:15765,cache,cache,15765,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['cache']
Performance," normals with 5M bins each, CombineReadCounts took ~1 min, CreatePanelOfNormals (with no QC) took ~4.5 minutes (although ~1 minute of this is writing target weights, which I haven't added to the new version yet) and generated a 2.7GB PoN, and NormalizeSomaticReadCounts took ~8 minutes (~7.5 minutes of which was spent composing/writing results, thanks to overhead from ReadCountCollection). In comparison, the new CreateReadCountPanelOfNormals took ~1 minute (which includes combining read-count files, which takes ~30s of I/O) and generated a 520MB PoN, and DenoiseReadCounts took ~30s (~10s of which was composing/writing results, as we are still forced to generate two ReadCountCollections). Resulting PTN and TN copy ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actuall",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1105,perform,performed,1105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['perform'],['performed']
Performance, org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3909:4489,concurren,concurrent,4489,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3909,2,['concurren'],['concurrent']
Performance, org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:15653,cache,cache,15653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['cache']
Performance," phasing, which is supported only for reference-model confidence output; 14:50:19.280 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.481 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.776 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:50:19.795 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:50:19.847 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:50:19.848 INFO IntelPairHmm - Available threads: 48; 14:50:19.848 INFO IntelPairHmm - Requested threads: 4; 14:50:19.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:50:19.926 INFO ProgressMeter - Starting traversal; 14:50:19.926 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:50:30.309 INFO ProgressMeter - chr17:740224 0.2 3010 17395.5; 14:50:41.016 INFO ProgressMeter - chr17:1675683 0.4 7020 19973.4; 14:50:51.041 INFO ProgressMeter - chr17:2415218 0.5 10100 19477.4; 14:51:01.041 INFO ProgressMeter - chr17:3591332 0.7 14920 21773.6; 14:51:11.059 INFO ProgressMeter - chr17:4574538 0.9 19100 22412.6; 14:51:21.089 INFO ProgressMeter - chr17:5381890 1.0 22460 22033.3; 14:51:31.097 INFO ProgressMeter - chr17:6474462 1.2 27070 22821.4; 14:51:41.535 INFO ProgressMeter - chr17:7455949 1.4 31150 22902.4; 14:51:51.542 INFO ProgressMeter - chr17:8073825 1.5 33820 22149.2; 14:52:01.549 INFO ProgressMeter - chr17:9138632 1.7 38220 22566.0; 14:52:11.962 INFO ProgressMeter - chr17:10514361 1.9 43840 23478.4; 14:52:21.975 INFO ProgressMeter - chr17:11679575 2.0 48560 23872.6; 1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678:7233,multi-thread,multi-threaded,7233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678,1,['multi-thread'],['multi-threaded']
Performance," physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; >; > Since the calculation takes quite long, I checked the WARN messages of the; > output above. Especially the last one about the AVX instruction set where; > it says that a *MUCH* slower implementation will be used. From the few; > WARN messages it seems like the root cause is the failure to load libgkl; > and that again seems to be related to my platform. Does anyone know more; > about this issue or how to work around it?; >; > Best ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:5872,load,load,5872,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance," reads (with reads below 10 bases in length being removed) have their base qualities farther modified in `PairHMMLikelihoodCalculationEngine.createQualityModifiedRead()` in various ways. This modification does not stick however since the base qualities are all modified on a clean partial copy of the read.; 4. Following this the reads (the ones from step 2) are realigned to the reference according to their best haplotypes. Sometimes this means as few as 11 bases of ""read"" are being realigned at this stage. . It is these realigned reads that are used for genotyping, where the only reads that are actually used to contribute likelihoods for calls are reads that overlap the variant event within 2 bases of overlap on either side. In DRAGEN they do something different that we had to replicate to achieve concordance. Dragen still performs equivalent modifications for steps 1-3 as they apply to the reads but rather than performing step 4 and using those reads for genotype assignment it instead for genotyping reaches back for each read (that has survived filtering) to its original BAM alignment (before being unclipped/hardclipped) and uses those reads for FRD/BQD calling. When running GATK with the new argument `--use-original-alignments-for-genotyping-overlap` this is what happens as well (step 4 is skipped entirely in addition). The results were somewhat surprising (listed below): . ![RealignmentPlotIndels](https://user-images.githubusercontent.com/16102845/87588690-13fc4680-c6b2-11ea-98e9-4c69259c2869.png); ![RealignmentPlotSNPs](https://user-images.githubusercontent.com/16102845/87588692-1494dd00-c6b2-11ea-96dc-ba06f45357c2.png). This says that running GATK in DRAGEN mode without realigning reads performs slightly better for low complexity region SNPs than it does with realignment. This could perhaps be a side effect of the BQD algorithm as it cares about the specific bases that are applied for SNPs. I have theorized that perhaps the explanation for this behavior has to d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6706:1887,perform,performs,1887,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6706,2,['perform'],"['performing', 'performs']"
Performance," remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoise",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:2269,perform,performed,2269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['perform'],['performed']
Performance," reports that this error still occurs even after the patch in https://github.com/broadinstitute/gatk/pull/5099. With that patch, we are now retrying on `UnknownHostException`, but the retries are all failing: . ```; [August 14, 2018 7:09:18 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 896.64 minutes.; Runtime.totalMemory()=3966238720; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(Geno",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:1019,concurren,concurrent,1019,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance," sc.setLogLevel(newLevel).; Failed to created SparkJLineReader: java.io.IOException: Permission denied; Falling back to SimpleReader.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 1.6.0; /_/. Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_91); Type in expressions to have them evaluated.; Type :help for more information.; Spark context available as sc (master = yarn-client, app id = application_1507683879816_0007).; Wed Oct 11 14:25:24 CST 2017 Thread[main,5,main] java.io.FileNotFoundException: derby.log (Permission denied); ----------------------------------------------------------------; Wed Oct 11 14:25:24 CST 2017:; Booting Derby version The Apache Software Foundation - Apache Derby - 10.11.1.1 - (1616546): instance a816c00e-015f-0a1b-f1bd-00002ce33928 ; on database directory /tmp/spark-98953d35-8594-4907-b4a5-0870f1d17b3e/metastore with class loader sun.misc.Launcher$AppClassLoader@5c647e05 ; Loaded from file:/opt/cloudera/parcels/CDH-5.12.1-1.cdh5.12.1.p0.3/jars/derby-10.11.1.1.jar; java.vendor=Oracle Corporation; java.runtime.version=1.8.0_91-b14; user.dir=/opt/Software/gatk; os.name=Linux; os.arch=amd64; os.version=3.10.0-514.el7.x86_64; derby.system.home=null; Database Class Loader started - derby.database.classpath=''; 17/10/11 14:25:33 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.1.0-cdh5.12.1; 17/10/11 14:25:33 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException; SQL context available as sqlContext. **./gradlew bundle**; **[root@com1 gatk]# ./gradlew bundle; when I executed the command ./gradlew bundle it appeared the error in the last did this matter**. .......; [loading ZipFileIndexFileObject[/root/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.6.5/d50be1723a09be903887099ff2014ea9020",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240:2049,Load,Loaded,2049,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240,1,['Load'],['Loaded']
Performance, scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/08/29 10:20:49 ERROR Executor: Exception in task 12.0 in stage 12.0 (TID 3228); ```. I am running version 4.0.8.1 of GATK using openjdk version 1.8.0_212.; The command I am using is:. ```; gatk StructuralVariationDiscoveryPipelineSpark \; --aligner-index-image refrance.fasta.img \; --contig-sam-file contigs-aligned.sam \; --spark-master local[30] \; --kmers-to-ignore kmers_to_ignore.txt \; -R $fasta \; -I $sample.bam \; -O $sample.vcf; ```. Thanks for taking a look!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5145:2527,concurren,concurrent,2527,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5145,2,['concurren'],['concurrent']
Performance," scheduler.TaskSetManager: Lost task 0.1 in stage 1.0 (TID 2, com2, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_1507683879816_0006_01_000002 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000002; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:28 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.; 17/10/11 14:19:28 INFO storage.BlockManagerMaster: Removal of executor 1 requested; 17/10/11 14:19:28 INFO cluster.YarnClientSchedulerBackend: Asked to remove non-existent executor 1; 17/10/11 14:19:28 INFO spark.ExecutorAllocationManager: Existing executor 1 has been removed (new total is 0); 17/10/11 14:19:35 INFO cluster.YarnClientSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (com2:35590) with ID 2; 17/10/11 14:19:35 INFO scheduler.TaskSetManager: Starting task 0.2 in stage 1.0 (TID 3, com2, executor 2, partition 0, NODE_LOCAL, 1990 bytes); 17/10/11 14:19:35",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:21092,concurren,concurrent,21092,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['concurren'],['concurrent']
Performance," scheduler.TaskSetManager: Lost task 0.3 in stage 1.0 (TID 4, com2, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:38 ERROR scheduler.TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job; 17/10/11 14:19:38 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 17/10/11 14:19:38 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.; 17/10/11 14:19:38 INFO storage.BlockManagerMaster: Removal of executor 2 requested; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Asked to remove non-existent executor 2; 17/10/11 14:19:38 INFO cluster.YarnScheduler: Cancelling stage 1; 17/10/11 14:19:38 INFO scheduler.DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:203) failed in 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:28105,concurren,concurrent,28105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['concurren'],['concurrent']
Performance, shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.i,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:5992,concurren,concurrent,5992,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance," should definitely provide defaults for typical data types in *documentation*.) And in the end, I think it is beneficial for users that wish to tweak knobs to do some work to understand what those knobs actually do (even if just at a basic level). The other downside of option 2 is that it might not be immediately obvious from the command line what parameters are being used. For example, if a user chooses a set of defaults but then overrides some of them, we should make it so they don't have to go digging through the logs to see what parameters are actually used in the end. Nor should they have to go back and check what the defaults were for whatever version of the jar they were using at the time. Option 2 might also make it easier to inadvertently override parameters, etc. via command-line typos or copy-and-paste errors---it's much more straightforward to require and check that every parameter is specified once and fallback to a default if not, as we do now. Not to say that we couldn't get around any of these issues in Barclay, but I think it'll require some thought and careful design. Would be interested to hear Engine team's opinions. Finally, one point that I think will become more relevant as our tools and pipelines become more flexible and parameterized: I think we should start thinking of ""Best Practices Recommendations"" less as ""here is the best set of parameters to use with your data"" and more as ""here is *how to find* the best set of parameters to use with your data (for a given truth set, sensitivity requirement, etc.)"". After all, if we are putting together pipelines to do hyperparameter optimization, there is no reason not to share them with the community. This would also relax the requirement that the defaults in the WDL (which have to be kept in sync with those in the GATK jar) represent some sort of Best Practices Recommendation, which is awkward in exactly scenarios like the one you highlight. @vdauwera @LeeTL1220 @sooheelee might have some thoughts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289:1977,optimiz,optimization,1977,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289,1,['optimiz'],['optimization']
Performance," should proceed here, if at all? @ldgauthier reminded me that this story was unfinished and is getting a little stale. @fleharty take note if we want to report progress on this front to our MalariaGEN collaborators. On my end, there are a couple of things to do:; - [x] rebase and resolve conflicts; - [x] change TSV input as discussed above; - [x] add doc strings for new arguments; - [x] add integration tests to make absolutely sure exposure was done correctly, perhaps? I'm open to discussion about how this should be done. Complete coverage here will be difficult and perhaps not worth the effort, but I can probably put in a few tests that make sure changing the hard-coded values in master and doing the same via the exposed parameters in this branch have the same effect on a few existing test cases. However, while I'm doing the last three, I wonder if we could run whatever canonical evaluations/optimizations we have to see whether it's worth consolidating some of the parameter sets at this stage? I think there's an argument for having at least two sets (haplotype-to-reference + read-to-haplotype), but I'm not sure how to justify having a separate set for dangling heads/tails. But also not sure which set the latter should be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/present",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:934,optimiz,optimizations,934,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,1,['optimiz'],['optimizations']
Performance," spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 19:01:43.728 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.1.0-10-g554a0e8-SNAPSHOT; 19:01:43.729 INFO HaplotypeCallerSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:01:43.729 INFO HaplotypeCallerSpark - Executing as hadoop@ip-xx.xx.xx.xx on Linux v4.9.85-38.58.amzn1.x86_64 amd64; 19:01:43.729 INFO HaplotypeCallerSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-b10; 19:01:43.729 INFO HaplotypeCallerSpark - Start Date/Time: April 8, 2019 7:01:43 PM UTC; 19:01:43.729 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.730 INFO Haplotyp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869:1896,Load,Loading,1896,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869,1,['Load'],['Loading']
Performance," src/main/java/org/broadinstitute/hellbender/tools/walkers/groundtruth/GroundTruthScorer.java:68: error: unmappable character (0x80) for encoding US-ASCII; * <li>Score : A flow-based alignment score. Since the alignment is per-flow, in the case that there???s a cycle skip, the read and reference flow signals will not be aligned, and therefore the score will be inaccurate.</li>; ^; src/main/java/org/broadinstitute/hellbender/tools/walkers/groundtruth/GroundTruthScorer.java:68: error: unmappable character (0x99) for encoding US-ASCII; * <li>Score : A flow-based alignment score. Since the alignment is per-flow, in the case that there???s a cycle skip, the read and reference flow signals will not be aligned, and therefore the score will be inaccurate.</li>; ^; ```. This test is skipped without any apparent reason:; ```; Running Test: Test method loadIndex(org.broadinstitute.hellbender.BwaMemIntegrationTest). Gradle suite > Gradle test > org.broadinstitute.hellbender.BwaMemIntegrationTest > loadIndex FAILED; java.lang.UnsatisfiedLinkError: 'boolean org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createReferenceIndex(java.lang.String, java.lang.String, java.lang.String)'; at org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createReferenceIndex(Native Method); at org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createIndexImageFromFastaFile(BwaMemIndex.java:227); at org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createIndexImageFromFastaFile(BwaMemIndex.java:196); at org.broadinstitute.hellbender.BwaMemIntegrationTest.loadIndex(BwaMemIntegrationTest.java:49); Running Test: Test method testChimericUnpairedMapping(org.broadinstitute.hellbender.BwaMemIntegrationTest). Gradle suite > Gradle test > org.broadinstitute.hellbender.BwaMemIntegrationTest > testChimericUnpairedMapping SKIPPED; Running Test: Test method testPerfectUnpairedMapping(org.broadinstitute.hellbender.BwaMemIntegrationTest). Gradle suite > Gradle test > org.broadinstitute.hellbender.BwaMemIntegra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8940:2137,load,loadIndex,2137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8940,1,['load'],['loadIndex']
Performance," stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:5870,concurren,concurrent,5870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['concurren'],['concurrent']
Performance," still generating the error which it does not (even on my side). However, my few tests made today resulted in interesting observations. ## Traces. Below the command and trace produced from my real case. I annonymized it but the number of characters in path were kept. ```java; (cerc_prod) [16:48 xxxxxxx@yyyyyy:test a]$ gatk MergeVcfs -I data/calling/cerc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; Using GATK jar /master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar MergeVcfs -I data/calling/cerc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; 16:48:58.710 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Mon Jun 22 16:48:58 CDT 2020] MergeVcfs --INPUT data/calling/cerc_prod2.SM_V7_1.vcf.gz --INPUT data/calling/cerc_prod2.SM_V7_ZW.vcf.gz --OUTPUT out.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Jun 22, 2020 4:48:58 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Mon Jun 22 16:48:58 CDT 2020] Executing as xxxxxxx@yyyyyy on Linux 3.10.0-693.11.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_152-release-1056-b12; D",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241:1199,Load,Loading,1199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241,1,['Load'],['Loading']
Performance," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:11392,load,loading,11392,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,12,"['Load', 'Perform', 'load']","['LoadBigQueryData', 'Perform', 'load', 'loading']"
Performance," the last developer who left this in a reasonable state a beverage of their choice.; # (This may be yourself, and you'll appreciate that beverage while you tinker with dependencies!); #; # When changing dependencies or versions in this file, check to see if the ""supportedPythonPackages"" DataProvider; # used by the testGATKPythonEnvironmentPackagePresent test in PythonEnvironmentIntegrationTest needs to be updated; # to reflect the changes.; #; name: gatk; channels:; # if channels other than conda-forge are added and the channel order is changed (note that conda channel_priority is currently set to flexible),; # verify that key dependencies are installed from the correct channel and compiled against MKL; - conda-forge; - defaults; dependencies:. # core python dependencies; - conda-forge::python=3.6.10 # do not update; - pip=20.0.2 # specifying channel may cause a warning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:1632,perform,performance,1632,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,1,['perform'],['performance']
Performance," the log file: ```*** Error in `java: munmap_chunk(): invalid pointer: 0x00007f685d06c840 ***```. The respective backtraces: . ```; *** Error in `java': double free or corruption (out): 0x00007f6364699340 ***; ======= Backtrace: =========; /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f636ba307e5]; /lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f636ba3937a]; /lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f636ba3d53c]; /cromwell_root/tmp.7626fbcf/libgkl_smithwaterman1454827346682980108.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f63123c8fa8]; /cromwell_root/tmp.7626fbcf/libgkl_smithwaterman1454827346682980108.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7f63123c8bf8]; [0x7f6355bff192]; ```. ```; *** Error in `java': munmap_chunk(): invalid pointer: 0x00007f685d06c840 ***; ======= Backtrace: =========; /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f68634c37e5]; /lib/x86_64-linux-gnu/libc.so.6(cfree+0x1a8)[0x7f68634d0698]; /cromwell_root/tmp.4eeeda3c/libgkl_smithwaterman7538158038428947321.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f6830cf2fa8]; /cromwell_root/tmp.4eeeda3c/libgkl_smithwaterman7538158038428947321.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7f6830cf2bf8]; [0x7f684dc31f92]; ```. In each of these occurrences, the filtered vcf file was produced, but the vcf.idx file was missing. Although the java errors occur, the last line of the log denotes the step as a success: (This might be true, but only when the option --create-output-variant-index is set to false.; `SetOperationStatus(copied 0 file(s) to <destinations_folder> succeeded""`. I also performed a test based on machine type. (outside of the full workflow, starting the steps on my own on a separate instance & replicating the steps of the workflow); - Using an instance with 2 vCPU's, 7.5 GB of ram, just ran out of memory.; - Using an instance with 8 vCPU's, 30 GB of ram finished successfully, producing both the filtered vcf & vcf.idx",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-652696262:2481,perform,performed,2481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-652696262,1,['perform'],['performed']
Performance," this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one to set the maximum number of values allowed per chunk, so that heap usage can be controlled, but the downside is that this translates into a corresponding limit on the number of columns (i.e., intervals). On the other hand, you could theoretically crank this number up to Integer.MAX_VALUE, as long as you set -Xmx high enough... In practice, it's very unlikely that we'll need to go to bins smaller than a read length.); - [ ] <s>Check that CreatePanelOfNormals works correctly on Spark cluster.</s> Implement Randomized SVD, which should give better performance on large matrices. See https://arxiv.org/pdf/1007.5510.pdf and https://research.fb.com/fast-randomized-svd/. For now, I'll require that the coverage matrix can fit in RAM, but more sophisticated versions of the algorithm could be implemented in the future.; - [ ] Update methods doc. Note that some of the CNV section is out of date and incorrect. In particular, we have been taking in PCOV as input to CreatePanelOfNormals for some time now, but the doc states that we take integer read counts. This already yields different results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:3383,perform,performance,3383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['perform'],['performance']
Performance," to me that the issue is due to memory management. Then I realized several test, by increasing both Java machine memory and docker job memory (10G) wich seems enough for mutect2 and my job was still failling. I also try to change CPU parameters but nothing change.; We did more test and try to run the same command with 2 others version of gatk (4.1.9.0 and 4.1.0.0). The job failed in 4.1.9.0 with the same log than 4.1.4.0 but the version 4.1.0.0 ran successfully. #### Steps to reproduce; you will find the command below. I'am not aware of the confidentiality about my input. If i can i will send it to you if needed.; `java -Xmx4000m -Xms4000m -XX:ParallelGCThreads=1 -XX:+AggressiveHeap -jar /usr/share/java/gatk-package-4.1.4.1-local.jar Mutect2 --smith-waterman FASTEST_AVAILABLE -I WES-T_S7_chr_1_bqsr.bam -I WGS-C_S12_chr_1_bqsr.bam -normal WGS-C -L 1 -O OUTPUT -R GRCh38.92.fa`; #### Expected behavior; _Tell us what should happen_. ### Description. > 10:29:22.302 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/share/java/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2021 10:29:22 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:29:22.407 INFO Mutect2 - ------------------------------------------------------------; 10:29:22.408 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.4.1; 10:29:22.408 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:29:22.408 INFO Mutect2 - Executing as spim@992fbecc5b50 on Linux v3.10.0-693.el7.x86_64 amd64; 10:29:22.408 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v11.0.6+10-post-Debian-1deb10u1; 10:29:22.408 INFO Mutect2 - Start Date/Time: January 6, 2021 at 10:29:22 AM UTC; 10:29:22.408 INFO Mutect2 - ------------------------------------------------------------; 10:29:22.408 INFO Mutect2 - --------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7032:1419,Load,Loading,1419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7032,1,['Load'],['Loading']
Performance," to messages in stdout. Includes # total records, number of records that were trimmed, # variant records skipped due to ref allele being too long and finally the max-indel-length value that needs to be set to include these in the leftalignandtrim. This is an improvement to previous stdout messaging. Upping max-indel-length; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --max-indel-length 250 -O zeta_snippet_leftalign_250_96branch.vcf.gz; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --max-indel-length 250 -O zeta_snippet_leftalign_250_96branch.vcf.gz; 14:03:44.243 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 06, 2018 2:03:44 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 14:03:44.358 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 14:03:44.358 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-25-g0c6f06f-SNAPSHOT; 14:03:44.359 INFO LeftAlignAndTrimVariants - For support and documentation go ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326:6768,Load,Loading,6768,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326,1,['Load'],['Loading']
Performance," to port this code and hook up to our tools via the standardized output writer creation methods. The VCF header lines look like this:. ```; ##GATKCommandLine.SelectVariants=<ID=SelectVariants,Version=3.4-46-gbc02625,Date=""Wed Aug 19 10:29:53 EDT 2015"",Epoch=1439994593766,CommandLineOptions=""analysis_type=SelectVariants input_file=[] showFullBamList=false read_buffer_size=null phone_home=AWS gatk_key=null tag=NA read_filter=[] disable_read_filter=[] intervals=[20:10000000-10250000, 21:10000000-10250000] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=human_g1k_v37.fasta nonDeterministicRandomSeed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=false never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=variant source=dbsnp_138.b37.vcf) discordance=(RodBind",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2269:1117,perform,performanceLog,1117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2269,1,['perform'],['performanceLog']
Performance," user: hdfs; 17/10/11 14:19:15 INFO yarn.Client: Application report for application_1507683879816_0006 (state: ACCEPTED); 17/10/11 14:19:15 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(null); 17/10/11 14:19:15 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> mg, PROXY_URI_BASES -> http://mg:8088/proxy/application_1507683879816_0006), /proxy/application_1507683879816_0006; 17/10/11 14:19:15 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 17/10/11 14:19:16 INFO yarn.Client: Application report for application_1507683879816_0006 (state: ACCEPTED); 17/10/11 14:19:17 INFO yarn.Client: Application report for application_1507683879816_0006 (state: RUNNING); 17/10/11 14:19:17 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: 10.131.101.159; 	 ApplicationMaster RPC port: 0; 	 queue: root.users.hdfs; 	 start time: 1507702753100; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088/proxy/application_1507683879816_0006/; 	 user: hdfs; 17/10/11 14:19:17 INFO cluster.YarnClientSchedulerBackend: Application application_1507683879816_0006 has started running.; 17/10/11 14:19:17 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34044.; 17/10/11 14:19:17 INFO netty.NettyBlockTransferService: Server created on 34044; 17/10/11 14:19:17 INFO storage.BlockManager: external shuffle service port = 7337; 17/10/11 14:19:17 INFO storage.BlockManagerMaster: Trying to register BlockManager; 17/10/11 14:19:17 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.131.101.159:34044 with 530.0 MB RAM, BlockManagerId(driver, 10.131.101.159, 34044); 17/10/11 14:19:17 INFO storage.BlockManagerMaster: Registered BlockManager; 17/10/11 14:19:17 INFO scheduler.EventLoggingListener: Logging ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:8063,queue,queue,8063,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['queue'],['queue']
Performance," via `-L PATH_TO_BED_FILE`, error is reported; * Scenario 2: run with the WGS bam and give intervals via `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 3: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L PATH_TO_BED_FILE`, no error; * Scenario 4: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 5: download the shrunken bam to local machine and run `PrintReadsSpark` with `-L PATH_TO_BED_FILE`, no error. Stack trace from scenario 1:; ```; ./gatk PrintReadsSpark \; -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam \; -O hdfs://shuang-small-m:8020/results/temp.bam \; -L hdfs://shuang-small-m:8020/data/intervals.bed \; -- \; --spark-runner GCS \; --cluster shuang-small \; --project broad-dsde-methods. Using GATK jar /Users/shuang/GATK/gatk/build/libs/gatk-spark.jar; found cached jar: gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar. Replacing spark-submit style args with dataproc style args. --cluster shuang-small --project broad-dsde-methods -> --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializer.buffer.max=512m,spark.driver.maxResultSize=0,spark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 ,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2. Running:; gcloud dataproc jobs submit spark --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:1501,cache,cached,1501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['cache'],['cached']
Performance," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:32136,queue,queued,32136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,3,['queue'],['queued']
Performance, | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9QcmltaXRpdmVDYWNoZU5vZGUuamF2YQ==) | `83.333% <71.429%> (+30.702%)` | `10 <7> (+3)` | :arrow_up: |; | [...er/tools/coveragemodel/cachemanager/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9DYWNoZU5vZGUuamF2YQ==) | `80.645% <76.923%> (+30.645%)` | `9 <8> (+4)` | :arrow_up: |; | [...overagemodel/cachemanager/ComputableCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlQ2FjaGVOb2RlLmphdmE=) | `89.189% <80%> (+32.779%)` | `18 <17> (+2)` | :arrow_up: |; | [...ols/coveragemodel/CoverageModelEMComputeBlock.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL0NvdmVyYWdlTW9kZWxFTUNvbXB1dGVCbG9jay5qYXZh) | `77.617% <82.558%> (-1.61%)` | `49 <2> (-1)` | |; | [...dinstitute/hellbender/utils/MathObjectAsserts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9NYXRoT2JqZWN0QXNzZXJ0cy5qYXZh) | `63.636% <84.615%> (+15.249%)` | `9 <3> (+4)` | :arrow_up: |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:3220,cache,cachemanager,3220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,1,['cache'],['cachemanager']
Performance,!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 01:39:03.364 INFO FilterAlignmentArtifacts - Initializing engine; 01:39:07.644 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-Filter/22.hg38-filtered.vcf; 01:39:08.399 INFO FilterAlignmentArtifacts - Done initializing engine; 01:39:09.523 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 01:39:09.565 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 01:39:09.566 INFO IntelPairHmm - Available threads: 4; 01:39:09.566 INFO IntelPairHmm - Requested threads: 4; 01:39:09.566 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 01:39:09.567 INFO ProgressMeter - Starting traversal; 01:39:09.567 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; munmap_chunk(): invalid pointer; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx11500m -jar /root/gatk.jar FilterAlignmentArtifacts -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -V gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-Filter/22.hg38-filtered.vcf -I gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/209d1183-ed9a-4755-a4b3-d595797640ea/PreProcessingForVariantDiscovery_GATK4/9f7c0ab6-b61b-4797-92f1-7929bbf677d8/call-GatherBamFiles/22.hg38.bam --bwa-mem-index-image /cromwe,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:5205,multi-thread,multi-threaded,5205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['multi-thread'],['multi-threaded']
Performance,"!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 20:12:42.725 INFO FilterAlignmentArtifacts - Initializing engine; 20:12:48.403 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-secure-024a1aae-a4f9-4025-aa93-f759f93a8203/50383670-4607-4e59-9bfc-4db970980f0e/Mutect2/773a91ea-25be-4d49-b97c-16527076250c/call-Filter/cacheCopy/TN-20-36-filtered.vcf; 20:12:50.117 INFO FilterAlignmentArtifacts - Done initializing engine; 20:12:51.042 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 20:12:51.099 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 20:12:51.100 INFO IntelPairHmm - Available threads: 14; 20:12:51.100 INFO IntelPairHmm - Requested threads: 4; 20:12:51.100 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 20:12:51.100 INFO ProgressMeter - Starting traversal; 20:12:51.100 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 20:20:25.766 INFO ProgressMeter - chr3:104142090 7.6 1000 132.0; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007efc9818177e, pid=24, tid=0x00007f13b3c76700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # C [libgkl_smithwaterman1809483713436863458.so+0x177e] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x60e; #; # Core dump written. Default location: /cromwell_root/core or core.24; #; # An error report file with more information is saved as:; # /cromwell_root/hs_err_pid24.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.jav",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098:1341,multi-thread,multi-threaded,1341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098,1,['multi-thread'],['multi-threaded']
Performance,!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 11:24:09.944 INFO FilterAlignmentArtifacts - Initializing engine; 11:24:10.534 INFO FeatureManager - Using codec VCFCodec to read file file:///raid/tmp/82/68cd46b704bab21cb8661465e5c2b8/WGS-NA12878.filtered.vcf; 11:24:10.814 INFO FilterAlignmentArtifacts - Done initializing engine; 11:24:10.816 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:24:10.817 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 11:24:10.818 INFO IntelSmithWaterman - Using CPU-supported AVX-512 instructions; 11:24:10.818 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 11:24:10.957 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:24:10.980 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 11:24:10.980 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:24:10.981 INFO IntelPairHmm - Available threads: 80; 11:24:10.981 INFO IntelPairHmm - Requested threads: 4; 11:24:10.981 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:24:10.981 INFO ProgressMeter - Starting traversal; 11:24:10.981 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 11:25:26.222 INFO ProgressMeter - chr1:32527418 1.3 1000 797.5; 11:26:14.235 INFO ProgressMeter - chr1:103944651 2.1 2000 973.6; 11:26:59.367 INFO ProgressMeter - chr1:121884881 2.8 3000 1069.0; 11:28:22.595 INFO ProgressMeter - chr1:124412677 4.2 4000 953.8; 11:30:27.936 INFO ProgressMeter - chr1:146326436 6.3 5000 795.9; 11:31:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8221:3967,Load,Loading,3967,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8221,1,['Load'],['Loading']
Performance,"![throughput vs core count](https://cloud.githubusercontent.com/assets/10458949/11080726/940ccfd2-87cb-11e5-99b8-a1ebc4bb6949.png). Thanks to @droazen for doing those measurements. I've taken the input and plotted the per-core throughput numbers above. What we expect to see is a drop after the non-distributed version, and then a steady decline as we add cores. This matches what we're seeing here: the walker throughput is 145 MB/min/core, and then we quickly drop to 50, 32, then finally 15 MB/min/core. This matches well with my earlier informal measurement on the cloud where I saw 15 MB/min/core on 128 cores. It looks like there is room for improvement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/995#issuecomment-155619576:2,throughput,throughput,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/995#issuecomment-155619576,3,['throughput'],['throughput']
Performance,"![weighted](https://user-images.githubusercontent.com/11076296/97032266-8ab9a300-152f-11eb-8d73-148ff99963be.png). Here is the result of optimizing for sensitivity in the high-confidence, low-compexity region of chr22 in CHM, allowing haplotype-to-reference and read-to-haplotype (match, mismatch, gap open) to range over ([1, 20], [-20, -1], [-20, -1]) and fixing gap extend penalties to -1. The optimal (match, mismatch, gap open) parameters found in this run appear to be:. haplotype-to-reference: 2, -8, -19; read-to-haplotype: 1, -4, -3. I wouldn't put much stock in interpreting these parameters or their exact values for now, but it does appear that the match values and the haplotype-to-reference gap-open penalty might be saturating the bounds of the search. Plots of the type suggested by @dalessioluca might be more illuminating. Compare with default performance:. ````; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 9.000 4003 4019 494 1036 0.8905 0.7944 0.8397; None 4009 4025 511 1030 0.8873 0.7956 0.8390; ````. That the corresponding curve with a precision/sensitivity endpoint of (0.8873, 0.7956) above isn't at the top of the pack means that we could squeeze out some extra calls by varying the SW parameters. Of course, this doesn't account for negative impact elsewhere. One could imagine writing a loss where this sensitivity is optimized while putting minimum constraints on precision, sensitivity, and/or F1 in the high-confidence, high-complexity regions (the assumption being the truth set is complete in those regions), or some weightings/variations thereof. EDIT: Actually, looks like overall performance in the high-confidence region improves:. ````; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715465692:137,optimiz,optimizing,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715465692,2,"['optimiz', 'perform']","['optimizing', 'performance']"
Performance,"""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 898, in __call__; storage_map=getattr(self.fn, 'storage_map', None)); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/gof/link.py"", line 325, in raise_with_op; reraise(exc_type, exc_value, exc_trace); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/six.py"", line 692, in reraise; raise value.with_traceback(tb); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 884, in __call__; self.fn() if output_subset is None else\; File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 989, in rval; r = p(n, [x[0] for x in i], o); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 978, in p; self, node); File ""theano/scan_module/scan_perform.pyx"", line 215, in theano.scan_module.scan_perform.perform (/home/shlee/.theano/compiledir_Linux-4.13--gcp-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64/scan_perform/mod.cpp:2628); NotImplementedError: We didn't implemented yet the case where scan do 0 iteration; Apply node that caused the error: forall_inplace,cpu,scan_fn}(Elemwise{minimum,no_inplace}.0, InplaceDimShuffle{0,2,1}.0, Subtensor{int64:int64:int64}.0, IncSubtensor{InplaceSet;:int64:}.0, Shape_i{0}.0); Toposort index: 97; Inputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(int64, scalar)]; Inputs shapes: [(), (0, 6, 6), (0, 6), (2, 6), ()]; Inputs strides: [(), (288, 8, 48), (48, 8), (48, 8), ()]; Inputs values: [array(0), array([], shape=(0, 6, 6), dtype=float64), array([], shape=(0, 6), dtype=float64), 'not shown', array(6)]; Outputs clients: [[Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})]]. HINT: Re-running with most Theano optimiza",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4840:11226,perform,perform,11226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840,1,['perform'],['perform']
Performance,"# . ----------------------------------------------------------------------------------------------------------------------------------. the variants.funcotated.maf:. #version 2.4; ##; ## fileformat=VCFv4.2; ## FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ## FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ## FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ## source=Funcotator; ## GATKCommandLine=<ID=Funcotator,CommandLine=""Funcotator --output ./my_data/variants.funcotated.maf --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output-file-format MAF --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --disable-sequence-dictionary-validation true --remove-filtered-variants false --five-prime-flank-size 5000 --three-prime-flank-size 0 --force-b37-to-hg19-reference-contig-conversion false --transcript-selection-mode CANONICAL --lookahead-cache-bp 100000 --min-num-bases-for-segment-funcotation 150 --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false"",Version=""4.2.0.0"",Date=""March 24, 2021 12:11:32 PM GMT"">; ## Funcotator 4.2.0.0 | Date 20211124T121132 | Gencode 34 CANONICAL | Achilles 110303 | CGC full_2012_03-15 | ClinVar 12.03.20 | C",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:19223,cache,cache-bp,19223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,1,['cache'],['cache-bp']
Performance,"# Actual behavior. > (base) [pkus@master1 mutect_test]$ ~/programs/gatk-4.1.8.0/gatk Funcotator --variant filtered_variants/P1.vcf.gz --reference ~/resources/hg38_for_bwa/hs38DH.fa --ref-version hg38 --data-sources-path ~/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s --output filtered_variants/P1.avcf.gz --output-file-format VCF; > Using GATK jar /home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar Funcotator --variant filtered_variants/P1.vcf.gz --reference /home/pkus/resources/hg38_for_bwa/hs38DH.fa --ref-version hg38 --data-sources-path /home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s --output filtered_variants/P1.avcf.gz --output-file-format VCF; > 15:16:39.460 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; > Jul 17, 2020 3:16:39 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; > INFO: Failed to detect whether we are running on Google Compute Engine.; > 15:16:39.785 INFO Funcotator - ------------------------------------------------------------; > 15:16:39.786 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.8.0; > 15:16:39.786 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; > 15:16:39.787 INFO Funcotator - Executing as xxx on Linux v3.10.0-957.5.1.el7.x86_64 amd64; > 15:16:39.787 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_251-b08; > 15:16:39.787 INFO Funcotator - Start Date/Time: July 17, 2020 3:16:39 PM CEST; > 15:16:39.787 INFO Funcotator - ------------------------------------------------------------; > 15",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708:1885,Load,Loading,1885,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708,1,['Load'],['Loading']
Performance,"# Extracting PGEN from GVS. ## The PGEN format; PGEN is a format written for and used by version 2 of [PLINK](https://www.cog-genomics.org/plink/2.0/). ***IT IS VERY IMPORTANT TO NOTE THAT VERSION 2 OF PLINK IS STILL IN ALPHA AND THE PGEN FORMAT IS STILL SUBJECT TO CHANGE.*** The format comprises 3 file types (or actually sometimes 4):; 1. A `.pgen` file. This is a binary file that stores a mapping of samples and sites to genotypes in a very cleverly compressed way that I can't explain super well because it's complicated.; 2. A `.pvar` file. This is essentially a sites-only VCF. It has information for each site referenced in the `.pgen` file. PLINK also provides an option to produce/use a zstd compressed version of the file (`.pvar.zst`), and we have opted to write that for performance purposes.; 3. A `.psam` file. This is a plaintext file that contains a list of samples referenced in the `.pgen` file. It also optionally includes some phenotype data.; 4. Optionally, a `.pgi` file. Typically, a `.pgen` file has an index at the top. Optionally, PLINK supports using a `.pgen` file with an index in a separate `.pgi` file. The PGEN format does not store all of the information that a VCF has. It leaves out a lot of the fields and annotations you can store in a VCF. As a result of this and the clever compression in the `.pgen` file, these files are typically much smaller than equivalent VCFs. For more information on the PGEN file format, see the official spec [here](https://github.com/chrchang/plink-ng/blob/master/pgen_spec/pgen_spec.pdf). ## The code; The code for the PGEN extract can be divided into 3 parts:; 1. The PGEN-JNI, a C++/JNI library that handles writing HTSJDK VariantContext objects to PGEN files,; 2. ExtractCohortToPgen, a GATK tool based on ExtractCohortToVcf that processes VariantContexts and passes them to PGEN-JNI for writing, and; 3. GvsExtractCallsetPgenMerged, a WDL workflow based on GvsExtractCallset that uses ExtractCohortToPgen to write a series of P",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8708:785,perform,performance,785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708,1,['perform'],['performance']
Performance,"# Steps to reproduce; Try to do a PostprocessGermlineCNVCalls with not all the autosomal chromosomes. #### Command. ```/home/tintest/miniconda2/bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/tintest/miniconda2/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar PostprocessGermlineCNVCalls -L Refseq_GrCh38_1-3-9-17-Y-M.bed -R hg38_1-3-9-17-Y-M.fasta --calls-shard-path GermlineCNVCaller/GermlineCNVCaller-calls/ --contig-ploidy-calls DetermineGermlineContigPloidy/DetermineGermlineContigPloidy-calls/ --model-shard-path GermlineCNVCaller/GermlineCNVCaller-model --sample-index 274 --autosomal-ref-copy-number 2 --allosomal-contig Y --output-genotyped-intervals intervals/SAMPLE_274_PostprocessGermlineCNVCalls_interval.vcf --output-genotyped-segments segments/SAMPLE_274_PostprocessGermlineCNVCalls_segments.vcf```. #### Output; ```14:21:35.446 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/tintest/miniconda2/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:21:35.534 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 14:21:35.536 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.0.5.1; 14:21:35.537 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:21:35.538 INFO PostprocessGermlineCNVCalls - Executing as tintest@dahu63 on Linux v4.9.0-6-amd64 amd64; 14:21:35.539 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 14:21:35.541 INFO PostprocessGermlineCNVCalls - Start Date/Time: August 1, 2018 2:21:35 PM CEST; 14:21:35.542 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 14:21:35.543 INFO PostprocessGermlineCNVCalls - ------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053#issuecomment-409558231:1968,Load,Loading,1968,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053#issuecomment-409558231,1,['Load'],['Loading']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=h1) Report; > Merging [#2811](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/56e6baa79b4e56ebee5fb8d2b2288373a4269fa8?src=pr&el=desc) will **increase** coverage by `0.022%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2811 +/- ##; =============================================; + Coverage 79.978% 80% +0.022% ; - Complexity 16726 16795 +69 ; =============================================; Files 1139 1139 ; Lines 60894 61155 +261 ; Branches 9436 9497 +61 ; =============================================; + Hits 48702 48924 +222 ; - Misses 8396 8422 +26 ; - Partials 3796 3809 +13; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree) | Coverage  | Complexity  | |; |---|---|---|---|; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <> ()` | `2 <0> ()` | :arrow_down: |; | [...ender/tools/spark/pipelines/BQSRPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQlFTUlBpcGVsaW5lU3BhcmsuamF2YQ==) | `100% <0%> ()` | `15% <0%> (+7%)` | :arrow_up: |; | [...nstitute/hellbender/utils/help/GATKHelpDoclet.java](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9oZWxwL0dBVEtIZWxwRG9jbGV0LmphdmE=) | `100% <0%> ()` | `9% <0%> (+3%)` | :arrow_up: |; | [...stitute/hellbender/cmdline/CommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGU,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2811#issuecomment-306008892:929,Perform,PerformAlleleFractionSegmentation,929,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2811#issuecomment-306008892,1,['Perform'],['PerformAlleleFractionSegmentation']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=h1) Report; > Merging [#3036](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/84fbda69bf9528059777496a415be8eb6db63e61?src=pr&el=desc) will **increase** coverage by `0.331%`.; > The diff coverage is `88.554%`. ```diff; @@ Coverage Diff @@; ## master #3036 +/- ##; ===============================================; + Coverage 79.973% 80.304% +0.331% ; - Complexity 16727 17771 +1044 ; ===============================================; Files 1139 1152 +13 ; Lines 60902 65165 +4263 ; Branches 9437 10284 +847 ; ===============================================; + Hits 48705 52330 +3625 ; - Misses 8401 8900 +499 ; - Partials 3796 3935 +139; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree) | Coverage  | Complexity  | |; |---|---|---|---|; | [...ome/segmentation/PerformCopyRatioSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUNvcHlSYXRpb1NlZ21lbnRhdGlvbi5qYXZh) | `86.667% <> (+6.667%)` | `4 <0> (+2)` | :arrow_up: |; | [...institute/hellbender/tools/exome/ACNVModeller.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9BQ05WTW9kZWxsZXIuamF2YQ==) | `97.143% <> (-0.079%)` | `17 <0> ()` | |; | [...ellbender/tools/exome/copyratio/CopyRatioData.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9jb3B5cmF0aW8vQ29weVJhdGlvRGF0YS5qYXZh) | `95.349% <0%> (-2.27%)` | `14 <1> (+1)` | |; | [...nder/tools/exome/segmentation/AFCRHiddenState.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3036#issuecomment-306513201:960,Perform,PerformCopyRatioSegmentation,960,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3036#issuecomment-306513201,1,['Perform'],['PerformCopyRatioSegmentation']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=h1) Report; > Merging [#3157](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/eab8761cbdfdaf24a5bf7551172b9f262d26d8cf?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3157 +/- ##; ===========================================; Coverage 80.132% 80.132% ; Complexity 16993 16993 ; ===========================================; Files 1145 1145 ; Lines 61641 61641 ; Branches 9606 9606 ; ===========================================; Hits 49394 49394 ; Misses 8419 8419 ; Partials 3828 3828; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=tree) | Coverage  | Complexity  | |; |---|---|---|---|; | [...te/hellbender/tools/exome/PerformSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QZXJmb3JtU2VnbWVudGF0aW9uLmphdmE=) | `100% <> ()` | `3 <0> ()` | :arrow_down: |; | [...bender/tools/exome/NormalizeSomaticReadCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9Ob3JtYWxpemVTb21hdGljUmVhZENvdW50cy5qYXZh) | `79.167% <> ()` | `6 <0> ()` | :arrow_down: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3157#issuecomment-311469829:887,Perform,PerformSegmentation,887,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3157#issuecomment-311469829,1,['Perform'],['PerformSegmentation']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=h1) Report; > Merging [#3183](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/64eba53c96ea739638d34222f0f2c61c39153a64?src=pr&el=desc) will **increase** coverage by `0.05%`.; > The diff coverage is `89.931%`. ```diff; @@ Coverage Diff @@; ## master #3183 +/- ##; ==============================================; + Coverage 80.415% 80.465% +0.05% ; - Complexity 17294 17368 +74 ; ==============================================; Files 1165 1165 ; Lines 62573 62785 +212 ; Branches 9763 9789 +26 ; ==============================================; + Hits 50318 50520 +202 ; - Misses 8350 8353 +3 ; - Partials 3905 3912 +7; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree) | Coverage  | Complexity  | |; |---|---|---|---|; | [...gemodel/cachemanager/ComputableGraphStructure.java](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlR3JhcGhTdHJ1Y3R1cmUuamF2YQ==) | `100% <> ()` | `63 <0> ()` | :arrow_down: |; | [...nder/cmdline/ExomeStandardArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0V4b21lU3RhbmRhcmRBcmd1bWVudERlZmluaXRpb25zLmphdmE=) | `0% <> ()` | `0 <0> ()` | :arrow_down: |; | [...der/tools/coveragemodel/nd4jutils/Nd4jIOUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL25kNGp1dGlscy9OZDRqSU9VdGlscy5qYXZh) | `81.731% <> ()` | `19 <0> ()` | :arrow_down: |; | [...bender/tools/exome/TargetAnnotationCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3183#issuecomment-314620643:932,cache,cachemanager,932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3183#issuecomment-314620643,1,['cache'],['cachemanager']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=h1) Report; > Merging [#3515](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/a218b6bbf6b3f243bce34a30f2458308319aadf3?src=pr&el=desc) will **increase** coverage by `0.012%`.; > The diff coverage is `84.946%`. ```diff; @@ Coverage Diff @@; ## master #3515 +/- ##; ===============================================; + Coverage 79.905% 79.917% +0.012% ; - Complexity 17918 17945 +27 ; ===============================================; Files 1199 1200 +1 ; Lines 65102 65195 +93 ; Branches 10142 10160 +18 ; ===============================================; + Hits 52020 52102 +82 ; - Misses 9042 9049 +7 ; - Partials 4040 4044 +4; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=tree) | Coverage  | Complexity  | |; |---|---|---|---|; | [...umber/utils/optimization/PersistenceOptimizer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL29wdGltaXphdGlvbi9QZXJzaXN0ZW5jZU9wdGltaXplci5qYXZh) | `84.946% <84.946%> ()` | `27 <27> (?)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `78.571% <0%> (+0.649%)` | `39% <0%> ()` | :arrow_down: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `73.973% <0%> (+2.74%)` | `11% <0%> ()` | :arrow_down: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3515#issuecomment-325021094:944,optimiz,optimization,944,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3515#issuecomment-325021094,1,['optimiz'],['optimization']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=h1) Report; > Merging [#3590](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/58108d0f3f1a760884201a62469105bc55c09a29?src=pr&el=desc) will **increase** coverage by `0.358%`.; > The diff coverage is `93.939%`. ```diff; @@ Coverage Diff @@; ## master #3590 +/- ##; ===============================================; + Coverage 79.736% 80.094% +0.358% ; - Complexity 18148 18799 +651 ; ===============================================; Files 1217 1226 +9 ; Lines 66602 69015 +2413 ; Branches 10429 11073 +644 ; ===============================================; + Hits 53106 55277 +2171 ; - Misses 9289 9415 +126 ; - Partials 4207 4323 +116; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=tree) | Coverage  | Complexity  | |; |---|---|---|---|; | [...umber/utils/optimization/PersistenceOptimizer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL29wdGltaXphdGlvbi9QZXJzaXN0ZW5jZU9wdGltaXplci5qYXZh) | `84.946% <> ()` | `27 <0> ()` | :arrow_down: |; | [...copynumber/utils/segmentation/KernelSegmenter.java](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL3NlZ21lbnRhdGlvbi9LZXJuZWxTZWdtZW50ZXIuamF2YQ==) | `93.939% <93.939%> ()` | `44 <44> (?)` | |; | [.../tools/spark/sv/evidence/QNamesForKmersFinder.java](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9RTmFtZXNGb3JLbWVyc0ZpbmRlci5qYXZh) | `83.333% <0%> (-16.667%)` | `7% <0%> ()` | |; | [...nder/tools/spark/pathseq/PSPathogenTaxonScore.java](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3590#issuecomment-330916077:954,optimiz,optimization,954,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3590#issuecomment-330916077,1,['optimiz'],['optimization']
Performance,"## Bug Report. ### Affected tool(s) or class(es). CNNScoreVariants. ### Affected version(s); - [x] Latest master branch as of [12/05/2021]. Same issue for an older master branch in v4.1.9. ### Description. **Issue with vqsr_cnn package in Conda environment.; AttributeError: module 'keras.backend' has no attribute 'clear_session'**. > Using GATK jar /lustre/home/regmova/tools/gatk/build/libs/gatk-package-4.2.0.0-19-ge60cdf8-SNAPSHOT-local.jar; >; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /lustre/home/regmova/tools/gatk/build ; >/libs/gatk-package-4.2.0.0-19-ge60cdf8-SNAPSHOT-local.jar CNNScoreVariants -V TR017.GL.vcf.gz -R /home/regmova; >/Scratch/RefGenome/hs37d5.fa -O TR017.CNNscored.vcf; 10:46:04.904 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/home/regmova/tools/gatk/build/libs/gatk-package-4.2.0.0-19-ge60cdf8-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 12, 2021 10:46:05 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:46:05.152 INFO CNNScoreVariants - ------------------------------------------------------------; 10:46:05.152 INFO CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.2.0.0-19-ge60cdf8-SNAPSHOT; 10:46:05.152 INFO CNNScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:46:05.152 INFO CNNScoreVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_92-b14; 10:46:05.152 INFO CNNScoreVariants - Start Date/Time: 12 May 2021 10:46:04 BST; 10:46:05.152 INFO CNNScoreVariants - ------------------------------------------------------------; 10:46:05.152 INFO CNNScoreVariants - ------------------------------------------------------------; 10:46:05.153 INFO CNNScoreVariants - HTSJDK Version: 2.24.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7250:876,Load,Loading,876,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7250,1,['Load'],['Loading']
Performance,## Bug Report. ### Affected tool(s) or class(es). HC java.lang.IllegalStateException: Padded span must contain active span. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description. ```; Runtime.totalMemory()=2494038016; java.lang.IllegalStateException: Padded span must contain active span.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:104); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:80); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popNextReadyAssemblyRegion(ActivityProfile.java:332); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popReadyAssemblyRegions(ActivityProfile.java:277); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:159); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:112); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:35); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:192); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7289:931,load,loadNextAssemblyRegion,931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7289,1,['load'],['loadNextAssemblyRegion']
Performance,"## Bug Report. ### Affected tool(s) or class(es); AnalyzeCovariates . ### Affected version(s); - [x] Latest public release version [v4.1.4.0] [hash:cec850f20311f0686fcf88510bc44e529590d78bec7076a603132115943c09e6]. ### Description ; AnalyzeCovariates fails with ; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.4.0-local.jar AnalyzeCovariates -bqsr /researchers/sebastian.hollizeck/lowcWGS/IN-PM01004/Bam/IN-PM01004_rmd.recal.bam.recalTable -plots /researchers/sebastian.hollizeck/lowcWGS/IN-PM01004/Bam/AnalyzeCovariates.pdf; 23:15:29.581 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 19, 2020 11:15:30 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 23:15:30.435 INFO AnalyzeCovariates - ------------------------------------------------------------; 23:15:30.437 INFO AnalyzeCovariates - The Genome Analysis Toolkit (GATK) v4.1.4.0; 23:15:30.437 INFO AnalyzeCovariates - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:15:30.438 INFO AnalyzeCovariates - Executing as shollizeck@papr-res-compute204.unix.petermac.org.au on Linux v3.10.0-1062.4.3.el7.x86_64 amd64; 23:15:30.438 INFO AnalyzeCovariates - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_212-8u212-b03-0ubuntu1.16.04.1-b03; 23:15:30.438 INFO AnalyzeCovariates - Start Date/Time: January 19, 2020 11:15:29 PM UTC; 23:15:30.439 INFO AnalyzeCovariates - ------------------------------------------------------------; 23:15:30.439 INFO AnalyzeCovariates - ------------------------------------------------------------; 23:15:30.439 INFO AnalyzeCovariates - HTSJDK Version: 2.20.3; 23:15:30.439 INFO AnalyzeCovariates - Picard Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6393:723,Load,Loading,723,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6393,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); Any tool that uses log4j2 and is compiled in java11. ### Affected version(s); - [ X ] Latest master branch as of 20210707. ### Description ; Runing almost everything (anything that makes use of log4j2) I get an UnsuportedOperationException. For example:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx120g -XX:ParallelGCThreads=20 -jar /opt/gatk-4.2.0.0-42-g2fc3b65-SNAPSHOT/gatk-package-4.2.0.0-42-g2fc3b65-SNAPSHOT-local.jar HaplotypeCaller -R Reference/Cork_oak_ref.fasta -I 03-Mapping/05-Deduped-Sorted/S_B_10-deduped-sorted.bam -O 04-SNPcalling/01-HaplotypeCaller/S_B_10.raw.g.vcf --emit-ref-confidence GVCF; WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.; Exception in thread ""main"" java.lang.ExceptionInInitializerError; at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:304); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.UnsupportedOperationException: No class provided, and an appropriate one cannot be found.; at org.apache.logging.log4j.LogManager.callerClass(LogManager.java:576); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:601); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:588); at org.broadinstitute.barclay.argparser.ClassFinder.<clinit>(ClassFinder.java:29); ... 4 more; ```. #### Steps to reproduce; Compile master with java11 and run ; `gatk CheckIlluminaDirectory --help`. #### Expected behavior; Normal execution. #### Actual behavior; UnsupportedOperationException",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7338:869,perform,performance,869,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7338,1,['perform'],['performance']
Performance,"## Bug Report. ### Affected tool(s) or class(es); CollectMultipleMetrics. ### Affected version(s); 4.1.2.0. ### Description ; CollectMultipleMetrics performs Percent-encoding of input paths. When running this tool as a step of a packed CWL workflow with Cromwell, this causes a `No such file or directory` error. The input file; ```; /cromwell-executions/transform_pack.cwl#main/0cd8a732-b482-4b8e-ba6e-34d244620ded/call-picard_collectmultiplemetrics/inputs/-733038737/dbsnp_144.hg38.vcf.gz; ```; becomes; ```; file:///cromwell-executions/transform_pack.cwl%23main/0cd8a732-b482-4b8e-ba6e-34d244620ded/call-picard_collectmultiplemetrics/inputs/-733038737/dbsnp_144.hg38.vcf.gz; ```; and can not be found. #### Expected behavior; The tool should collect metrics without error. #### Actual behavior; `CollectMultipleMetrics`; ```; Job main.metrics.metrics.cwl.gatk_collectmultiplemetrics:NA:1 exited with return code 3 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: /mnt/scratch/runpack/cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_collectmultiplemetrics/execution/stderr.; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_collectmultiplemetrics/tmp.a2640a46; 20:19:59.771 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/bin/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu May 09 20:20:00 UTC 2019] CollectMultipleMetrics --INPUT /cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_colle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5931:149,perform,performs,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5931,1,['perform'],['performs']
Performance,"## Bug Report. ### Affected tool(s) or class(es); CollectWgsMetrics. ### Affected version(s); - 4.1.3.0. ### Description ; CollectWgsMetrics crashes on input that has worked for previous versions (including 4.1.2.0). #### Steps to reproduce; ```bash; gatk CollectWgsMetrics \; --java-options ""-Xms6G -Xmx10G -Djava.io.tmpdir=."" \; --INPUT=example.bam \; --OUTPUT=example.seq_metrics.txt \; --REFERENCE_SEQUENCE=ucsc.hg19.fasta \; --USE_FAST_ALGORITHM=true \; --LOCUS_ACCUMULATION_CAP 25000 \; --COVERAGE_CAP=100; ```; #### Expected behavior; CollectWgsMetrics should not crash. #### Actual behavior; ```terminal; 02:33:12.885 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/conda/envs/base-v1.4.1/share/gatk4-4.1.3.0-0/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Mon Sep 16 02:33:13 UTC 2019] CollectWgsMetrics --INPUT example.bam --OUTPUT example.seq_metrics.txt --COVERAGE_CAP 100 --LOCUS_ACCUMULATION_CAP 25000 --USE_FAST_ALGORITHM true --REFERENCE_SEQUENCE ucsc.hg19.fasta --MINIMUM_MAPPING_QUALITY 20 --MINIMUM_BASE_QUALITY 20 --STOP_AFTER -1 --INCLUDE_BQ_HISTOGRAM false --COUNT_UNPAIRED false --SAMPLE_SIZE 10000 --ALLELE_FRACTION 0.001 --ALLELE_FRACTION 0.005 --ALLELE_FRACTION 0.01 --ALLELE_FRACTION 0.02 --ALLELE_FRACTION 0.05 --ALLELE_FRACTION 0.1 --ALLELE_FRACTION 0.2 --ALLELE_FRACTION 0.3 --ALLELE_FRACTION 0.5 --READ_LENGTH 150 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 8 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Sep 16, 2019 2:33:15 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Mon Sep 16 02:33:15 UTC 2019] Executing as user@server on Linux 3.10.0-693.21.1.el7.x86_64 amd64; OpenJDK 64-Bi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6163:653,Load,Loading,653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6163,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); CreateReadCountPanelOfNormals. ### Affected version(s); - [ ] Latest public release version [4.1.0.0]. ### Description ; When you run it on a single machine, it trys to use _hadoop_ and failed. ```; $ java -jar ../gatk-package-4.1.0.0-local.jar CreateReadCountPanelOfNormals --input in.counts.hdf5 --output out.pon.hdf5; 12:33:52.103 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 12:33:52.162 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5686:610,Load,Loading,610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); GATK 4.1.0.0. ### Description ; Funcotator does not perform any annotation on a minimal VCF with canonical cancer variants and returns the following error:. ```; 23:28:30.519 INFO Funcotator - Initializing Funcotator Engine...; 23:28:30.523 INFO Funcotator - Creating a VCF file for output: file:xxx/sandbox/idh.funcotated.vcf; 23:28:30.541 INFO ProgressMeter - Starting traversal; 23:28:30.541 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:28:30.652 INFO ProgressMeter - unmapped 0.0 15 8108.1; 23:28:30.652 INFO ProgressMeter - Traversal complete. Processed 15 total variants in 0.0 minutes.; 23:28:30.652 WARN Funcotator - ================================================================================; 23:28:30.652 WARN Funcotator - _ _ _ __ __ _ _ _ _; 23:28:30.652 WARN Funcotator - | || || | \ \ / /_ _ _ __ _ __ (_)_ __ __ _ | || || |; 23:28:30.652 WARN Funcotator - | || || | \ \ /\ / / _` | '__| '_ \| | '_ \ / _` | | || || |; 23:28:30.653 WARN Funcotator - |_||_||_| \ \V V / (_| | | | | | | | | | | (_| | |_||_||_|; 23:28:30.653 WARN Funcotator - (_)(_)(_) \_/\_/ \__,_|_| |_| |_|_|_| |_|\__, | (_)(_)(_); 23:28:30.653 WARN Funcotator - |___/; 23:28:30.653 WARN Funcotator - --------------------------------------------------------------------------------; 23:28:30.653 WARN Funcotator - Only IGRs were produced for this dataset. This STRONGLY indicates that this; 23:28:30.653 WARN Funcotator - run was misconfigured.; 23:28:30.653 WARN Funcotator - You MUST check your data sources to make sure they are correct for these data.; 23:28:30.653 WARN Funcotator - ================================================================================; ```. There is no reason to assume that there is any issue with the data sources or run parameters. They have worked fine using a different VCF that had completed INFO tags. #### Steps to reproduce; Run Funcotator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5777:139,perform,perform,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5777,1,['perform'],['perform']
Performance,"## Bug Report. ### Affected tool(s) or class(es); GATK PostprocessGermlineCNVCalls. ### Affected version(s); v4.4.0.0. ### Description ; Run GTAK on a batch of WES samples with `PostprocessGermlineCNVCalls` encountered: ""Records were not strictly sorted in dictionary order.""; I tried to detect germline CNV in cohort mode on 25 WES samples by the official tutorial. At first, I didn't perform scatter and the step `PostprocessGermlineCNVCalls` was very time-consuming but eventually worked. So I split the reference genome into 45 parts to save time. It's OK for the first sample but there was an error ""Records were not strictly sorted in dictionary order."" from the second sample. I was really annoyed by it. `03:12:39.275 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xiangxd/project/software/callers/gatk_4.4/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 03:12:39.467 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.473 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.4.0.0; 03:12:39.474 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:12:39.475 INFO PostprocessGermlineCNVCalls - Executing as xiangxd@cu07 on Linux v3.10.0-327.el7.x86_64 amd64; 03:12:39.475 INFO PostprocessGermlineCNVCalls - Java runtime: Java HotSpot(TM) 64-Bit Server VM v20.0.2+9-78; 03:12:39.477 INFO PostprocessGermlineCNVCalls - Start Date/Time: April 15, 2024, 3:12:39AM CST; 03:12:39.477 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.478 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.495 INFO PostprocessGermlineCNVCalls - HTSJDK Version: 3.0.5; 03:12:39.496 INFO PostprocessGermlineCNVCalls - Picard Version: 3.0.0; 03:12:39.497 INFO PostprocessGermlineCNVCalls - Built for Spark Vers",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8776:386,perform,perform,386,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776,2,"['Load', 'perform']","['Loading', 'perform']"
Performance,"## Bug Report. ### Affected tool(s) or class(es); GATK v4.1.4.0 using FilterMutectCalls. ### Affected version(s); - [x] Latest public release version `4.1.4.0` installed from conda release `gatk4-4.1.4.0-1`; - [ ] Latest master branch as of [date of test?]. ### Description ; This issue reports the same error that is reported in #6237, but on the latest release, and in a mitochondrial calling setting. My command is:; ```bash; gatk FilterMutectCalls -V MT.vcf.gz\; -R human_g1k_v37.main.fasta\; -O MT.filtered.vcf.gz\; --stats MT.vcf.gz.stats\; --mitochondria-mode; ```. I get the following output to STDERR:; ```; 11:15:57.152 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/warkre/miniconda3/envs/gatk4.1.4.0/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 07, 2019 11:15:57 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:15:57.328 INFO FilterMutectCalls - ------------------------------------------------------------; 11:15:57.328 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.4.0; 11:15:57.328 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:15:57.328 INFO FilterMutectCalls - Executing as warkre@fuji on Linux v4.9.0-9-amd64 amd64; 11:15:57.328 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 11:15:57.329 INFO FilterMutectCalls - Start Date/Time: November 7, 2019 11:15:57 AM CET; 11:15:57.329 INFO FilterMutectCalls - ------------------------------------------------------------; 11:15:57.329 INFO FilterMutectCalls - ------------------------------------------------------------; 11:15:57.329 INFO FilterMutectCalls - HTSJDK Version: 2.20.3; 11:15:57.329 INFO FilterMutectCalls - Picard Version: 2.21.1; 11:15:57.329 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6255:657,Load,Loading,657,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6255,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller. ### Affected version(s); - GATK4 4.0.5.1. ### Description ; I'm trying to do a germline CNV calling with 387 exomes samples (I know it's a lot). The CollectReadCounts and DetermineGermlineContigPloidy were successfull. But for the GermlineCNVCaller I got what I think is a Python ""cannot allocate memory"" error. I tried to specify to the JVM a max memory to allocate ``` --java-options ""-Xmx192G"" ``` , but no improvements. The machine I'm working on got 32 threads and 192 Gb RAM. #### Steps to reproduce; I guess try to do a CNV calling with a large cohort. #### Output; ```10:56:25.124 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/tintest/miniconda2/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:56:25.342 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.343 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.0.5.1; 10:56:25.344 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:56:25.345 INFO GermlineCNVCaller - Executing as tintest@dahu39 on Linux v4.9.0-6-amd64 amd64; 10:56:25.346 INFO GermlineCNVCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 10:56:25.347 INFO GermlineCNVCaller - Start Date/Time: July 25, 2018 10:56:24 AM CEST; 10:56:25.348 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.349 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.350 INFO GermlineCNVCaller - HTSJDK Version: 2.15.1; 10:56:25.351 INFO GermlineCNVCaller - Picard Version: 2.18.2; 10:56:25.352 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:56:25.353 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:56:25.354 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053:685,Load,Loading,685,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller --output-mode EMIT_ALL_SITES. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I'm trying to generate a VCF (not a gVCF) that contains calls spanning all the sites in my regions. Each region is small, and is more or less equivalent to a single variant. Ideally I'd use `GENOTYPE_GIVEN_ALLELES`, but I don't know the alleles, and in some cases the variant location is approximate (e.g. somewhere in _this_ 10bp window). I've been trying to use HaplotypeCaller to produce a VCF that contains calls covering my entire set of regions, but nothing seems to work. I started with just `--output-mode` and eventually ended up with:. ```; gatk HaplotypeCaller \; -R ref.fasta \; -L regions.interval_list \; --disable-optimizations \; --force-active \; --output-mode EMIT_ALL_SITES \; -I my.bam \; -O my.vcf.gz; ```. This does output considerably more records, including a lot of hom-ref records, but still nowhere near to the full set of bases within my regions. E.g. in one test this emits variants spanning 3,468bp which is way better than the ~120bp I get without those options, but nowhere near the 293,570bp with the regions I'm supplying. It would be great if `--output-mode EMIT_ALL_SITES` did as the documentation described, but if that's not possible, then perhaps that mode should simply be removed?. #### Steps to reproduce; Try calling a BAM file with HaplotypeCaller with a 100-1000bp region with `--output-mode EMIT_ALL_SITES`. #### Expected behavior; VCF should contain records spanning the entire input region. #### Actual behavior; VCF contains a minority of sites from the region.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6059:866,optimiz,optimizations,866,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6059,1,['optimiz'],['optimizations']
Performance,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller GVCF mode. ### Affected version(s); GATK 4.1.8.0 . ### Description ; Discussed on the GATK forum: https://gatk.broadinstitute.org/hc/en-us/community/posts/360072760032-HaplotypeCaller-NullPointerException-Error. Command: ; `gatk --java-options ""-Xmx4g"" HaplotypeCaller -R hg19.fa.gz -I test.bam -O test.g.vcf.gz -ERC GVCF`. #### Stack Trace. ```; 17:08:11.229 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/zepengmu/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 27, 2020 5:08:12 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:08:12.021 INFO HaplotypeCaller - ------------------------------------------------------------; 17:08:12.028 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.0; 17:08:12.028 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:08:12.038 INFO HaplotypeCaller - Executing as zepengmu@midway2-0243.rcc.local on Linux v3.10.0-1127.8.2.el7.x86_64 amd64; 17:08:12.038 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 17:08:12.039 INFO HaplotypeCaller - Start Date/Time: August 27, 2020 5:08:11 PM CDT; 17:08:12.039 INFO HaplotypeCaller - ------------------------------------------------------------; 17:08:12.039 INFO HaplotypeCaller - ------------------------------------------------------------; 17:08:12.039 INFO HaplotypeCaller - HTSJDK Version: 2.22.0; 17:08:12.039 INFO HaplotypeCaller - Picard Version: 2.22.8; 17:08:12.039 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:08:12.040 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:08:12.040 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:08:12.040 INFO HaplotypeCal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6783:453,Load,Loading,453,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6783,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCallerSpark. ### Affected version(s); - [ 4.6.0.0] . ### Description ; spark task failed, here is the stack trace:; ```shell; java.lang.NullPointerException: Cannot invoke ""java.util.List.size()"" because ""cache"" is null; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypesCache.ensureCapacity(GenotypesCache.java:84); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypesCache.get(GenotypesCache.java:43); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.makeGenotypeCall(GATKVariantContextUtils.java:341); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.AlleleSubsettingUtils.subsetAlleles(AlleleSubsettingUtils.java:133); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.AlleleSubsettingUtils.subsetAlleles(AlleleSubsettingUtils.java:48); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:191); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:263); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:979); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$assemblyFunction$0(HaplotypeCallerSpark.java:179); 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197); 	at java.base/java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1856); 	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:292); 	at java.base/java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); 	at java.base/java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); 	at java.base/java.util.stream.StreamSp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8961:264,cache,cache,264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8961,1,['cache'],['cache']
Performance,"## Bug Report. ### Affected tool(s) or class(es); Haplotypecaller. ### Affected version(s); 4.1.1 or later. ### Description ; if you look at haplotypecaller.java::callRegion(), you will see these lines:; ```; if (trimmingResult.hasLeftFlankingRegion()) {; result.addAll(referenceModelForNoVariation(trimmingResult.nonVariantLeftFlankRegion(), false, VCpriors));; }; // output variant containing region.; result.addAll(referenceConfidenceModel.calculateRefConfidence(assemblyResult.getReferenceHaplotype(),; calledHaplotypes.getCalledHaplotypes(), assemblyResult.getPaddedReferenceLoc(), regionForGenotyping,; readLikelihoods, genotypingEngine.getPloidyModel(), calledHaplotypes.getCalls(), hcArgs.standardArgs.genotypeArgs.supportVariants != null,; VCpriors));; // output right-flanking non-variant section:; if (trimmingResult.hasRightFlankingRegion()) {; result.addAll(referenceModelForNoVariation(trimmingResult.nonVariantRightFlankRegion(), false, VCpriors));; }. ```; We trim the region left and right and calculate the Reference confidence. The problem is when we call calculateRefConfidence , we might trim reads for the left region and calculate TransientAttributes and put them in the cache. Next time, when we call calculateRefConfidence, we trim the reads for the right region but we still look at the cache which is prepopulated with data for the left region and we use the same data which might result in the wrong output. . if you turn USE_CACHED_READ_INDEL_INFORMATIVENESS_VALUES off, you might get different results which I do not think is the expected outcome. Let me know if you want more information. Thank you.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5908:1194,cache,cache,1194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5908,2,['cache'],['cache']
Performance,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [x] 4.1.1.0..4.1.9.0. ### Description ; I am evaluating Mutect2 variant calling performance in GiaB mixtures (target capture, no UMI, 2000x avg coverage). In particular, I am comparing 4.0.12.0 against 4.1.9.0 with default parameters. Below, I am providing data from a representative sample.; 4.1.9.0 misses variants that 4.0.12.0 was able to call. When feeding a reference VCF with option `--alleles` the variants are detected with decent quality scores. It is unclear why 4.1.9.0 does not make these variant calls and if this could be changed by modifying input parameters. Unlike in this issue https://github.com/broadinstitute/gatk/issues/6724 the variants were not called with the option `--force-active`. . These are the variants that are only called by 4.1.9.0 when the reference VCF is fed as input:. ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Sample; 2	25458546	.	C	T	.	.	AS_SB_TABLE=723,503|25,14;DP=1302;ECNT=1;MBQ=20,20;MFRL=189,190;MMQ=60,60;MPOS=36;POPAF=7.3;TLOD=61.58	GT:AD:AF:DP:F1R2:F2R1:SB	0/1:1226,39:0.033:1265:576,19:554,19:723,503,25,14; 4	55152040	.	C	T	.	.	AS_SB_TABLE=1102,1078|15,13;DP=2349;ECNT=2;MBQ=20,20;MFRL=180,164;MMQ=60,60;MPOS=35;POPAF=7.3;TLOD=31.85	GT:AD:AF:DP:F1R2:F2R1:SB	0/1:2180,28:0.012:2208:1003,16:1104,10:1102,1078,15,13; 5	170833472	.	AAT	A	.	.	AS_SB_TABLE=201,501|7,17;DP=750;ECNT=1;MBQ=20,26;MFRL=203,209;MMQ=60,60;MPOS=20;POPAF=7.3;RPA=2,1;RU=AT;STR;TLOD=45.4	GT:AD:AF:DP:F1R2:F2R1:SB	0/1:702,24:0.035:726:329,12:311,12:201,501,7,17; 7	101844851	.	A	G	.	.	AS_SB_TABLE=1022,1178|25,25;DP=2406;ECNT=1;MBQ=20,20;MFRL=189,198;MMQ=60,60;MPOS=46;POPAF=7.3;TLOD=65.52	GT:AD:AF:DP:F1R2:F2R1:SB	0/1:2200,50:0.021:2250:854,29:906,19:1022,1178,25,25; 7	101916798	.	C	A	.	.	AS_SB_TABLE=91,916|1,37;DP=1060;ECNT=1;MBQ=32,32;MFRL=213,195;MMQ=60,60;MPOS=26;POPAF=7.3;TLOD=54.92	GT:AD:AF:DP:F1R2:F2R1:SB	0/1:1007,38:0.033:1045:438,17:511,18:91,916,1,37; 7	148506396	.	A	C	.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7015:166,perform,performance,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7015,1,['perform'],['performance']
Performance,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); I test this problem in two versions, V4.1.4.1 and V4.3.0.0.They all have this problem. ### Description ; Following the recommendations of the 'Best Practice Workflows', I run mutect2 in the following command. java -jar -Djava.io.tmpdir=${tmpDir} -Xms2g -Xmx16g ; /mnt/bin/gatk-4.1.4.1/gatk-package-4.1.4.1-SNAPSHOT-local.jar Mutect2 ; --native-pair-hmm-threads 32 ; -R ${Fasta} ; -I ${cancer_bam} ; -I ${normal_bam}; --tumor-sample cancer --normal-sample normal ; -L ${all_chrome_bed}; --bam-output ${bam_output} ; -O ${vcf_output}. To improve parallelism, I try to split my all chrome bed to 25 files.Parallel running the flowing command brings me signficient performance improvement. java -jar -Djava.io.tmpdir=${tmpDir} -Xms2g -Xmx16g ; /mnt/bin/gatk-4.1.4.1/gatk-package-4.1.4.1-SNAPSHOT-local.jar Mutect2 ; --native-pair-hmm-threads 32 ; -R ${Fasta} ; -I ${cancer_bam} ; -I ${normal_bam}; --tumor-sample cancer --normal-sample normal ; -L ${chr1_bed}; --bam-output ${chr1_bam_output} ; -O ${chr1_vcf_output}. java -jar -Djava.io.tmpdir=${tmpDir} -Xms2g -Xmx16g ; /mnt/bin/gatk-4.1.4.1/gatk-package-4.1.4.1-SNAPSHOT-local.jar Mutect2 ; --native-pair-hmm-threads 32 ; -R ${Fasta} ; -I ${cancer_bam} ; -I ${normal_bam}; --tumor-sample cancer --normal-sample normal ; -L ${chr2_bed}; --bam-output ${chr2_bam_output} ; -O ${chr2_vcf_output}. But when I examined the vcf results produced by both modes of operation, I found consistency issues. #### Expected behavior; Let's focus on chromosome 2.I expect 100% consistency between the following two runs.; 1. The vcf file is obtained using a bed file containing only chromosome 2.; 2. Use bed file with all chromosomes to get all calling results, then filter to get chromesome 2 calling result. #### Actual behavior; 1. The first method above gives one more result than the second.; 2. There are 168 vcf results inconsistent, out of 1247 total.One of the inconsistencie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8152:745,perform,performance,745,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8152,1,['perform'],['performance']
Performance,"## Bug Report. ### Affected tool(s) or class(es); The docker image: `broadinstitute/gatk`. ### Affected version(s); `latest`. ### Description ; - The current GATK image has 44 layers; - In [the Azure Container Registry standard service tier](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling), ""ReadOps per minute"" is limited to 3000; - ""A `docker pull` translates to multiple read operations based on the number of layers in the image, plus the manifest retrieval.""; - 3000 / 45 = 66. That means that the image can only be pulled 66 times per minute. This is problematic for running many concurrent workflows that also have many shards. Once that limit is exceeded, the task can fail, which can cause the entire workflow to fail. ; - Layers can be viewed here: `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`; [gatk-image-layers.txt](https://github.com/broadinstitute/gatk/files/14212774/gatk-image-layers.txt). #### Steps to reproduce; `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`. #### Expected behavior; `--squash` shall be added to `build_docker_base_cloud.sh`, like has been added to `build_docker_base_locally.sh` already: https://github.com/broadinstitute/gatk/blob/a353e49f218e675f331abf629f0bb46df1d5151d/scripts/docker/gatkbase/build_docker_base_locally.sh#L24. #### Workaround; Users can pull the existing image, and [use `docker-squash` to squash the image to a single layer](https://github.com/goldmann/docker-squash), then push it into their private ACR, then update their WDLs to reference the new image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684:334,throughput,throughput-and-throttling,334,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684,2,"['concurren', 'throughput']","['concurrent', 'throughput-and-throttling']"
Performance,"## Bug Report. ### Affected tool(s) or class(es); VariantRecalibrator. ### Affected version(s); GATK 4.2.0.0 . ### Description . When running VariantRecalibrator on a joint-called gVCF with 2000 samples, the following java.lang.IllegalStateException occurs: **Gaussian mean vector does not have the same size as the list of annotations**. ```; 17:56:38.072 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 28, 2021 5:56:38 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:56:38.485 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.487 INFO VariantRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 17:56:38.487 INFO VariantRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:56:38.488 INFO VariantRecalibrator - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.25.1.el7.x86_64 amd64; 17:56:38.488 INFO VariantRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 17:56:38.488 INFO VariantRecalibrator - Start Date/Time: July 28, 2021 5:56:38 PM EDT; 17:56:38.489 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.489 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.490 INFO VariantRecalibrator - HTSJDK Version: 2.24.0; 17:56:38.491 INFO VariantRecalibrator - Picard Version: 2.25.0; 17:56:38.491 INFO VariantRecalibrator - Built for Spark Version: 2.4.5; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRIT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7380:384,Load,Loading,384,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); _FilterMutectCalls_. ### Affected version(s); - gatk-4.1.0.0 (_latest_). ### Description . Hi,. I am using _Mutect2_ (v4.1.0.0) and similar to a previous bug reported on `AF=.`, _FilterMutectCalls_ seems to complain about MPOS fields having a value of `.`. No intermediate processing was done between _Mutect2_ and _FilterMutectCalls_. Below the error stack trace :. ```; 17:13:28.491 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data-ddn/home/anthony/sbx/mutect2/work/conda/gatk4-mutect2-nf-bcf605d6af4c0524a368d3d105898641/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:13:30.503 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.503 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.0.0; 17:13:30.504 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:13:30.504 INFO FilterMutectCalls - Executing as anthony@node063 on Linux v2.6.32-220.el6.x86_64 amd64; 17:13:30.504 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 17:13:30.504 INFO FilterMutectCalls - Start Date/Time: February 17, 2019 5:13:28 PM CET; 17:13:30.504 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.505 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Version: 2.18.2; 17:13:30.505 INFO FilterMutectCalls - Picard Version: 2.18.25; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:13:30.506 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:13:30.506 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5684:462,Load,Loading,462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5684,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); _Mutect2_. ### Affected version(s); - GATK 4.1.4.1. ### Description ; When running Mutect2 (from GATK v4.1.4.1) using the following command:. `gatk Mutect2 -R [path to grch37-1kg.fa] -I testcase.bam -O pon.vcf`. to create a PoN on NovaSeq WGS-data processed through the best practice pipeline (with the BQSR-steps run through the Spark-enabled tools, and bwa mem with -Y flag) I get the following error in multiple regions:. [Stacktrace](https://www.dropbox.com/s/d2n5zflj9u11oj8/stacktrace.png?dl=0). AFAIK this is related to the new code path introduced in #6240 and seem to be triggered when there are more than 2 reads supporting a fragment but all of them are either duplicate reads or supplemntary/secondary alignments. Any input is greatly appreciated. I guess a temporary fix is to use the --independent-mates flag (although haven't tried it yet -- how much worse mutation calling performance do one incur when using that flag?). #### Steps to reproduce; Use the following small test case .bam-file as input to the command specified above:. [Testcase](https://www.dropbox.com/s/hilcj3aj0jnjdmh/testcase.bam?dl=0). #### Expected behavior; Completion of mutect2 without Exception. #### Actual behavior; Early termination of the mutect2 run due to raising an exception when trying to create a fragment with no read data to back it up. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6310:939,perform,performance,939,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6310,1,['perform'],['performance']
Performance,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. FilterAlignmentArtifacts. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. 4.3.0.0. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. ```; Using GATK jar /gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -XX:+UseNUMA -jar /gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar FilterAlignmentArtifacts -R /raid/bundle/hg38/Homo_sapiens_assembly38.fasta.gz -O WGS-NA12878.FilterAlignmentArtifacts.vcf --tmp-dir . -V WGS-NA12878.filtered.vcf -I WGS-NA12878.sorted.dedup.recal.bam --bwa-mem-index-image /raid/bundle/hg38/Homo_sapiens_assembly38.fasta.img; 11:24:09.761 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:24:09.942 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.942 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.3.0.0; 11:24:09.943 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:24:09.943 INFO FilterAlignmentArtifacts - Executing as root@D52BV-2U on Linux v4.15.0-202-generic amd64; 11:24:09.943 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_352-8u352-ga-1~18.04-b08; 11:24:09.943 INFO FilterAlignmentArtifacts - Start Date/Time: February 24, 2023 11:24:09 AM CST; 11:24:09.943 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.943 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.943 INFO Filter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8221:986,Load,Loading,986,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8221,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); _womtool-47, grpah_; or; _version 1.0 wdls_. ### Description . I've been trying to graph mutect2 wdls using the latest womtool, but each time the tool throws an exception. Command-line:; `java -jar womtool-47.jar graph mutect2.wdl`. Exception:; ```; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: Unrecognized token on line 445, column 42:. samtools view -h -T ~{ref_fasta} ~{cram} |; ^; 	at wdl.draft2.parser.WdlParser.unrecognized_token(WdlParser.java:6975); 	at wdl.draft2.parser.WdlParser.lex(WdlParser.java:7048); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:263); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:170); 	at scala.util.Try$.apply(Try.scala:213); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:630); 	at womtool.graph.GraphPrint$.generateWorkflowDigraph(GraphPrint.scala:19); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:131); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:162); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:167); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:24); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:24); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```. for multi sample:; ```; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: Unrecognized token on line 31, colu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6261:693,load,load,693,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6261,4,['load'],"['load', 'loadUsingSource']"
Performance,"## Bug Report. ### Affected tool(s) or class(es); gatk MarkDuplicatesSpark. ### Affected version(s); - GATK 4.2.6.1; - Spark 3.2.1. ### Description ; File sizes are different between MarkDuplicates and MarkDuplicatesSpark (run locally). file sizes:; input cram: 1094584927; output bam (MarkDuplicates): 2839215419; output bam (MarkDuplicatesSpark): 3536690732. #### Steps to reproduce; command:. `java -Xmx200G -jar /opt/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar MarkDuplicatesSpark \; -I file.cram \; -O file_sorted_markduplicates.bam \; -M file_markduplicates_metrics.txt \; -R homo_sapiens.fa`. #### Expected behavior; output bam should be the same size (or very similar) between MarkDuplicates and MarkDuplicatesSpark. Note: this is when using the local version of the gatk package, using the spark version I get the following error:. `Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Partitioner; 	at java.lang.Class.getDeclaredConstructors0(Native Method); 	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671); 	at java.lang.Class.getConstructors(Class.java:1651); 	at org.broadinstitute.hellbender.utils.ClassUtils.canMakeInstances(ClassUtils.java:31); 	at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:319); 	at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Partitioner; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:387); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 8 more`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8007:1659,load,loadClass,1659,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8007,3,['load'],['loadClass']
Performance,"## Bug Report. ### Affected tool(s) or class(es); gatk SplitNCigarReads. ### Affected version(s); - gatk 4.2.6.1. ### Description ; I produced the bam files using STAR, and adjusted the MQ value to 60. I then used sambamba markdup to mark duplicate, then I proceeded to use SplitNCigarReads. The CPU load for SplitNCigarReads was very high and at certain times can spike up to 2400%. I tried limiting the cpu usage with commands like `-XX:ParallelGCThreads=1` and `-XX:ConcGCThreads=1`, but it doesn't seem to have an effect. (The cpu usage sometimes do stay at 100%) I also adjusted the MQ value in STAR to lessen the load in SplitNCigarReads. I also tried to increase the read size to reduce I/O time.; ![image](https://user-images.githubusercontent.com/106958825/175206165-08b28567-d671-45fa-b033-f20c4792edb7.png). #### Steps to reproduce; STAR; ```; STAR \; --genomeDir ${star_reference_path} \; --runThreadN 16 \; --readFilesIn ${file_1} ${file_2} \; --readFilesCommand ""gunzip -c"" \; --sjdbOverhang 149 \; --outSAMtype BAM SortedByCoordinate \; --outBAMsortingThreadN 16 \; --outSAMmultNmax 1 \; --outSAMmapqUnique 60 \; --outSAMattrRGline ID:${id} LB:RNASEQ SM:${sample_name} PL:ILLUMINA PU:${platform_unit} PM:${instrument_id} \; --limitBAMsortRAM 50000000000 \; --twopassMode Basic \; --outFileNamePrefix /rawdata/rnaseq/clean/bam/1.; ```. Mark Duplicate; ```; sambamba markdup \; -t 4 \; --tmpdir=/tmp \; --hash-table-size=262144 \; --overflow-list-size=67108864 \; /rawdata/rnaseq/clean/bam/1.Aligned.sortedByCoord.out.bam \; /rawdata/rnaseq/clean/bam/1.aligned.duplicate_marked.sorted.bam \; ```. SplitNCigarReads; ```; gatk --java-options ""-Djava.io.tmpdir=/tmp -Xmx20G -XX:ParallelGCThreads=1 -XX:ConcGCThreads=1"" SplitNCigarReads \; -R ${reference_path} \; --tmp-dir /tmp \; -I /rawdata/rnaseq/clean/bam/1.aligned.duplicate_marked.sorted.bam \; -O /rawdata/rnaseq/clean/bam_gatk/1.aligned.duplicate_marked.sorted.bam \; --create-output-bam-md5 TRUE \; --max-reads-in-memory 1000000 \; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7914:300,load,load,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7914,2,['load'],['load']
Performance,"## Bug Report. ### Affected tool(s); GenotypeGVCFs . ### Affected version(s); 4.0.2.0. ### Description ; After running GenomicsDBImport which takes a short time, GenotypeGVCFs takes a really long time to genotype a short interval. It should not take so long. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11471/performance-troubleshooting-tips-for-genotypegvcfs/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4512:375,perform,performance-troubleshooting-tips-for-genotypegvcfs,375,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4512,1,['perform'],['performance-troubleshooting-tips-for-genotypegvcfs']
Performance,"## Bug Report. Dear developers,. I tried to update the GENCODE database and used the getGencode.sh scripts to get the data. However, I was not able to index the feature-file: Do you have any idea why that happens and how to get it done?. Code:; /home/robby/Tools/NGS/gatk-4.2.0.0/gatk IndexFeatureFile -I /home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; Using GATK jar /home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar IndexFeatureFile -I /home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; 18:53:59.113 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 08, 2021 6:53:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:53:59.283 INFO IndexFeatureFile - ------------------------------------------------------------; 18:53:59.283 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 18:53:59.284 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:53:59.290 INFO IndexFeatureFile - Initializing engine; 18:53:59.290 INFO IndexFeatureFile - Done initializing engine; 18:53:59.417 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but err",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7134:948,Load,Loading,948,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7134,1,['Load'],['Loading']
Performance,"## Bug Report. java.lang.OutOfMemoryError when 'java -jar /usr/hpc-bio/gatk/gatk-package-4.1.2.0-local.jar' sometimes, but there is a lot of memeory yet. And then all features can not be used. This is the call stack.; ```; java -jar gatk/gatk-package-4.1.2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6050:506,load,load,506,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050,11,"['Load', 'load']","['LoadType', 'LoadersManager', 'load', 'loaders']"
Performance,"## Bug Report; GenotypeGVCFs stuck indefinitely at ""Initializing engine"" step. ### Affected tool(s) or class(es); gatk GenotypeGVCFs; ### Affected version(s); GATK v4.1.4.1 (installed in a `conda` convironment from the bioconda channel), on a RHEL server 7.6 (Maipo). ### Description ; Following the recommended pipeline of HaplotypeCaller, GenomicsDBImport and then GenotypeGVCFs, the last command hangs indefinitely and from the log file, it seems like it doesn't get past the ""Initialize engine"" step. This is an example of the standard error stream (after the `GenotypeGVCFs` job reached 20 hours wall time and was killed) :; ```; 22:28:44.293 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/export/user/home/miniconda3/envs/aDNA/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.; jar!/com/intel/gkl/native/libgkl_compression.so; Dec 17, 2020 10:28:44 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 22:28:44.639 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:28:44.640 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.4.1; 22:28:44.640 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:28:44.640 INFO GenotypeGVCFs - Executing as user@gc-prd-hpcn002 on Linux v3.10.0-957.27.2.el7.x86_64 amd64; 22:28:44.640 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 22:28:44.640 INFO GenotypeGVCFs - Start Date/Time: December 17, 2020 10:28:44 PM AEST; 22:28:44.640 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:28:44.640 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:28:44.640 INFO GenotypeGVCFs - HTSJDK Version: 2.21.0; 22:28:44.640 INFO GenotypeGVCFs - Picard Version: 2.21.2; 22:28:44.640 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7007:675,Load,Loading,675,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7007,1,['Load'],['Loading']
Performance,"## Bug Report; when I run the MarkDuplicatesSpark, it throws me an error: basically it shows the spark engine stopped when run this function. ; the part of the error log is here:; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/rnaseq_pipeline_app/Apps/GATK/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar) to method java.nio.Bits.unaligned(); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 21/01/12 15:50:31 INFO SparkContext: Running Spark version 2.4.5; 21/01/12 15:50:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 21/01/12 15:50:31 INFO SparkContext: Submitted application: MarkDuplicatesSpark; 21/01/12 15:50:31 INFO SecurityManager: Changing view acls to: root; 21/01/12 15:50:31 INFO SecurityManager: Changing modify acls to: root; 21/01/12 15:50:31 INFO SecurityManager: Changing view acls groups to: ; 21/01/12 15:50:31 INFO SecurityManager: Changing modify acls groups to: ; 21/01/12 15:50:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); groups with view permissions: Set(); users with modify permissions: Set(root); groups with modify permissions: Set(); 21/01/12 15:50:31 INFO Utils: Successfully started service 'sparkDriver' on port 36657.; 21/01/12 15:50:31 INFO SparkEnv: Registering MapOutputTracker; 21/01/12 15:50:31 INFO SparkEnv: Registering BlockManagerMaster; 21/01/12 15:50:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 21/01/12 15:50:31 INFO B",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7035:904,load,load,904,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7035,1,['load'],['load']
Performance,"## Feature request. ### Tool(s) or class(es) involved. SV pipeline, Funcotator, etc. ### Description. In trying to build test data for SV, time and time again we face the problem of not being able to find actual desired events on the two chromosomes 20 and 21, hence end up having to painfully perform all kinds of coordinate hacks in order to have enough test coverage. It seems that the Funcotator team is also facing a similar issue. Therefore it will be great if the whole reference genome for HG38, and maybe HG19 as well, can be included in the tests, so that tool developers spend less time worrying about hassles in moving real events to chr20 and chr21. One of the potential downside is obvious: it increases the repo size and time for running tests (downloading a bigger file) on Travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5111:294,perform,perform,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5111,1,['perform'],['perform']
Performance,"## Feature request. ### Tool(s) or class(es) involved; Funcotator. ### Description. Currently, there is a Caching mechanism in VcfFuncotationFactory. This could be refactored into a separate component and shared with other funcotation factories. Not all funcotation factories would benefit from this cache, but certainly LocatableXsvFuncotationFactory would. If the refactoring is to put the cache into DataSourceFuncotationFactory, then there needs to be a mechanism for subclasses to disable it. Additionally, the cache should be made into a separate class that can be used from DataSourceFuncotationFactory. Just to encapsulate the cache functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4974:300,cache,cache,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4974,4,['cache'],['cache']
Performance,"## Feature request. ### Tool(s) or class(es) involved; GenomicsDBImport. ### Description; Users get confused by this error message: `A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader`; `Caused by: java.io.IOException: GenomicsDB JNI Error: VariantQueryProcessorException : Could not open array 1$1$249250621 at workspace: ...; TileDB error message : [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading tile offsets failed`. In one of our [docs](https://gatk.broadinstitute.org/hc/en-us/articles/360035889971--How-to-Consolidate-GVCFs-for-joint-calling-with-GenotypeGVCFs), we offer this advice, but this is not a proper argument in the GATK tool docs yet:; _If youre working on a POSIX filesystem (e.g. Lustre, NFS, xfs, ext4 etc), you must set the environment variable TILEDB_DISABLE_FILE_LOCKING=1 before running any GenomicsDB tool. If you dont, you will likely see an error like Could not open array genomicsdb_array at workspace:[...]_. **This request is to add a proper argument to deal with this scenario in GenomicsDBImport and to document it in the tool docs.**",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6519:403,load,load,403,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6519,1,['load'],['load']
Performance,"## Feature request. ### Tool(s) or class(es) involved; VariantRecalibrator. ### Description; Currently VariantRecalibrator only accepts vcf as input. Previously when performing Joint Genoptying using GenotypeGVCFs, the outputs were vcf hence this behavior made sense. . Now that we have GenomicsDB import (which is quite fast) we still have to use GenotypeGVCFs to extract a vcf for Variant Recalibrator. So, let's just skip a step and let VariantRecalibrator use GenomicsDB as an input!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7169:166,perform,performing,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7169,1,['perform'],['performing']
Performance,"## Feature request. ### Tool(s) or class(es) involved; _FindBreakpointEvidenceSpark_, _StructuralVariationDiscoveryPipelineSpark_, when using _XGBoostEvidenceFilter.java_. ### Description; The SV pipeline filters BreakpointEvidence based on BreakpointDensityFilter, or optionally XGBoostEvidenceFilter. The XGBoostEvidenceFilter uses a saved classifier model trained with Python code external to the GATK. This poses two main problems:; 1) The external Python code was designed for proof-of-principle and method development, not maintainability or ease of use. Additionally, GATK users and developers are assumed to be familiar with Java, not necessarily Python.; 2) The external Python code must share heterogeneous data with Java for unit/integration tests (supplying test BreakpointEvidence, expected classifier features, and expected classifier probabilities). Currently this is done via JSON files organized to (invertibly) store Pandas or Numpy objects. The resulting code to load these JSON files in on the Java side is complex.; These problems can be resolved by; 1) Replacing external python code by porting to an **experimental** tool in the GATK.; 2) Replacing JSON files with a serialization strategy currently supported by the GATK (e.g. Kryo). Additional benefits can be obtained by ensuring that the classifier-training subroutines are sufficiently general to speed development for other projects that may want to use boosted decision trees for classification.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4922:982,load,load,982,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4922,1,['load'],['load']
Performance,"## Feature request. ### Tool(s) or class(es) involved; _StructuralVariationDiscoveryPipelineSpark, XGBoostEvidenceFilter.java_. ### Description; Currently there are two unresolved large structural decisions about features for the XGBoostEvidenceFilter classifier. At the moment these decisions are switched by static member booleans, however that results in bad software engineering with one active code path and one inactive code path. The decisions to be made are:; 1. Whether to merge templateSize and readCount; * Yes: avoid NaN properties and decrease the number of columns by 1; * No: properties are easier to understand; 2. Whether to merge overlapping mappability k-mers for the mappability score; * Yes: the property is much easier to understand and explain. This seems like a no-brainer.; * No: unfortunately the currently best-performing classifier was trained on unmerged k-mers. Resolving this issue requires training new classifiers (altering feature design and training approach) in order to come to a definitive decision on these decisions (with the strong hope that decision 2 is to merge overlapping k-mers). Then inactive code paths and boolean switches can be removed. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5041:838,perform,performing,838,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5041,1,['perform'],['performing']
Performance,"## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_; [DepthOfCoverage](https://gatk.broadinstitute.org/hc/en-us/articles/360041851491-DepthOfCoverage-BETA-). ### Description. Are there plans to make DepthOfCoverage multi-threaded? If not, would it be possible to require such improvements?. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7890:260,multi-thread,multi-threaded,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7890,1,['multi-thread'],['multi-threaded']
Performance,"## Feature request. I used gatk4 docker file to perform the germline CNV cohort analysis. I got the individual vcf file for each sample. But I cannot find the tool to combine the CNV, just like combining gvcf in SNV analysis. Is there any such tool that I missed? And, is there any visualization scripts I can use? Thank you so much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5373:48,perform,perform,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5373,1,['perform'],['perform']
Performance,"## Feature request. Related to #6239. I'm interested in performing joint genotyping on a set of given alleles in order to avoid deflating rare variants (previously genotype given alleles, now force call filtered alleles). In particular, I'd like to regenotype on all of the alleles in the input gVCF output by CombineGVCFs, which I'll define in Proposal 1. To be more in line with HaplotypeCaller/Mutect, I've also provided Proposal 2. If this is something you'd be open to having in the GATK, I'd be happy to submit a PR. ### Tool(s) or class(es) involved. #### Proposal 1. If we move the `force-call-filtered-alleles` argument from `AssemblyBasedCallerArgumentCollection` to `GenotypeCalculationArgumentCollection`, this will expose it from `GenotypeGVCFs`. If this argument is true, we use the input alleles for regenotyping in `GenotypeGVCFsEngine` in `genotypingEngine. calculateGenotypes`. #### Proposal 2. If we move the `force-call-filtered-alleles` and `alleles` arguments from `AssemblyBasedCallerArgumentCollection` to `GenotypeCalculationArgumentCollection`, this will expose them from `GenotypeGVCFs`. If provided, the features can then be used for regenotyping in `GenotypeGVCFsEngine` in `genotypingEngine. calculateGenotypes`. ### Description. When performing joint genotyping, the user could tell GenotypeGVCFs to regenotype on a given set of alleles, similar to how they would for HaplotypeCaller. #### Proposal 1. ```; GenotypeGVCFs; --force-call-filtered-alleles true; --input combined.g.vcf; ```. #### Proposal 2. ```; GenotypeGVCFs; --force-call-filtered-alleles true; --alleles rare.combined.g.vcf; --input combined.g.vcf; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6550:56,perform,performing,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6550,2,['perform'],['performing']
Performance,"## Tool(s) or class(es) involved; GenomicsDBImport(v4.1.8.1). ### Description; Hello, I was construct to genomicdb using GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Ser",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7324:1010,optimiz,optimizations,1010,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324,1,['optimiz'],['optimizations']
Performance,"## Update numpy\scipy\pymc3 python package. ### Tool(s) or class(es) involved. python\gcnvkernel; python\vqsr_cnn. ### Description; want to use the newer numpy 1.19.4, but I found that gatk uses conda-force to install the older numpy 1.17.5, and it is not allowed to upgrade numpy because of scipy version restrictions. And scipy cannot be upgraded because of the version limitation of pymc3. I think we should use the new version of the software (in the new version, some bugs are fixed, the performance is better), we need to deal with the difficulties and help the software upgrade.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6978:493,perform,performance,493,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6978,1,['perform'],['performance']
Performance,"### Affected tool(s) or class(es); HaplotypeCallerSpark. gatk HaplotypeCallerSpark -R GRCh38_full_analysis_set_plus_decoy_hla.fa -I GatherBamFiles.bam -O g.vcf.gz. The HaplotypeCaller works, but not HaplotypeCallerSpark.; Tried to use the docker image, and different server; tried to build the newest gatk, same error message. ### Affected version(s); - [ x ] Latest public release version 4.1.8.1; java version ; ```; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12); OpenJDK 64-Bit Server VM (build 25.152-b12, mixed mode); ```. ### Description ; keep get the error message like below. ```; Using GATK jar /cache/home/xc278/p/GATK/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Dsamjdk.compression_level=5 -Xms10G -jar /cache/home/xc278/p/GATK/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCallerSpark -R GRCh38_full_analysis_set_plus_decoy_hla.fa -I SRR1573206.GatherBamFiles.bam -O SRR1573206.g.vcf.gz -G StandardAnnotation -G StandardHCAnnotation -G AS_StandardAnnotation -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -ERC GVCF; 09:38:05.617 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 09:38:05.617 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 09:38:05.655 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cache/home/xc278/p/GATK/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 15, 2020 9:38:05 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 09:38:05.911 INFO HaplotypeCallerSpark -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6750:660,cache,cache,660,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6750,2,['cache'],['cache']
Performance,### Affected tool(s) or class(es); _EstimateDragstrParameters_. ### Affected version(s); - [ ] Latest public release version [version?]; - [X] Latest master branch as of [after PR 6634 has been merged in]. ### Description . Look for usages of ```Utils.runInParallel```. Change those to use Spark instead. There is a possibility of removing multi-threading all together if we change the way we decimate and filter sites.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6876:340,multi-thread,multi-threading,340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6876,1,['multi-thread'],['multi-threading']
Performance,"### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### ate of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; **GenotypeGVCFs stuck at Starting traversal for coulple of days:**. Using GATK jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR4_gvcf_database -G StandardAnnotation -O fat_ALL_MATERIALS_chr4.g.vcf.gz; 11:58:13.194 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:58:14.522 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 20, 2022 11:58:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:58:19.059 INFO GenotypeGVCFs - ------------------------------------------------------------; 11:58:19.060 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.9.0; 11:58:19.060 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:58:19.060 INFO GenotypeGVCFs - Executing as gaoshibin@fat1 on Linux v3.10.0-693.el7.x86_64 amd64; 11:58:19.060 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 11:58:19.060 INFO GenotypeGVCFs - Start Date/Time: 2022520 115813; 11:58:19.060 INFO GenotypeGVCFs - -------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866:2185,Load,Loading,2185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866,1,['Load'],['Loading']
Performance,"### Affected tool(s) or class(es); docker version GATK:4.1.1.0. ### Affected version(s); ; latest release. ### Description ; Funcotator shuts down part way through job. A configuration problem @ google?; [funcotator_crash.txt](https://github.com/broadinstitute/gatk/files/3652568/funcotator_crash.txt). RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: www.googleapis.com; ; ### Description . 04:13:19.667 INFO ProgressMeter - 15:85753672 1834.2 199000 108.5; 04:17:42.593 INFO VcfFuncotationFactory - dbSNP 9606_b150 cache hits/total: 0/0; 04:17:42.593 INFO VcfFuncotationFactory - gnomAD_exome 2.1 cache hits/total: 0/1402; 04:17:42.593 INFO VcfFuncotationFactory - gnomAD_genome 2.1 cache hits/total: 0/162233; 04:17:42.665 INFO Funcotator - Shutting down engine; [September 25, 2019 4:17:42 AM UTC] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 1,845.78 minutes.; Runtime.totalMemory()=4523032576; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: www.googleapis.com; at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:318); at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182:331,concurren,concurrent,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182,5,"['cache', 'concurren']","['cache', 'concurrent']"
Performance,"### Affected tool(s) or class(es); gatk PrintReads. ### Affected version(s); v4.1.4.1. ### Description ; Command like; ```; java -Xms2g -Xmx3g -jar gatk.jar PrintReads --gcs-project-for-requester-pays my-project -R hg38.fa -I gs://some-bucket/data.cram -L loci.interval_list -L UNMAPPED -O data.loci.bam; ```; crashes near the end with this error:; ```. 05:03:04.672 INFO PrintReads - Shutting down engine; [March 1, 2020 5:03:04 AM EST] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 18.22 minutes.; Runtime.totalMemory()=3094872064; htsjdk.samtools.util.RuntimeEOFException: java.nio.channels.ClosedChannelException; 	at htsjdk.samtools.CRAMFileReader.queryUnmapped(CRAMFileReader.java:413); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryUnmapped(SamReader.java:543); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:129); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:111); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:151); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:27); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:13); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6475:892,load,loadNextIterator,892,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6475,1,['load'],['loadNextIterator']
Performance,"### Bug Report. Hi, after installing the conda environment and running `conda activate gatk` without errors, I seem to still have a problem importing the gcnvkernel module. Is there a way I can install it through pip or what is something I may have done wrong? I already went over the README and standard documentation, and don't think I missed a step. ### Affected tool(s) or class(es); gvnvkernel, other expected modules. #### Expected behavior; Generate output file from my VCF. #### Actual behavior; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /home/gamer456148/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar GermlineCNVCaller --input var.vcf --run-mode CASE --contig-ploidy-calls X/prefix-calls --output-prefix regular.vcf --output testfile.vcf; 21:21:12.277 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/gamer456148/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 23, 2020 9:21:12 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:21:12.543 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:21:12.544 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.1.4.1; 21:21:12.544 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:21:12.544 INFO GermlineCNVCaller - Executing as gamer456148@gamer456148-Inspiron-15-7579 on Linux v4.15.0-88-generic amd64; 21:21:12.544 INFO GermlineCNVCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_191-b12; 21:21:12.544 INFO GermlineCNVCaller - Start Date/Time: February 23, 2020 9:21:12 PM EST; 21:21:12.544 INFO GermlineCNVCaller - -------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6467:965,Load,Loading,965,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6467,1,['Load'],['Loading']
Performance,"### Bug Report; When running the StructuralVariationDiscoveryPipelineSpark, I am getting the following error:. ```; java.lang.UnsatisfiedLinkError:`/tmp/jp102/libfml.833188020007107749.jnilib: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/jp102/libfml.833188020007107749.jnilib); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at org.broadinstitute.hellbender.utils.fermi.FermiLiteAssembler.loadNativeLibrary(FermiLiteAssembler.java:157); at org.broadinstitute.hellbender.utils.fermi.FermiLiteAssembler.<init>(FermiLiteAssembler.java:24); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FermiLiteAssemblyHandler.apply(FermiLiteAssemblyHandler.java:72); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FermiLiteAssemblyHandler.apply(FermiLiteAssemblyHandler.java:23); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.sp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5145:340,load,load,340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5145,4,['load'],"['load', 'loadLibrary', 'loadNativeLibrary']"
Performance,"### Instructions. Dear GATK team,. I am encountering an error when using the mutect2 function in ""gatk-4.2.6.1"". Whenever I enable the ""--disable-tool-default-read-filters"" option, I receive a java.lang.ArrayIndexOutOfBoundsException error. Since I need to call SNVs for RNA-seq data, I first split the bam file by chromosome, and then perform markduplicate and splitNcigar in two steps. I found that when I use the bam file obtained after using the splitNcigar function to run mutect2, the same error still occurs, even if I don't disable the -read-filters. Therefore, I suspect that the error may be introduced by splitNcigar. Thus, I tried running mutect2 directly on the bam file after markduplicate. If the --disable-tool-default-read-filters option is not set, the command runs successfully. However, once it is set, the same error occurs. ----. ## Bug Report; [February 28, 2023 10:46:30 AM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2394947584; java.lang.ArrayIndexOutOfBoundsException; at java.lang.System.arraycopy(Native Method); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.applyHardClipBases(ClippingOp.java:216); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.apply(ClippingOp.java:69); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:142); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipLowQualEnds(ReadClipper.java:251); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipLowQualEnds(ReadClipper.java:255); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipLowQualEnds(ReadClipper.java:263); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.finalizeRegion(AssemblyBasedCallerUtils.java:132); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:270); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8224:336,perform,perform,336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8224,1,['perform'],['perform']
Performance,### Instructions; I have run java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx64g -jar /rawdata/software-wes/software/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar MarkDuplicates -I /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.bam --REMOVE_DUPLICATES TRUE --VALIDATION_STRINGENCY SILENT -O /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.rmdup.bam -M /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.rmdup.stat. ----. ## Bug Report; it just returns ; 01:41:21.972 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/rawdata/software-wes/software/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Sep 14 01:41:21 PDT 2023] MarkDuplicates --INPUT /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.bam --OUTPUT /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.rmdup.bam --METRICS_FILE /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.rmdup.stat --REMOVE_DUPLICATES true --VALIDATION_STRINGENCY SILENT --MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP 50000 --MAX_FILE_HANDLES_FOR_READ_ENDS_MAP 8000 --SORTING_COLLECTION_SIZE_RATIO 0.25 --TAG_DUPLICATE_SET_MEMBERS false --REMOVE_SEQUENCING_DUPLICATES false --TAGGING_POLICY DontTag --CLEAR_DT true --DUPLEX_UMI false --ADD_PG_TAG_TO_READS true --ASSUME_SORTED false --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --PROGRAM_RECORD_ID MarkDuplicates --PROGRAM_GROUP_NAME MarkDuplicates --READ_NAME_REGEX <optimized capture of last three ':' separated fields as numeric values> --OPTICAL_DUPLICATE_PIXEL_DISTANC,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8520:795,Load,Loading,795,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8520,1,['Load'],['Loading']
Performance,"### Summary; This user was able to access the GenomicsDB workspace but is having performance issues with SelectVariants. They tried the same command locally and it took less than a minute. Are there any changes with how the user is running SelectVariants to improve the performance?. ### GATK Info; GATK 4.1.9.0; . This request was created from a contribution made by Lucas Taniguti on February 01, 2021 22:41 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360076845511-How-do-I-SelectVariants-from-GenomicsDB-stored-in-GCS-#community\_comment\_360014183291](https://gatk.broadinstitute.org/hc/en-us/community/posts/360076845511-How-do-I-SelectVariants-from-GenomicsDB-stored-in-GCS-#community_comment_360014183291). \--. Thank you, it has started to work with gendb.gs://. But now I think it does not run. I have only one sample stored into the database and I'm selecting only chr20:1-1000000 and it is running for more than 30 minutes. Is it expected?. I'm using a VM from GCE, in the same region as the GCS bucket. Using GATK jar /home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar ; ; ```; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx10g -Xms5g - ; ; jar /home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar SelectVariants -R Homo\_sapiens\_assembly38.fasta -V gendb.gs://mybucket/genomicsdb -L chr20:1-1000000 -O teste. ; ; vcf.gz ; ; 23:01:23.595 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl\_compres ; ; sion.so ; ; 23:01:23.914 INFO SelectVariants - ------------------------------------------------------------ ; ; 23:01:23.915 INFO SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.9.0 ; ; 23:01:23.915 INFO SelectVariants - For support and documentation go to [https://software.bro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7070:81,perform,performance,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7070,2,['perform'],['performance']
Performance,"### code; docker run -it -v /media/sj/14t1:/data2 broadinstitute/gatk-dev:NVSCOREVARIANTS-PREVIEW-SNAPSHOT /bin/bash; ####result; 08:37:42.884 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-NVSCOREVARIANTS-PREVIEW-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 08:37:42.995 INFO NVScoreVariants - ------------------------------------------------------------; 08:37:42.995 INFO NVScoreVariants - The Genome Analysis Toolkit (GATK) vNVSCOREVARIANTS-PREVIEW-SNAPSHOT; 08:37:42.995 INFO NVScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 08:37:42.995 INFO NVScoreVariants - Executing as root@0e48fe56d3ce on Linux v5.15.0-79-generic amd64; 08:37:42.995 INFO NVScoreVariants - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 08:37:42.995 INFO NVScoreVariants - Start Date/Time: August 29, 2023 8:37:42 AM GMT; 08:37:42.995 INFO NVScoreVariants - ------------------------------------------------------------; 08:37:42.995 INFO NVScoreVariants - ------------------------------------------------------------; 08:37:42.996 INFO NVScoreVariants - HTSJDK Version: 3.0.1; 08:37:42.996 INFO NVScoreVariants - Picard Version: 2.27.5; 08:37:42.996 INFO NVScoreVariants - Built for Spark Version: 2.4.5; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:37:42.996 INFO NVScoreVariants - Deflater: IntelDeflater; 08:37:42.996 INFO NVScoreVariants - Inflater: IntelInflater; 08:37:42.996 INFO NVScoreVariants - GCS max retries/reopens: 20; 08:37:42.996 INFO NVScoreVariants - Requester pays: disabled; 08:37:42.996 WARN NVScoreVariants - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8501:170,Load,Loading,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8501,1,['Load'],['Loading']
Performance,"###### | 100%; termcolor-1.1.0 | 8 KB | ########## | 100%; protobuf-3.11.2 | 635 KB | ########## | 100%; keras-applications-1 | 33 KB | ########## | 100%; readline-6.2 | 606 KB | ########## | 100%; libgfortran-ng-7.3.0 | 1006 KB | ########## | 100%; numpy-1.13.3 | 3.1 MB | ########## | 100%; ```. numpy-1.13.3 is corectly installed . but then . ```; Collecting numpy (from biopython==1.70->-r /root/gatk-4.1.4.0/condaenv.g1uyq0ce.requirements.txt (line 1)); Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB); ```. that does . ```; Found existing installation: numpy 1.13.3; Uninstalling numpy-1.13.3:; Successfully uninstalled numpy-1.13.3; ```. this causes ```gatk DetermineGermlineContigPloidy ```; to exit with an error related to numpy.testing.decorators which is deprecated since numpy 1.15.0 see https://docs.scipy.org/doc/numpy-1.15.0/release.html. ```; Deprecations. Aliases of builtin pickle functions are deprecated, in favor of their unaliased pickle.<func> names:; numpy.loads; numpy.core.numeric.load; numpy.core.numeric.loads; numpy.ma.loads, numpy.ma.dumps; numpy.ma.load, numpy.ma.dump - these functions already failed on python 3 when called with a string.; Multidimensional indexing with anything but a tuple is deprecated. This means that the index list in ind = [slice(None), 0]; arr[ind] should be changed to a tuple, e.g., ind = [slice(None), 0]; arr[tuple(ind)] or arr[(slice(None), 0)]. That change is necessary to avoid ambiguity in expressions such as arr[[[0, 1], [0, 1]]], currently interpreted as arr[array([0, 1]), array([0, 1])], that will be interpreted as arr[array([[0, 1], [0, 1]])] in the future.; Imports from the following sub-modules are deprecated, they will be removed at some future date.; numpy.testing.utils; numpy.testing.decorators; numpy.testing.nosetester; numpy.testing.noseclasses; numpy.core.umath_tests; ````. regards. Eric",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6396:1818,load,loads,1818,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6396,5,['load'],"['load', 'loads']"
Performance,"########code:; docker run -it -v /media/sj/14t1:/data2 broadinstitute/gatk:latest. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.4.0.0-local.jar CNNVariantWriteTensors --output-tensor-dir /data2/example/results --reference /data2/example/1/hg19.fa --truth-bed /data2/example/hg19.hybrid.bed --truth-vcf /data2/example/hg19.hybrid.vcf.gz --variant /data2/example/NA12877.vcf.gz. ##########result:; 02:02:31.316 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 02:02:31.342 INFO CNNVariantWriteTensors - ------------------------------------------------------------; 02:02:31.345 INFO CNNVariantWriteTensors - The Genome Analysis Toolkit (GATK) v4.4.0.0; 02:02:31.345 INFO CNNVariantWriteTensors - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:02:31.345 INFO CNNVariantWriteTensors - Executing as root@d768be9a3fc5 on Linux v5.15.0-79-generic amd64; 02:02:31.345 INFO CNNVariantWriteTensors - Java runtime: OpenJDK 64-Bit Server VM v17.0.6+10-Ubuntu-0ubuntu118.04.1; 02:02:31.345 INFO CNNVariantWriteTensors - Start Date/Time: August 30, 2023 at 2:02:31 AM GMT; 02:02:31.345 INFO CNNVariantWriteTensors - ------------------------------------------------------------; 02:02:31.345 INFO CNNVariantWriteTensors - ------------------------------------------------------------; 02:02:31.346 INFO CNNVariantWriteTensors - HTSJDK Version: 3.0.5; 02:02:31.346 INFO CNNVariantWriteTensors - Picard Version: 3.0.0; 02:02:31.346 INFO CNNVariantWriteTensors - Built for Spark Version: 3.3.1; 02:02:31.346 INFO CNNVariantWriteTensors - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 02:02:31.346 INFO CNNVariantWriteTensors - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:02:31.346 INFO CNNVariantWriteTensors - HTSJDK Defaults.USE_A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8506:577,Load,Loading,577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8506,1,['Load'],['Loading']
Performance,"##The CalculateContamination Bug Report. Hello, I have a problem to ask you:. I running this command in the gatk4-4.2.3.0-0:; `gatk CalculateContamination -I gewb.tumor.pileups.table -matched gewb.normal.pileups.table -O gewb.contamination.table`. the following information is displayed:. Using GATK jar /cluster/home/jialu/miniconda3/envs/wes2/share/gatk4-4.2.3.0-0/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /cluster/home/jialu/miniconda3/envs/wes2/share/gatk4-4.2.3.0-0/gatk-package-4.2.3.0-local.jar CalculateContamination -I gewb.tumor.pileups.table -matched gewb.normal.pileups.table -O gewb.contamination.table; 19:10:31.163 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cluster/home/jialu/miniconda3/envs/wes2/share/gatk4-4.2.3.0-0/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 06, 2022 7:10:31 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:10:31.437 INFO CalculateContamination - ------------------------------------------------------------; 19:10:31.437 INFO CalculateContamination - The Genome Analysis Toolkit (GATK) v4.2.3.0; 19:10:31.437 INFO CalculateContamination - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:10:31.438 INFO CalculateContamination - Executing as haojie@node1 on Linux v3.10.0-957.el7.x86_64 amd64; 19:10:31.438 INFO CalculateContamination - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_302-b08; 19:10:31.438 INFO CalculateContamination - Start Date/Time: March 6, 2022 7:10:31 PM CST; 19:10:31.438 INFO CalculateContamination - ------------------------------------------------------------; 19:10:31.438 INFO CalculateContamination - --------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7707:823,Load,Loading,823,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7707,1,['Load'],['Loading']
Performance,"#1629 @akiezun @droazen @lbergelson. Added a substring search to `SWPairwiseAlignment.align`to avoid running the full Smith-Waterman when the query is found in the reference without any indels. The performance benefit of this code will be data dependent. In the current HaplotypeCaller test, >80% of the Smith-Waterman calls are filtered by the substring search. Added tests to cover all of the overhang strategies. **Note:** The substring search only works for the `SOFTCLIP`and `IGNORE`overhang strategies. The `INDEL`and `LEADING_INDEL`can result in more complicated CIGAR strings. See the `SWPairwiseAlignmentUnitTest.testSubstringMatchIndelLong` and `SWPairwiseAlignmentUnitTest.testSubstringMatchLeadingIndelLong` tests for examples.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1677:198,perform,performance,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1677,1,['perform'],['performance']
Performance,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:16949,load,loading,16949,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['load'],['loading']
Performance,"#Bug Report. ### Affected tool(s) or class(es); CNNScoreVariant. ### Affected version(s); 4.0.7.0. ### Description ; For 1,297,033 variant sites, the CNN_1D=-16.118. . #### Steps to reproduce; /run_cnn.sh adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz; Using GATK jar /share/pkg/gatk/4.0.7.0/install/bin/gatk-package-4.0.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /share/pkf/adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O cnn/adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz; 11:29:43.339 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.7.0/install/bin/gatk-package-4.0.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:29:43.467 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.467 INFO CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.0.7.0; 11:29:43.467 INFO CNNScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:29:43.468 INFO CNNScoreVariants - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-696.28.1.el6.x86_64 amd64; 11:29:43.468 INFO CNNScoreVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 11:29:43.469 INFO CNNScoreVariants - Start Date/Time: August 13, 2018 11:29:43 AM UTC; 11:29:43.469 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.469 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.469 INFO CNNScoreVariants - HTSJDK Version: 2.16.0; 11:29:43.469 INFO CNNScoreVariants - Picard Version: 2.18.7; 11:29:43.469 INFO CNNScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:29:43",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5101:836,Load,Loading,836,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5101,1,['Load'],['Loading']
Performance,"$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:132); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCal",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7330,concurren,concurrent,7330,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,"$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:132); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); ```. However, when trying to run the unit tests that failed using commands like:; ```; ./gradlew test --tests VctOutputRendererUnitTest; ```; The same tests will pass. Following the stack trace, I found that several of these failures were because the FeatureManager class threw a GATKException. Per the source code in FeatureManager.java, the exception was thrown because of either an InstantiationException, IllegalAccessException, NoSuchMethodException, or an InvocationTargetException caught when trying to d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6748:5639,concurren,concurrent,5639,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6748,1,['concurren'],['concurrent']
Performance,$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5979#issuecomment-498620174:3959,concurren,concurrent,3959,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5979#issuecomment-498620174,2,['concurren'],['concurrent']
Performance,"$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:8984,concurren,concurrent,8984,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['concurren'],['concurrent']
Performance,$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Opti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:13425,concurren,concurrent,13425,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['concurren'],['concurrent']
Performance,$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Full stacktrace [here](https://console.cloud.google.com/dataproc/jobs/333e650177dc48dd95474c37316a5bf2?organizationId=548622027621&project=broad-dsde-methods&region=global),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:11119,concurren,concurrent,11119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,2,['concurren'],['concurrent']
Performance,% +0.051% ; - Complexity 38899 38963 +64 ; ===============================================; Files 2336 2336 ; Lines 182709 182730 +21 ; Branches 20060 20066 +6 ; ===============================================; + Hits 158213 158325 +112 ; + Misses 17441 17365 -76 ; + Partials 7055 7040 -15 ; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/8074?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage  | |; |---|---|---|; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <45.455%> (-3.801%)` | :arrow_down: |; | [...vqsr/scalable/LabeledVariantAnnotationsWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvTGFiZWxlZFZhcmlhbnRBbm5vdGF0aW9uc1dhbGtlci5qYXZh) | `86.822% <46.154%> (+0.208%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <66.667%> (-2.991%)` | :arrow_down: |; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_sourc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323:1878,scalab,scalable,1878,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323,1,['scalab'],['scalable']
Performance,"' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 4 invalid unit(s) of work during this build to ensure correctness.; Please consult deprecation warnings for more details.; ```; The warnings show up in at least these tasks: gatkTabComplete, installDist, gatkDoc, shadowJar, sparkJar. Seems like it should be easy to fix, I'm not sure how we didn't see them when doing the upgrade.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7625:2092,optimiz,optimizations,2092,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625,1,['optimiz'],['optimizations']
Performance,'java/lang/NoSuchMethodError': java.lang.Object.lambda$comparingInt$7b0bb60$1(Ljava/util/function/ToIntFunction;Ljava/lang/Object;Ljava/lang/Object;)I> (0x000000067b219168) thrown at [/home/buildozer/aports/community/openjdk8/src/icedtea-3.6.; Event: 3.491 Thread 0x00005648765c2000 Exception <a 'java/lang/NoSuchMethodError': java.lang.Object.lambda$thenComparing$36697e65$1(Ljava/util/Comparator;Ljava/lang/Object;Ljava/lang/Object;)I> (0x000000067b220588) thrown at [/home/buildozer/aports/community/openjdk8/src/icedtea-3.6.0/openjdk/. Events (10 events):; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmmOMP; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmmOMP done; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmm; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmm done; Event: 4.326 loading class com/intel/gkl/IntelGKLUtils; Event: 4.326 loading class com/intel/gkl/IntelGKLUtils done; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder done; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder done. Dynamic libraries:; 3c0000000-41b600000 rw-p 00000000 00:00 0 ; 41b600000-66ab00000 ---p 00000000 00:00 0 ; 66ab00000-6aef00000 rw-p 00000000 00:00 0 ; 6aef00000-7c0000000 ---p 00000000 00:00 0 ; 7c0000000-7c0520000 rw-p 00000000 00:00 0 ; 7c0520000-800000000 ---p 00000000 00:00 0 ; 2b5f56cd5000-2b5f56d5e000 r-xp 00000000 07:00 565 /lib/ld-musl-x86_64.so.1; 2b5f56d5e000-2b5f56d60000 ---p 00000000 00:00 0 ; 2b5f56d60000-2b5f56d63000 ---p 00000000 00:00 0 ; 2b5f56d63000-2b5f56e61000 rw-p 00000000 00:00 0 [stack:85483]; 2b5f56e61000-2b5f56e62000 r--p 00000000 00:00 0 ; 2b5f56e62000-2b5f56e63000 rw-p 00000000 00:00 0 ; 2b5f56e63000-2b5f56e6b000 rw-s 00000000 08:01 69704,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:29404,load,loading,29404,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['load'],['loading']
Performance,'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] -----------------------------------------,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4686:1220,Queue,Queue,1220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686,1,['Queue'],['Queue']
Performance,"(1) I am studying GMS mappability scores. To the best of my knowledge, it is the only such analysis that considers both paired-end reads and base calling error rate characteristic of Illumina machines. We could feed the GMS score as a feature file to the coverage collector tool for filtering. (2) I am also working on the ""optimal strategy"" for different SV types. (3) @samuelklee, do we get the same wavy pattern in other samples in the same region? in other words, it is sample-specific or region-specific?. (4) While fragment-based GC correction is difficult (and probably unnecessary) to perform without keeping a full index of aligned reads (like GS), it might be worthwhile to at least collect per-sample per-interval fragment-based average GC content (perhaps along with other summaries such as average fragment length, MQ, etc). It is easy to show that that the difference between full fragment-based GC correction and correction only using the observed average fragment GC content for the pile-up is of the order of the curvature of the GC curve, which is presumably small. We could collect these statistics either on-the-go during coverage collection, or from the sparse counts table as you suggested before (most sensible approach, once we figure out a way to represent sparse tensors).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372413152:593,perform,perform,593,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372413152,1,['perform'],['perform']
Performance,"(90 and counting, ~1% of jobs) which seems to match with the above numbers:. `htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset, for input source: gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/1e300bb3-6990-4342-8959-118826efb3dd/PairedEndSingleSampleWorkflow/3b32519a-f910-49a6-a5fc-b7ec9700d281/call-GatherVCFs/S153-2.g.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:102); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:86); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.loadHeaderFromVCFUri(GenomicsDBImport.java:252); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.initializeHeaderAndSampleMappings(GenomicsDBImport.java:223); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onStartup(GenomicsDBImport.java:202); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:114); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Conne",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:1014,load,loadHeaderFromVCFUri,1014,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['load'],['loadHeaderFromVCFUri']
Performance,"(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935); at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-02-17 16:25:50 INFO MemoryStore:54 - MemoryStore cleared; 2019-02-17 16:25:50 INFO BlockManager:54 - BlockManager stopped; 2019-02-17 16:25:50 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-02-17 16:25:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-02-17 16:25:50 INFO SparkContext:54 - Successfully stopped SparkContext; 16:25:50.893 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 17, 2019 4:25:50 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 5.28 minutes.; Runtime.totalMemory()=505937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:46078,concurren,concurrent,46078,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['concurren'],['concurrent']
Performance,(DefaultTaskExecutionGraph.java:355); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:343); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); Caused by: org.gradle.api.GradleException: Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'; at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:58); at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:31); at org.gradle.api.tasks.javadoc.Javadoc.executeExternalJavadoc(Javadoc.java:158); at org.gradle.api.tasks.javadoc.Javadoc.generate(Javadoc.java:146); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.intern,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716:5748,concurren,concurrent,5748,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716,1,['concurren'],['concurrent']
Performance,(DefaultTaskExecutionGraph.java:355); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:343); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); Caused by: org.gradle.api.GradleException: Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/usr/bin/gatk/build/tmp/gatkDoc/javadoc.options'; at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:58); at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:31); at org.gradle.api.tasks.javadoc.Javadoc.executeExternalJavadoc(Javadoc.java:158); at org.gradle.api.tasks.javadoc.Javadoc.generate(Javadoc.java:146); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.interna,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:4622,concurren,concurrent,4622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,1,['concurren'],['concurrent']
Performance,(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3679:4175,concurren,concurrent,4175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3679,1,['concurren'],['concurrent']
Performance,(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); 17:43:23.161 INFO FeatureManager - Using codec VCFCodec to read file file:///scratch/tmp/spark-ecd63991-68be-4879-b481-68e6789a2004/userFiles-b72d4821-5e36-4d36-aa79-aa6263768669/1000G_phase1.indels.hg19.sites.vcf; 20/01/05 17:43:23 INFO NewHadoopRDD: Input split: file:/panfs/roc/groups/6/clinicalmdl/shared/wgs_exome_v1.0/projects/BT_WGS_Flex_S1/data/exome_dedup_reads.bam:167436615680+33554432; 20/01/05 17:43:23 ERROR Executor: Exception in task 4990.0 in stage 0.0 (TID 4990); java.io.FileNotFoundException: /panfs/roc/groups/6/clinicalmdl/shared/v1.0/projects/BT_WGS_Flex_S1/data/exome_dedup_reads.bam (Too many open files); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.j,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-570992855:4867,concurren,concurrent,4867,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-570992855,1,['concurren'],['concurrent']
Performance,(ReadMetadata.java:431); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata.lambda$new$1dcab782$1(ReadMetadata.java:57); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3462:1788,concurren,concurrent,1788,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3462,1,['concurren'],['concurrent']
Performance,"(added here for easier tracking). In bwajni.c; - [ ] lift `(*env)->GetArrayLength(env,baseArray)` out of loops; - [ ] pass pointers directly not wrapped in classes, eg. BwaIndex could be passed as a pointer; - [ ] cache all fieldIDs, methodIDs and classes; - [ ] pass in data directly without the ShortRead; - [ ] batch multiple calls to align (1 read) ; - [ ] pass in a struct for the native code to fill rather then allocate a new AlnRgn everytime",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1857:214,cache,cache,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1857,1,['cache'],['cache']
Performance,"(also, rebase to pick up the fix for the race condition you see)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1701#issuecomment-211044105:41,race condition,race condition,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1701#issuecomment-211044105,1,['race condition'],['race condition']
Performance,") get rid of the warnings about missing .so files. As an aside, I'm curious whether PowerPC architecture has an instruction; set similar to AVX. This is something I might actually be able to; contribute to the project so I'm excited by the prospect!. -Dan. On Fri, Sep 4, 2020, 11:53 AM R-obert <notifications@github.com> wrote:. > Hello,; >; > I'm trying to use GATK4 (4.1.8.1) on an Ubuntu (16.04) machine. The; > machine is a ""PowerLinux"" machine and I'm guessing that the most relevant; > info for the following problem is that it is a ppc64le system. When I use; > HaplotypeCaller, I see the following messages on the screen:; >; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-b",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:1427,Load,Loading,1427,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['Load'],['Loading']
Performance,)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)** ; **20/03/05 09:28:58 INFO ShutdownHookManager: Shutdown hook called** ; **20/03/05 09:28:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-9e0e0327-45a3-46e8-872a-f5a63c3c7a98** ; **Using GATK jar /mnt/clinix1/Analysis/mongol/phenomata/Tools/Anaconda3/envs/gatk4/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar** ; **Running:** ; **java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx200G -jar /mnt/clinix1/Analysis/mongol/phenomata/Tools/Anaconda3/envs/gatk4/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar PathSeqPipelineSpark --input /clinix1/Analysis/mongol/phenomata/04.GC\_CC/01.Alignment/Aligned/17039\_N.bam --filter-bwa-image /clinix1/Analysis/mongol/phenomata/04.GC\_CC/PathSeq/hg19\_custom/ucsc.hg19.fasta.img --kmer-file /clinix1/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:51522,concurren,concurrent,51522,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,")** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)** ; **20/03/05 09:28:58 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40, localhost, executor driver, partition 40, PROCESS\_LOCAL, 7972 bytes)** ; **20/03/05 09:28:58 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)** ; **20/03/05 09:28:58 WARN TaskSetManager: Lost task 34.0 in stage 0.0 (TID 34, localhost, executor driver): com.esotericsoftware.kryo.KryoException: Buffer underflow.** ; **at com.esotericsoftware.kryo.io.Input.require(Input.java:199)** ; **at com.esotericsoftware.kryo.io.Input.readLong(Input.java:686)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet.<init>(LongHopscotchSet.java:83)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet$Serializer.read(LongHopscotchSet.java:527)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet$Serializer.read(LongHopscotchSet.java:519)** ; **",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:29090,concurren,concurrent,29090,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,")** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **20/03/05 09:28:58 ERROR TaskSetManager: Task 34 in stage 0.0 failed 1 times; aborting job** ; **20/03/05 09:28:58 INFO TaskSchedulerImpl: Cancelling stage 0** ; **20/03/05 09:28:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 0.0 in stage 0.0 (TID 0), reason: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 30.0 in stage 0.0 (TID 30), reason: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 9.0 in stage 0.0 (TID 9), reason: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 1.0 in stage 0.0 (TID 1), reason: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 31.0 in stage 0.0 (TID 31), reason: Stage cancelled** ; **20/03/05 0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:32538,concurren,concurrent,32538,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,")** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **20/03/05 09:28:58 INFO DAGScheduler: Job 0 failed: count at PathSeqPipelineSpark.java:245, took 63.806676 s** ; **20/03/05 09:28:58 INFO SparkUI: Stopped Spark web UI at http://cm132:4040** ; **20/03/05 09:28:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!** ; **20/03/05 09:28:58 INFO NewHadoopRDD: Input split: file:/clinix1/Analysis/mongol/phenomata/04.GC\_CC/01.Alignment/Aligned/17039\_N.bam:1342177280+33554432** ; **20/03/05 09:28:58 INFO MemoryStore: MemoryStore cleared** ; **20/03/05 09:28:58 INFO BlockManager: BlockManager stopped** ; **20/03/05 09:28:58 INFO BlockManagerMaster: BlockManagerMaster stopped** ; **20/03/05 09:28:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!** ; **20/03/05 09:28:58 INFO SparkContext: Successfully stopped SparkContext** ; **09:28:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:41152,concurren,concurrent,41152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)** ; **at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)** ; **at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)** ; **at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at scala.Option.foreach(Option.scala:257)**,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:45579,concurren,concurrent,45579,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,"); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:135); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion wi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2946:11633,concurren,concurrent,11633,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946,1,['concurren'],['concurrent']
Performance,); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at htsjdk.samtools.cram.ref.CRAMLazyReferenceSource.getReferenceBases,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7060:2683,concurren,concurrent,2683,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060,1,['concurren'],['concurrent']
Performance,"); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /local/scratch/rieder/spark-bb59423b-0368-4de5-85e0-e6641fb25380/userFiles-a91d5958-33f5-4685-bf9d-c8fc0924f7c6/Homo_sapiens_assembly38.known_indels.vcf: Too many open files, for input source: /local/scratch/rieder/spark-bb59423b-0368-4de5-85e0-e6641fb25380/userFiles-a91d5958-33f5-4685-bf9d-c8fc0924f7c6/Homo_sapiens_assembly38.known_indels.vcf; at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:263); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:102); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:127); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:121); at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6578:5982,concurren,concurrent,5982,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6578,1,['concurren'],['concurrent']
Performance,"* Added MinGqVariantFilterBase; * * loads VCF, pedigree, UCSC genome tract, and truth data; * * calculates variant overlap with genome tracts; * * forms matrices, tensors, and other helping data for machine learning; * * provides for TRAIN and FILTER modes; * * provides functions for calculating loss given assigned min GQ values; * * computes best estimate of truth data used for training xgboost model; * Added XGBoostMinGqVariantFilter; * * calculates new GQ based on gradient boosting; * Added PropertiesTable for loading VCF properties into tensors; * Added TractOverlapDetector for computing overlap properties with; UCSC genome tracts. Training loss is based on weighted combination of heredity and truth; data, broken down by variant category.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7705:36,load,loads,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7705,2,['load'],"['loading', 'loads']"
Performance,"* Added the possibility of requesting a gridded output intervals set in ProcessIntervals.; * Also added a min-interval-length argument in case we want to skip smallish intervals (e.g.; at the end of contig).; * Some possible performance improvements in filtering bins that only contain Ns (was using a Stream<Byte>, a bit abusive). * IntervalUtils has now a rutine to write a interval file out of an stream of intervals. Works with NIO.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5701:225,perform,performance,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5701,1,['perform'],['performance']
Performance,* Cache the result of getBestAvailableSequenceDictionary instead of calling it on every variant.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6672:2,Cache,Cache,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6672,1,['Cache'],['Cache']
Performance,"* Currently things are in a weird state, picard style interval lists are handled either as tribble files if they are named correctly as .interval_list; If they are named .intervals, .picard, or .list they are loaded with a different code path.; This unifies it so that picard files are only loaded as .interval_list and .intervals is always considered a Gatk style list. * This removes the work around for broken 0 length intervals that was put in place a long time ago. However, the workaround was effectively removed; for all .interval_list files in 4.1.3.0 when we started reading those through the tribble plugin. Either the broken files no longer are used or they; are misnamed as .intervals. * fix tests to deal correctly with .inverval_list vs .intervals",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6465:209,load,loaded,209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6465,2,['load'],['loaded']
Performance,"* Fixing a non-deterministic point in HaplotypeCaller's KBestHaplotypeFinder; * It uses a priority queue to compare scores, if there are ties the tie breaking is arbitrary and seems to be different depending on circumstances of the run.; * For some as of yet unknown reason reading from a gs path vs a local path can cause this to be triggered.; * Adding a tie breaker which uses the entirety of the bases in the Path in cases where the score is tied, this is unique per path.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6104:99,queue,queue,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6104,1,['queue'],['queue']
Performance,"* Modifies gCNV WDLs to improve Cromwell performance when running on a large number of intervals, as in WGS; * Adds optional `disabled_read_filters` input to `CollectCounts`; * Enables GCS streaming for `CollectCounts` and `CollectAllelicCounts`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6607:41,perform,performance,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6607,1,['perform'],['performance']
Performance,"* Should be runnable on-demand using a convenient mechanism (eg., reviewer types a command on a github PR). * Should be robust enough to provide confidence that a substantial change to a stable variant-calling tool is safe to merge. * Should cover performance as well as correctness. * Output may be a report that a human has to read (do not need automated pass/fail). * Implement for `HaplotypeCaller` and CNV tools first (with help of @LeeTL1220), then work with other teams to get test coverage for their tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4630:248,perform,performance,248,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4630,1,['perform'],['performance']
Performance,* Successful run in PMI land that finds nothing to clean up [here](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20Echo%20First%20Look/job_history/ba861882-96ee-4635-b522-2fe9489b0076).; * Successful run in Integration land that finds loads to clean up [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/01667ae7-fd85-4a12-abcb-69e892500fa3).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8644:253,load,loads,253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8644,1,['load'],['loads']
Performance,"* The performance should be fine - TileDB/GenomicsDB stores each field in a separate file (columnar storage) and so adding MIN_DP file to the list of queried fields (~5-10 INFO fields) should be fine.; * One possible source of performance improvement - I was querying the PL field in the sites only query (not producing it in the output VariantContext objects). I think it can be dropped from the query. I was assuming that the PL field would be needed to correctly handle spanning deletions (spanning deletion corresponds to deletion allele with min PL). However, for spanning deletions, all INFO fields are dropped. Hence, any INFO fields that depend on the allele order (allele specific annotations) would be dropped for the spanning deletion. Hence, the exact deletion allele corresponding to the spanning deletion is irrelevant making it possible to drop the PL field from the query as well. Is that correct?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377305568:6,perform,performance,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377305568,2,['perform'],['performance']
Performance,"* There was an ""optimization"" put in place in SelectVariants which accidentally added a quadraticly scaling check on the genotypes.; * This keeps the optimization but makes it linear instead of quadratic on the number of samples.; * On one example with several thousand samples there was a speed up from ~5 minutes to .1 minutes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6729:16,optimiz,optimization,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6729,2,['optimiz'],['optimization']
Performance,"* _Large number of open file handles_: this was an issue in TileDB which got fixed as part of the restructuring that @nalinigans did for supporting HDFS/S3/GCS (#5017). I was too lazy to fix this again. If it's going to take some time for PR #5017 to be merged, I can submit a separate fix for this. This would fix any crashes/termination issues.; * _Performance of a single import process with a large number of intervals_; * Restating the obvious, but this is a single process (and by default, a single thread) with many intervals to import. As you increase the number of samples, this will become a performance pain point.; * More important than the number of intervals is the amount of data imported per interval. Each interval import involves opening the VCF files (loading index structures while creating FeatureReader objects), writing to TileDB/GenomicsDB. and closing the VCF file handles (destroying FeatureReader objects). If the amount of data written for each interval is sufficiently large, the cost of opening/closing the VCF files (creating/destroying FeatureReaders) is small relative to the total time taken.; * In the test cases I and Chris were trying, the amount of data written per interval was small (or 0 in many cases). The time taken in opening/closing the VCF files (and loading/destroying the index) dominates the total time.; * For a single import process (single thread), creating a large interval is better (or no worse) than passing several small intervals. TileDB/GenomicsDB has 0 overhead for regions with no data (for example, WES gVCFs). Having larger intervals will likely avoid issues described above. Hence, an advisory message will be beneficial.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-410576757:602,perform,performance,602,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-410576757,3,"['load', 'perform']","['loading', 'performance']"
Performance,"* added a reference parameter to FeatureData source and FeatureManager methods; * genomicsDB requires a reference, previously this was being passed; through by hardcoding it in the required json files; * json files are now autogenerated by the importer tool, but the; reference wasn't being handled correctly. * updated the various walkers to pass the reference through if available. * gendb:// paths now point to the workspace directory instead of a; directory of jsons. * removed the ability to specify array, vidmap.json, and; callset.json paths in the importer tool since we now rely on the; structure and naming of the files when loading; moved some constants to GenomicsDBConstants. * updated GenomicsDBIntegration tests to use the new importer instead of a; prepackaged and very brittle set of json files. fixed a bug in GenomicsDBImporterIntegrationTests that made both tests; write to the same workspace",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2626:635,load,loading,635,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2626,1,['load'],['loading']
Performance,"************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; [ccastane9@andersserver-01 GenomicsDB]$ bash *_genotype.3.sh; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx16g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 16:27:53.573 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2021 4:27:54 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:27:54.132 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:27:54.133 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 16:27:54.133 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:27:54.143 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 16:27:54.143 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_275-b01; 16:27:54.144 INFO GenotypeGVCFs - Start Date/Time: January 6, 2021 4:27:53 PM CST; 16:27:54.144 INFO GenotypeGVCFs - -------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:4325,Load,Loading,4325,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['Load'],['Loading']
Performance,"**Initial integration of GKL**; - Removed native build related items from `build.gradle`; - Removed native code from src tree; - Refactored `PairHMM.java` and `VectorLoglessPairHMM.java` to use GKL; - Updated `VectorPairHMMUnitTest.java` to use GKL; - Added integration tests to `IntelDeflaterIntegrationTest.java`. **Notes**; - PairHMM has been tested in HaplotypeCaller and GVCF output is md5sum equivalent to the PairHMM currently in GATK; - PairHMM in GKL is still single threaded, but about **_1.4x faster**_ than existing PairHMM, due to fixing a performance issue in the native code; - Next steps are captured in #1903 #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1935:553,perform,performance,553,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1935,1,['perform'],['performance']
Performance,"**The following work has been done:**; - We performed a round of evaluations against XHMM and cn.MOPS on a cohort of 160 samples from SFARI project (which is described in our ASHG poster). For ground truth we used a callset generated from Talkowski lab SV pipeline on matched whole genome samples. Unfortunately, SFARI cohort is not public and cannot be used for public facing evaluations.; - Some hyperparameter tweaking was necessary to achieve good performance. Hyperparameters changed were contained mostly only to `psi_t` parameter.; - We developed a clustering procedure that is based on coverage profile at the set of targets that are highly variable across different capture kits. ; - We found that filtering on a QS metric on a final callset significantly boosted the specificity while lowering sensitivity insignificantly.; - We developed a hyperparameter optimization framework prototype that could be used in a future for general optimizations of cost/performance parameters for all GATK pipelines.; - We resolved several memory issues that came up during validations. **A few issues were encountered along the way:**; - The sensitivity and specificity on multiallellic (common) sites was significantly lower than on rare events.; - Single target calling sensitivity was lower than 20%.; - Pipeline WDL required optimization in order to handle whole genome data, however these changes were not consolidated in the official WDL. **Currently the ongoing work is focused on the following:**; - Improving sensitivity/specificity of calls on common regions. One solution being tested involves setting a prior for common regions derived from a high quality callset. Second solution is to set a different filtering threshold for common regions.; - Consolidating validation scripts to process gCNV output and outputs of competing tools measure their performances against ground truth.; - Analyzing 1000 Genomes exomes, which could be potentially used for public facing automatic evaluations. **The",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-532500502:44,perform,performed,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-532500502,5,"['optimiz', 'perform']","['optimization', 'optimizations', 'performance', 'performed']"
Performance,"**changes in this PR:**; - resolves specops issue #247 - ImportGenomes.wdl takes Array[File] from data table as vcf input; - refactor LoadBigQueryData.wdl back into ImportGenomes; - returns an error if the `bq load` step fails (workflow was silently succeeding when this step failed); - checks existence of tables using `bq show` rather than the csv file - this should still be safe against a race condition because of @ericsong 's refactoring to prevent the `CreateTables` step from being scattered; - run CreateTables at the start (don't wait for CreateImportTsvs); - does NOT use a preemptible VM for the LoadTables step, to minimize (though not eliminate) the possibility of loading a duplicate set of data (see specops issue #248 for further discussion). **testing:**; - these changes were tested in Terra, BQ outputs checked and verified",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7112:134,Load,LoadBigQueryData,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7112,5,"['Load', 'load', 'race condition']","['LoadBigQueryData', 'LoadTables', 'load', 'loading', 'race condition']"
Performance,"+00 --active_class_padding_hybrid_mode=50000 --enable_bias_factors=True --disable_bias_factors_in_active_class=False --p_alt=1.000000e-06 --cnv_coherence_length=1.000000e+04 --max_copy_number=5 --p_active=0.010000 --class_coherence_length=10000.000000 --learning_rate=5.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=100 --max_advi_iter_subsequent_epochs=100 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=2.000000e+00 --num_thermal_epochs=20 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_admixing_rate=7.500000e-01 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:35:09.182 INFO cohort_denoising_calling - Loading 4 read counts file(s)...; 10:35:12.176 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; sample_names. Stderr: Traceback (most recent call last):; File ""/tmp/wujh/cohort_denoising_calling.7794651839449939395.py"", line 114, in <module>; n_st, sample_names, sample_metadata_collection); File ""/opt/NfsDir/BioDir/Anaconda3/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 379, in __init__; sample_metadata_collection, sample_names, self.contig_list); File ""/opt/NfsDir/BioDir/Anaconda3/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 586, in _get_baseline_copy_number_and_read_depth; ""Some samples do not have read depth metadata""; AssertionError: Some samples do not have read depth metadata. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCura",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4457:1972,Load,Loading,1972,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4457,1,['Load'],['Loading']
Performance,+32 ; Lines 146768 147415 +647 ; Branches 16223 16225 +2 ; ================================================; - Hits 127666 55100 -72566 ; - Misses 13189 87388 +74199 ; + Partials 5913 4927 -986; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5732?src=pr&el=tree) | Coverage  | Complexity  | |; |---|---|---|---|; | [...s/copynumber/models/AlleleFractionInitializer.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL21vZGVscy9BbGxlbGVGcmFjdGlvbkluaXRpYWxpemVyLmphdmE=) | `89.063% <> ()` | `17 <0> ()` | :arrow_down: |; | [...r/tools/copynumber/models/AlleleFractionState.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL21vZGVscy9BbGxlbGVGcmFjdGlvblN0YXRlLmphdmE=) | `100% <> ()` | `7 <0> ()` | :arrow_down: |; | [...umber/utils/optimization/PersistenceOptimizer.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL29wdGltaXphdGlvbi9QZXJzaXN0ZW5jZU9wdGltaXplci5qYXZh) | `77.419% <> (-10.753%)` | `24 <0> (-4)` | |; | [...bender/tools/copynumber/models/CopyRatioState.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL21vZGVscy9Db3B5UmF0aW9TdGF0ZS5qYXZh) | `100% <> ()` | `5 <0> ()` | :arrow_down: |; | [...copynumber/utils/segmentation/KernelSegmenter.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL3NlZ21lbnRhdGlvbi9LZXJuZWxTZWdtZW50ZXIuamF2YQ==) | `95.671% <> (-2.164%)` | `45 <0> (-3)` | |; | [...s/copynumber/models/AlleleFractionLikelihoods.java](https://codecov.io/gh/br,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5732#issuecomment-470293496:1595,optimiz,optimization,1595,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5732#issuecomment-470293496,1,['optimiz'],['optimization']
Performance,+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9OYXR1cmFsTG9nVXRpbHMuamF2YQ==) | `77.143% <0.000%> ()` | |; | [...ls/clustering/BayesianGaussianMixtureModeller.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jbHVzdGVyaW5nL0JheWVzaWFuR2F1c3NpYW5NaXh0dXJlTW9kZWxsZXIuamF2YQ==) | `0.000% <0.000%> ()` | |; | [.../tools/walkers/vqsr/scalable/data/VariantType.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9WYXJpYW50VHlwZS5qYXZh) | `60.000% <60.000%> ()` | |; | [.../walkers/vqsr/scalable/SystemCommandUtilsTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU3lzdGVtQ29tbWFuZFV0aWxzVGVzdC5qYXZh) | `60.870% <60.870%> ()` | |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `72.222% <72.222%> ()` | |; | ... and [20 more](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834:4733,scalab,scalable,4733,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834,1,['scalab'],['scalable']
Performance,", et al. Nature. 2022 Jul;607(7920):732-740. doi: 10.1038/s41586-022-04965-x. Epub 2022 Jul 20.PMID: 35859178. On page 69+ of this pdf, they describe the problem and how they cleverly worked around it. ; ; https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-022-04965-x/MediaObjects/41586_2022_4965_MOESM1_ESM.pdf. _It should be noted that running GATK out of the box will cause every job to read the entire; gVCF index file (.tbi) for each of the 150,119 samples. The average size of the index files is ; 4.15MB, so each job would have to read 4.15*150,126 = 623GB of data on top of the actual; gVCF slice data. For 60,000 jobs, this would amount to 623GB*60,000 = 37PB or 25.2GB/sec; of additional read overhead if the jobs are run on 20,000 cores in 17 days. This read; overhead will definitely prevent 20,000 cores from being used simultaneously. However,; this problem was avoided by pre-processing the .tbi files and modifying the software; reading the gVCF files from the central storage in a similar fashion as we did for GraphTyper; and the CRAM index files (.crai)._. This explains why chr1 requires more memory than chr22 despite running on the same number of samples. The larger chr1 tbi index is the source of the memory problem. The Decode solution is too limit the reading of the tbi index to the part that indexes the scattered region. There is a long pause at the beginning of the running GenotypeGVCFs which I never understood. GATK must be the reading of all the sample's gvcfs tbi into memory during that pause. So the reblocking of the gvcfs above reduced the memory foot print by decreasing the tbi size. Decode reduced it by chopping up the index so for each scattered region, GATK could only read a small subset of the index needed for that region. The combination of reblocking and chopping up the tbi would help with the memory requirements even more. However, it is clear that GATK's present reading of the full tbi is not scalable given the memory requirements.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579:2214,scalab,scalable,2214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579,1,['scalab'],['scalable']
Performance,", which didn't work because I hadn't built yet. gatk-launch told me to run `/humgen/gsa-scr1/gauthier/workspaces/gatk/gradlew installDist`, which I did and it threw the following error (sorry for the huge stacktrace, but I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.inter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:1100,cache,caches,1100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['caches']
Performance,", you'll see it's running with Xmx178g. We added 60G to the cluster memory request to leave buffer for the C layer. We're on v4.2.5.0. Does this error look familiar, and/or do you have any troubleshooting suggestions? Thanks in advance. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations. 12:31:14.647 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2022 12:31:14 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:31:14.783 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.783 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 12:31:14.783 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:31:14.784 INFO GenotypeGVCFs - Executing as labkey_submit@exanode-6-25 on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 12:31:14.784 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_60-b27; 12:31:14.784 INFO GenotypeGVCFs - Start Date/Time: February 15, 2022 12:31:14 PM PST; 12:31:14.784 INFO GenotypeGVCFs - --------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674:1421,Load,Loading,1421,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674,1,['Load'],['Loading']
Performance,",67] [info] 1 new workflows fetched by cromid-ca5c695: 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,68] [info] WorkflowManagerActor Starting workflow 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,69] [info] WorkflowManagerActor Successfully started WorkflowActor-968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,69] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2020-07-14 05:09:30,72] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2020-07-14 05:09:30,83] [info] MaterializeWorkflowDescriptorActor [968be82c]: Parsing workflow as WDL 1.0; [2020-07-14 05:09:31,60] [info] MaterializeWorkflowDescriptorActor [968be82c]: Call-to-Backend assignments: ValidateBamsWf.ValidateBAM -> Local; [2020-07-14 05:09:31,82] [warn] Local [968be82c]: Key/s [memory, disks] is/are not supported by backend. Unsupported attributes will not be part of job executions.; [2020-07-14 05:09:35,38] [info] Not triggering log of token queue status. Effective log interval = None; [2020-07-14 05:09:37,15] [info] WorkflowExecutionActor-968be82c-eef3-4bdb-a1ab-3d4e2ca70674 [968be82c]: Starting ValidateBamsWf.ValidateBAM; [2020-07-14 05:09:37,39] [info] Assigned new job execution tokens to the following groups: 968be82c: 1; [2020-07-14 05:09:41,61] [warn] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: Unrecognized runtime attribute keys: disks, memory; [2020-07-14 05:09:41,71] [info] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: /gatk/gatk \; ValidateSamFile \; --INPUT /cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/inputs/-1942028726/test.bam \; --OUTPUT test.validation_.txt \; --MODE SUMMARY; [2020-07-14 05:09:41,76] [info] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: executing: # make sure there is no preexisting Docker CID file; rm -f /gatk/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:4213,queue,queue,4213,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,1,['queue'],['queue']
Performance,"- Built for Spark Version: 2.4.5; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:55:36.415 INFO HaplotypeCaller - Deflater: IntelDeflater; 12:55:36.415 INFO HaplotypeCaller - Inflater: IntelInflater; 12:55:36.415 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:55:36.415 INFO HaplotypeCaller - Requester pays: disabled; 12:55:36.415 INFO HaplotypeCaller - Initializing engine; 12:55:36.508 INFO IntervalArgumentCollection - Processing 1 bp from intervals; 12:55:36.511 INFO HaplotypeCaller - Done initializing engine; 12:55:36.515 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 12:55:36.523 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:55:36.524 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:55:36.552 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:55:36.553 INFO IntelPairHmm - Available threads: 12; 12:55:36.553 INFO IntelPairHmm - Requested threads: 4; 12:55:36.553 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 12:55:36.569 INFO ProgressMeter - Starting traversal; 12:55:36.569 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:55:36.587 INFO HaplotypeCaller - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityAvailableReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7229:2112,Load,Loading,2112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7229,1,['Load'],['Loading']
Performance,"- Create `PairHMMNativeArguments` in HaplotypeCaller and pass to `VectorLoglessPairHMM` in `PairHMM.java`.; - Supply GATK temp directory in `VectorLoglessPairHMM.java`. Currently passing `null`, which uses the system temp directory. ```; final boolean isSupported = new IntelPairHmm().load(null);; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1946:285,load,load,285,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1946,1,['load'],['load']
Performance,"- Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:57:41.739 INFO PathSeqPipelineSpark - Initializing engine; 18:57:41.739 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/03/05 18:57:41 INFO SparkContext: Running Spark version 2.2.0; 18:57:41.968 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18:57:42.155 INFO PathSeqPipelineSpark - Shutting down engine; [5 March, 2019 6:57:42 PM IST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=645922816; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5802:2344,load,load,2344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802,1,['load'],['load']
Performance,"- Fixed a NPE in the `problem` variant case, which is now resolved.; This was due to not filling out the dataSourceName field.; - Tested with b37 gnomAD matching against b37 variants with hg19 data; sources.; - Fixed some issues with the default problem variant annotations.; - Adding in per-data source cache settings.; - Fixing logger in DataSourceUtils. Fixes #5456",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5491:304,cache,cache,304,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5491,1,['cache'],['cache']
Performance,"- I sneaked in another change where I pass in a single file containing a list of input_vcfs instead of an array of input_vcfs. I made this because Terra couldn't save my inputs when I passed in 700 samples.; - Most of the logic was moved into `CreateTables`, including the determination for what files to load. It would have been cleaner to move all of the file loading logic into `LoadTable` but the current approach cuts down the on the number of `gsutil ls` calls made and more importantly, only spins up a shard if there are files to load.; - I pushed the logic into a separate workflow because I wanted to refactor it as two tasks and I couldn't find a way to get a Task to call another Task without wrapping it in a workflow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7056:305,load,load,305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7056,4,"['Load', 'load']","['LoadTable', 'load', 'loading']"
Performance,"- Initializing engine; > 25 15:07:52.848 INFO FeatureManager - Using codec VCFCodec to read file file://ref_nobackup/af-only-gnomad.hg38.vcf.gz; > 26 15:07:53.126 INFO Mutect2 - Done initializing engine; > 27 15:07:53.196 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 28 15:07:53.201 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 29 15:07:53.223 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 30 15:07:53.223 INFO IntelPairHmm - Available threads: 2; > 31 15:07:53.224 INFO IntelPairHmm - Requested threads: 4; > 32 15:07:53.224 WARN IntelPairHmm - Using 2 available threads, but 4 were requested; > 33 15:07:53.224 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; > 34 15:07:53.231 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; > 35 15:07:53.314 INFO ProgressMeter - Starting traversal; > 36 15:07:53.314 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; > 37 15:07:54.410 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 1.8392900000000002E-4; > 38 15:07:54.412 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.03020143; > 39 15:07:54.412 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.05 sec; > 40 15:07:54.413 INFO Mutect2 - Shutting down engine; > 41 [June 19, 2023 at 3:07:54 PM CEST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; > 42 Runtime.totalMemory()=285212672; > 43 java.lang.IndexOutOfBoundsException: Index -1 out of bounds for length 1; > 4",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632:3628,multi-thread,multi-threaded,3628,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632,1,['multi-thread'],['multi-threaded']
Performance,- Intervals specified...; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 21:06:12.479 INFO FeatureManager - Using codec IntervalListCodec to read file file:///paedyl01/disk1/louisshe/work/NGS/wdl/test_workflow_cnv/germline/cromwell-executions/CNVGuts/-947966988/Homo_sapiens_assembly38.bed.preprocessed.filtered.scattered.0154.interval_list; 21:06:12.640 DEBUG FeatureDataSource - Cache statistics for FeatureInput /paedyl01/disk1/louisshe/work/NGS/wdl/test_workflow_cnv/germline/cromwell-executions/CNVGermlineCohort8/Homo_sapiens_assembly38.bed.preprocessed.filtered.scattered.0154.interval_list:/paedyl01/disk1/louisshe/work/NGS/wdl/test_workflow_cnv/germline/cromwell-executions/CNVGermli947966988/Homo_sapiens_assembly38.bed.preprocessed.filtered.scattered.0154.interval_list:; 21:06:12.640 DEBUG FeatureCache - Cache hit rate was 0.00% (0 out of 0 total queries); 21:06:12.645 INFO IntervalArgumentCollection - Processing 4999155 bp from intervals; 21:06:12.656 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 21:06:18.914 WARN GermlineCNVCaller - Sequence dictionary in annotated-intervals file does not match the master sequence dictionary.; 21:06:19.130 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 21:06:19.200 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 21:06:19.200 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 21:07:11.897 DEBUG ScriptExecutor - Executing:; 21:07:11.897 DEBUG ScriptExecutor - python; 21:07:11.897 DEBUG ScriptExecutor - /paedyl01/disk1/louisshe/tmp/gatk/cohort_denoising_calling.418897092082188314.py; 21:07:11.897 DEBUG ScriptExecutor - --ploidy_calls_path=/paedyl01/disk1/louisshe/work/NGS/wdl/test_workflow_cnv/germline/cr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:6427,Cache,Cache,6427,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Cache'],['Cache']
Performance,- Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; 15:41:50.158 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; WARNING 2020-08-19 15:41:50 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 15:41:50.159 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/hg19_All_20180423.vcf.gz -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 15:41:50.159 INFO DataSourceUtils - Setting lookahead cache for data source: dbSNP : 100000; 15:41:50.163 INFO FeatureManager - Using codec VCFCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 15:41:50.277 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/hg19_All_20180423.vcf.gz -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 15:41:50.375 INFO FeatureManager - Using codec VCFCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 15:41:50.490 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/gencode_xhgnc_v75_37.hg19.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/gencode_xhgnc/hg19/gencode_xhgnc_v75_37.hg19.tsv; 15:41:51.07,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6758:15794,cache,cache,15794,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6758,1,['cache'],['cache']
Performance,"- Shutdown hook called; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-69cc5c72-eff6-4259-8b3b-12fa6f8c42b0; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-0bd07e00-4f6d-43bd-b9d2-b1999376c72b; ```. Just to verify, the non-spark version still runs fine with the compressed fasta.... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference GRCh38_full_analysis_set_plus_decoy_hla.fa.gz; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference GRCh38_full_analysis_set_plus_decoy_hla.fa.gz; 13:38:54.168 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:38:55.869 INFO CountReads - ------------------------------------------------------------; 13:38:55.870 INFO CountReads - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:38:55.870 INFO CountReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:38:55.871 INFO CountReads - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:38:55.871 INFO CountReads - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:38:55.871 INFO CountReads - Start Date/Time: January 9, 2019 1:38:54 PM EST; 13:38:55.871 INFO CountReads - ------------------------------------------------------------; 13:38:55.871 INFO CountReads - ------------------------------------------------------------; 13:38:55.872 INFO CountReads - HTSJDK Version: 2.18.1; 13:38:55.873 INFO CountReads - ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:44003,Load,Loading,44003,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['Load'],['Loading']
Performance,"- When querying VCFs, the VcfFuncotationFactory will consider the allele (i.e. Number=""R"" and ""A"") in the output. This allows single-allele queries hitting datasource multiallelic variant contexts to be rendered properly. Closes #4957 ; - Added very simple caching to VCF FuncotationFactory; - VCF Funcotation factory can recognize when alleles are not exactly the same, but equivalent. ; - Fixed small speed bottleneck where Set.equals(...) could be used instead of more complex method. This was happening in validation of funcotation metadata.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4977:409,bottleneck,bottleneck,409,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4977,1,['bottleneck'],['bottleneck']
Performance,- [ ] Sample-specific unexplained variance for all samples + quantile in the cohort; - [ ] HMM log likelihood + quantile in the cohort; - [ ] Bias factor loading quantiles in the cohort. This issues requires calculating and saving a number of summary statistics in the COHORT mode in conjunction with the coverage model parameters bundle.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4060:154,load,loading,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4060,1,['load'],['loading']
Performance,- [ ] make a call to `segment_gcnv_calls.py`; - [ ] generate VCF file from .tsv files generated by `segment_gcnv_calls.py`. The python CLI scripts takes 3 arguments:; - ploidy calls; - model shards; - calls shards. Note:; - gcnvkernel 0.6.0 (PR #4335) now writes the baseline copy number to `baseline_copy_number_t.tsv` in the calls output path (for each sample). `PostprocessGermlineCNVCalls` can simply load this table (without needing to load ploidy calls) to determine the baseline copy-number state. `PostprocessGermlineCNVCalls` could take an additional argument `--allosomal-contigs` to specify sex chromosomes. `PostprocessGermlineCNVCalls` would then set the `REF` copy-number state on sex chromosomes appropriately. It could further take `--ref-autosomal-copy-number` (optional) to allow the users set the appropriate REF autosomal copy-number (for non-homo-sapiens species).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4336:405,load,load,405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4336,2,['load'],['load']
Performance,"- `GvsImportGenomes.wdl` - renamed WDL, fixed issue with poorly returned bq load string; - `GvsCreateFilterSet.wdl` - renamed WDL, added SA support, fixed a lot of issues in ExtractFeatures tool surrounding permissions - BQ projectIDs are now being properly passed through, freq_table UDF defined in repo rather than in BQ; - `GvsPrepareCallset.wdl` (done in previous PR); - `GvsExtractCallset.wdl` - renamed WDL, added SA support; - SA testing README added. all 4 tested with SA in this Terra workspace: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_sa_testing; testing also without SA in this workspace: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_testing_no_sa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7205:76,load,load,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7205,1,['load'],['load']
Performance,- `localization_optional` for indexes localization in `CreateImportTsvs`; - check to make sure there are more than 0 samples to load before going forward in `GetSampleIds`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7574:128,load,load,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7574,1,['load'],['load']
Performance,"- adds a scatter to the `SNPsVariantRecalibrator` call when the number of samples is over a certain threshold; - separates out ""generate tsv/csv files"" and ""load tsv/csv files into BigQuery"" steps of `UploadFilterSetToBQ` into two different tasks, `CreateFilterSetFiles` (scattered) and `UploadFilterSetFilesToBQ` (not). Closes https://github.com/broadinstitute/dsp-spec-ops/issues/326. - [ ] To do before merging: remove change to `.dockstore.yml`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7320:157,load,load,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7320,1,['load'],['load']
Performance,"- adds three new tables: vcf_header_lines_temp, vcf_header_lines, and sample_vcf_header; - populates vcf_header_lines_temp table during CreateVariantIngestFiles (LoadData WDL task); - parses data in vcf_header_lines_temp table in python script and populates vcf_header_lines and sample_vcf_header tables, then cleans up (ProcessVCFHeaders WDL task); - fixed ""bug"" where BigQueryUtils.doRowsExistFor() assumed value was a String that was really an Long, did not work for actual String values; - all behind feature flag (set to false by default) so to not break Beta. Successful run of `GvsJointVariantCalling`: https://job-manager.dsde-prod.broadinstitute.org/jobs/7d5e7b30-7b7c-475c-bf4e-86d6d38cfc8d; Successful run of `GvsQuickstartVcfIntegration`: https://job-manager.dsde-prod.broadinstitute.org/jobs/a17e171e-f8a7-465b-8214-52630f9ec9d1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8321:162,Load,LoadData,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8321,1,['Load'],['LoadData']
Performance,- also added it to the PGEN export WDL to analyze performance easier. test run of filter correction: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/f7de418a-05c7-4720-8dc9-a997b1cfd456; test run of PGEN extract: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/e2b2c875-4cfd-4cbe-879a-18fb91c1518e,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8954:50,perform,performance,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8954,1,['perform'],['performance']
Performance,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - CacheNode keys and tags have their own classes now for code clarity",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3034:124,cache,caches,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3034,3,"['Cache', 'cache']","['CacheNode', 'cache', 'caches']"
Performance,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - keys and tags have their own classes now",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035:124,cache,caches,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035,2,['cache'],"['cache', 'caches']"
Performance,- use aou service account to access gvcf and to load to BQ; - do manual localization with aou service account in create ingest tsv task to minimize vm spin up,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7133:48,load,load,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7133,1,['load'],['load']
Performance,-------------------------------------------; 10:29:22.408 INFO Mutect2 - ------------------------------------------------------------; 10:29:22.409 INFO Mutect2 - HTSJDK Version: 2.21.0; 10:29:22.409 INFO Mutect2 - Picard Version: 2.21.2; 10:29:22.409 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:29:22.409 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:29:22.409 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:29:22.409 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:29:22.409 INFO Mutect2 - Deflater: IntelDeflater; 10:29:22.409 INFO Mutect2 - Inflater: IntelInflater; 10:29:22.409 INFO Mutect2 - GCS max retries/reopens: 20; 10:29:22.409 INFO Mutect2 - Requester pays: disabled; 10:29:22.409 INFO Mutect2 - Initializing engine; 10:29:22.609 INFO IntervalArgumentCollection - Processing 170805979 bp from intervals; 10:29:22.613 INFO Mutect2 - Done initializing engine; 10:29:22.622 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/usr/share/java/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 10:29:22.624 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/usr/share/java/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 10:29:22.625 INFO IntelSmithWaterman - Using CPU-supported AVX-512 instructions; 10:29:22.625 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 10:29:22.631 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/usr/share/java/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 10:29:22.660 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 10:29:22.660 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 10:29:22.660 INFO IntelPairHmm - Available threads: 40; 10:29:22.660 INFO IntelPairHmm - Requested threads: 4; 10:29:22.660 INFO PairHMM - Using the OpenMP multi-threaded AVX-accel,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7032:3341,Load,Loading,3341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7032,1,['Load'],['Loading']
Performance,"-------------------------------------------; 16:26:35.422 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 16:26:35.423 INFO GenotypeGVCFs - Picard Version: 2.22.8; 16:26:35.423 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:26:35.423 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:26:35.426 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:26:35.426 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:26:35.427 INFO GenotypeGVCFs - Deflater: IntelDeflater; 16:26:35.427 INFO GenotypeGVCFs - Inflater: IntelInflater; 16:26:35.427 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 16:26:35.427 INFO GenotypeGVCFs - Requester pays: disabled; 16:26:35.427 INFO GenotypeGVCFs - Initializing engine; 16:26:37.201 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading MBR failed.; 16:26:39.459 INFO GenotypeGVCFs - Shutting down engine; [January 6, 2021 4:26:39 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=2303197184; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; [ccastane9@andersserver-01 GenomicsDB]$ bash *_genotype.3.sh; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx16g -jar /data1/_software/gatk-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:3044,load,load,3044,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['load'],['load']
Performance,"-------------------------------------------; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 16:27:54.145 INFO GenotypeGVCFs - Picard Version: 2.22.8; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:27:54.146 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:27:54.146 INFO GenotypeGVCFs - Deflater: IntelDeflater; 16:27:54.146 INFO GenotypeGVCFs - Inflater: IntelInflater; 16:27:54.146 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 16:27:54.146 INFO GenotypeGVCFs - Requester pays: disabled; 16:27:54.146 INFO GenotypeGVCFs - Initializing engine; 16:27:55.873 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading MBR failed.; 16:27:58.483 INFO GenotypeGVCFs - Shutting down engine; [January 6, 2021 4:27:58 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=2231894016; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Couldn't create GenomicsDBFeatureReader; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:410); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:326); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.initializeDrivingVariants(VariantLocusWalker.java:76); 	at",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:6399,load,load,6399,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['load'],['load']
Performance,"-------------------------------------------; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 21:16:35.498 INFO GenotypeGVCFs - Picard Version: 2.22.8; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:16:35.498 INFO GenotypeGVCFs - Deflater: IntelDeflater; 21:16:35.499 INFO GenotypeGVCFs - Inflater: IntelInflater; 21:16:35.499 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 21:16:35.499 INFO GenotypeGVCFs - Requester pays: disabled; 21:16:35.499 INFO GenotypeGVCFs - Initializing engine; 21:16:36.737 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading MBR failed.; 21:16:38.472 INFO GenotypeGVCFs - Shutting down engine; [January 17, 2021 9:16:38 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=2551709696; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Couldn't create GenomicsDBFeatureReader; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:410); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:326); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.initializeDrivingVariants(VariantLocusWalker.java:76); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839:4512,load,load,4512,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839,1,['load'],['load']
Performance,"---------------------------------------; 13:56:52.187 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 13:56:52.187 INFO GenotypeGVCFs - Picard Version: 2.22.8; 13:56:52.187 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 13:56:52.187 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 13:56:52.187 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:56:52.187 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:56:52.187 INFO GenotypeGVCFs - Deflater: IntelDeflater; 13:56:52.188 INFO GenotypeGVCFs - Inflater: IntelInflater; 13:56:52.188 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 13:56:52.188 INFO GenotypeGVCFs - Requester pays: disabled; 13:56:52.188 INFO GenotypeGVCFs - Initializing engine; 13:56:53.115 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading tile offsets failed.; 13:57:15.762 INFO GenotypeGVCFs - Shutting down engine; [December 21, 2020 1:57:15 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.40 minutes.; Runtime.totalMemory()=2119696384; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Couldn't create GenomicsDBFeatureReader; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:410); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:326); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.initializeDrivingVariants(VariantLocusWalker.java:76",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012:3212,load,load,3212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012,1,['load'],['load']
Performance,"-----------------------------------. funcotator output:. (gatk) root@75181703d894:/gatk# ./gatk Funcotator \; > --variant ./my_data/test_b37.vcf \; > --reference ./my_data/human_g1k_v37.fasta \; > --ref-version hg19 \; > --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s \; > --output ./my_data/variants.funcotated.maf \; > --output-file-format MAF \; > --disable-sequence-dictionary-validation; Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Funcotator --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output ./my_data/variants.funcotated.maf --output-file-format MAF --disable-sequence-dictionary-validation; 12:11:19.732 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 24, 2021 12:11:19 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:11:19.904 INFO Funcotator - ------------------------------------------------------------; 12:11:19.904 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 12:11:19.904 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:11:19.905 INFO Funcotator - Executing as root@75181703d894 on Linux v4.15.0-132-generic amd64; 12:11:19.905 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 12:11:19.905 INFO Funcotator - Start Date/Time: March 24, 2021 12:11:19 PM GMT; 12:11:19.905 INFO Funcotator - ------------------------------------------------------------; 12:11:19.905 INFO Funcot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:2642,Load,Loading,2642,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,1,['Load'],['Loading']
Performance,"------------------------------; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Version: 2.24.1; 10:49:50.814 INFO PathSeqBuildKmers - Picard Version: 2.25.4; 10:49:50.814 INFO PathSeqBuildKmers - Built for Spark Version: 2.4.5; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:49:50.814 INFO PathSeqBuildKmers - Deflater: IntelDeflater; 10:49:50.814 INFO PathSeqBuildKmers - Inflater: IntelInflater; 10:49:50.815 INFO PathSeqBuildKmers - GCS max retries/reopens: 20; 10:49:50.815 INFO PathSeqBuildKmers - Requester pays: disabled; 10:49:50.815 INFO PathSeqBuildKmers - Initializing engine; 10:49:50.815 INFO PathSeqBuildKmers - Done initializing engine; 10:49:50.816 INFO PathSeqBuildKmers - Loading reference kmers...; 10:50:49.423 INFO PSKmerUtils - Generating kmers from 3713370968 bases in 412468 records...; 10:51:03.515 INFO PSKmerUtils - 6.7% complete - 249.0 Mbp at 1060.1 Mbp/min, 3.27 min remaining; 10:51:19.888 INFO PSKmerUtils - 13.2% complete - 491.1 Mbp at 967.3 Mbp/min, 3.33 min remaining; 10:51:35.575 INFO PSKmerUtils - 18.6% complete - 689.4 Mbp at 896.3 Mbp/min, 3.37 min remaining; 10:51:47.982 INFO PSKmerUtils - 23.7% complete - 879.7 Mbp at 901.3 Mbp/min, 3.14 min remaining; 10:52:01.291 INFO PSKmerUtils - 28.6% complete - 1061.2 Mbp at 886.0 Mbp/min, 2.99 min remaining; 10:52:10.127 INFO PSKmerUtils - 33.2% complete - 1232.0 Mbp at 916.0 Mbp/min, 2.71 min remaining; 10:52:21.475 INFO PSKmerUtils - 37.5% complete - 1391.4 Mbp at 906.9 Mbp/min, 2.56 min remaining; 10:52:28.848 INFO PSKmerUtils - 41.4% complete - 1536.5 Mbp at 927.2 Mbp/min, 2.35 min remaining; 10:52:34.246 INFO PSKmerUtils - 45.1% complete - 1674.9 Mbp at 958.7 Mbp/min, 2.13 min remaining;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8204:2573,Load,Loading,2573,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8204,1,['Load'],['Loading']
Performance,---------------; 09:49:05.901 INFO Mutect2 - HTSJDK Version: 2.18.2; 09:49:05.901 INFO Mutect2 - Picard Version: 2.18.25; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:49:05.902 INFO Mutect2 - Deflater: IntelDeflater; 09:49:05.902 INFO Mutect2 - Inflater: IntelInflater; 09:49:05.902 INFO Mutect2 - GCS max retries/reopens: 20; 09:49:05.902 INFO Mutect2 - Requester pays: disabled; 09:49:05.902 INFO Mutect2 - Initializing engine; 09:49:06.887 INFO Mutect2 - Done initializing engine; 09:49:06.935 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/Tools/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:49:06.937 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 09:49:06.937 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 09:49:07.007 INFO ProgressMeter - Starting traversal; 09:49:07.007 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:49:17.023 INFO ProgressMeter - 1:139173 0.2 480 2875.7; 09:49:27.704 INFO ProgressMeter - 1:763661 0.3 2590 7508.3; 09:49:38.001 INFO ProgressMeter - 1:958723 0.5 3290 6369.0; 09:49:49.182 INFO ProgressMeter - 1:981050 0.7 3380 4808.5; 09:50:02.383 INFO ProgressMeter - 1:988991 0.9 3440 3727.3; 09:50:13.586 INFO ProgressMeter - 1:1227096 1.1 4290 3866.1; 09:50:23.594 INFO ProgressMeter - 1:1460850 1.3 5240 4105.1; 09:50:34.165 INFO ProgressMeter - 1:1960541 1.5 7060 4860.1; 09:50:46.537 INFO ProgressMeter - 1:2489135 1.7 8930 5383.3; 09:50:56.541 INFO ProgressMeter - 1:3195743 1.8 11330 6206,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844:1985,multi-thread,multi-threaded,1985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844,1,['multi-thread'],['multi-threaded']
Performance,------------; 12:18:11.388 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - HTSJDK Version: 2.14.3; 12:18:11.388 INFO Mutect2 - Picard Version: 2.17.2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:18:11.388 INFO Mutect2 - Deflater: IntelDeflater; 12:18:11.388 INFO Mutect2 - Inflater: IntelInflater; 12:18:11.389 INFO Mutect2 - GCS max retries/reopens: 20; 12:18:11.389 INFO Mutect2 - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:18:11.389 INFO Mutect2 - Initializing engine; 12:18:11.724 INFO Mutect2 - Done initializing engine; 12:18:12.288 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 12:18:12.290 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 12:18:12.290 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 12:18:12.290 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 12:18:12.368 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:18:12.368 WARN IntelPairHmm - Ignoring request for 4 threads; not using OpenMP implementation; 12:18:12.369 INFO PairHMM - Using the AVX-accelerated native PairHMM implementation; 12:18:12.403 INFO ProgressMeter - Starting traversal; 12:18:12.403 INFO ProgressMeter - Current Locus Elapsed Minute,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665:2650,Load,Loading,2650,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665,1,['Load'],['Loading']
Performance,---------; 13:24:08.557 INFO Mutect2 - HTSJDK Version: 2.23.0; 13:24:08.557 INFO Mutect2 - Picard Version: 2.22.8; 13:24:08.558 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 13:24:08.558 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 13:24:08.558 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:24:08.558 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:24:08.558 INFO Mutect2 - Deflater: IntelDeflater; 13:24:08.558 INFO Mutect2 - Inflater: IntelInflater; 13:24:08.558 INFO Mutect2 - GCS max retries/reopens: 20; 13:24:08.558 INFO Mutect2 - Requester pays: disabled; 13:24:08.558 INFO Mutect2 - Initializing engine; 13:24:09.048 INFO FeatureManager - Using codec VCFCodec to read file file://ref/1000g_pon.hg38.vcf.gz; 13:24:09.207 INFO FeatureManager - Using codec VCFCodec to read file file://ref/af-only-gnomad.hg38.vcf.gz; 13:24:09.374 INFO Mutect2 - Done initializing engine; 13:24:09.435 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 13:24:09.438 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 13:24:09.472 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 13:24:09.472 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 13:24:09.473 INFO IntelPairHmm - Available threads: 24; 13:24:09.473 INFO IntelPairHmm - Requested threads: 4; 13:24:09.473 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 13:24:09.501 INFO ProgressMeter - Starting traversal; 13:24:09.502 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 13:24:19.721 INFO ProgressMeter - chr1:634040 0.2 2460 14443.7; 13:24:29.736 INFO ProgressMeter - chr1:1564703 0.3 7220 21,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6851:3065,Load,Loading,3065,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6851,1,['Load'],['Loading']
Performance,"----. ## Bug Report. ### Affected tool(s) or class(es); - Tool/class name(s), special parameters: GenomicsDBImport. ### Affected version(s); - Version: gatk4-4.4.0.0-0. ### Description ; Hello,. I have been having an issue come up when utilizing `GenomicsDBImport`. This issue has happened when using a range of samples and shard counts (8 - 1000 samples, shard count of up to 2000). My current example is an attempt to joint call 1000 samples together. I will submit the jobs and 1-2 of the shards (of the ~100 concurrently running) will throw a `malloc(): unaligned tcache chunk detected`. When I resubmit that shard, it will usually rerun without a problem. Or if I kill all jobs and resubmit, a different shard will throw the malloc error. . I have run approximately 20 tests and I seem to get this failure 2/3 times. However, it only arises on the initial submission and not when additional jobs are submitted as previous shards complete. Please note that the 1000 samples have successfully been imported into the GenomicsDB but this error seems to persist somewhat randomly across multiple machines. . Thank you for your assistance! . #### Steps to reproduce. - Command used (omitting paths to 1000 samples for brevity) for one of the failed shards. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8g -jar /gpfs/gpfs_de6000/home/dalegre/miniconda3/envs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar GenomicsDBImport -V [samples 1-1002] --genomicsdb-workspace-path results/jointcalling/genomicsDB/temp_0882_of_2000_DB --merge-input-intervals false --bypass-feature-reader --tmp-dir temp --max-num-intervals-to-import-in-parallel 10 --batch-size 50 --intervals results/germline/interval/temp_0882_of_2000/scattered.interval_list --genomicsdb-shared-posixfs-optimizations true; ```. #### Expected behavior; All shards are imported into the GenomicsDB successfu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8683:512,concurren,concurrently,512,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683,1,['concurren'],['concurrently']
Performance,----. ## Bug Report. ### Affected tool(s) or class(es); GATK LiftoverVcf. ### Affected version(s); gatk/4.1.7.0. ### Description . The LiftoverVcf generates the following error. The error occurs with SVs where the INFO/END is not also lifted over. This results in INFO/END before the site start position which triggers the error.; ```; Using GATK jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar LiftoverVcf -I b37/HG002_SVs_Tier1_v0.6.vcf.gz -O b38/HG002_SVs_Tier1_v0.6.hg38.vcf.gz -CHAIN grch37_to_grch38.over.chain.gz --REJECT b38/HG002_SVs_Tier1_v0.6.rejected.vcf.gz -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; 10:20:35.165 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Sun Jul 26 10:20:35 EDT 2020] LiftoverVcf --INPUT b37/HG002_SVs_Tier1_v0.6.vcf.gz --OUTPUT b38/HG002_SVs_Tier1_v0.6.hg38.vcf.gz --CHAIN grch37_to_grch38.over.chain.gz --REJECT b38/HG002_SVs_Tier1_v0.6.rejected.vcf.gz --REFERENCE_SEQUENCE /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --WARN_ON_MISSING_CONTIG false --LOG_FAILED_INTERVALS true --WRITE_ORIGINAL_POSITION false --WRITE_ORIGINAL_ALLELES false --LIFTOVER_MIN_MATCH 1.0 --ALLOW_MISSING_FIELDS_IN_HEADER false --RECOVER_SWAPPED_REF_ALT false --TAGS_TO_REVERSE AF --TAGS_TO_DROP MAX_AF --DISABLE_SORT false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATE,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6725:958,Load,Loading,958,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6725,1,['Load'],['Loading']
Performance,"--master spark://ip-xxx-xx-xx-xxx:xxxx --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true /curr/tianj/gatk/build/libs/gatk-spark.jar MarkDuplicatesSpark -I /curr/tianj/data/sortedbam/xx_sort.bam -M xx.m -O xx_markduplicatespark.bam --TMP_DIR tmp --sparkMaster spark://ip-xxx-xx-xx-xxx:xxxx; 00:48:13.577 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/curr/tianj/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [June 7, 2017 12:48:13 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark --output xx_markduplicatespark.bam --METRICS_FILE xx.m --input /curr/tianj/data/sortedbam/xx_sort.bam --sparkMaster spark://ip-xxx-xx-xx-xxx:xxxx --TMP_DIR tmp --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --READ_NAME_REGEX [a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 7, 2017 12:48:13 AM UTC] Executing as tian",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:1955,Load,Loading,1955,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['Load'],['Loading']
Performance,"-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Using GATK jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms454m -Xmx3181m -XX:+UseSerialGC -jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/mnt/isilon/cbmi/variome/tmp/rathik; 11:49:24.784 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:49:25.130 INFO GenomicsDBImport - ------------------------------------------------------------; 11:49:25.130 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.6.0; 11:49:25.131 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:49:25.134 INFO GenomicsDBImport - Executing as rathik@reslnrefo01.research.chop.edu on Linux v3.10.0-862.el7.x86_64 amd64; 11:49:25.134 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-b10; 11:49:25.134 INFO GenomicsDBImport - Start Date/Time: July 20, 2018 11:49:24 AM EDT; 11:49:25.134 INFO GenomicsDBImport - ------------------------------------------------------------; 11:49:25.134 INFO GenomicsDBImport - ------------------------------------------------------------; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045:1697,Load,Loading,1697,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045,1,['Load'],['Loading']
Performance,"-23590f0ab31f/call-PathSeqAlign/MMRF_2072_2_BM.microbe_aligned.paired.bam:33554432+33554432 20/07/17 09:38:46 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 5) java.util.NoSuchElementException: next on empty iterator at scala.collection.Iterator$$anon$2.next(Iterator.scala:39) at scala.collection.Iterator$$anon$2.next(Iterator.scala:37) at scala.collection.Iterator$$anon$13.next(Iterator.scala:469) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$PeekingImpl.next(Iterators.java:1155) at org.broadinstitute.hellbender.utils.spark.SparkUtils.lambda$putReadsWithTheSameNameInTheSamePartition$7bd206b0$1(SparkUtils.java:190) at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153) at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); `. Looking at the aligned bams that go into the scoring task, they don't appear to be empty or different to the rest of the cohort. Any thoughts?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6709:2024,concurren,concurrent,2024,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6709,2,['concurren'],['concurrent']
Performance,"-31003 queried with: 1:68590-70510; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:769); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:165); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:604); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; 04:37:39.167 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.170 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.170 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.171 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.171 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.171 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.171 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.172 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.172 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.172 INFO GenomicsDBImport - Starting batch inp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5300:5297,concurren,concurrent,5297,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300,1,['concurren'],['concurrent']
Performance,"-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-FilterAlignmentArtifacts/attempt-3/script -> /cromwell_root/script; 2020/07/25 01:38:45 Localization script execution complete.; 2020/07/25 01:38:58 Done localization.; 2020/07/25 01:38:59 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= us.gcr.io/broad-gatk/gatk@sha256:8051adab0ff725e7e9c2af5997680346f3c3799b2df3785dd51d4abdd3da747b /bin/bash /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.6c58e0ba; 01:39:02.909 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 01:39:02.925 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 01:39:02.927 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 01:39:03.142 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 01:39:03.361 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.361 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.1; 01:39:03.361 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:39:03.362 INFO FilterAlignmentArtifacts - Executing as root@3f245e278eba on Linux v4.19.112+ amd64; 01:39:03.362 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 01:39:03.362 INFO FilterAlignmentArtifacts - Start Date/Time: July 25, 2020 1:39:03 AM GMT; 01:39:03.362 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.362 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.363 INFO",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:2358,Load,Loading,2358,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['Load'],['Loading']
Performance,"-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-BenchmarkVCFControlSample/Benchmark/06cbfab4-17a7-4415-9118-d0ebbe156bfd/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""84.26158888888888"",; ""CHM evalHCsystemhours"": ""0.19243055555555555"",; ""CHM evalHCwallclockhours"": ""60.242008333333345"",; ""CHM evalHCwallclockmax"": ""3.176513888888889"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM evalindelF1Score"": ""0.8724"",; ""CHM evalindelPrecision"": ""0.8814"",; ""CHM evalsnpF1Score"": ""0.9784"",; ""CHM evalsnpPrecision"": ""0.9706"",; ""CHM evalsnpRecall"": ""0.9863"",; ""CHM evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-BenchmarkVCFTestSample/Benchmark/362a3e75-6a39-4bde-bb79-e6562dc66dd9/call-CombineSummaries/summary.csv"",; ""EXOME1 controlindelF1Score"": ""0.727"",; ""EXOME1 controlindelPrecision"": ""0.632"",; ""EXOME1 controlsnpF1Score"": ""0.9878"",; ""EXOME1 controlsnpPrecision"": ""0.9815"",; ""EXOME1 controlsnpRecall"": ""0.9941"",; ""EXOME1 controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-EXOME1Sampl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202:18366,cache,cacheCopy,18366,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202,1,['cache'],['cacheCopy']
Performance,"-CompareSAMs not ported because ReadWalker traversal is not suited; for it. -SplitNCigarReads not ported because of the way it uses the reference; (could be ported to ReadWalker with some refactoring, however). There were a few engine changes as well to accomodate the new ReadWalker tools:. -Method to allow walkers to access the SAM header from the reads data source. -No longer require an index for BAM/SAM files when no intervals are; provided and no queries are performed. -onTraversalDone() now allows tools to return a value, which is printed; out by the engine. Resolves #113",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/122:467,perform,performed,467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/122,1,['perform'],['performed']
Performance,"-Created a new base class for Spark tools, GATKSparkTool, that centrally manages; and validates standard tool inputs (reads, reference, and intervals). This allows; us to enforce consistency across tools, delete duplicated boilerplate code from tools; to load inputs, and perform standard kinds of validation (eg., sequence dictionary; validation) in one place. -Tools that don't fit into the pattern established by GATKSparkTool can still extend; SparkCommandLineProgram directly. -This is just a first step -- there is still much work to be done to unify our data source; classes and transparently handle inputs from different sources (GCS, hdfs, files), but; having inputs centrally managed should make the remaining tasks much easier.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/955:255,load,load,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/955,2,"['load', 'perform']","['load', 'perform']"
Performance,"-Created a new class of tool, IntervalWalker, that processes a single interval at a time,; with the ability to query optional overlapping sources of reads, reference data, and/or; features/variants. Current implementation is simple/naive with no special caching;; performance issues will be addressed once we port this traversal type to dataflow. -Added the ability for VariantWalkers to access contextual reads/reference/feature data. -To enable the above changes, migrated most of the engine to use SimpleIntervals rather; than GenomeLocs. This allows for the creation of Context objects in traversals where there; is not necessarily a sequence dictionary available (eg., VariantWalker). -Moved shared arguments/code from Walker classes up into GATKTool. Still some issues; related to marking engine-wide arguments as optional/required on a per-traversal or; per-tool basis, but tickets have been created for these. -Since there isn't yet an htsjdk release that contains SimpleInterval, temporarily; checked a copy of it into our repo, which we can remove the next time we; rev htsjdk. TODOs:. -We currently still require a sequence dictionary to actually parse intervals in; IntervalArgumentCollection. This is due entirely to our support of intervals without; specific stop positions (eg., ""chr1"" and ""chr1:1+"") -- for these intervals we must; look up the stop position in a sequence dictionary. This means that IntervalWalkers; currently require at least one input that contains a sequence dictionary (although; VariantWalkers do not). We should look into ways of relaxing this restriction. Resolves #109",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/297:264,perform,performance,264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/297,1,['perform'],['performance']
Performance,"-Dev, Python, libgomp) and tried running HaplotypeCaller on a sample of mine to test everything. . Unfortunately, it always breaks when running the steps with the JNI libraries. If I start running things on native Java (without the C++ libraries), things work well. . I copied my log file and the command + hs_err file in here. . Anything I missed here?. ``` ; Using GATK jar /usr/bin/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Xmx16g -jar /usr/bin/gatk-package-4.0.0.0-local.jar HaplotypeCaller -I file.bam -R human_g1k_v37.fasta -O test.vcf -ERC GVCF --create-output-variant-index --annotation MappingQualityRankSumTest --annotation QualByDepth --annotation ReadPosRankSumTest --annotation RMSMappingQuality --annotation FisherStrand --annotation Coverage --dbsnp dbsnp_138.b37.vcf --verbosity INFO; 12:18:41.830 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:18:42.092 INFO HaplotypeCaller - ------------------------------------------------------------; 12:18:42.092 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.0.0; 12:18:42.092 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:18:42.092 INFO HaplotypeCaller - Executing as iiipe01@node050 on Linux v3.10.0-693.2.2.el7.x86_64 amd64; 12:18:42.092 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_151-b12; 12:18:42.092 INFO HaplotypeCaller - Start Date/Time: January 15, 2018 12:18:41 PM GMT; 12:18:42.092 INFO HaplotypeCaller - ------------------------------------------------------------; 12:18:42.092 INFO HaplotypeCaller - ------------------------------------------------------------; 12:18:42.093 INFO HaplotypeCaller - HTSJDK Version: 2.13.2; 12:18:42.093 INFO HaplotypeCaller",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:1146,Load,Loading,1146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['Load'],['Loading']
Performance,"-Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar FilterAlignmentArtifacts --reference /home/gatk/references/Sars_cov_2.ASM985889v3.dna_sm.toplevel.fa.gz --variant /data/filteredVCF/in2510-8.orientationFilter.vcf --input /data/rawVCF/mutectBAM/in2510-8.mutect2.bam --bwa-mem-index-image /home/gatk/references/Sars_cov_2.ASM985889v3.dna_sm.toplevel.fa.img --output /data/alignmentArtifactFilteredVCF/in2510-8.orientationFilter.alignmentArtifactFilter.vcf; 08:33:36.572 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_utils.so; 08:33:36.591 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 08:33:36.592 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 08:33:36.826 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 25, 2021 8:33:37 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 08:33:37.130 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 08:33:37.130 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.9.0-SNAPSHOT; 08:33:37.130 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 08:33:37.131 INFO FilterAlignmentArtifacts - Executing as gatk@1ff04a9b2ba9 on Linux v5.4.72-microsoft-standard-WSL2 amd64; 08:33:37.131 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 08:33:37.131 INFO FilterAlignmentArtifacts - Start Date/Time: March 25, 2021 8:33:36 AM GMT; 08:33",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7162:2291,Load,Loading,2291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162,1,['Load'],['Loading']
Performance,"-Ported the remaining pieces of the HaplotypeCaller and assembled them; into a runnable tool. Fixed many bugs in our ported code in the process; of doing so. -Extracted a separate ""engine"" that does all the work of the HC and is; separate from the runnable walker. -Added a new walker class, ReadWindowWalker, as a prospective replacement; for active region traversal. -Hooked up the native VECTOR_LOGLESS_CACHING PairHmm to the HaplotypeCaller,; and activated it by default (this speeds up performance by ~3x in my tests).; Also added a fallback mode when AVX is not present.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1567:491,perform,performance,491,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1567,1,['perform'],['performance']
Performance,"-Published a jbwa snapshot to the Broad artifactory, which we now depend on; via gradle. This snapshot contains builds of the native jbwa code for both; Mac and Linux. -Added utility methods to NativeUtils to load this library at runtime,; and a test proving that it can be loaded successfully. Also switched; to the new NativeUtils methods for loading the PairHMM, and confirmed; that it loads successfully with the HaplotypeCaller in protected. -Included a gradle script to publish a new jbwa snapshot, should it; become necessary, along with instructions on how to do so. Resolves #1838",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1847:209,load,load,209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847,4,['load'],"['load', 'loaded', 'loading', 'loads']"
Performance,"-Reduce memory usage of AssemblyRegion traversal by an order of magnitude; by loading the reads for each shard more lazily. -Add a sharding mode that creates one shard per user interval (or per contig,; if there are no explicit intervals), and make it the default for both HaplotypeCaller; and Mutect2. -When determining active regions, only consider loci within the user's intervals (but; still include surrounding reads in the final region). This mimics GATK3.x behavior. -Serve up empty pileup objects for uncovered loci (this also mimics GATK3.x behavior).; The fact that we weren't doing this before was responsible for much of the remaining; difference vs. the GATK 3.x HaplotypeCaller. -Ported GATK 3 PR 1389 (use median rather than the second-best likelihood for the; NON_REF allele). -Ported a change to the ReferenceConfidenceModel from GATK3. -Fixed a bug in ReadLikelihoods that was causing ArrayIndexOutOfBoundsException. -Added special handling of RawMQ to HaplotypeCaller (mirrors the handling of RawMQ; from GenotypeGVCFs). -Added updated concordance test data generated with HaplotypeCaller 3.8-4-g7b0250253f. Resolves #1950; Resolves #3516; Resolves #3517; Resolves #3518; Resolves #3233; Resolves #2848",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3519:78,load,loading,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3519,1,['load'],['loading']
Performance,"-coded values in master and doing the same via the exposed parameters in this branch have the same effect on a few existing test cases. However, while I'm doing the last three, I wonder if we could run whatever canonical evaluations/optimizations we have to see whether it's worth consolidating some of the parameter sets at this stage? I think there's an argument for having at least two sets (haplotype-to-reference + read-to-haplotype), but I'm not sure how to justify having a separate set for dangling heads/tails. But also not sure which set the latter should be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/presentation/d/1t5WOAEOMp0xAzJgpKbP68BUnclNYfIVRrDSL9wl1-3A/edit?usp=sharing); this entailed running parameter scans using a local Cromwell on my desktop. Probably this optimization work could be redone relatively easily using the Neptune framework put together by @dalessioluca, which was still in development at the time I did this work. Happy to share the resources and scripts I used if we go down this route; they are pretty lightweight. See more discussion starting here: https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566. Alternatively, we could merge this branch to expose the parameters now and punt on consolidating/optimizing them. I'm not compl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:1679,optimiz,optimized,1679,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,1,['optimiz'],['optimized']
Performance,"-dont-trim-active-regions true`:. ```; chr11 6411935 rs3838786 TGCTGGC CGCTGGC,T,<NON_REF> 4029.06 . DB;DP=118;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQandDP=424800,118;REF_BASES=ATGGGCCTGGTGCTGGCGCTG GT:AD:DP:F1R2:F2R1:GQ:PL:SB 1/2:0,62,40,0:102:0,31,23,0:0,31,17,0:99:4046,1646,1982,2435,0,2437,4113,1933,2560,4431:0,0,54,48; ```. and the second one didn't:. ```; chr11 6411935 rs3838786 TGCTGGC T,CGCTGGC,<NON_REF> 2308.64 . BaseQRankSum=-1.312;ClippingRankSum=0.877;DB;DP=119;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=428400,119;REF_BASES=ATGGGCCTGGTGCTGGCGCTG;ReadPosRankSum=0.255 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 0/2:7,0,65,0:72:1,0,34,0:6,0,31,0:99:2316,2364,2996,0,269,1897,2506,2977,1274,3798:1,6,34,31; ```. Note how in the second case, there are two alts in the gVCF, but only one of them has depth!. The only way to recover these cases is to run with `--dont-trim-active-regions`, but that make the HC run approximately 5 times slower, which is obviously not ideal. What I'd like to suggest is that the HC have some automated way to detect when this kind of error is likely to happen or has happened, and work around it. My suggestion(s) would be:. 1. I _think_ this really only happens in repetitive regions. I wonder if it would be possible to have the HC automatically trim active regions when assembly at kmer size 10 works, and disable it when it has to escalate to a higher kmer size? . 2. Trim the active region, but retain the untrimmed active region also. Genotype using the trimmed region. If any allele receives count=0, re-genotype using the untrimmed regions. My thought here is that I think not trimming the active regions really only makes a difference at a small fraction of sites, on the order of 1/1000, but to rescue those sites we have to pay a 5x performance penalty at every site. It would be great if trimming could be auto-disabled at only those sites that are problematic, so we could have our cake and eat it too.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5791:2884,perform,performance,2884,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5791,1,['perform'],['performance']
Performance,"-driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:274); at org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:259); at org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:175); at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentB",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2183:1417,load,loadClass,1417,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183,1,['load'],['loadClass']
Performance,"-g00a40ea-SNAPSHOT-spark.jar CountReadsSpark -I hdfs://arlab174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(Na",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1865,load,load,1865,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,"-initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:20:12.111 INFO case_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 10:20:12.273 INFO root - Loading modeling interval list from the provided model...; 10:20:12.475 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 10:20:12.491 INFO root - The model contains 11901 intervals and 23 contig(s); 10:20:12.491 INFO root - Loading 1 read counts file(s)...; 10:20:12.545 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 10:20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC curve knobs: 20; 10:20:12.555 INFO root - - GC curve prior standard deviation: 1.0; 10:20:12.954 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the denoising model...; 10:20:15.806 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the sampler...; 10:20:15.807 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the copy number caller...; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Global model variables: {'log_me",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8740:6318,Load,Loading,6318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740,1,['Load'],['Loading']
Performance,"-output-prefix {params.prefix} -imr OVERLAPPING_ONLY -O {output.ploidy_calls}`. When solved, {params.files} creates something like ""-I sample1.hdf5 -I sample2.hdf5"" etc... Involved **software versions**:; **gcnvkernel** = 0.7; **gatk** = 4.2.0.0; **Python** = 3.6.10. **Complete log**: . ```; Using GATK jar /software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -jar /software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar DetermineGermlineContigPloidy -L results/cnv/targets.preprocessed.interval_list -I results/cnv/hdf5/MGM20-0848_S4.hdf5 -I results/cnv/hdf5/MGM20-0872_S2.hdf5 -I results/cnv/hdf5/MGM20-1121_S4.hdf5 -I results/cnv/hdf5/MGM20-1543_S10.hdf5 --contig-ploidy-priors resources/contig_ploidy_priors.tsv --output-prefix ploidy -imr OVERLAPPING_ONLY -O results/cnv/ploidy; 15:09:27.326 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 18, 2021 3:09:27 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:09:27.686 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 15:09:27.686 INFO DetermineGermlineContigPloidy - The Genome Analysis Toolkit (GATK) v4.2.0.0; 15:09:27.687 INFO DetermineGermlineContigPloidy - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:09:27.687 INFO DetermineGermlineContigPloidy - Executing as n.liorni@hpc001 on Linux v3.10.0-1127.el7.x86_64 amd64; 15:09:27.687 INFO DetermineGermlineContigPloidy - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_302-b08; 15:09:27.687 INFO DetermineGermlineContigPloidy - Start Date/Time: 18 ottobre 2021 15.09.27 CEST; 15:09:27.68",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:1303,Load,Loading,1303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['Load'],['Loading']
Performance,". After running the given below code, I am not able to find the output file (summary file). This is the link, where the code is given \[[https://gatk.broadinstitute.org/hc/en-us/articles/4405451404699-Concordance#--summary\](/hc/en-us/articles/4405451404699-Concordance#--summary)](https://gatk.broadinstitute.org/hc/en-us/articles/4405451404699-Concordance#--summary](/hc/en-us/articles/4405451404699-Concordance#--summary)). Please also find the log file below. Is the summary file required as input file to run the below script? Please advice. gatk Concordance \\ ; ;  -R /scicore/home/cichon/GROUP/memory\_optimization/data/reference/gch38.fa \\ ; ;  -eval /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/sample1\_affect.filtered.vcf \\ ; ;  --truth /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/NA12878.vcf.gz \\ ; ;  --summary /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/summary.tsv ; 11:26:21.545 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/scicore/soft/apps/GATK/4.2.2.0-foss-2018b-Java-1.8/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 11, 2021 11:26:21 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:26:21.681 INFO Concordance - ------------------------------------------------------------ ; ; 11:26:21.682 INFO Concordance - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 11:26:21.682 INFO Concordance - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:26:21.682 INFO Concordance - Executing as [thirun0000@shi85.cluster.bc2.ch](mailto:thirun0000@shi85.cluster.bc2.ch) on Linux v3.10.0-1062.18.1.el7.x86\_64 amd64 ; ; 11:26:21.682 INFO Concordance - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_212-b03 ; ; 11:26:21.682 INFO Concordance - S",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7562:1601,Load,Loading,1601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7562,1,['Load'],['Loading']
Performance,". CombineGVCFs truns to a error ""Exception in thread ""main"" java.lang.OutOfMemoryError"" .; then I chose to use GenomicsDBImport to do this job. It still doesn't work. First error is ""read_one_line_fully && ""Buffer did not have space to hold a line fully - increase buffer size""; I add ""--genomicsdb-vcf-buffer-size 16384000"" , it causes different error ""Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space"". This is my command and work log.; My java version is ; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12). GATK is very helpful in my research, and I really need some help to get it work. gatk --java-options ""-Xmx48g -Xms48G"" GenomicsDBImport -V C1_sentieon_gvcf.gz .......... -V SCAU-106.gvcf.gz -V SCAU-107.gvcf.gz -V SCAU-108.gvcf.gz -V SCAU-128.gvcf.gz --genomicsdb-workspace-path my_database.chr01 -R IRGSP-1.0_genome.fasta --genomicsdb-vcf-buffer-size 16384000 --intervals chr01. 11:48:08.245 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ayu/anaconda3/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.5.1; 11:48:09.327 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:48:09.327 INFO GenomicsDBImport - Executing as ayu@ayu on Linux v5.15.90.1-microsoft-standard-WSL2 amd64; 11:48:09.327 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 11:48:09.327 INFO GenomicsDBImport - Start Date/Time: November 26, 2023 11:48:08 AM CST; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - H",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8593:1232,Load,Loading,1232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8593,1,['Load'],['Loading']
Performance,. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); 	... 3 more; Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:602); 	... 8 more; Caused by: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:640); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lam,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:2260,concurren,concurrent,2260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,. The exception I get is:; org.apache.commons.math3.exception.TooManyEvaluationsException: illegal state: maximal count (20) exceeded: evaluations; 	at org.apache.commons.math3.optim.BaseOptimizer$MaxEvalCallback.trigger(BaseOptimizer.java:242); 	at org.apache.commons.math3.util.Incrementor.incrementCount(Incrementor.java:155); 	at org.apache.commons.math3.optim.BaseOptimizer.incrementEvaluationCount(BaseOptimizer.java:191); 	at org.apache.commons.math3.optim.univariate.UnivariateOptimizer.computeObjectiveValue(UnivariateOptimizer.java:148); 	at org.apache.commons.math3.optim.univariate.BrentOptimizer.doOptimize(BrentOptimizer.java:225); 	at org.apache.commons.math3.optim.univariate.BrentOptimizer.doOptimize(BrentOptimizer.java:43); 	at org.apache.commons.math3.optim.BaseOptimizer.optimize(BaseOptimizer.java:153); 	at org.apache.commons.math3.optim.univariate.UnivariateOptimizer.optimize(UnivariateOptimizer.java:70); 	at org.broadinstitute.hellbender.utils.OptimizationUtils.max(OptimizationUtils.java:40); 	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationModel.lambda$calculateContamination$13(ContaminationModel.java:214); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationModel.calculateContamination(ContaminationModel.java:215); 	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationModel.<init>(ContaminationModel.java:67); 	at org.broadinstitute.hellbende,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6282:1055,Optimiz,OptimizationUtils,1055,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6282,1,['Optimiz'],['OptimizationUtils']
Performance,"..... . 12:38:08.377 INFO KnownSitesCache - Number of variants read: 60200001; 12:38:09.341 INFO KnownSitesCache - Number of variants read: 60300001; 12:38:10.131 INFO KnownSitesCache - Number of variants read: 60400001; 12:38:10.940 INFO KnownSitesCache - Number of variants read: 60500001; 12:38:11.719 INFO KnownSitesCache - Number of variants read: 60600001; 12:38:12.568 INFO KnownSitesCache - Number of variants read: 60700001; 12:38:13.432 INFO KnownSitesCache - Number of variants read: 60800001; 12:38:14.137 INFO KnownSitesCache - Number of variants read: 60900001; 12:38:14.833 INFO KnownSitesCache - Number of variants read: 61000001; 12:39:23.200 INFO BaseRecalibrationEngine - The covariates being used here: ; 12:39:23.200 INFO BaseRecalibrationEngine - 	ReadGroupCovariate; ```; Based on the time stamps, the observation is that it took 10 min to read the KnowSites.vcf file (about 10GB), for the code which is a filtering and copy operation:. ```java; private static List<GATKVariant> wrapQueryResults(final Iterator<VariantContext> queryResults ) {; final List<GATKVariant> wrappedResults = new ArrayList<>();; long count = 0;; while ( queryResults.hasNext() ) {; if (count++ % 100000 == 0) {; log.info(""Number of variants read: "" + count);; }; wrappedResults.add(VariantContextVariantAdapter.sparkVariantAdapter(queryResults.next()));; }; return wrappedResults;; }. ```; Seems to me this is awfully slow. The vcf file resides on HDFS (with a 100Gbps switch and backed up by a NVMe storage with over 1GBps bandwidth). Assuming we can achieve a persistent 100MBps bandwidth (which IMO is quite modest), it would take 100 sec to fetch the file. Added the overhead, it should take only 2 to 3 minutes to finish this process. If it turns out that IO performance cannot be improved, I have the impression that knownSites file is relatively stable. Would it make sense to convert the KnownSites into an intermediate format which can be read much faster? Or smaller than the 2GB threshold?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4264:2511,perform,performance,2511,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4264,1,['perform'],['performance']
Performance,../coveragemodel/cachemanager/DuplicableNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTkRBcnJheS5qYXZh) | `81.818% <100%> (+38.068%)` | `6 <2> (+2)` | :arrow_up: |; | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9QcmltaXRpdmVDYWNoZU5vZGUuamF2YQ==) | `83.333% <71.429%> (+30.702%)` | `10 <7> (+3)` | :arrow_up: |; | [...er/tools/coveragemodel/cachemanager/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9DYWNoZU5vZGUuamF2YQ==) | `80.645% <76.923%> (+30.645%)` | `9 <8> (+4)` | :arrow_up: |; | [...overagemodel/cachemanager/ComputableCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlQ2FjaGVOb2RlLmphdmE=) | `89.189% <80%> (+32.779%)` | `18 <17> (+2)` | :arrow_up: |; | [...ols/coveragemodel/CoverageModelEMComputeBlock.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL0NvdmVyYWdlTW9kZWxFTUNvbXB1dGVCbG9jay5qYXZh) | `77.617% <82.558%> (-1.61%)` | `49 <2> (-1)` | |; | [...dinstitute/hellbender/utils/MathObjectAsserts.java](https://c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:2910,cache,cachemanager,2910,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,2,"['Cache', 'cache']","['CacheNode', 'cachemanager']"
Performance,".0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; GenotypeGVCFs \; -R ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/1017648146/Homo_sapiens_assembly38.fasta"" \; -O ""$tmp_vcf"" \; -D ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/1017648146/Homo_sapiens_assembly38.dbsnp138.vcf"" \; -G StandardAnnotation \; --only-output-calls-starting-in-intervals \; --use-new-qual-calculator \; -V gendb://""$genomicsdb"" \; -L ""chr18:1-80373285"". ""/share/ClusterShare/software/contrib/evaben/gatk/prebuilt/4.0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; VariantFiltration \; --filter-expression ""ExcessHet > 54.69"" \; --filter-name ExcessHet \; -O ""output.vcf.gz"" \; -V ""$tmp_vcf""; ```. And a SGE hard memory limit of 40G (GenotypeGVCFs has -Xmx8g).; On gatk 4.0.4.0 I see peak memory usage of 15.7G, while with gatk 4.0.6.0 I get:. ```; ...; 19:06:23.757 INFO GenotypeGVCFs - Initializing engine; 19:06:24.785 INFO FeatureManager - Using codec VCFCodec to read file file:///share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e9bf8c5e-3e70-476a-99a2-833f9d38cb2f/call-GenotypeGVCFs/shard-0/inputs/1017648146/Homo_sapiens_assembly38.dbsnp138.vcf; terminate called after throwing an instance of 'std::length_error'; what(): vector::_M_default_append; ```. It seems unlikely to be just a performance regression, maybe something is wrong with my commandline/inputs that only the new version is revealing. This may be in the genomicsdb part of the codebase, as that is the input file I am reading. . [stderr of failure (4.0.6.0) ](https://github.com/broadinstitute/gatk/files/2204252/gengvcferr.txt); [stderr of success (4.0.4.0) ](https://github.com/broadinstitute/gatk/files/2204253/gengvcfgood.txt); [script of failure](https://github.com/broadinstitute/gatk/files/2204254/gengvcfscript.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5024:2240,perform,performance,2240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024,1,['perform'],['performance']
Performance,.018 INFO FeatureManager - Using codec VCFCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/dbsnp/hg38/hg38_All_20170710.vcf.gz; > 12:28:19.213 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/CancerGeneCensus_Table_1_full_2012-03-15.txt -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/cancer_gene_census/hg38/CancerGeneCensus_Table_1_full_2012-03-15.txt; > 12:28:19.227 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/Cosmic.db -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/cosmic/hg38/Cosmic.db; > 12:28:19.401 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/cosmic_tissue.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/cosmic_tissue/hg38/cosmic_tissue.tsv; > 12:28:19.487 INFO DataSourceUtils - Setting lookahead cache for data source: chr1_a_bed : 100000; > 12:28:19.495 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_a_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.tsv; > 12:28:19.500 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.config; > 12:28:19.505 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_a_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.tsv; > 12:28:19.507 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_a_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.tsv; > WARNING 2020-07-21 12:28:19 AsciiLineReader Creating an index,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:13006,cache,cache,13006,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,".0; 10:46:05.153 INFO CNNScoreVariants - Built for Spark Version: 2.4.5; 10:46:05.153 INFO CNNScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:46:05.153 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:46:05.153 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:46:05.153 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:46:05.153 INFO CNNScoreVariants - Deflater: IntelDeflater; 10:46:05.153 INFO CNNScoreVariants - Inflater: IntelInflater; 10:46:05.153 INFO CNNScoreVariants - GCS max retries/reopens: 20; 10:46:05.153 INFO CNNScoreVariants - Requester pays: disabled; 10:46:05.153 INFO CNNScoreVariants - Initializing engine; 10:46:05.598 INFO FeatureManager - Using codec VCFCodec to read file file:///lustre/scratch/scratch/regmova/tmp/TEST_DATA/TR017_GERMLINE_VARIANTS/TR017.GL.vcf.gz; 10:46:05.638 INFO CNNScoreVariants - Done initializing engine; 10:46:05.639 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/lustre/home/regmova/tools/gatk/build/libs/gatk-package-4.2.0.0-19-ge60cdf8-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_utils.so; 10:46:35.436 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:/tmp/1d_cnn_mix_train_full_bn.8208762367402959162.json and weights:/tmp/1d_cnn_mix_train_full_bn.2787226329292768726.hd5; 10:46:35.438 INFO CNNScoreVariants - Done scoring variants with CNN.; 10:46:35.438 INFO CNNScoreVariants - Shutting down engine; [12 May 2021 10:46:35 BST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.51 minutes.; Runtime.totalMemory()=2132279296; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: A nack was received from the Python process (most likely caused by a raised exception caused by): nkm received. ```; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/regmova/miniconda3/envs/gatk/lib/python3.6/site-packages/vq",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7250:3066,Load,Loading,3066,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7250,1,['Load'],['Loading']
Performance,".1.5 or early version as far as we know) when running `FilterAlignmentArtifacts` in one of our cluster but not the other. We narrowed down the issue, using the CPU differences (the working one does not support AVX2), to `libgkl_smithwaterman.so`. Paths are shortened for clarity in the following commands. ```; bash faa.sh ; Using GATK jar /app/gatk-package-4.1.8.0-local.jar; Running:; /bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /app/gatk-package-4.1.8.0-local.jar FilterAlignmentArtifacts -V /output/sample.FilterMutectCalls.vcf.gz -R /db/hs37d5.fa --bwa-mem-index-image /db/hg38.fa.img -I /output/sample.Mutect2.bam -O sample.somatic_filter.test.vcf.gz --use-jdk-inflater true; 19:11:56.929 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 19:11:56.943 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 19:11:56.944 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 19:11:57.168 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 19, 2020 7:11:57 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:11:57.324 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 19:11:57.324 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:11:57.325 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:11:57.325 INFO FilterAlignmentArtifacts - Executing as foo@bar.local on Linux v2.6.32-696.6.3.el6.x86_",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:1086,Load,Loading,1086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['Load'],['Loading']
Performance,".2; 06:47:33.116 INFO ProgressMeter - NC_038255.2:27256247 21.2 122736000 5795288.5; 06:47:42.432 INFO CombineGVCFs - Shutting down engine; [March 13, 2024 at 6:47:42 AM GMT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 21.46 minutes.; Runtime.totalMemory()=920649728; htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Transport endpoint is not connected; at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:48); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:205); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:149); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:24); at org.broadinstitute.hellbender.engine.MultiVariantDataSource$1.next(MultiVariantDataSource.java:408); at org.broadinstitute.hellbender.engine.MultiVariantDataSource$1.next(MultiVariantDataSource.java:393); at htsjdk.samtools.util.PeekableIterator.advance(PeekableIterator.java:71); at htsjdk.samtools.util.PeekableIterator.next(PeekableIterator.java:57); at htsjdk.samtools.util.MergingIterator.next(MergingIterator.java:101); at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1845); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8735:22797,load,loadNextNovelFeature,22797,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735,1,['load'],['loadNextNovelFeature']
Performance,".679 INFO HaplotypeCaller - Requester pays: disabled 13:39:56.680 INFO HaplotypeCaller - Initializing engine 13:39:56.968 INFO HaplotypeCaller - Done initializing engine 13:39:56.971 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled 13:39:57.000 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output 13:39:57.000 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output 13:39:57.020 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so 13:39:57.026 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported 13:39:57.026 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation! 13:39:57.108 INFO ProgressMeter - Starting traversal 13:39:57.110 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute 13:40:07.119 INFO ProgressMeter - chr19:8969701 0.2 29900 179382.1 13:40:17.116 INFO ProgressMeter - chr19:20264701 0.3 67550 202609.5 13:40:27.115 INFO ProgressMeter - chr19:31874701 0.5 106250 212471.7 13:40:37.116 INFO ProgressMeter - chr19:44792701 0.7 149310 223937.0 13:40:49.251 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr19:55910646 and possibly subsequent; at least 10 samples must have called genotypes 13:40:49.413 INFO ProgressMeter - chr19:55910600 0.9 186370 213817.0; 13:40:55.466 INFO HaplotypeCaller - 0 read(s) filtered by: MappingQualityReadFilter; 0 r",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195:13703,multi-thread,multi-threaded,13703,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195,1,['multi-thread'],['multi-threaded']
Performance,".689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4686:1979,Queue,Queue,1979,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686,1,['Queue'],['Queue']
Performance,".Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:40 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:41 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:42 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:43 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:44 INFO yarn.Client: Application report for application_1603353714322_0004 (state: FAILED); 20/10/22 12:02:44 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: Application application_1603353714322_0004 failed 2 times due to AM Container for appattempt_1603353714322_0004_000002 exited with exitCode: 13; For more detailed output, check application tracking page:http://jacky:8088/cluster/app/application_1603353714322_0004Then, click on links to logs of each attempt.; Diagnostics: Exception from container-launch.; Container id: container_1603353714322_0004_02_000001; Exit code: 13; Stack trace: ExitCodeException exitCode=13: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:545); 	at org.apache.hadoop.util.Shell.run(Shell.java:456); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:722); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:211); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6906:6625,concurren,concurrent,6625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6906,3,['concurren'],['concurrent']
Performance,.ExceptionInInitializerError; 	at org.genomicsdb.GenomicsDBUtils.createTileDBWorkspace(GenomicsDBUtils.java:37); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.overwriteCreateOrCheckWorkspace(GenomicsDBImport.java:883); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onTraversalStart(GenomicsDBImport.java:605); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: org.genomicsdb.exception.GenomicsDBException: Could not load genomicsdb native library; 	at org.genomicsdb.GenomicsDBUtilsJni.<clinit>(GenomicsDBUtilsJni.java:33); 	... 10 more; Caused by: java.lang.UnsatisfiedLinkError: /tmp/libtiledbgenomicsdb8918780584607909502.so: libcurl.so.4: cannot open shared object file: No such file or directory; 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at org.genomicsdb.GenomicsDBLibLoader.loadLibraryFromJar(GenomicsDBLibLoader.java:147); 	at org.genomicsdb.GenomicsDBLibLoader.loadLibrary(GenomicsDBLibLoader.java:47); 	at org.genomicsdb.GenomicsDBUtilsJni.<clinit>(GenomicsDBUtilsJni.java:30); 	... 10 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24378/error-while-running-genomicsdbimport/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6122:1961,load,load,1961,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6122,6,['load'],"['load', 'loadLibrary', 'loadLibraryFromJar']"
Performance,.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$Def,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:14575,cache,cache,14575,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['cache']
Performance,.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:7750,cache,cache,7750,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['cache']
Performance,.ReadStateManager.collectPendingReads(ReadStateManager.java:159); 	at org.broadinstitute.hellbender.utils.locusiterator.LocusIteratorByState.lazyLoadNextAlignmentContext(LocusIteratorByState.java:315); 	at org.broadinstitute.hellbender.utils.locusiterator.LocusIteratorByState.hasNext(LocusIteratorByState.java:252); 	at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.advanceAlignmentContext(IntervalAlignmentContextIterator.java:104); 	at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.advanceAlignmentContextToCurrentInterval(IntervalAlignmentContextIterator.java:99); 	at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.next(IntervalAlignmentContextIterator.java:69); 	at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.next(IntervalAlignmentContextIterator.java:21); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:143); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:135); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665:5432,load,loadNextAssemblyRegion,5432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665,1,['load'],['loadNextAssemblyRegion']
Performance,.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ```; `,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:45293,concurren,concurrent,45293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,2,['concurren'],['concurrent']
Performance,.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:159); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4648:2310,concurren,concurrent,2310,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648,2,['concurren'],['concurrent']
Performance,.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:2173,concurren,concurrent,2173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575,2,['concurren'],['concurrent']
Performance,".WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:24); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:24); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```. for multi sample:; ```; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: Unrecognized token on line 31, column 50:. <title>gatk/mutect2_multi_sample.wdl at master ? broadinstitute/gatk ? GitHub</title>; ^; 	at wdl.draft2.parser.WdlParser.unrecognized_token(WdlParser.java:6975); 	at wdl.draft2.parser.WdlParser.lex(WdlParser.java:7048); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:263); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:170); 	at scala.util.Try$.apply(Try.scala:213); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:630); 	at womtool.graph.GraphPrint$.generateWorkflowDigraph(GraphPrint.scala:19); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:131); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:162); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:167); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:24); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6261:2333,load,load,2333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6261,1,['load'],['load']
Performance,".apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 15/07/14 13:14:53 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected; at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getOutputFile(TemplatedTextOutputFormat.java:50); at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getDefaultWorkFile(TemplatedTextOutputFormat.java:46); at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:995); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 15/07/14 13:14:53 ERROR scheduler.TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 15/07/14 13:14:53 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool; 15/07/14 13:14:53 INFO scheduler.TaskSchedulerImpl: Cancelling stage 1; 15/07/14 13:14:53 INFO scheduler.DAGScheduler: Stage 1 (saveAsNewAPIHadoopFile at TransformTranslator.java:432) failed in 0.155 s; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713:31707,concurren,concurrent,31707,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713,2,['concurren'],['concurrent']
Performance,".b37.NA12878.20.21.bam`. ```; [May 18, 2016 5:10:55 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=318242816; java.net.BindException: Failed to bind to: /10.1.5.39:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. we should make it not fail or fail more gracefully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1839:1443,concurren,concurrent,1443,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1839,5,['concurren'],['concurrent']
Performance,".csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9843"",; ""NIST controlindelPrecision"": ""0.9895"",; ""NIST controlsnpF1Score"": ""0.9908"",; ""NIST controlsnpPrecision"": ""0.992"",; ""NIST controlsnpRecall"": ""0.9896"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-BenchmarkVCFControlSample/Benchmark/eaf4d582-e197-4e13-8122-5e1ec22591ae/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""73.06777222222223"",; ""NIST evalHCsystemhours"": ""0.1622555555555555"",; ""NIST evalHCwallclockhours"": ""46.65241388888888"",; ""NIST evalHCwallclockmax"": ""2.7461055555555554"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9843"",; ""NIST evalindelPrecision"": ""0.9895"",; ""NIST evalsnpF1Score"": ""0.9908"",; ""NIST evalsnpPrecision"": ""0.992"",; ""NIST evalsnpRecall"": ""0.9896"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-BenchmarkVCFTestSample/Benchmark/87985440-93fa-4a33-ac09-e4cbead32bfb/call-CombineSummaries/summary.csv""; }; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494:14468,cache,cacheCopy,14468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494,1,['cache'],['cacheCopy']
Performance,".driver.maxResultSize=0,spark.driver.userClassPathFirst=true,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true ,spark.executor.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true --jar /Users/markw/IdeaProjects/gatk/build/libs/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar -- PrintReadsSpark -I gs://mw-pathseq-test/hs37d5cs.reads.sorted.bam -O hs37d5cs.reads.txt --apiKey XXXXXXXXXXXXXXXXXXXXX --verbosity DEBUG --sparkMaster yarn; Copying file:///Users/markw/IdeaProjects/gatk/build/libs/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar [Content-Type=application/java-archive]...; - [1 files][ 95.3 MiB/ 95.3 MiB] 9.0 MiB/s; Operation completed over 1 objects/95.3 MiB.; Job [5b3d4225-0547-4aa9-8a83-ab26460aa2d2] submitted.; Waiting for job output...; 21:42:45.768 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/tmp/5b3d4225-0547-4aa9-8a83-ab26460aa2d2/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 21:42:45.791 DEBUG IntelGKLUtils - Extracted Intel GKL to /tmp/root/libgkl_compression6493251482684327282.so. 21:42:45.792 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [February 6, 2017 9:42:45 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output hs37d5cs.reads.txt --input gs://mw-pathseq-test/hs37d5cs.reads.sorted.bam --apiKey XXXXXXXXXXXXXXXX --sparkMaster yarn --verbosity DEBUG --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [February 6, 2017 9:42:45 PM UTC] Executing as root@mw-test-m on Linux 3.16.0-4-amd64 amd64; OpenJDK ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277823929:2301,load,load,2301,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277823929,1,['load'],['load']
Performance,".exome.cram \; > -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; > -readIndex gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.crai \; > -O HG00190_cram.bam \; > -L chr17; Using GATK jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar PrintReads -I gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa -readIndex gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.crai -O HG00190_cram.bam -L chr17; 15:00:28.170 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; [October 5, 2017 3:00:28 PM EDT] PrintReads --output HG00190_cram.bam --intervals chr17 --input gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram --readIndex gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.crai --reference /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:21367,Load,Loading,21367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['Load'],['Loading']
Performance,.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:17459,concurren,concurrent,17459,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['concurren'],['concurrent']
Performance,".gz --tmp-dir=/tmp --sample-ploidy 24 -L chrom2; ```. It failed at the same region it was failing before, with this error message:. ```; 01:15:27.623 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),8476.664214527651,Cpu time(s),8391.206707930733; [January 14, 2020 1:15:30 AM BRT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 279.78 minutes.; Runtime.totalMemory()=16865820672; htsjdk.tribble.TribbleException: Invalid block size -122708061; at htsjdk.variant.bcf2.BCF2Decoder.readNextBlock(BCF2Decoder.java:66); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:134); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:181); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:49); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490); at org.broadinsti",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574113941:1697,load,loadNextFeature,1697,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574113941,1,['load'],['loadNextFeature']
Performance,.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24); at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96); at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:38); at org.gradle.internal.execution.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:9183,Cache,CacheStep,9183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Cache'],['CacheStep']
Performance,.java:91); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:144); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObj,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545:2088,concurren,concurrent,2088,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545,2,['concurren'],['concurrent']
Performance,.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Existing mirrorFile and resourceId don't match isDirectory status! '/hadoop_gcs_connector_metadata_cache/hellbender/test/output/gatk4-spark/recalibrated.bam' (dir: 'false') vs 'gs://hellbender/test/output/gatk4-spark/recalibrated.bam/' (dir: 'true'); 	at com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache.getCacheEntryInternal(FileSystemBackedDirectoryListCache.java:198); 	at com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache.putResourceId(FileSystemBackedDirectoryListCache.java:363); 	at com.google.cloud.hadoop.gcsio.CacheSupplementedGoogleCloudStorage.createEmptyObjects(CacheSupplementedGoogleCloudStorage.java:150); 	at com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:578); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.mkdirs(GoogleHadoopFileSystemBase.java:1372); 	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1881); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:313); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1150); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1078); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1078); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271419191:2473,Cache,CacheSupplementedGoogleCloudStorage,2473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271419191,1,['Cache'],['CacheSupplementedGoogleCloudStorage']
Performance,.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594); at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183); Caused by: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at htsjdk.samtools.cram.ref.ReferenceSource.getReferenceBasesByRegion(ReferenceSource.java:207); at htsjdk.samtools.cram.build.CRAMReferenceRegion.fetchReferenceBasesByRegion(CRAMReferenceRegion.java:169); at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:502); at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.getNextRecord(CRAMFileReader.java:589); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.initial,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139:8636,concurren,concurrent,8636,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139,1,['concurren'],['concurrent']
Performance,".reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); 	at org.broadinstitute.hellbender.tools.walkers.haplo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7699,concurren,concurrent,7699,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,".reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); ```. However, when trying to run the unit tests that failed using commands like:; ```; ./gradlew test --tests VctOutputRendererUnitTest; ```; The same tests will pass. Following the stack trace, I found that several of these failures were because the FeatureManager class threw a GATKException. Per the source code in FeatureManager.java, the exception was thrown because of either an InstantiationException, IllegalAccessException, NoSuchMethodException, or an InvocationTargetException caught when trying to determine candidate codecs for reading a VCF file. The unit test files FeatureDataSourceUnitTest and FeatureManagerUnitTest pass when running the unit tests all at once, and also pass individually. The test files correctly generate under appropriate directories under src/test/resources, as far as I can tell. . Attached is a zip archive of the test results:; [test_resu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6748:6008,concurren,concurrent,6008,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6748,1,['concurren'],['concurrent']
Performance,".reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); 	at org.broadinstitute.hellbender.tools.walkers.hapl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7519,concurren,concurrent,7519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,".reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); ```. However, when trying to run the unit tests that failed using commands like:; ```; ./gradlew test --tests VctOutputRendererUnitTest; ```; The same tests will pass. Following the stack trace, I found that several of these failures were because the FeatureManager class threw a GATKException. Per the source code in FeatureManager.java, the exception was thrown because of either an InstantiationException, IllegalAccessException, NoSuchMethodException, or an InvocationTargetException caught when trying to determine candidate codecs for reading a VCF file. The unit test files FeatureDataSourceUnitTest and FeatureManagerUnitTest pass when running the unit tests all at once, and also pass indivi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6748:5828,concurren,concurrent,5828,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6748,1,['concurren'],['concurrent']
Performance,".samtools.metrics.MetricsFile.read(MetricsFile.java:356); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalStart(FilterByOrientationBias.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:777); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); Caused by: java.lang.ClassNotFoundException: gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at htsjdk.samtools.metrics.MetricsFile.loadClass(MetricsFile.java:471); 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:353); 	... 8 more; ```. If it is replaced, the tool still errors but with a different error:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030:1994,load,loadClass,1994,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030,1,['load'],['loadClass']
Performance,.scala:1137); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 19/03/26 20:02:39 INFO ShutdownHookManager: Shutdown hook called; 19/03/26 20:02:39 INFO ShutdownHookManager: Deleting directory /docker/working/7dd5e9aa-fa24-45ca-9979-13623c0ff8d5/a0d4bfdf-66b4-47af-b002-3c3935a7b633/spark-44911f4d-4d54-42b0-b6d1-35614170c1fc; Using GATK jar /docker/reference/Apps/GATK/4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx10876M -Djava.io.tmpdir=./ -jar /docker/reference/Apps/GATK/4.1.0.0/gatk-package-4.1.0.0-local.jar BaseRecalibratorSpark --spark-master local[8] -R /docker/reference/Data/B37/GATKBundle/2.8_subset_arup_v0.1/human_g1k_v37_decoy_phiXAdaptr.fasta -I bam/rmdup_gatkrealign.bam -O gatk_base_recal_table.txt --known-sites /docker/reference/Data/B37/DbSNP/dbSNP_147_20160601/All_,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854:7113,concurren,concurrent,7113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854,1,['concurren'],['concurrent']
Performance,.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1798951329-10.128.1.77-1564169124618:blk_1073741844_1020 file=/reference/Homo_sapiens_assembly38.fasta; 	at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1085); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1068); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1047); 	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655); 	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:949); 	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1004); 	at java.io.DataInputStream.read(DataInputStream.java:149); 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6064:3584,concurren,concurrent,3584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6064,1,['concurren'],['concurrent']
Performance,.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3228,load,loadClass,3228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['load'],['loadClass']
Performance,.socketRead0; 1.7% 0 + 266 sun.nio.ch.EPollArrayWrapper.epollWait; 0.7% 0 + 99 sun.nio.ch.NativeThread.current; 0.6% 0 + 96 java.util.zip.Deflater.reset; 0.6% 0 + 88 java.util.zip.Inflater.reset; 0.4% 0 + 58 org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray; 0.3% 0 + 47 java.io.FileInputStream.readBytes; 0.3% 0 + 41 sun.nio.ch.FileDispatcherImpl.read0; 0.2% 0 + 26 java.lang.Throwable.fillInStackTrace; 0.1% 0 + 19 java.io.UnixFileSystem.getLength; 0.1% 0 + 13 java.lang.Object.getClass; 0.1% 0 + 12 java.lang.Object.hashCode; 0.1% 11 + 0 java.lang.ClassLoader.defineClass1; 0.1% 3 + 7 java.lang.Class.forName0; 0.1% 0 + 9 sun.nio.ch.FileDispatcherImpl.size0; 0.1% 0 + 9 java.util.zip.ZipFile.getEntry; 0.1% 0 + 8 java.lang.Class.isArray; 0.0% 0 + 7 java.io.FileOutputStream.open0; 0.0% 0 + 6 java.lang.Class.isPrimitive; 0.0% 0 + 6 java.io.FileOutputStream.close0; 73.9% 16 + 11236 Total stub (including elided). Thread-local ticks:; 60.2% 23027 Blocked (of total); 0.0% 1 Class loader; 0.0% 1 Unknown: thread_state; ```. and on igzip:. ```; Flat profile of 425.43 secs (38916 total ticks): Executor task launch worker-4. Interpreted + native Method ; 0.1% 0 + 23 java.net.Inet6AddressImpl.lookupAllHostAddr; 0.1% 0 + 16 java.io.UnixFileSystem.delete0; 0.1% 14 + 0 org.apache.spark.util.collection.TimSort.sort; 0.1% 10 + 0 org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp; 0.1% 10 + 0 htsjdk.samtools.BAMRecordCodec.encode; 0.0% 0 + 5 java.net.SocketInputStream.socketRead0; 0.0% 5 + 0 org.apache.spark.util.collection.TimSort$SortState.mergeHi; 0.0% 0 + 3 java.net.Inet6AddressImpl.getHostByAddr; 0.0% 3 + 0 org.apache.spark.util.collection.ExternalSorter.insertAll; 0.0% 0 + 2 htsjdk.samtools.util.zip.IntelDeflater.deflateBytes; 0.0% 2 + 0 sun.misc.Unsafe.defineClass; 0.0% 2 + 0 sun.reflect.MethodAccessorGenerator.emitInvoke; 0.0% 2 + 0 org.apache.spark.deploy.SparkHadoopUtil$$anonfun$2$$anonfun$apply$mc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581:5424,load,loader,5424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581,1,['load'],['loader']
Performance,.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.available(AppInputStream.java:60); 	at java.io.BufferedInputStream.available(BufferedInputStream.java:410); 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.clou,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:6580,concurren,concurrent,6580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance,.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685:6092,concurren,concurrent,6092,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685,1,['concurren'],['concurrent']
Performance,.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:526); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:114); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.UnknownHostException: www.googleapis.com; 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:668); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:264); 	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.p,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094:6223,concurren,concurrent,6223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094,1,['concurren'],['concurrent']
Performance,.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:526); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:114); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; 443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to broad-jg-dev-11k-call-set/JointGenotyping/0cb36821-b8bf-4e6d-a352-07b101f6b7d1/call-ApplyRecalibration/shard-1734/GMKF_Seidman_CHD_WGS_904.filtered.1734.vcf.gz.; 	at shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.Abstra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3735:4758,concurren,concurrent,4758,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3735,1,['concurren'],['concurrent']
Performance,.storage.contrib.nio.CloudStorageFileSystemProvider.newFileSystem(CloudStorageFileSystemProvider.java:192); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newFileSystem(CloudStorageFileSystemProvider.java:83); 	at java.nio.file.FileSystems.newFileSystem(FileSystems.java:336); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:40); 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:140); 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:121); 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:170); 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:130); 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:67); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-285797337:6410,concurren,concurrent,6410,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-285797337,2,['concurren'],['concurrent']
Performance,.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)** ; **20/03/05 09:28:58 INFO ShutdownHookManager: Shutdown hook called** ; **20/03/05 09:28:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-9e0e0327-45a3-46e8-872a-f5a63c3c7a98** ; **Using GATK jar /mnt/clinix1/Analysis/mongol/phenomata/Tools/Anaconda3/envs/gatk4/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar** ; **Running:** ; **java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx200G -jar /mnt/clinix1/Analysis/mongol/phenomata/Tools/Anaconda3/envs/gatk4/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar PathSeqPipelineSpark --input /clinix1/Analysis/mongol/phenomata/04.GC\_CC/01.Alignment/Aligned/17039\_N.bam --filter-bwa-image /clinix1/Analysis/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:51433,concurren,concurrent,51433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,".tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)** ; **20/03/05 09:28:58 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40, localhost, executor driver, partition 40, PROCESS\_LOCAL, 7972 bytes)** ; **20/03/05 09:28:58 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)** ; **20/03/05 09:28:58 WARN TaskSetManager: Lost task 34.0 in stage 0.0 (TID 34, localhost, executor driver): com.esotericsoftware.kryo.KryoException: Buffer underflow.** ; **at com.esotericsoftware.kryo.io.Input.require(Input.java:199)** ; **at com.esotericsoftware.kryo.io.Input.readLong(Input.java:686)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet.<init>(LongHopscotchSet.java:83)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet$Serializer.read(LongHopscotchSet.java:527)** ; **at org.broadinstitute.hellbe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:29001,concurren,concurrent,29001,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,".tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **20/03/05 09:28:58 ERROR TaskSetManager: Task 34 in stage 0.0 failed 1 times; aborting job** ; **20/03/05 09:28:58 INFO TaskSchedulerImpl: Cancelling stage 0** ; **20/03/05 09:28:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 0.0 in stage 0.0 (TID 0), reason: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 30.0 in stage 0.0 (TID 30), reason: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 9.0 in stage 0.0 (TID 9), reason: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 1.0 in stage 0.0 (TID 1), reason: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:32449,concurren,concurrent,32449,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,".tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **20/03/05 09:28:58 INFO DAGScheduler: Job 0 failed: count at PathSeqPipelineSpark.java:245, took 63.806676 s** ; **20/03/05 09:28:58 INFO SparkUI: Stopped Spark web UI at http://cm132:4040** ; **20/03/05 09:28:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!** ; **20/03/05 09:28:58 INFO NewHadoopRDD: Input split: file:/clinix1/Analysis/mongol/phenomata/04.GC\_CC/01.Alignment/Aligned/17039\_N.bam:1342177280+33554432** ; **20/03/05 09:28:58 INFO MemoryStore: MemoryStore cleared** ; **20/03/05 09:28:58 INFO BlockManager: BlockManager stopped** ; **20/03/05 09:28:58 INFO BlockManagerMaster: BlockManagerMaster stopped** ; **20/03/05 09:28:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!*",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:41063,concurren,concurrent,41063,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)** ; **at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)** ; **at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)** ; **at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:45490,concurren,concurrent,45490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,".tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 19/02/01 21:28:28 INFO TaskSetManager: Starting task 701.0 in stage 5.0 (TID 4406, localhost, executor driver, partition 701, PROCESS_LOCAL, 4940 bytes); 19/02/01 21:28:28 INFO Executor: Running task 701.0 in stage 5.0 (TID 4406)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5647:3444,concurren,concurrent,3444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5647,2,['concurren'],['concurrent']
Performance,".url = jdbc:hsqldb:mem:f10b64bd-d8ca-4428-917b-311fca24c372;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-07-14 05:09:29,37] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-07-14 05:09:29,47] [info] Running with database db.url = jdbc:hsqldb:mem:e337a356-2f0c-4389-92c5-255465180f24;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,89] [info] Slf4jLogger started; [2020-07-14 05:09:30,10] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-ca5c695"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2020-07-14 05:09:30,23] [info] Metadata summary refreshing every 1 second.; [2020-07-14 05:09:30,23] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2020-07-14 05:09:30,25] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2020-07-14 05:09:30,26] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,26] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,36] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2020-07-14 05:09:30,46] [info] SingleWorkflowRunnerActor: Version 51; [2020-07-14 05:09:30,48] [info] SingleWorkflowRunnerActor: Submitting workflow; [2020-07-14 05:09:30,55] [info] Unspecified type (Unspecified version) workflow 968be82c-eef3-4bdb-a1ab-3d4e2ca70674 submitted; [2020-07-14 05:09:30,66] [info] SingleWorkflowRunnerActor: Workflow submitted 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,67] [info] 1 new workflows fetched by cromid-ca5c695: 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:2279,throttle,throttle,2279,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,1,['throttle'],['throttle']
Performance,".walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:302); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:135); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ----------------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2946:11571,concurren,concurrent,11571,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946,1,['concurren'],['concurrent']
Performance,///home/shiyang/softwares/gatk-4.1.8.1/clinvar_20180401.vcf -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:49.851 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar_VCF : 100000; 15:41:49.852 INFO FeatureManager - Using codec VCFCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:49.938 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_20180401.vcf -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:50.021 INFO FeatureManager - Using codec VCFCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:50.092 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar : 100000; 15:41:50.093 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; 15:41:50.093 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.config; 15:41:50.158 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; 15:41:50.158 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6758:14246,cache,cache,14246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6758,1,['cache'],['cache']
Performance,/3146?src=pr&el=tree) | Coverage  | Complexity  | |; |---|---|---|---|; | [...ellbender/tools/exome/CalculateTargetCoverage.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9DYWxjdWxhdGVUYXJnZXRDb3ZlcmFnZS5qYXZh) | `92.265% <> ()` | `32 <0> ()` | :arrow_down: |; | [...bender/tools/exome/NormalizeSomaticReadCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9Ob3JtYWxpemVTb21hdGljUmVhZENvdW50cy5qYXZh) | `77.143% <0%> (-2.024%)` | `10% <0%> (+4%)` | |; | [...bender/tools/exome/germlinehmm/xhmm/XHMMModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9nZXJtbGluZWhtbS94aG1tL1hITU1Nb2RlbC5qYXZh) | `100% <0%> ()` | `14% <0%> (+7%)` | :arrow_up: |; | [...te/hellbender/tools/exome/PerformSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QZXJmb3JtU2VnbWVudGF0aW9uLmphdmE=) | `100% <0%> ()` | `6% <0%> (+3%)` | :arrow_up: |; | [...ender/utils/hmm/segmentation/HMMPostProcessor.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9obW0vc2VnbWVudGF0aW9uL0hNTVBvc3RQcm9jZXNzb3IuamF2YQ==) | `85.439% <0%> (+1.215%)` | `92% <0%> (+15%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.649% <0%> (+2.027%)` | `34% <0%> ()` | :arrow_down: |; | [.../coverage/pca/HDF5PCACoveragePoNCreationUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3146#issuecomment-311542216:1843,Perform,PerformSegmentation,1843,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3146#issuecomment-311542216,1,['Perform'],['PerformSegmentation']
Performance,/CNVGuts/-947966988/Homo_sapiens_assembly38.bed.preprocessed.filtered.scattered.0154.interval_list; 21:06:12.640 DEBUG FeatureDataSource - Cache statistics for FeatureInput /paedyl01/disk1/louisshe/work/NGS/wdl/test_workflow_cnv/germline/cromwell-executions/CNVGermlineCohort8/Homo_sapiens_assembly38.bed.preprocessed.filtered.scattered.0154.interval_list:/paedyl01/disk1/louisshe/work/NGS/wdl/test_workflow_cnv/germline/cromwell-executions/CNVGermli947966988/Homo_sapiens_assembly38.bed.preprocessed.filtered.scattered.0154.interval_list:; 21:06:12.640 DEBUG FeatureCache - Cache hit rate was 0.00% (0 out of 0 total queries); 21:06:12.645 INFO IntervalArgumentCollection - Processing 4999155 bp from intervals; 21:06:12.656 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 21:06:18.914 WARN GermlineCNVCaller - Sequence dictionary in annotated-intervals file does not match the master sequence dictionary.; 21:06:19.130 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 21:06:19.200 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 21:06:19.200 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 21:07:11.897 DEBUG ScriptExecutor - Executing:; 21:07:11.897 DEBUG ScriptExecutor - python; 21:07:11.897 DEBUG ScriptExecutor - /paedyl01/disk1/louisshe/tmp/gatk/cohort_denoising_calling.418897092082188314.py; 21:07:11.897 DEBUG ScriptExecutor - --ploidy_calls_path=/paedyl01/disk1/louisshe/work/NGS/wdl/test_workflow_cnv/germline/cromwell-executions/CNVGermlineCohortWorkflow/d53c0a; 21:07:11.897 DEBUG ScriptExecutor - --output_calls_path=/paedyl01/disk1/louisshe/out/NMD/batch1_2023/batch1_all/cnv/cohort_calls/batch1_all-calls; 21:07:11.897 DEBUG ScriptExecutor - --output_tracking_path=/paedyl01/disk1/louisshe/out/NMD/batch1_2023/batch1_all/cnv/cohort_calls/batch1_all-tracking; 21:07:11.897 DEBUG ScriptExecutor - --random_seed=1984; 21:07:11.897 DEBUG S,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:6902,perform,performed,6902,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['perform'],['performed']
Performance,"/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/resources/sv_classifier_test_data.json; src/test/resources/sv_features_test_data.json",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:3093,perform,performance,3093,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,1,['perform'],['performance']
Performance,"/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). 16/11/16 23:25:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Cancelling stage 1; 16/11/16 23:25:11 INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) failed in 0.276 s; 16/11/16 23:25:11 INFO DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 1.029776 s; 16/11/16 23:25:11 INFO SparkContext: SparkContext already stopped.; [November 16, 2016 11:25:11 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2058354688; org.apache.spark.SparkException: Job aborted due to stage failure:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:15994,concurren,concurrent,15994,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['concurren'],['concurrent']
Performance,/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:18256,concurren,concurrent,18256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['concurren'],['concurrent']
Performance,"/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]; java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:13005,concurren,concurrent,13005,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['concurren'],['concurrent']
Performance,"/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 INFO SparkContext: Invoking stop() from shutdown hook; 16/11/16 23:25:11 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:14485,concurren,concurrent,14485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['concurren'],['concurrent']
Performance,/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 INFO SparkUI: Stopped Spark web UI at http://172.32.65.22:4040; 16/11/16 23:25:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/11/16 23:25:11 INFO MemoryStore: MemoryStore cleared; 16/11/16 23:25:11 INFO BlockManager: BlockManager stopped; 16/11/16 23:25:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/11/16 23:25:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/11/16 23:25:11 INFO SparkContext: Successfully stopped SparkContext; 16/11/16 23:25:11 INFO ShutdownHookManager: Shutdown hook called; 16/11/16 23:25:11 INFO ShutdownHookManager: Deleting directory /gpfs/ngsdata/sparkcache/spark-29e7cb29-06dd-4145-ad9a-aa75971badb8; 16/11/16 23:25:11 INFO ShutdownHookManager: Deleting direc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:24642,concurren,concurrent,24642,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['concurren'],['concurrent']
Performance,"/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 17/10/11 14:19:28 INFO scheduler.TaskSetManager: Starting task 0.1 in stage 1.0 (TID 2, com2, executor 1, partition 0, NODE_LOCAL, 1990 bytes); 17/10/11 14:19:28 INFO cluster.YarnClientSchedulerBackend: Disabling executor 1.; 17/10/11 14:19:28 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 1); 17/10/11 14:19:28 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.; 17/10/11 14:19:28 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, com2, 38568); 17/10/11 14:19:28 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor; 17/10/11 14:19:28 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1507683879816_0006_01_000002 on host: com2. Exit status: 50. Diagnostics:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:16830,concurren,concurrent,16830,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['concurren'],['concurrent']
Performance,"/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 17/10/11 14:19:37 INFO scheduler.TaskSetManager: Starting task 0.3 in stage 1.0 (TID 4, com2, executor 2, partition 0, NODE_LOCAL, 1990 bytes); 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Disabling executor 2.; 17/10/11 14:19:38 INFO scheduler.DAGScheduler: Executor lost: 2 (epoch 1); 17/10/11 14:19:38 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.; 17/10/11 14:19:38 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(2, com2, 46254); 17/10/11 14:19:38 INFO storage.BlockManagerMaster: Removed 2 successfully in removeExecutor; 17/10/11 14:19:38 ERROR cluster.YarnScheduler: Lost executor 2 on com2: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:23843,concurren,concurrent,23843,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['concurren'],['concurrent']
Performance,"/arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.l",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1995,load,load,1995,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,/codecov.io/gh/broadinstitute/gatk/pull/4902?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage  | |; |---|---|---|; | [...e/hellbender/engine/FeatureDataSourceUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2VVbml0VGVzdC5qYXZh) | `88.318% <> ()` | |; | [...institute/hellbender/engine/FeatureDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2UuamF2YQ==) | `74.603% <57.143%> (+24.603%)` | :arrow_up: |; | [...ender/engine/cache/SideReadInputCacheStrategy.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvU2lkZVJlYWRJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `81.481% <81.481%> ()` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `91.667% <84.615%> (+33.333%)` | :arrow_up: |; | [...ellbender/engine/VariantWalkerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3J,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741:2336,cache,cache,2336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741,1,['cache'],['cache']
Performance,/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.967 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.config; > 12:28:17.995 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.997 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > WARNING 2020-07-21 12:28:17 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 12:28:18.002 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; > 12:28:18.009 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > 12:28:18.020 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.config; > 12:28:18.120 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > 12:28:18.121 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > WARNING 2020-07-21 12:28:18 AsciiLineReader Creating an indexable source for an Asci,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:9578,cache,cache,9578,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,"/gatk:471: SyntaxWarning: ""is not"" with a literal. Did you mean ""!=""?; if not len(filesToAdd) is 0:; Using GATK jar /home/warkre/miniconda3/envs/gatk4.1.4.0/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/warkre/miniconda3/envs/gatk4.1.4.0/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar FilterMutectCalls -V MT.vcf.gz -R human_g1k_v37.main.fasta -O MT.filtered.vcf.gz --stats MT.vcf.gz.stats --mitochondria-mode; ```. #### Steps to reproduce. The data is sensitive human data, so I will not share that data. However, I have made up some data that appears to fail as well. Using the command:. ```bash; gatk FilterMutectCalls -V mu.2.vcf -R human_g1k_v37.main.fasta -O MT.filtered.vcf.gz --stats MT.vcf.gz.stats --mitochondria-mode; ```; I get an output to STDERR:; ```; 11:30:15.521 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/warkre/miniconda3/envs/gatk4.1.4.0/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 07, 2019 11:30:15 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:30:15.673 INFO FilterMutectCalls - ------------------------------------------------------------; 11:30:15.674 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.4.0; 11:30:15.674 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:30:15.674 INFO FilterMutectCalls - Executing as warkre@fuji on Linux v4.9.0-9-amd64 amd64; 11:30:15.674 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 11:30:15.674 INFO FilterMutectCalls - Start Date/Time: November 7, 2019 11:30:15 AM CET; 11:30:15.674 INFO FilterMutectCalls - --------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6255:6919,Load,Loading,6919,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6255,1,['Load'],['Loading']
Performance,"/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; -I /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram \; -O /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/test_decram_20171002.bam \; -L chr17; ...; Using GATK jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar PrintReads -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa -I /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram -O /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/test_decram_20171002.bam -L chr17; 14:52:10.763 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; [October 5, 2017 2:52:10 PM EDT] PrintReads --output /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/test_decram_20171002.bam --intervals chr17 --input /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram --reference /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:1354,Load,Loading,1354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['Load'],['Loading']
Performance,"/hg19-homopolymers-save.interval_list`. Stacktrace:; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded; 	at java.util.BitSet.initWords(BitSet.java:166); 	at java.util.BitSet.<init>(BitSet.java:161); 	at htsjdk.samtools.GenomicIndexUtil.regionToBins(GenomicIndexUtil.java:90); 	at htsjdk.samtools.BinningIndexContent.getChunksOverlapping(BinningIndexContent.java:121); 	at htsjdk.samtools.CachingBAMFileIndex.getSpanOverlapping(CachingBAMFileIndex.java:71); 	at htsjdk.samtools.BAMFileReader.getFileSpan(BAMFileReader.java:898); 	at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:915); 	at htsjdk.samtools.BAMFileReader.query(BAMFileReader.java:575); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:528); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:400); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:125); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:66); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.prepareIteratorsForTraversal(ReadsDataSource.java:404); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.iterator(ReadsDataSource.java:330); 	at java.lang.Iterable.spliterator(Iterable.java:101); 	at org.broadinstitute.hellbender.utils.Utils.stream(Utils.java:1055); 	at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:252); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:93); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.Comma",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4334:1294,load,loadNextIterator,1294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4334,1,['load'],['loadNextIterator']
Performance,/hg38/gencode_xhgnc_v90_38.hg38.tsv; > 12:28:17.925 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/achilles_lineage_results.import.txt -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/achilles/hg38/achilles_lineage_results.import.txt; > 12:28:17.932 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode_xrefseq_v90_38.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/gencode_xrefseq/hg38/gencode_xrefseq_v90_38.tsv; > 12:28:17.939 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/hgnc_download_Nov302017.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/hgnc/hg38/hgnc_download_Nov302017.tsv; > 12:28:17.939 INFO Funcotator - Finalizing data sources (this step can be long if data sources are cloud-based)...; > 12:28:17.940 INFO DataSourceUtils - Setting lookahead cache for data source: chr1_b_bed : 100000; > 12:28:17.951 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.967 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.config; > 12:28:17.995 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.997 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > WARNING 2020-07-21 12:28:17 AsciiLineReader Creating an index,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:8388,cache,cache,8388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:41.615 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg_lof.tsv -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/acmg_lof/hg38/acmg_lof.tsv; 06:42:41.617 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg59_test_cleaned.txt -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/acmg_rec/hg38/acmg59_test_cleaned.txt; 06:42:41.617 INFO Funcotator - Finalizing data sources (this step can be long if data sources are cloud-based)...; 06:42:41.618 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.618 INFO DataSourceUtils - Setting lookahead cache for data source: LMMKnown : 100000; 06:42:41.622 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.641 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.652 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.663 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.annotation.REORDERED.gtf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencod,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7090:5288,cache,cache,5288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090,1,['cache'],['cache']
Performance,/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; Using GATK jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/uksza_2022_Nature -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/uksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:36:33.528 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.547 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.550 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.551 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.669 INFO BaseRecalibrator - ------------------------------------------------------------; 13:36:33.670 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:36:33.670 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:36:33.670 INFO BaseRecalibrator - Executin,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:6490,Load,Loading,6490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['Load'],['Loading']
Performance,"/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; + /spark-1.6.2-bin-hadoop2.6//bin/spark-submit --master spark://hpcgenomicn24:6311 --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; 23:25:07.475 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/gpfs/software/spark/gatk4onspark.jar!/com/intel/gkl/native/libIntelGKL.so; 23:25:07.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [November 16, 2016 11:25:07 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /gpfs/home/tpathare/test/ --input /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [November 16, 2016 11:25:07 PM AST] Executing as root@hpcgenomicn24 on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_66-b17; Version: Version:4.alpha.2-98-g8fa5092-SNAPSHOT; 23:25:07.556 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; 23:25:07.556 INFO PrintReadsSpark - Defaults.COMPRESSION_LEVEL : 5; 23:25:07.556 INFO PrintReadsSpark - Defaults.CREA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:1408,load,loaded,1408,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['load'],['loaded']
Performance,"/vidmap.json; 16:28:04.155 INFO GenomicsDBImport - Callset Map JSON file will be written to forkTest/callset.json; 16:28:04.156 INFO GenomicsDBImport - Complete VCF Header will be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(Geno",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3603,concurren,concurrent,3603,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['concurren'],['concurrent']
Performance,/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;   java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam ; ; 00:11:11.683 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.697 WARN NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.700 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.700 WARN NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.812 INFO BaseRecalibrator - ------------------------------------------------------------ ; ; 00:11:11.813 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:11:11.813 INFO BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://so,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:9102,Load,Loading,9102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['Load'],['Loading']
Performance,"0 samples; 04:37:37.079 INFO GenomicsDBImport - Starting batch input file preload; 04:37:38.712 INFO GenomicsDBImport - Finished batch preload; 04:37:38.712 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:39.162 INFO GenomicsDBImport - Shutting down engine; [October 8, 2018 4:37:39 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=4116185088; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:769); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:165); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:604); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; 04:37:39.167 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.170 INFO GenomicsDBImpor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5300:4699,concurren,concurrent,4699,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300,1,['concurren'],['concurrent']
Performance,0); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); ... 34 more; Caused by:; java.util.ConcurrentModificationException; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513:8705,Concurren,ConcurrentModificationException,8705,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513,1,['Concurren'],['ConcurrentModificationException']
Performance,0); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); ... 34 more; Caused by:; java.util.ConcurrentModificationException; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:8716,Concurren,ConcurrentModificationException,8716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['Concurren'],['ConcurrentModificationException']
Performance,"0-0/gatk-package-4.2.5.0-local.jar --version ; ; The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; HTSJDK Version: 2.24.1 ; ; Picard Version: 2.25.4. Exact command:. gatk CNNScoreVariants -I 73318\_WES\_hg19\_recalibrated.sorted.bam -V 73318\_80\_IDTv1.vcf.gz -R /media/analyst/Data/Reference\_data/hg19.fa -O /media/analyst/Data/73318\_CNNScore\_test.vcf.gz -tensor-type read\_tensor > /media/analyst/Data/CNNScoreVariants.log. Entire console output:. Running: ; ;   java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar CNNScoreVariants -I 73318\_WES\_hg19\_recalibrated.sorted.bam -V 73318\_80\_IDTv1.vcf.gz -R /media/analyst/Data/Reference\_data/hg19.fa -O /media/analyst/Data/73318\_CNNScore\_test.vcf.gz -tensor-type read\_tensor ; ; 11:17:58.509 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Apr 25, 2022 11:17:58 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:17:58.668 INFO CNNScoreVariants - ------------------------------------------------------------ ; ; 11:17:58.668 INFO CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 11:17:58.669 INFO CNNScoreVariants - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:17:58.669 INFO CNNScoreVariants - Executing as analyst@WGS on Linux v5.13.0-40-generic amd64 ; ; 11:17:58.669 INFO CNNScoreVariants - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13 ; ; 11:17:58.669 INFO CNNScoreVariants - Start Date/Time: April 25, 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811:2430,Load,Loading,2430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811,1,['Load'],['Loading']
Performance,"0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Mar 07 16:08:17 UTC 2019] ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5807:1791,perform,performed,1791,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807,1,['perform'],['performed']
Performance,0.3 35000 101621.1; 14:56:26.027 INFO ProgressMeter - 1:5856032 0.5 55000 105867.6; ...; 19:37:05.295 INFO ProgressMeter - GL000209.1:48811 281.2 30739000 109323.8; 19:37:15.543 INFO ProgressMeter - GL000224.1:65537 281.3 30758000 109324.9; 19:37:25.847 INFO ProgressMeter - GL000248.1:21736 281.5 30768000 109293.8; 19:37:25.906 INFO FilterMutectCalls - Finished pass 0 through the variants; 19:50:04.590 INFO FilterMutectCalls - Shutting down engine; [9 January 2020 7:50:04 PM] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 294.19 minutes.; Runtime.totalMemory()=14966849536; java.lang.IllegalArgumentException: Values in probability array sum to a negative number NaN; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:731); 	at org.broadinstitute.hellbender.utils.MathUtils.normalizeSumToOne(MathUtils.java:731); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.performEMIteration(SomaticClusteringModel.java:336); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:306); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:158); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:159); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runC,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341:4476,perform,performEMIteration,4476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341,1,['perform'],['performEMIteration']
Performance,0/11 14:19:28 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor; 17/10/11 14:19:28 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1507683879816_0006_01_000002 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000002; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:28 ERROR cluster.YarnScheduler: Lost executor 1 on com2: Container marked as failed: container_1507683879816_0006_01_000002 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000002; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.co,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:18556,concurren,concurrent,18556,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['concurren'],['concurrent']
Performance,"0/13 18:11:40 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:41 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 17/10/13 18:11:41 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> mg, PROXY_URI_BASES -> http://mg:8088/proxy/application_1507856833944_0003), /proxy/application_1507856833944_0003; 17/10/13 18:11:41 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 17/10/13 18:11:41 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:42 INFO yarn.Client: Application report for application_1507856833944_0003 (state: RUNNING); 17/10/13 18:11:42 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: 10.131.101.145; 	 ApplicationMaster RPC port: 0; 	 queue: root.users.hdfs; 	 start time: 1507889497661; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088/proxy/application_1507856833944_0003/; 	 user: hdfs; 17/10/13 18:11:42 INFO cluster.YarnClientSchedulerBackend: Application application_1507856833944_0003 has started running.; 17/10/13 18:11:42 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44818.; 17/10/13 18:11:42 INFO netty.NettyBlockTransferService: Server created on 10.131.101.159:44818; 17/10/13 18:11:42 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 17/10/13 18:11:42 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.131.101.159, 44818, None); 17/10/13 18:11:42 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.131.101.159:44818 with 366.3 MB RAM, BlockManagerId(driver, 10.131.101.159, 44818, None); 17/10/13 18:11:42 INF",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:13009,queue,queue,13009,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['queue'],['queue']
Performance,"0/757d9552-3464-11e5-8244-46931fed8383.png). Logs indicated it was running on a single worker, regardless of how many were specified for the job. **The Cause**. The underlying cause is a combination of ReadBAM's design and Dataflow's own perhaps over-eager optimization. ReadBAM is implemented as a series of transforms. It could also have been implemented as a Dataflow BoundedSource, but the latter is much more complicated. The transforms are as follows:; Start with a collection of filenames and a collection of contigs.; Transform 1 -- input: filenames, side input: contigs. Generates a list of regions to read (""BAMShard""); Transform 2 -- input: `PCollection<BAMShard>`, output: `PCollection<Read>`. Each worker reads from the BAM file, using the index to find where to read from. Dataflow sees that transform 2 takes as input transform 1's output, and so these two can be run in sequence on the same machines, skipping a serialization/deserialization step. This optimization is called ""fusing"" and it's generally a very good thing. However in this case, the input PCollection has a single element (the file we want to read), so only one worker is involved. Because of the fusion, that same worker then ends up doing all of the reading work, ruining our day. **The Solutions**. There are multiple ways to solve this problem. ; 1. change transform 1 to have the contig collection as a primary input in the hope that we always have more than one contig. ; This solution's very brittle (our benchmark, for example, reads a single chromosome so the contig list has effectively only one element). I did not pursue it.; 2. Insert a groupby step between the two transforms.; pro: this gets all the workers involved again; con: the groupby itself takes some time, unnecessarily.; 3. Compute the BAMShards at the client and then send those to workers.; pro: this gets all the workers involved again, and they do not have to spend any time on groupby; con: an existing Dataflow bug will cause the program ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/756:1214,optimiz,optimization,1214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/756,1,['optimiz'],['optimization']
Performance,00 Exception <a 'java/lang/NoSuchMethodError': java.lang.Object.lambda$thenComparing$36697e65$1(Ljava/util/Comparator;Ljava/lang/Object;Ljava/lang/Object;)I> (0x000000067b220588) thrown at [/home/buildozer/aports/community/openjdk8/src/icedtea-3.6.0/openjdk/. Events (10 events):; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmmOMP; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmmOMP done; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmm; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmm done; Event: 4.326 loading class com/intel/gkl/IntelGKLUtils; Event: 4.326 loading class com/intel/gkl/IntelGKLUtils done; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder done; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder done. Dynamic libraries:; 3c0000000-41b600000 rw-p 00000000 00:00 0 ; 41b600000-66ab00000 ---p 00000000 00:00 0 ; 66ab00000-6aef00000 rw-p 00000000 00:00 0 ; 6aef00000-7c0000000 ---p 00000000 00:00 0 ; 7c0000000-7c0520000 rw-p 00000000 00:00 0 ; 7c0520000-800000000 ---p 00000000 00:00 0 ; 2b5f56cd5000-2b5f56d5e000 r-xp 00000000 07:00 565 /lib/ld-musl-x86_64.so.1; 2b5f56d5e000-2b5f56d60000 ---p 00000000 00:00 0 ; 2b5f56d60000-2b5f56d63000 ---p 00000000 00:00 0 ; 2b5f56d63000-2b5f56e61000 rw-p 00000000 00:00 0 [stack:85483]; 2b5f56e61000-2b5f56e62000 r--p 00000000 00:00 0 ; 2b5f56e62000-2b5f56e63000 rw-p 00000000 00:00 0 ; 2b5f56e63000-2b5f56e6b000 rw-s 00000000 08:01 69704933 /tmp/hsperfdata_iiipe01/85482; 2b5f56e6b000-2b5f56f3c000 rw-p 00000000 00:00 0 ; 2b5f56f3c000-2b5f56f3d000 r--s 00000000 08:01 67151449 /etc/localtime; 2b5f56f3d000-2b5f56f58000 r--s 001d6000 07:00 6686 /usr/lib/jvm/java-1.8-openjdk/jre/lib/ext/nashorn.jar; 2b5f56f58000-2b5f5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:29684,load,loading,29684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['load'],['loading']
Performance,"00 pipeline. But when I run the command above, it throws out an error, seems something related with malformat. I check my VCF file and think it should be OK. So I wonder if you can kindly tell me how to fix this bug?; The ERROR is:; `Using GATK jar /home/shiyang/softwares/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/shiyang/softwares/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar Funcotator --variant /home/shiyang/Project/BGB900_101/TSO_result/TSO_somatic_vcf/112-0005-0031-B1_L1.UP12.tmb.tsv.tso.somatic.vcf --reference /storage01/ref_genome/hg19/bwa/ucsc.hg19.fasta --ref-version hg19 --data-sources-path /home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s --output /home/shiyang/Project/BGB900_101/TSO_result/test.maf --output-file-format MAF; 15:41:48.793 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/shiyang/softwares/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 19, 2020 3:41:49 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:41:49.028 INFO Funcotator - ------------------------------------------------------------; 15:41:49.028 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.8.1; 15:41:49.028 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:41:49.028 INFO Funcotator - Executing as shiyang@r740 on Linux v3.10.0-957.el7.x86_64 amd64; 15:41:49.028 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_252-b09; 15:41:49.028 INFO Funcotator - Start Date/Time: August 19, 2020 3:41:48 PM CST; 15:41:49.029 INFO Funcotator - ------------------------------------------------------------; 15:41:49.029 INFO Fun",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6758:2591,Load,Loading,2591,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6758,1,['Load'],['Loading']
Performance,004.bam -O test.dup.bam -M marked_dup_metrics.txt; **********. 15:26:21.393 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Wed Jul 03 15:26:21 CEST 2024] MarkDuplicates INPUT=[WA02_i5-537_i7-98_S11819_L004.bam] OUTPUT=test.dup.bam METRICS_FILE=marked_dup_metrics.txt MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 TAG_DUPLICATE_SET_MEMBERS=false REMOVE_SEQUENCING_DUPLICATES=false TAGGING_POLICY=DontTag CLEAR_DT=true DUPLEX_UMI=false FLOW_MODE=false FLOW_DUP_STRATEGY=FLOW_QUALITY_SUM_STRATEGY USE_END_IN_UNPAIRED_READS=false USE_UNPAIRED_CLIPPED_END=false UNPAIRED_END_UNCERTAINTY=0 UNPAIRED_START_UNCERTAINTY=0 FLOW_SKIP_FIRST_N_FLOWS=0 FLOW_Q_IS_KNOWN_END=false FLOW_EFFECTIVE_QUALITY_THRESHOLD=15 ADD_PG_TAG_TO_READS=true REMOVE_DUPLICATES=false ASSUME_SORTED=false DUPLICATE_SCORING_STRATEGY=SUM_OF_BASE_QUALITIES PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates READ_NAME_REGEX=<optimized capture of last three ':' separated fields as numeric values> OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 MAX_OPTICAL_DUPLICATE_SET_SIZE=300000 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=2 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false USE_JDK_DEFLATER=false USE_JDK_INFLATER=false; [Wed Jul 03 15:26:21 CEST 2024] Executing as qnick@Barbus on Linux 6.5.0-41-generic amd64; OpenJDK 64-Bit Server VM 17.0.11+9-Ubuntu-122.04.1; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: 4.6.0.0; INFO 2024-07-03 15:26:21 MarkDuplicates Start of doWork freeMemory: 189357416; totalMemory: 234881024; maxMemory: 32178700288; INFO 2024-07-03 15:26:21 MarkDuplicates Reading input file and constructing read end information.; INFO 2024-07-03 15:26:21 MarkDuplicates Will retain up to 116589493 data points before spilling to disk.; [Wed Jul ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8904:7110,optimiz,optimized,7110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8904,1,['optimiz'],['optimized']
Performance,"00; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; >; > INFO: Failed to detect whether we are running on Google Compute Engine.; >; > 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.843 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; >; > 16:17:05.843 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; >; > 16:17:05.843 INFO HaplotypeCaller - Executing as robert@powerlinux on Linux v4.4.0-184-generic ppc64le; >; > 16:17:05.843 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_252-8u252-b09-1~16.04-b09; >; > 16:17:05.843 INFO HaplotypeCaller - Start Date/Time: September 4, 2020 4:17:04 PM UTC; >; > 16:17:05.843 INFO HaplotypeCaller - -------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:2412,load,load,2412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,"01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM models, as well. @LeeTL1220 @mbabadi @davidbenjamin I'd be interested to hear your thoughts, if you have any.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:1522,perform,performs,1522,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666,1,['perform'],['performs']
Performance,"015f-0a1b-f1bd-00002ce33928 ; on database directory /tmp/spark-98953d35-8594-4907-b4a5-0870f1d17b3e/metastore with class loader sun.misc.Launcher$AppClassLoader@5c647e05 ; Loaded from file:/opt/cloudera/parcels/CDH-5.12.1-1.cdh5.12.1.p0.3/jars/derby-10.11.1.1.jar; java.vendor=Oracle Corporation; java.runtime.version=1.8.0_91-b14; user.dir=/opt/Software/gatk; os.name=Linux; os.arch=amd64; os.version=3.10.0-514.el7.x86_64; derby.system.home=null; Database Class Loader started - derby.database.classpath=''; 17/10/11 14:25:33 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.1.0-cdh5.12.1; 17/10/11 14:25:33 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException; SQL context available as sqlContext. **./gradlew bundle**; **[root@com1 gatk]# ./gradlew bundle; when I executed the command ./gradlew bundle it appeared the error in the last did this matter**. .......; [loading ZipFileIndexFileObject[/root/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.6.5/d50be1723a09be903887099ff2014ea9020333/jackson-databind-2.6.5.jar(com/fasterxml/jackson/databind/annotation/JsonSerialize$Inclusion.class)]]; [loading ZipFileIndexFileObject[/root/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-core/2.5/7ed845de1dfe070d43511fab1784e6c4118398/log4j-core-2.5.jar(org/apache/logging/log4j/core/config/plugins/PluginVisitorStrategy.class)]]; [done in 5759 ms]; 1 error; :gatkTabComplete FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/opt/Software/gatk/build/tmp/gatkTabComplete/jadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 7.431 secs",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240:2897,load,loading,2897,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240,4,"['cache', 'load']","['caches', 'loading']"
Performance,"03:15:02.578 INFO IntervalArgumentCollection - Processing <redacted> bp from intervals; 03:15:02.588 INFO HaplotypeCaller - Done initializing engine; 03:15:02.590 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 03:15:02.593 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/conda/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 03:15:02.598 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/conda/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 03:15:02.599 INFO IntelSmithWaterman - Using CPU-supported AVX-512 instructions; 03:15:02.599 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 03:15:02.601 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 03:15:02.601 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 03:15:02.623 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:conda/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 03:15:02.667 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 03:15:02.667 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 03:15:02.667 INFO IntelPairHmm - Available threads: 16; 03:15:02.667 INFO IntelPairHmm - Requested threads: 32; 03:15:02.667 WARN IntelPairHmm - Using 16 available threads, but 32 were requested; 03:15:02.667 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; ```. Any insight into what's going on and how to diagnose it would be greatly appreciated.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6889:6117,Load,Loading,6117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6889,2,"['Load', 'multi-thread']","['Loading', 'multi-threaded']"
Performance,04.231 INFO GenotypeGVCFs - Initializing engine; > 21:13:11.834 INFO GenotypeGVCFs - Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 21:14:17.635 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.858 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 13 > 12 expanding to 26; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 27 > 26 expanding to 54; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 55 > 54 expanding to 110; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 111 > 110 expanding to 222; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 223 > 222 expanding to 446; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 447 > 446 expanding to 894; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 895 > 894 expanding to 1790; > 21:14:17.874 DEBUG MathUtils$Log10Cache - cache miss 1791 > 1790 expanding to 3582; > 21:14:17.894 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.930 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.937 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:18.507 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:3->4; > 21:14:18.510 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:27:38.720 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:28:26.332 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:30:24.296 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:4->5; > 21:30:24.299 DEBUG Gen,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:6034,cache,cache,6034,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['cache'],['cache']
Performance,0418.tsv /tmp/tintest/sample-3678073345547351139852.tsv /tmp/tintest/sample-3687670170611066239891.tsv /tmp/tintest/sample-3697896582170286914413.tsv /tmp/tintest/sample-3704126767513966037718.tsv /tmp/tintest/sample-3718480935718374256974.tsv /tmp/tintest/sample-3729118447183914284068.tsv /tmp/tintest/sample-3734484951576950052743.tsv /tmp/tintest/sample-3745007638909244994571.tsv /tmp/tintest/sample-3758817480300622528681.tsv /tmp/tintest/sample-3765561422653477541111.tsv /tmp/tintest/sample-377681127346074691924.tsv /tmp/tintest/sample-3788006936711929575536.tsv /tmp/tintest/sample-3794598448303416401276.tsv /tmp/tintest/sample-380910670101098136635.tsv /tmp/tintest/sample-3815864583095389374312.tsv /tmp/tintest/sample-3821063008346821202582.tsv /tmp/tintest/sample-3836550848258521825191.tsv /tmp/tintest/sample-3842488752532231097400.tsv /tmp/tintest/sample-3855124216409092357090.tsv /tmp/tintest/sample-3866989755460133829309.tsv ; Stdout: 10:58:52.820 INFO cohort_denoising_calling - Loading 387 read counts file(s)...; 11:01:01.618 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:02:11.422 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 11:04:53.672 ERROR theano.gof.cmodule - [Errno 12] Cannot allocate memory. Stderr: Problem occurred during compilation with the command line below:; /usr/bin/g++ -shared -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -fopenmp -march=knl -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mavx512f -mno-avx512er -mavx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mavx512dq -mavx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053:61415,Load,Loading,61415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053,1,['Load'],['Loading']
Performance,"05f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-BenchmarkVCFControlSample/Benchmark/3b068fb2-7140-4c1e-8860-df8df21821ec/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""80.5165222222222"",; ""CHM evalHCsystemhours"": ""0.1713305555555555"",; ""CHM evalHCwallclockhours"": ""53.10978888888891"",; ""CHM evalHCwallclockmax"": ""2.7458416666666667"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM evalindelF1Score"": ""0.8724"",; ""CHM evalindelPrecision"": ""0.8814"",; ""CHM evalsnpF1Score"": ""0.9784"",; ""CHM evalsnpPrecision"": ""0.9706"",; ""CHM evalsnpRecall"": ""0.9863"",; ""CHM evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-BenchmarkVCFTestSample/Benchmark/7f7c4522-e293-4a03-ada8-9541a585250b/call-CombineSummaries/summary.csv"",; ""EXOME1 controlindelF1Score"": ""0.727"",; ""EXOME1 controlindelPrecision"": ""0.632"",; ""EXOME1 controlsnpF1Score"": ""0.9878"",; ""EXOME1 controlsnpPrecision"": ""0.9815"",; ""EXOME1 controlsnpRecall"": ""0.9941"",; ""EXOME1 controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-EXOME1Sampl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207:11445,cache,cacheCopy,11445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207,1,['cache'],['cacheCopy']
Performance,"06669 Sep 16 11:32 genome.fa.amb; -rw-r----- 1 kh3 kh3 3276 Sep 16 11:32 genome.fa.ann; -rw-r----- 1 kh3 kh3 3137454592 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 3:47:28 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.; 2-45-ga",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171:1345,load,loaded,1345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171,1,['load'],['loaded']
Performance,07/25 01:37:55 Done container setup.; 2020/07/25 01:37:56 Starting localization.; 2020/07/25 01:38:02 Localization script execution started...; 2020/07/25 01:38:02 Localizing input gs://gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle -> /cromwell_root/gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle; 2020/07/25 01:38:40 Localizing input gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-FilterAlignmentArtifacts/attempt-3/script -> /cromwell_root/script; 2020/07/25 01:38:45 Localization script execution complete.; 2020/07/25 01:38:58 Done localization.; 2020/07/25 01:38:59 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= us.gcr.io/broad-gatk/gatk@sha256:8051adab0ff725e7e9c2af5997680346f3c3799b2df3785dd51d4abdd3da747b /bin/bash /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.6c58e0ba; 01:39:02.909 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 01:39:02.925 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 01:39:02.927 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 01:39:03.142 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 01:39:03.361 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.361 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.1; 01:39:03.361 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:39:03.362 INFO FilterAlignmentArtifacts - Executing as root@3f245e278eba on Linux v4.19.112+ amd64; 01:39:03.362 INFO FilterAlig,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:1941,Load,Loading,1941,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['Load'],['Loading']
Performance,"08 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:37:16.408 INFO GermlineCNVCaller - Deflater: IntelDeflater; 08:37:16.409 INFO GermlineCNVCaller - Inflater: IntelInflater; 08:37:16.409 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 08:37:16.409 INFO GermlineCNVCaller - Requester pays: disabled; 08:37:16.409 INFO GermlineCNVCaller - Initializing engine; 08:37:21.698 INFO GermlineCNVCaller - Done initializing engine; 08:37:22.015 INFO GermlineCNVCaller - Retrieving intervals from read-count file (results/200219_X008378.counts.tsv)...; 08:37:22.119 INFO GermlineCNVCaller - No annotated intervals were provided...; 08:37:22.120 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 08:37:22.194 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 08:37:22.195 INFO GermlineCNVCaller - Shutting down engine; [February 26, 2019 8:37:22 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.29 minutes.; Runtime.totalMemory()=330301440; java.lang.IllegalArgumentException: Output directory results/190226.181217_K00178.CNVCaller does not exist.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.validateArguments(GermlineCNVCaller.java:361); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:281); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:15383,perform,performed,15383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['perform'],['performed']
Performance,08.558 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 13:24:08.558 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:24:08.558 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:24:08.558 INFO Mutect2 - Deflater: IntelDeflater; 13:24:08.558 INFO Mutect2 - Inflater: IntelInflater; 13:24:08.558 INFO Mutect2 - GCS max retries/reopens: 20; 13:24:08.558 INFO Mutect2 - Requester pays: disabled; 13:24:08.558 INFO Mutect2 - Initializing engine; 13:24:09.048 INFO FeatureManager - Using codec VCFCodec to read file file://ref/1000g_pon.hg38.vcf.gz; 13:24:09.207 INFO FeatureManager - Using codec VCFCodec to read file file://ref/af-only-gnomad.hg38.vcf.gz; 13:24:09.374 INFO Mutect2 - Done initializing engine; 13:24:09.435 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 13:24:09.438 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 13:24:09.472 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 13:24:09.472 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 13:24:09.473 INFO IntelPairHmm - Available threads: 24; 13:24:09.473 INFO IntelPairHmm - Requested threads: 4; 13:24:09.473 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 13:24:09.501 INFO ProgressMeter - Starting traversal; 13:24:09.502 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 13:24:19.721 INFO ProgressMeter - chr1:634040 0.2 2460 14443.7; 13:24:29.736 INFO ProgressMeter - chr1:1564703 0.3 7220 21409.5; .; .; .; 15:28:55.286 INFO ProgressMeter - chrM:12891 124.8 11474080 91967.0; 15:29:08.985 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 10.162159898; 15:29:08.986 ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6851:3250,Load,Loading,3250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6851,1,['Load'],['Loading']
Performance,097.log; Use `git lfs logs last` to view the log.; portage$ cat /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; git-lfs/2.3.4 (GitHub; linux amd64; go 1.10); git version 2.16.3. $ git-lfs pull --include src/main/resources/large; No default remote. No remotes defined. Current time in UTC: ; 2018-04-20 20:10:32. ENV:; LocalWorkingDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999; LocalGitDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalGitStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalMediaDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects; LocalReferenceDir=; TempDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/tmp; ConcurrentTransfers=3; TusTransfers=false; BasicTransfersOnly=false; SkipDownloadErrors=false; FetchRecentAlways=false; FetchRecentRefsDays=7; FetchRecentCommitsDays=0; FetchRecentRefsIncludeRemotes=true; PruneOffsetDays=3; PruneVerifyRemoteAlways=false; PruneRemoteName=origin; LfsStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs; AccessDownload=none; AccessUpload=none; DownloadTransfers=basic; UploadTransfers=basic. Client IP addresses:; xx.xx.xx.xx; xx.xx.xx.xx; xx.xx.xx.xx; portage$ ls -latr; total 188; -rw-r--r-- 1 portage portage 428 Apr 20 22:05 codecov.yml; -rwxr-xr-x 1 portage portage 5741 Apr 20 22:05 build_docker.sh; -rw-r--r-- 1 portage portage 32161 Apr 20 22:05 build.gradle; -rw-r--r-- 1 portage portage 37502 Apr 20 22:05 README.md; -rw-r--r-- 1 portage portage 1502 Apr 20 22:05 LICENSE.TXT; -rw-r--r-- 1 portage portage 1555 Apr 20 22:05 Dockerfile; -rw-r--r-- 1 portage portage 1128 Apr 20 22:05 AUTHORS; -rw-r--r-- 1 portage portage 8237 Apr 20 22:05 .travis.yml; -rw-r--r-- 1 portage portage 395 Apr 20 22:05 .gitignore; -rw-r--r-- 1 portage portage 128 Apr 20 22:05 .gitattributes; -rw-r--r-- 1 portage portage ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:15983,Concurren,ConcurrentTransfers,15983,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['Concurren'],['ConcurrentTransfers']
Performance,"0; 15/07/14 13:14:53 INFO spark.ContextCleaner: Cleaned broadcast 0; 15/07/14 13:14:53 ERROR executor.Executor: Exception in task 0.0 in stage 1.0 (TID 1); java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected; at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getOutputFile(TemplatedTextOutputFormat.java:50); at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getDefaultWorkFile(TemplatedTextOutputFormat.java:46); at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:995); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 15/07/14 13:14:53 ERROR util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]; java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected; at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getOutputFile(TemplatedTextOutputFormat.java:50); at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getDefaultWorkFile(TemplatedTextOutputFormat.java:46); at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:995); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713:29404,concurren,concurrent,29404,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713,1,['concurren'],['concurrent']
Performance,0b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `0% <0%> (-89.583%)` | `0% <0%> (-12%)` | |; | [...adinstitute/hellbender/engine/ReadContextData.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZENvbnRleHREYXRhLmphdmE=) | `0% <0%> (-70.37%)` | `0% <0%> (-6%)` | |; | [...n/java/org/broadinstitute/hellbender/utils/KV.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9LVi5qYXZh) | `0% <0%> (-57.143%)` | `0% <0%> (-5%)` | |; | [...walkers/genotyper/afcalc/AFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQUZDYWxjdWxhdG9yUHJvdmlkZXIuamF2YQ==) | `22.222% <0%> (-44.444%)` | `2% <0%> (-2%)` | |; | [...notyper/afcalc/ConcurrentAFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQ29uY3VycmVudEFGQ2FsY3VsYXRvclByb3ZpZGVyLmphdmE=) | `50% <0%> (-33.333%)` | `1% <0%> (-1%)` | |; | [...ls/funcotator/metadata/VcfFuncotationMetadata.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21ldGFkYXRhL1ZjZkZ1bmNvdGF0aW9uTWV0YWRhdGEuamF2YQ==) | `71.429% <0%> (-28.571%)` | `8% <0%> (+3%)` | |; | [...titute/hellbender/engine/TwoPassVariantWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvVHdvUGFzc1ZhcmlhbnRXYWxrZXIuamF2YQ==) | `69.231% <0%> (-26.007%)` | `6% <0%> (+2%)` | |; | ... and [600 more](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5251#issuecomment-426437671:3052,Concurren,ConcurrentAFCalculatorProvider,3052,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5251#issuecomment-426437671,1,['Concurren'],['ConcurrentAFCalculatorProvider']
Performance,"1	NM:i:7	MQ:i:60	AS:i:105	XS:i:20; EOF. bind 'set disable-completion off'. samtools view reads.sam -b > reads.bam; samtools index reads.bam. gatk HaplotypeCaller -R chr19.fa -I reads.bam -O output.g.vcf -ERC GVCF ; bcftools view output.g.vcf -c1 | bcftools annotate -x INFO,FORMAT/SB,FORMAT/PL | tail ; gatk GenotypeGVCFs -R chr19.fa -V output.g.vcf -O output.vcf ; bcftools view output.vcf -c1 | bcftools annotate -x INFO,FORMAT/SB,FORMAT/PL | tail. ```. output:; ```; Using GATK jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar Running: java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar HaplotypeCaller -R chr19.fa -I reads.bam -O output.g.vcf -ERC GVCF Picked up JAVA_TOOL_OPTIONS: -Djava.net.useSystemProxies=true 13:39:56.569 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so 13:39:56.647 INFO HaplotypeCaller - ------------------------------------------------------------ 13:39:56.660 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0 13:39:56.666 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/ 13:39:56.666 INFO HaplotypeCaller - Executing as gleixner@odcf-worker02 on Linux v3.10.0-1160.76.1.el7.x86_64 amd64 13:39:56.666 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17+35-2724 13:39:56.667 INFO HaplotypeCaller - Start Date/Time: October 26, 2023 at 1:39:56 PM CEST 13:39:56.667 INFO HaplotypeCaller - ------------------------------------------------------------ 13:39:56.667 INFO HaplotypeCaller - ------------------------------------------------------------ 13:39:56.669 INFO HaplotypeCaller - HTSJDK Ver",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195:11059,Load,Loading,11059,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195,1,['Load'],['Loading']
Performance,1 from BlockManagerMaster.; 18/01/09 18:31:12 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1515493209401_0001_01_000003 on host: tele-2. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_1515493209401_0001_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:262); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 1. 18/01/09 18:31:12 INFO storage.BlockManagerMaster: Removal of executor 2 requested; 18/01/09 18:31:12 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 2; 18/01/09 18:31:12 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1515493209401_0001_01_000004 on host: tele-6. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_1515493209401_0001_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.n,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:18426,concurren,concurrent,18426,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['concurren'],['concurrent']
Performance,"1) we demonstrate that parameters don't have much of an effect and can be consolidated, or 2) we find more optimal sets of parameters. Potentially we could also show that 3) our parameters are already optimal (I'd say this would be by pure dumb luck), in which case we could at least demonstrate and document some justification for them. If the parameters don't have much of an impact on NA12878, I'm curious to see whether this holds for low coverage or messier data---and ultimately, in malaria. Just starting with NA12878 because of the availability of truth and the potential impact for the primary use case of calling in human data. Some preliminary results: I ran the aforementioned comparison on chr22 with 1) 4.1.8.1 master and 2) 4.1.8.1 with haplotype-to-reference SW parameters changed from `NEW_SW_PARAMETERS` to `STANDARD_NGS` on two replicates of NA12878 (O1D1 and O2D2 from the 2018 NovaSeq snapshot experiment). On each replicate, 2) demonstrated slightly lower performance, but it was well within the sample-to-sample variation between these two replicates. Here are the corresponding vcfeval summaries:. ```; ::::::::::::::; NA12878/O1D1/4.1.8.1/summary.txt; ::::::::::::::; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 84.000 40778 40780 35116 1412 0.5373 0.9665 0.6907; None 41994 41994 43760 196 0.4897 0.9954 0.6564; ::::::::::::::; NA12878/O1D1/STANDARD_NGS/summary.txt; ::::::::::::::; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 84.000 40743 40745 35255 1447 0.5361 0.9657 0.6895; None 41955 41954 43903 235 0.4886 0.9944 0.6553; ::::::::::::::; NA12878/O1D2/4.1.8.1/summary.txt; ::::::::::::::; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sen",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566:1711,perform,performance,1711,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566,1,['perform'],['performance']
Performance,1); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [5838bd7dec2d4533ad090ce03ecc7c0c] entered state [ERROR] while waiting for [DONE].; ```. #### Steps to reproduce. See command given in stack trace above.; WGS bam is available at ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam` ; and ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam.bai`. Interval list BED file content given below. ```; chrX	67113957	67114130; chrX	71903370	71903687; chrX	74330484	74330552; chrX	75379902	75379965; chrX	78441355	78441953; ```. #### Expected behavior; Pass. #### Actual behavior; Error! This could be related to ticket #2722,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:22562,concurren,concurrent,22562,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,2,['concurren'],['concurrent']
Performance,"1-07 11:33:52 INFO YarnClientSchedulerBackend:54 - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> scc-hsn1.scc.bu.edu, PROXY_URI_BASES -> https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0153), /proxy/application_1542127286896_0153; 2019-01-07 11:33:52 INFO JettyUtils:54 - Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 2019-01-07 11:33:52 INFO Client:54 - Application report for application_1542127286896_0153 (state: ACCEPTED); 2019-01-07 11:33:53 INFO YarnSchedulerBackend$YarnSchedulerEndpoint:54 - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 2019-01-07 11:33:53 INFO Client:54 - Application report for application_1542127286896_0153 (state: RUNNING); 2019-01-07 11:33:53 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: 192.168.18.193; ApplicationMaster RPC port: 0; queue: default; start time: 1546878818531; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0153/; user: farrell; 2019-01-07 11:33:53 INFO YarnClientSchedulerBackend:54 - Application application_1542127286896_0153 has started running.; 2019-01-07 11:33:53 INFO Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45270.; 2019-01-07 11:33:53 INFO NettyBlockTransferService:54 - Server created on scc-hadoop.bu.edu:45270; 2019-01-07 11:33:53 INFO BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 2019-01-07 11:33:53 INFO BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, scc-hadoop.bu.edu, 45270, None); 2019-01-07 11:33:53 INFO BlockManagerMasterEndpoint:54 - Registering block manager scc-hadoop.bu.edu:45270 with 408.6 MB RAM, BlockManagerId(driver, scc-hadoop.bu.edu, 45270, None); 2019-01-07 11:33:53 INFO BlockManagerMaster:54 - Register",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:15896,queue,queue,15896,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['queue'],['queue']
Performance,"1-09 13:35:32 INFO Client:54 - Application report for application_1542127286896_0166 (state: ACCEPTED); 2019-01-09 13:35:33 INFO YarnClientSchedulerBackend:54 - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> scc-hsn1.scc.bu.edu, PROXY_URI_BASES -> https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0166), /proxy/application_1542127286896_0166; 2019-01-09 13:35:33 INFO JettyUtils:54 - Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 2019-01-09 13:35:33 INFO YarnSchedulerBackend$YarnSchedulerEndpoint:54 - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 2019-01-09 13:35:33 INFO Client:54 - Application report for application_1542127286896_0166 (state: RUNNING); 2019-01-09 13:35:33 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: 192.168.18.195; ApplicationMaster RPC port: 0; queue: default; start time: 1547058922320; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0166/; user: farrell; 2019-01-09 13:35:33 INFO YarnClientSchedulerBackend:54 - Application application_1542127286896_0166 has started running.; 2019-01-09 13:35:33 INFO Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43627.; 2019-01-09 13:35:33 INFO NettyBlockTransferService:54 - Server created on scc-hadoop.bu.edu:43627; 2019-01-09 13:35:33 INFO BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 2019-01-09 13:35:33 INFO BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, scc-hadoop.bu.edu, 43627, None); 2019-01-09 13:35:33 INFO BlockManagerMasterEndpoint:54 - Registering block manager scc-hadoop.bu.edu:43627 with 372.6 MB RAM, BlockManagerId(driver, scc-hadoop.bu.edu, 43627, None); 2019-01-09 13:35:33 INFO BlockManagerMaster:54 - Register",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:15195,queue,queue,15195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['queue'],['queue']
Performance,"1. Add new arg `genomicsdb-shared-posixfs-optimizations` to help with shared posix filesystems like NFS and Lustre. This turns on `disable file locking` and for GenomicsDB import it minimizes writes to disks. The performance on some of the gatk datasets for the import of about 10 samples went from 23.72m to 6.34m on NFS which was comparable to importing to a local filesystem. Hopefully this helps with Issue #6487 and #6627. Also, fixes Issue #6519.; 2. This version of GenomicsDB also uses pre-compression filters for offset and compression files for new workspaces and genomicsdb arrays. The total sizes for a GenomicsDB workspace using the same dataset as above and the 10 samples went from 313MB to 170MB with no change in import and query times. Smaller GenomicsDB arrays also help with performance on distributed and cloud file systems.; 3. This version has added support to handle MNVs similar to deletions as described in Issue #6500. See [GenomicsDB PR 88](https://github.com/GenomicsDB/GenomicsDB/pull/88). Thanks @kgururaj.; 4. There is added support in the GenomicsDBImporter to have multiple contigs in the same GenomicsDB partition/array. This will hopefully help import times in cases where users have many thousands of contigs. See [GenomicsDB PR 91](https://github.com/GenomicsDB/GenomicsDB/pull/91). Changes will be needed from the gatk side to avail this support. Thanks @mlathara.; 5. Logging has been improved somewhat with the native C/C++ code using [spdlog](https://github.com/gabime/spdlog) and [fmt](https://github.com/fmtlib/fmt) and the Java layer using apache log4j and log4j.properties provided by the application. Also, info messages like `No valid combination operation found for INFO field AA - the field will NOT be part of INFO fields in the generated VCF records` will only be output once for the operation. Also see open PR #6514 for code to actually suppress these warnings for known fields.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6654:42,optimiz,optimizations,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6654,3,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,1. Allow for using separate threads for reading / processing / writing (max 3); 2. Use NM SAM tag instead of edit distance; 3. Perform likelihood scoring on trimmed read. Additional changes include:; 1. CachingIndexedFastaSequenceFile is enhanced to be thread safe and allow for adjusting its cache size. The change involved synchronizing the main query method (getSubsequenceAt) and deriving the cache size from a settable variable rather than from a constant.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8982:127,Perform,Perform,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8982,3,"['Perform', 'cache']","['Perform', 'cache']"
Performance,"1. Why can't standard tools operate on the distributed workspaces? You could run `GenotypeGVCFs` on these workspaces in a distributed fashion and then concatenate the results together if you want. I think you were initially considering this route - and still think it would be more performant. 2. Again, you can process/query a whatever set of intervals you want -- in a distributed fashion, right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-640791782:282,perform,performant,282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-640791782,1,['perform'],['performant']
Performance,"1.774 INFO HaplotypeCaller - HTSJDK Version: 2.20.3; 02:07:51.774 INFO HaplotypeCaller - Picard Version: 2.21.1; 02:07:51.774 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 02:07:51.774 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:07:51.774 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 02:07:51.774 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:07:51.774 INFO HaplotypeCaller - Deflater: IntelDeflater; 02:07:51.774 INFO HaplotypeCaller - Inflater: IntelInflater; 02:07:51.774 INFO HaplotypeCaller - GCS max retries/reopens: 20; 02:07:51.775 INFO HaplotypeCaller - Requester pays: disabled; 02:07:51.775 INFO HaplotypeCaller - Initializing engine; 02:07:52.246 INFO HaplotypeCaller - Done initializing engine; 02:07:52.303 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 02:07:52.312 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 02:07:52.314 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 02:07:52.355 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 02:07:52.355 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 02:07:52.356 INFO IntelPairHmm - Available threads: 104; 02:07:52.356 INFO IntelPairHmm - Requested threads: 4; 02:07:52.356 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 02:07:52.408 INFO ProgressMeter - Starting traversal; 02:07:52.408 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 02:07:53.316 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genoty",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6292:2940,Load,Loading,2940,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6292,1,['Load'],['Loading']
Performance,"1.993 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:40:21.993 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:40:21.993 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 14:40:21.993 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 14:40:21.993 INFO MarkDuplicatesSpark - GCS max retries/reopens: 20; 14:40:21.993 INFO MarkDuplicatesSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:40:21.994 WARN MarkDuplicatesSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: MarkDuplicatesSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 14:40:21.994 INFO MarkDuplicatesSpark - Initializing engine; 14:40:21.994 INFO MarkDuplicatesSpark - Done initializing engine; 14:40:22.338 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 15:24:12.735 INFO ReadsSparkSink - Finished sorting the bam file and dumping read shards to disk, proceeding to merge the shards into a single file using the master thread; 15:41:27.766 INFO ReadsSparkSink - Finished merging shards into a single output bam; 15:41:34.351 INFO MarkDuplicatesSpark - Shutting down engine; [May 7, 2018 3:41:34 PM EDT] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 61.21 minutes.; Runtime.totalMemory()=13635682304; ```. With native libraries (note the lack of the usual warning):. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark --java-options ""-Djava.library.path=${HADOOP_DIR}/hadoop-2.6.5-src/hadoop-common-project/hadoop-common/target/hadoop-common-2.6.5/lib/native"" -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam -- --spark-runner LOCAL --spark-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746:3882,load,load,3882,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746,1,['load'],['load']
Performance,"10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:20:12.111 INFO case_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 10:20:12.273 INFO root - Loading modeling interval list from the provided model...; 10:20:12.475 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 10:20:12.491 INFO root - The model contains 11901 intervals and 23 contig(s); 10:20:12.491 INFO root - Loading 1 read counts file(s)...; 10:20:12.545 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 10:20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC curve knobs: 20; 10:20:12.555 INFO root - - GC curve prior standard deviation: 1.0; 10:20:12.954 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the denoising model...; 10:20:15.806 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the sampler...; 10:20:15.807 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the copy number caller...; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Global model variables: {'log_mean_bias_t', 'psi_t_log__', 'W_tu', 'ard_u_log__'}; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'read_depth_s_log__', 'psi_s_log__', 'z_sg', 'z_su'}",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8740:6489,Load,Loading,6489,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740,1,['Load'],['Loading']
Performance,"113 / 643 of them were CNV test files that I deleted in #3907, which is a shade bit more than 1%... only a small fraction of these are loaded dynamically or are index/dict files, which tests will catch (and they have already, now that I check---6 / 113). I will concede that it is likely that most of the remaining files are index files, etc., but I do see a few more bams, etc. that could stand deletion. I'm curious as to what the best way to do this sort of thing actually is!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905#issuecomment-348598077:135,load,loaded,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905#issuecomment-348598077,1,['load'],['loaded']
Performance,"119579965 4.4 5479000 1242549.2; 15:39:35.700 INFO ProgressMeter - 11:118752077 4.6 5530000 1207397.2; 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; [March 2, 2023 3:40:16 PM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 5.27 minutes.; Runtime.totalMemory()=3432513536; java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to java.lang.Comparable; 	at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.util.TimSort.sort(TimSort.java:234); 	at java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); 	at java.util.Arrays.parallelSort(Arrays.java:1180); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.lang.Thread.run(Thread.java:750); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 		at htsjdk.samtools.AsyncSAM",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:6178,concurren,concurrent,6178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['concurren'],['concurrent']
Performance,11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 718; 11:39:07.668 DEBUG ReadThreadingGraph - Recovered 32 of 33 dangling tails; 11:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:13197,cache,cache,13197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance,"11:57.862 INFO IntelPairHmm - Requested threads: 4; 19:11:57.862 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 19:11:57.862 INFO ProgressMeter - Starting traversal; 19:11:57.862 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; *** glibc detected *** /for/bar/bin/java: double free or corruption (out): 0x00007f450af58700 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x3d01675dee)[0x7f45058afdee]; /lib64/libc.so.6(+0x3d01678c80)[0x7f45058b2c80]; /tmp/libgkl_smithwaterman410767516409374085.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f4499f4cfa8]; /tmp/libgkl_smithwaterman410767516409374085.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7f4499f4cbf8]; [0x7f44f58be6a2]; ======= Memory map: ========; ```. Then we **disabled** AVX2 in the newer cluster using Intels [sde64](https://software.intel.com/en-us/articles/intel-software-development-emulator) with `-ivb`, which directed GATK to use the Java implementation, and the filter worked without core dump. ```; sde64 -ivb -- faa.sh; Using GATK jar /app/gatk-package-4.1.8.0-local.jar; Running:; /bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /app/gatk-package-4.1.8.0-local.jar FilterAlignmentArtifacts -V /output/sample.FilterMutectCalls.vcf.gz -R /db/hs37d5.fa --bwa-mem-index-image /ref/hg38.fa.img -I /output/sample.Mutect2.bam -O sample.somatic_filter2.test.vcf.gz --use-jdk-inflater true --use-jdk-deflater true; 19:41:38.956 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 19:41:39.332 INFO SmithWatermanAligner - AVX accelerated SmithWaterman implementation is not supported, falling back to the Java implementation; ```; Hope this helps and we're looking forward the GKL fix. Cheers,; Richard",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:5815,Load,Loading,5815,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['Load'],['Loading']
Performance,"12** 25380275 . T G . . AS\_SB\_TABLE=3911,5343|26,21;DP=9485;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=18;POPAF=7.30;TLOD=53.53 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:9254,47:4.970e-03:9301:5321,21:3867,26:3911,5343,26,21. The input and the output BAMs show this call with the variant. ![](https://gatk.broadinstitute.org/hc/user_images/FVlI3WhNIzYK7NB7PakCmw.png). In the logs, it shows the detection of an active region here:. 08:01:23.642 INFO Mutect2Engine - Assembling chr12:**2538**0238-**2538**0327 with 19912 reads: (with overlap region = chr12:**2538**0138-**2538**0427). 08:01:24.119 INFO EventMap - >> Events = EventMap{chr12:**2538**0275-**2538**0275 \[T\*, G\],}. 08:01:24.154 INFO AssemblyResultSet - Trimming active region AssemblyRegion chr12:**2538**0238-**2538**0327 active?=true nReads=19912 with 2 haplotypes. 08:01:24.154 INFO AssemblyResultSet - Trimmed region to chr12:**2538**0255-**2538**0295 and reduced number of haplotypes from 2 to only 2. 08:01:25.383 INFO EventMap - >> Events = EventMap{chr12:**2538**0275-**2538**0275 \[T\*, G\],}. I have tried troubleshooting with the steps stated in this \[blog\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant). However, it does not change the output vcf. I used the force-calling mode by giving the above call in an input vcf and the call did appear in the vcf file. **chr12** 25398285 . C A . . AS\_SB\_TABLE=4312,3630|14,8;DP=8096;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=22;POPAF=7.30;TLOD=14.69 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:7942,22:2.576e-03:7964:3609,8:4268,13:4312,3630,14,8. However, I cannot rely on force-calling mutations on a set of calls. I am unsure if I am missing out more calls that are not showing up. Are there any parameters I need to tune so that I do not miss calls like above?<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/136765'>Zendesk ticket #136765</a>)<br>gz#136765</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7232:3992,tune,tune,3992,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232,1,['tune'],['tune']
Performance,"12:52:37.524 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:52:37.524 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:52:37.524 INFO GenomicsDBImport - Deflater: IntelDeflater; 12:52:37.524 INFO GenomicsDBImport - Inflater: IntelInflater; 12:52:37.524 INFO GenomicsDBImport - GCS max retries/reopens: 20; 12:52:37.524 INFO GenomicsDBImport - Requester pays: disabled; 12:52:37.524 INFO GenomicsDBImport - Initializing engine; 12:52:38.096 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:43.641 INFO IntervalArgumentCollection - Processing 134492644 bp from intervals; 12:52:43.720 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 12:52:43.722 INFO GenomicsDBImport - Done initializing engine; 12:52:44.113 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vidmap.json; 12:52:44.113 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/callset.json; 12:52:44.114 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vcfheader.vcf; 12:52:44.114 INFO GenomicsDBImport - Importing to array - /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/genomicsdb_array; 12:52:44.114 INFO ProgressMeter - Starting traversal; 12:52:44.115 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 13:03:44.100 INFO GenomicsDBImport - Importing batch 1 with 2 samples; [TileDB::FileSystem] Error: (sync_path) Cannot sync file; File syncing ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5740:3652,perform,performance,3652,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740,1,['perform'],['performance']
Performance,133GB file on NFS; 1490.96 seconds (95.4 MBytes per second). 30 GB file on NFS; real 5m18.298s (116.6 Mbytes per second); user 5m5.198s. same 30GB file on gcloud:; real 6m46.984s (91.3 Mbytes per second); user 6m6.124s; sys 0m35.352s. 1.6GB on NFS ; real 0m18.243s (91.9 Mbytes per second); user 0m17.073s; sys 0m0.595s. From this I conclude for now that it's not a huge deal - it will become a bottleneck at some point but we're a while away from that point.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1690#issuecomment-211945863:395,bottleneck,bottleneck,395,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1690#issuecomment-211945863,1,['bottleneck'],['bottleneck']
Performance,13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; > 21:13:04.231 INFO GenotypeGVCFs - Initializing engine; > 21:13:11.834 INFO GenotypeGVCFs - Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 21:14:17.635 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.858 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 13 > 12 expanding to 26; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 27 > 26 expanding to 54; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 55 > 54 expanding to 110; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 111 > 110 expanding to 222; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 223 > 222 expanding to 446; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 447 > 446 expanding to 894; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 895 > 894 expanding to 1790; > 21:14:17.874 DEBUG MathUtils$Log10Cache - cache miss 1791 > 1790 expanding to 3582; > 21:14:17.894 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.930 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.937 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:18.507 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:3->4; > 21:14:18.510 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:27:38.720 DEBUG GenotypeLikelihoodCalculators - Expanding capa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:5787,cache,cache,5787,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['cache'],['cache']
Performance,"14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8778"",; ""CHM controlindelPrecision"": ""0.8968"",; ""CHM controlsnpF1Score"": ""0.9813"",; ""CHM controlsnpPrecision"": ""0.9774"",; ""CHM controlsnpRecall"": ""0.9852"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-BenchmarkVCFControlSample/Benchmark/16cd1efe-5cea-403e-8e85-aec15e71bd1d/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""67.35536666666667"",; ""CHM evalHCsystemhours"": ""0.1557166666666667"",; ""CHM evalHCwallclockhours"": ""42.53388888888889"",; ""CHM evalHCwallclockmax"": ""2.7197444444444443"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM evalindelF1Score"": ""0.8778"",; ""CHM evalindelPrecision"": ""0.8968"",; ""CHM evalsnpF1Score"": ""0.9813"",; ""CHM evalsnpPrecision"": ""0.9774"",; ""CHM evalsnpRecall"": ""0.9852"",; ""CHM evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-BenchmarkVCFTestSample/Benchmark/2071078a-158e-4c3e-9b2f-907bd501821b/call-CombineSummaries/summary.csv"",; ""EXOME1 controlindelF1Score"": ""0.7573"",; ""EXOME1 controlindelPrecision"": ""0.6882"",; ""EXOME1 controlsnpF1Score"": ""0.9896"",; ""EXOME1 controlsnpPrecision"": ""0.9852"",; ""EXOME1 controlsnpRecall"": ""0.9941"",; ""EXOME1 controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-EXOME1Sam",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494:11446,cache,cacheCopy,11446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494,1,['cache'],['cacheCopy']
Performance,"14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.S",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:2068,load,load,2068,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,"1508#issuecomment-259727960). I did find that using SWParameterSet.STANDARD_NGS as the parameters instead of CigarUtils.NEW_SW_PARAMETERS will resolve this particular case, but a more comprehensive analysis would be required before we make the change since this will impact a lot of other events too. ---. @ldgauthier commented on [Thu Nov 10 2016](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-259744146). And just to provide a little more context, this is a problem in the clinical context because there's often an analysis step where potential causal variants are filtered out if they are seen in a normal reference panel (like ExAC) at more than some threshold allele frequency. If the representation doesn't match, then that step gets a lot more complicated than the way it's currently done. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-287818162). I head our Intel friends are working on a new SW implementation (or maybe ""just"" optimization?). . @droazen is this relevant to the Intel SW work? And should any/all improvements along these lines be done in GATK4 going forward?. ---. @droazen commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-287885846). @vdauwera Yes, Intel is working on a native implementation of `SmithWaterman` at our request -- it should materialize this quarter. Any changes/fixes to the Java implementation are relevant to Intel's work, since that is the implementation whose output they will have to match :) Of course, we'd certainly like such improvements to be implemented in GATK4 -- whether they are backported to GATK3 as well is up to you. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-287889598). @ldgauthier Any objections to this getting tackled in GATK4?. ---. @ldgauthier commented on [Mon Mar 20 2017](https://github.com/broadinstitu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2498:2677,optimiz,optimization,2677,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2498,1,['optimiz'],['optimization']
Performance,"1625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-BenchmarkVCFControlSample/Benchmark/5388d7b6-6bcd-451d-9a4e-925b386ecd0c/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""95.03499722222222"",; ""NIST evalHCsystemhours"": ""0.17304166666666665"",; ""NIST evalHCwallclockhours"": ""67.81165555555557"",; ""NIST evalHCwallclockmax"": ""3.691061111111111"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-EVALRuntimeTask/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-BenchmarkVCFTestSample/Benchmark/faae76f3-8378-4271-9822-5d2587113415/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-CreateHTMLReport/cacheCopy/report.html""; },; ""errors"": null; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1194801748:21372,cache,cacheCopy,21372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1194801748,1,['cache'],['cacheCopy']
Performance,1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpCl,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:6077,concurren,concurrent,6077,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"17:06.503 INFO FeatureManager - Using codec VCFCodec to read file file:///home/robert/test/snps.vcf. 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals. 16:17:06.551 INFO HaplotypeCaller - Done initializing engine. 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output. 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so. 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded. 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported. 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so. 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded. 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; ```. Since the calculation takes quite long, I checked the WARN messages of the output above. Especially the last one about the AVX instruction set where it says that a **MUCH** slow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794:4606,multi-thread,multi-threaded,4606,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794,1,['multi-thread'],['multi-threaded']
Performance,19-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0775.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0784.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0793.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ1479.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/00000. ...(all the shards fail in the same way). (this is stderr.background for one shard; all 10 shards log the same error). lee04110@ln0005 \[/scratch.global/lee04110/batch\] % cat /scratch.global/lee04110/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/execution/stderr.background. INFO:  Using cached SIF image. INFO:  Using cached SIF image. Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar. Running:.   java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms8000m -Xmx25000m -DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true -jar /gatk/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/inputs/-1806236336/0009-scattered.interval\_list \[...list of input gvcs\]--reader-threads 1 --merge-input-intervals true --consolidate false. Picked up \_JAVA\_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/tmp.bd1b0bc7. 20:38:55.819 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076:14167,cache,cached,14167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076,1,['cache'],['cached']
Performance,"19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:53:34.608 INFO ValidateVariants - Deflater: IntelDeflater; 19:53:34.608 INFO ValidateVariants - Inflater: IntelInflater; 19:53:34.608 INFO ValidateVariants - GCS max retries/reopens: 20; 19:53:34.608 INFO ValidateVariants - Requester pays: disabled; 19:53:34.608 INFO ValidateVariants - Initializing engine; 19:53:35.169 INFO FeatureManager - Using codec VCFCodec to read file file://chr1-22.phased.rename.reheader.vcf.gz; 19:53:35.594 INFO ValidateVariants - Done initializing engine; 19:53:35.594 WARN ValidateVariants - IDS validation cannot be done because no DBSNP file was provided; 19:53:35.594 WARN ValidateVariants - Other possible validations will still be performed; 19:53:35.594 INFO ProgressMeter - Starting traversal; 19:53:35.595 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 19:53:35.660 INFO ValidateVariants - Shutting down engine; [October 25, 2020 7:53:35 PM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2114453504; java.lang.ArrayIndexOutOfBoundsException: -87; 	at org.broadinstitute.hellbender.utils.BaseUtils.convertIUPACtoN(BaseUtils.java:123); 	at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.getSubsequenceAt(CachingIndexedFastaSequenceFile.java:340); 	at org.broadinstitute.hellbender.engine.ReferenceFileSource.queryAndPrefetch(ReferenceFileSource.java:78); 	at org.broadinstitute.hellbender.engine.ReferenceDataSource.queryAndPrefetch(ReferenceDataSource.java:64); 	at org.broadinstitute.hellbender.engine.ReferenceContext.getBases(ReferenceContext.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:3055,perform,performed,3055,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['perform'],['performed']
Performance,2 INFO GermlineCNVCaller - Inflater: IntelInflater; 17:28:28.782 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 17:28:28.782 INFO GermlineCNVCaller - Requester pays: disabled; 17:28:28.782 INFO GermlineCNVCaller - Initializing engine; 17:28:34.716 INFO GermlineCNVCaller - Done initializing engine; 17:28:34.723 INFO GermlineCNVCaller - Intervals specified...; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 17:28:35.689 INFO FeatureManager - Using codec IntervalListCodec to read file file:///media/Data/AnnotationDBs/CNV/Genom/hdf5/../Genom.filtered.interval_list; 17:28:42.892 INFO IntervalArgumentCollection - Processing 2741406000 bp from intervals; 17:28:43.237 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 17:28:51.740 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 17:28:57.410 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 17:28:57.410 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 17:28:57.940 INFO GermlineCNVCaller - Aggregating read-count file 0028-21.hdf5 (1 / 44); 17:29:00.837 INFO GermlineCNVCaller - Aggregating read-count file 0045-21.hdf5 (2 / 44); 17:29:03.690 INFO GermlineCNVCaller - Aggregating read-count file 0098-18.hdf5 (3 / 44); 17:29:06.658 INFO GermlineCNVCaller - Aggregating read-count file 0156-21.hdf5 (4 / 44); 17:29:09.435 INFO GermlineCNVCaller - Aggregating read-count file 0429-20.hdf5 (5 / 44); 17:29:12.235 INFO GermlineCNVCaller - Aggregating read-count file 0779-18.hdf5 (6 / 44); 17:29:14.939 INFO GermlineCNVCaller - Aggregating read-count file 1030-20.hdf5 (7 / 44); 17:29:17.822 INFO GermlineCNVCaller - Aggregating read-count file 1098-13.hdf5 (8 / 44); 17:29:20.668 INFO GermlineCNVCaller - Aggregating,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7234:5465,perform,performed,5465,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234,1,['perform'],['performed']
Performance,"2.0 => GCCcore/12.2.0. WARNING: GATK v4.1.4.1 support for Java 11 is in beta state. Use at your own risk. The following have been reloaded with a version change:; 1) GATK/4.5.0-java-17 => GATK/4.1.4.1-GCCcore-8.3.0-Java-11; 2) GCCcore/12.2.0 => GCCcore/8.3.0; 3) GMP/6.2.1-GCCcore-11.2.0 => GMP/6.1.2-GCCcore-8.3.0; 4) Java/17.0.4 => Java/11.0.16; 5) Python/3.9.6-GCCcore-11.2.0 => Python/3.7.4-GCCcore-8.3.0; 6) SQLite/3.36-GCCcore-11.2.0 => SQLite/3.29.0-GCCcore-8.3.0; 7) Tcl/8.6.11-GCCcore-11.2.0 => Tcl/8.6.9-GCCcore-8.3.0; 8) XZ/5.2.5-GCCcore-11.2.0 => XZ/5.2.4-GCCcore-8.3.0; 9) binutils/2.37-GCCcore-11.2.0 => binutils/2.32-GCCcore-8.3.0; 10) bzip2/1.0.8-GCCcore-11.2.0 => bzip2/1.0.8-GCCcore-8.3.0; 11) libffi/3.4.2-GCCcore-11.2.0 => libffi/3.2.1-GCCcore-8.3.0; 12) libreadline/8.1-GCCcore-11.2.0 => libreadline/8.0-GCCcore-8.3.0; 13) ncurses/6.2-GCCcore-11.2.0 => ncurses/6.1-GCCcore-8.3.0; 14) zlib/1.2.11-GCCcore-11.2.0 => zlib/1.2.11-GCCcore-8.3.0. 13:26:41.785 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/nbt_main/share/module_new/modules/software/GATK/4.1.4.1-GCCcore-8.3.0-Java-11/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Sep 09, 2024 1:26:42 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:26:42.114 INFO CombineGVCFs - ------------------------------------------------------------; 13:26:42.115 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.4.1; 13:26:42.115 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:26:42.115 INFO CombineGVCFs - Executing as ----@gb-fat-01.nbthpc.local on Linux v4.18.0-305.el8.x86_64 amd64; 13:26:42.115 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.16+8; 13:26:42.115 INFO CombineGVCFs - Start Date/Time: September 9, 2024 at 1:26:41 PM ICT; 13:26:42.115 INFO CombineGVCFs - -----------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8974:1289,Load,Loading,1289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8974,1,['Load'],['Loading']
Performance,2.544 DEBUG GenomeLocParser - chrUn_KI270752v1 (27745 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270753v1 (62944 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270754v1 (40191 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270755v1 (36723 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270756v1 (79590 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270757v1 (71251 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_GL000214v1 (137718 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270742v1 (186739 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_GL000216v2 (176608 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_GL000218v1 (161147 bp); 23:44:42.545 DEBUG GenomeLocParser - chrEBV (171823 bp); 23:44:42.632 INFO FeatureManager - Using codec IntervalListCodec to read file file:///gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list; 23:44:42.739 DEBUG FeatureDataSource - Cache statistics for FeatureInput /gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list:/gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list:; 23:44:42.740 DEBUG FeatureCache - Cache hit rate was 0.00% (0 out of 0 total queries); 23:44:42.743 INFO IntervalArgumentCollection - Processing 1022379 bp from intervals; 23:44:42.756 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 23:44:43.119 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 23:44:43.160 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 23:44:43.160 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 23:44:43.160 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 23:44:43.160 DEBUG GenomeLocParser - chr1 (248956422 bp); 23:44:43.161 DEBUG GenomeLocParser - chr2 (242193529 bp); 23:44:43.161 DEBUG GenomeLoc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:19485,Cache,Cache,19485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Cache'],['Cache']
Performance,"20$17960187$17981445/genomicsdb_meta_dir; hdfsBuilderConnect(forceNewInstance=0, nn=gs://hellbender-test-logs, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:; java.io.IOException: Must supply a value for configuration setting: fs.gs.project.id; 	at com.google.cloud.hadoop.util.ConfigurationUtil.getMandatoryConfig(ConfigurationUtil.java:39); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createOptionsBuilderFromConfig(GoogleHadoopFileSystemBase.java:2185); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1832); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1013); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:976); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2812); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:100); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2849); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2831); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:171); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:168); 	at java.base/java.security.AccessController.doPrivileged(Native Method); 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:168); 	at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.generateHea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6522:1234,Cache,Cache,1234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6522,1,['Cache'],['Cache']
Performance,"20-g00a40ea-SNAPSHOT-spark.jar; Running:; /mnt/raid5/frankliu/code/SPARK/spark-2.0.2//bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true --conf spark.executor.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true --deploy-mode client --num-executors 59 --executor-cores 4 --executor-memory 24180M --driver-memory 10G /mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar CountReadsSpark -I hdfs://arlab174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1121,load,load,1121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,20/10/22 12:02:33 INFO spark.SecurityManager: Changing modify acls to: jacky; 20/10/22 12:02:33 INFO spark.SecurityManager: Changing view acls groups to: ; 20/10/22 12:02:33 INFO spark.SecurityManager: Changing modify acls groups to: ; 20/10/22 12:02:33 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jacky); groups with view permissions: Set(); users with modify permissions: Set(jacky); groups with modify permissions: Set(); 20/10/22 12:02:33 INFO yarn.Client: Submitting application application_1603353714322_0004 to ResourceManager; 20/10/22 12:02:33 INFO impl.YarnClientImpl: Submitted application application_1603353714322_0004; 20/10/22 12:02:34 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:34 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: default; 	 start time: 1603360953394; 	 final status: UNDEFINED; 	 tracking URL: http://jacky:8088/proxy/application_1603353714322_0004/; 	 user: jacky; 20/10/22 12:02:35 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:36 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:37 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:38 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:39 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:40 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:41 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:42 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6906:4261,queue,queue,4261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6906,1,['queue'],['queue']
Performance,"20200521s/clinvar/hg38/clinvar_20180429_hg38.vcf; > 15:16:55.199 INFO FeatureManager - Using codec VCFCodec to read file file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/clinvar/hg38/clinvar_20180429_hg38.vcf15:16:55.375 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode_xhgnc_v90_38.hg38.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/gencode_xhgnc/hg38/gencode_xhgnc_v90_38.hg38.tsv; > 15:16:57.746 INFO Funcotator - Initializing Funcotator Engine...; > 15:16:57.777 INFO Funcotator - Creating a VCF file for output: file:/home/pkus/mutect_test/filtered_variants/P1.avcf.gz; > 15:16:57.894 INFO ProgressMeter - Starting traversal; > 15:16:57.894 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 15:16:57.979 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/0; > 15:16:57.981 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/0; > 15:16:57.991 INFO Funcotator - Shutting down engine; > [July 17, 2020 3:16:57 PM CEST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.31 minutes.; > Runtime.totalMemory()=883949568; > java.lang.IllegalArgumentException: Unexpected value: lncRNA; > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$GeneTranscriptType.getEnum(GencodeGtfFeature.java:1052); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.<init>(GencodeGtfFeature.java:158); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfGeneFeature.<init>(GencodeGtfGeneFeature.java:19); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfGeneFeature.create(GencodeGtfGeneFeature.java:23); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureType$1.create(GencodeGtfFeature.java:753); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.create(GencodeGtfFeature.java:320); > at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708:17096,cache,cache,17096,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708,1,['cache'],['cache']
Performance,"27 WARN Funcotator - _ _ _ __ __ _ _ _ _ ; 12:11:32.827 WARN Funcotator - | || || | \ \ / /_ _ _ __ _ __ (_)_ __ __ _ | || || | ; 12:11:32.828 WARN Funcotator - | || || | \ \ /\ / / _` | '__| '_ \| | '_ \ / _` | | || || | ; 12:11:32.828 WARN Funcotator - |_||_||_| \ \V V / (_| | | | | | | | | | | (_| | |_||_||_| ; 12:11:32.828 WARN Funcotator - (_)(_)(_) \_/\_/ \__,_|_| |_| |_|_|_| |_|\__, | (_)(_)(_) ; 12:11:32.828 WARN Funcotator - |___/ ; 12:11:32.828 WARN Funcotator - --------------------------------------------------------------------------------; 12:11:32.828 WARN Funcotator - Only IGRs were produced for this dataset. This STRONGLY indicates that this ; 12:11:32.828 WARN Funcotator - run was misconfigured. ; 12:11:32.828 WARN Funcotator - You MUST check your data sources to make sure they are correct for these data.; 12:11:32.828 WARN Funcotator - ================================================================================; 12:11:32.829 INFO VcfFuncotationFactory - ClinVar_VCF 20180401 cache hits/total: 0/0; 12:11:32.829 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/0; 12:11:32.830 INFO Funcotator - Shutting down engine; [March 24, 2021 12:11:32 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.22 minutes.; Runtime.totalMemory()=1793064960; Tool returned:; true; (gatk) root@75181703d894:/gatk# . ----------------------------------------------------------------------------------------------------------------------------------. the variants.funcotated.maf:. #version 2.4; ##; ## fileformat=VCFv4.2; ## FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ## FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ## FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ## source=Funcotator; ## GATKCommandLine=<ID=Funcotator,CommandLine=""Funcotator --output ./my_data/variants.funcotated.maf --ref-version hg19 --data-sources-path ./my",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:17851,cache,cache,17851,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,1,['cache'],['cache']
Performance,"29876123--Did-not-inflate-expected-amount-Error). \--. Hi! I'm doing WGS analysis of a pedigree of three individuals using GATK 4.2.0.0. Everything went on well for the first individual. However, in the step of generating gvcf file from bam file, I encountered the error \[htsjdk.samtools.SAMFormatException: Did not inflate expected amount\] in the other two of the individuals. Please help me! Thank you in advance!. a) GATK version used:. GATK 4.2.0.0. b) Exact command used:. java -jar /home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar \\ ; ; HaplotypeCaller \\ ; ; \-R /media/ngs/NGS0/Database/RefSeq/Homo\_sapiens\_NCBI\_GRCh38Decoy/Homo\_sapiens/NCBI/GRCh38Decoy/Sequence/WholeGenomeFasta/NewIndex/genome.fa \\ ; ; \-I /media/ngs/BAM5T/WGS\_analysis/Data/9\_BQSRBam/Ped-San-3\_merged\_realigned\_bqsr.bam \\ ; ; \-ERC GVCF \\ ; ; \-O /media/ngs/BAM5T/WGS\_analysis/Data/10\_gvcf/Ped-San-3\_merged\_realigned\_bqsr.g.vcf. c) Entire error log:. 14:14:32.075 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 01, 2021 2:14:32 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:14:32.573 INFO HaplotypeCaller - ------------------------------------------------------------ ; ; 14:14:32.573 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 14:14:32.573 INFO HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:14:32.573 INFO HaplotypeCaller - Executing as ngs@ngs-linux on Linux v5.8.0-59-generic amd64 ; ; 14:14:32.573 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_292-8u292-b10-0ubuntu1~20.04-b10 ; ; 14:14:32.573 INFO HaplotypeCaller - Start Date/Time: 2021111 021431 ; ; 14",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582:1439,Load,Loading,1439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582,1,['Load'],['Loading']
Performance,"29_hg38.vcf -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/clinvar/hg38/clinvar_20180429_hg38.vcf; > 15:16:55.199 INFO FeatureManager - Using codec VCFCodec to read file file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/clinvar/hg38/clinvar_20180429_hg38.vcf15:16:55.375 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode_xhgnc_v90_38.hg38.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/gencode_xhgnc/hg38/gencode_xhgnc_v90_38.hg38.tsv; > 15:16:57.746 INFO Funcotator - Initializing Funcotator Engine...; > 15:16:57.777 INFO Funcotator - Creating a VCF file for output: file:/home/pkus/mutect_test/filtered_variants/P1.avcf.gz; > 15:16:57.894 INFO ProgressMeter - Starting traversal; > 15:16:57.894 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 15:16:57.979 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/0; > 15:16:57.981 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/0; > 15:16:57.991 INFO Funcotator - Shutting down engine; > [July 17, 2020 3:16:57 PM CEST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.31 minutes.; > Runtime.totalMemory()=883949568; > java.lang.IllegalArgumentException: Unexpected value: lncRNA; > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$GeneTranscriptType.getEnum(GencodeGtfFeature.java:1052); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.<init>(GencodeGtfFeature.java:158); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfGeneFeature.<init>(GencodeGtfGeneFeature.java:19); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfGeneFeature.create(GencodeGtfGeneFeature.java:23); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureType$1.create(GencodeGtfFeature.java:753); > at org.broadinstitut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708:17013,cache,cache,17013,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708,1,['cache'],['cache']
Performance,"2:07:51.774 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:07:51.774 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 02:07:51.774 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:07:51.774 INFO HaplotypeCaller - Deflater: IntelDeflater; 02:07:51.774 INFO HaplotypeCaller - Inflater: IntelInflater; 02:07:51.774 INFO HaplotypeCaller - GCS max retries/reopens: 20; 02:07:51.775 INFO HaplotypeCaller - Requester pays: disabled; 02:07:51.775 INFO HaplotypeCaller - Initializing engine; 02:07:52.246 INFO HaplotypeCaller - Done initializing engine; 02:07:52.303 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 02:07:52.312 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 02:07:52.314 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 02:07:52.355 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 02:07:52.355 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 02:07:52.356 INFO IntelPairHmm - Available threads: 104; 02:07:52.356 INFO IntelPairHmm - Requested threads: 4; 02:07:52.356 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 02:07:52.408 INFO ProgressMeter - Starting traversal; 02:07:52.408 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 02:07:53.316 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes; 02:07:53.598 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 1.49244E-4; 02:07:53.598 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0078887480",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6292:3126,Load,Loading,3126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6292,1,['Load'],['Loading']
Performance,"2; > 15:39:25.463 INFO ProgressMeter - 10:119579965 4.4 5479000 1242549.2; > 15:39:35.700 INFO ProgressMeter - 11:118752077 4.6 5530000 1207397.2; > 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; > 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; > 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; > 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; > [March 2, 2023 3:40:16 PM EST]; > org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done.; > Elapsed time: 5.27 minutes.; > Runtime.totalMemory()=3432513536; > java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to; > java.lang.Comparable; > at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); > at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); > at java.util.TimSort.sort(TimSort.java:234); > at; > java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); > at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); > at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); > at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); > at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); > at java.util.Arrays.parallelSort(Arrays.java:1180); > at; > htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); > at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); > at; > htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); > at; > htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); > at java.lang.Thread.run(Thread.java:750); > Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record; > to closed writer.; > at; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:6543,concurren,concurrent,6543,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['concurren'],['concurrent']
Performance,2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:12535,cache,cache,12535,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance,"2_BM.microbe_aligned.paired.bam:33554432+33554432; 20/07/17 09:38:46 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 5); java.util.NoSuchElementException: next on empty iterator; 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:39); 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:37); 	at scala.collection.Iterator$$anon$13.next(Iterator.scala:469); 	at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$PeekingImpl.next(Iterators.java:1155); 	at org.broadinstitute.hellbender.utils.spark.SparkUtils.lambda$putReadsWithTheSameNameInTheSamePartition$7bd206b0$1(SparkUtils.java:190); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)`. Looking at the aligned bams that go into the scoring task, they don't appear to be empty or different to the rest of the cohort. Any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6319#issuecomment-660292360:2064,concurren,concurrent,2064,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6319#issuecomment-660292360,2,['concurren'],['concurrent']
Performance,3 from BlockManagerMaster.; 18/01/09 18:31:15 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1515493209401_0001_01_000005 on host: tele-2. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_1515493209401_0001_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:262); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 1. 18/01/09 18:31:15 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1515493209401_0001_01_000006 on host: tele-6. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_1515493209401_0001_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:21470,concurren,concurrent,21470,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['concurren'],['concurrent']
Performance,3); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:95); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:58); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230); at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1104); ... 31 more; Caused by: java.lang.IllegalAccessError: no such method: org.broadinstitute.hellbender.too,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1315:2535,concurren,concurrent,2535,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315,1,['concurren'],['concurrent']
Performance,3); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.ca,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:14399,cache,cache,14399,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['cache']
Performance,3); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.Defa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:7574,cache,cache,7574,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['cache']
Performance,"30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE"". $min_width; [1] 2. $alpha; [1] 0.01. $nperm; [1] 10000. $pmethod; [1] ""hybrid"". $kmax; [1] 25. $nmin; [1] 200. $eta; [1] 0.05. $trim; [1] 0.025. $undosplits; [1] ""none"". $undoprune; [1] ""0.05"". $undoSD; [1] 3. $help; [1] FALSE. Stderr: Error in sort(abs(diff(genomdat)))[1:n.keep] : ; only 0's may be mixed with negative subscripts; Calls: source ... segment -> inherits -> smooth.CNA -> trimmed.variance; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:163); 	at org.broadinstitute.hellbender.utils.segmenter.RCBSSegmenter.writeSegmentFile(RCBSSegmenter.java:114); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.applySegmentation(PerformSegmentation.java:185); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.doWork(PerformSegmentation.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2944:3008,Perform,PerformSegmentation,3008,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944,4,['Perform'],['PerformSegmentation']
Performance,34); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:89); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:140); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:264); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); ... 44 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:123); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:93); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.handleStorageException(CloudStorageReadChannel.java:242); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(Cloud,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5631:4695,concurren,concurrent,4695,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631,1,['concurren'],['concurrent']
Performance,"359270660945146060; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); 	at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55); 	at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144); 	at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219); 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276); 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322); 	at java.nio.file.Files.walkFileTree(Files.java:2662); 	at java.nio.file.Files.walkFileTree(Files.java:2742); 	at htsjdk.samtools.util.IOUtil.recursiveDelete(IOUtil.java:1344); 	... 3 more; 15:51:41.426 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gss1/home/ldl20190322/a_haoxiaoshuai/z_software/gatk/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 09, 2020 3:51:43 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:51:43.109 INFO ApplyVQSR - ------------------------------------------------------------; 15:51:43.109 INFO ApplyVQSR - The Genome Analysis Toolkit (GATK) v4.1.1.0; 15:51:43.109 INFO ApplyVQSR - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:51:43.109 INFO ApplyVQSR - Executing as ldl20190322@compute20 on Linux v2.6.32-642.el6.x86_64 amd64; 15:51:43.109 INFO ApplyVQSR - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_261-b12; 15:51:43.109 INFO ApplyVQSR - Start Date/Time: November 9, 2020 3:51:41 PM CST; 15:51:43.109 INFO ApplyVQSR - -----------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6948:4552,Load,Loading,4552,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6948,1,['Load'],['Loading']
Performance,38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; Using GATK jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=./ -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/uksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:46:24.742 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.761 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.764 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.764 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.884 INFO BaseRecalibrator - ------------------------------------------------------------; 13:46:24.884 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:46:24.885 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:46:24.885 INFO BaseRecalibrator - Executin,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:12798,Load,Loading,12798,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['Load'],['Loading']
Performance,38:02 Localizing input gs://gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle -> /cromwell_root/gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle; 2020/07/25 01:38:40 Localizing input gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-FilterAlignmentArtifacts/attempt-3/script -> /cromwell_root/script; 2020/07/25 01:38:45 Localization script execution complete.; 2020/07/25 01:38:58 Done localization.; 2020/07/25 01:38:59 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= us.gcr.io/broad-gatk/gatk@sha256:8051adab0ff725e7e9c2af5997680346f3c3799b2df3785dd51d4abdd3da747b /bin/bash /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.6c58e0ba; 01:39:02.909 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 01:39:02.925 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 01:39:02.927 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 01:39:03.142 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 01:39:03.361 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.361 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.1; 01:39:03.361 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:39:03.362 INFO FilterAlignmentArtifacts - Executing as root@3f245e278eba on Linux v4.19.112+ amd64; 01:39:03.362 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 01:39:03.362 INFO FilterAlignmentArtifacts - Start Date/Time:,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:2095,Load,Loading,2095,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['Load'],['Loading']
Performance,3YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `89.062% <> (-3.125%)` | :arrow_down: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `63.158% <> (-5.263%)` | :arrow_down: |; | [...lable/modeling/VariantAnnotationsModelBackend.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zTW9kZWxCYWNrZW5kLmphdmE=) | `100.000% <> ()` | |; | [...sr/scalable/modeling/VariantAnnotationsScorer.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zU2NvcmVyLmphdmE=) | `64.706% <> (-13.072%)` | :arrow_down: |; | [...able/ExtractVariantAnnotationsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9uc0ludGVncmF0aW9uVGVzdC5qYXZh) | `98.214% <> (+1.548%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=gith,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333:2711,scalab,scalable,2711,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333,1,['scalab'],['scalable']
Performance,"3a2bb0d"" in the JAR file name is why I presume that it's based off commit 3a2bb0d. . From the gatk-protected repo code (and also ""gatk"" repo) I added some debug/print statements and saved to a differently named JAR ""eddie.jar"" to help me distinguish my hacking from the original JAR. . The JAVA file where I added the most helpful statements was in CommandLineProgram.java which is actually in ""gatk"" repo (not ""gatk-protected"" repo). If I look at a LOG, I can see ""EAS"" my initials and see c40e75b which appears to be a more recent commit compared to 3a2bb0d. ```; EAS in main!!!!; EAS to call instanceMain second....; EAS to call instanceMain first....; 17:28:40.295 INFO SparkGenomeReadCounts - EAS ABOUT TO CALL instanceMainPostParseArgs in instanceMain in clp.java ; 17:28:40.396 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/cromwell_root/fc-7ac504fc-7fe4-4bc1-89d3-7f16317b8ff4/eddie.jar!/com/intel/gkl/native/libIntelGKL.so; 17:28:40.498 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [December 1, 2016 5:28:40 PM UTC] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts --binsize 5000 --outputFile this.entity_id.coverage.tsv --reference Homo_sapiens_assembly19.fasta --input firecloud-tcga-open-access/tutorial/bams/C835.HCC1143_BL.4.bam --keepXYMT false --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [December 1, 2016 5:28:40 PM UTC] Executing as root@71bfa07f6996 on Linux 3.16.0-0.bpo.4-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: Version:c40e75b-SNAPSHOT; 17:28:40.501 INFO SparkGenomeReadCounts - Defaults.BUFFER_SIZE : 131072; ```. ---. @eddiebroad commented on [Wed Dec 07 2016](https",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2922:4983,load,loaded,4983,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922,1,['load'],['loaded']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 14; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 0; cpu cores	: 14; apicid		: 32; initial apicid	: 32; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 15; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 1; cpu cores	: 14; apicid		: 34; initial apicid	: 34; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:59432,cache,cache,59432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 9; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 10; cpu cores	: 14; apicid		: 20; initial apicid	: 20; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 10; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 11; cpu cores	: 14; apicid		: 22; initial apicid	: 22; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat ps",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:53557,cache,cache,53557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 15; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 1; cpu cores	: 14; apicid		: 34; initial apicid	: 34; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 16; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 2; cpu cores	: 14; apicid		: 36; initial apicid	: 36; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:60606,cache,cache,60606,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 16; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 2; cpu cores	: 14; apicid		: 36; initial apicid	: 36; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 17; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 3; cpu cores	: 14; apicid		: 38; initial apicid	: 38; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:61780,cache,cache,61780,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 17; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 3; cpu cores	: 14; apicid		: 38; initial apicid	: 38; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 18; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 4; cpu cores	: 14; apicid		: 40; initial apicid	: 40; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:62954,cache,cache,62954,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 18; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 4; cpu cores	: 14; apicid		: 40; initial apicid	: 40; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 19; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 5; cpu cores	: 14; apicid		: 42; initial apicid	: 42; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:64128,cache,cache,64128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 19; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 5; cpu cores	: 14; apicid		: 42; initial apicid	: 42; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 20; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.875; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 6; cpu cores	: 14; apicid		: 44; initial apicid	: 44; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:65302,cache,cache,65302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 20; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.875; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 6; cpu cores	: 14; apicid		: 44; initial apicid	: 44; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 21; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 8; cpu cores	: 14; apicid		: 48; initial apicid	: 48; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:66476,cache,cache,66476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 21; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 8; cpu cores	: 14; apicid		: 48; initial apicid	: 48; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 22; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 9; cpu cores	: 14; apicid		: 50; initial apicid	: 50; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:67650,cache,cache,67650,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 22; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 9; cpu cores	: 14; apicid		: 50; initial apicid	: 50; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 23; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 10; cpu cores	: 14; apicid		: 52; initial apicid	: 52; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:68824,cache,cache,68824,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"4 05:09:55,53] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,53] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,53] [info] SubWorkflowStoreActor stopped; [2020-07-14 05:09:55,54] [info] JobStoreActor stopped; [2020-07-14 05:09:55,53] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,54] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,54] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2020-07-14 05:09:55,54] [info] CallCacheWriteActor stopped; [2020-07-14 05:09:55,54] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,54] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2020-07-14 05:09:55,54] [info] IoProxy stopped; [2020-07-14 05:09:55,54] [info] DockerHashActor stopped; [2020-07-14 05:09:55,55] [info] Shutting down connection pool: curAllocated=1 idleQueues.size=1 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2020-07-14 05:09:55,55] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2020-07-14 05:09:55,55] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2020-07-14 05:09:55,55] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2020-07-14 05:09:55,55] [info] KvWriteActor Shutting down: 0 queued messages to process; [2020-07-14 05:09:55,55] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2020-07-14 05:09:55,55] [info] ServiceRegistryActor stopped; [2020-07-14 05:09:55,58] [info] Database closed; [2020-07-14 05:09:55,58] [info] Stream materializer shut down; [2020-07-14 05:09:55,58] [info] WDL HTTP import resolver closed; Workflow 968be82c-eef3-4bdb-a1ab-3d4e2ca70674 transitioned to state Failed",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:10077,queue,queued,10077,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,2,['queue'],['queued']
Performance,"4 INFO HaplotypeCaller - Initializing engine; 03:58:34.384 INFO FeatureManager - Using codec BEDCodec to read file file:///data/b37.chr13.bed; 03:58:34.461 INFO IntervalArgumentCollection - Processing 595907 bp from intervals; 03:58:34.491 INFO HaplotypeCaller - Done initializing engine; 03:58:34.509 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 03:58:34.532 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 03:58:34.536 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 03:58:34.580 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 03:58:34.582 INFO IntelPairHmm - Available threads: 8; 03:58:34.582 INFO IntelPairHmm - Requested threads: 4; 03:58:34.582 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 03:58:34.623 INFO ProgressMeter - Starting traversal; 03:58:34.623 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 03:58:35.812 INFO HaplotypeCaller - 58 read(s) filtered by: ((((((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter) AND NotDuplicateReadFilter) AND PassesVendorQualityCheckReadFilter) AND NonZeroReferenceLengthAlignmentReadFilter) AND GoodCigarReadFilter) AND WellformedReadFilter); 58 read(s) filtered by: (((((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter) AND NotDuplicateReadFilter) AND PassesVendorQualityCheckReadFilter) AND NonZeroReferenceLengthAlignmentReadFilter) AND GoodCigarReadFilter); 58 read(s) filtered by: ((((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8149:11765,multi-thread,multi-threaded,11765,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149,1,['multi-thread'],['multi-threaded']
Performance,"4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like its performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so its not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the task data - or use a Spark accumulator), and then seeing if there's some way to make it more uniform.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:2454,optimiz,optimizing,2454,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884,2,"['cache', 'optimiz']","['cached', 'optimizing']"
Performance,"4.0.0/test/./02/callset.json; 10:19:39.951 INFO GenomicsDBImport - Complete VCF Header will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vcfheader.vcf; 10:19:39.951 INFO GenomicsDBImport - Importing to workspace - /home/test/Software/gatk-4.4.0.0/test/./02; 10:19:40.060 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 10:19:40.075 INFO GenomicsDBImport - Shutting down engine; org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; java.lang.NumberFormatException: For input string: ""G""; 	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67); 	at java.base/java.lang.Integer.parseInt(Integer.java:668); 	at java.base/java.lang.Integer.parseInt(Integer.java:786); 	at htsjdk.tribble.readers.TabixReader.getIntv(TabixReader.java:337); 	at htsjdk.tribble.readers.TabixReader.access$500(TabixReader.java:48); 	at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:438); 	at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:46); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.<init>(TabixFeatureReader.java:159); 	at htsjdk.tribble.TabixFeatureReader.query(TabixFeatureReader.java:133); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$1.query(GenomicsDBImport.java:971); 	at org.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); 	at org.genomicsdb.importer.GenomicsDBImporter.lambda$null$4(GenomicsDBImporter.java:732); 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635); 	at java.base/java.lang.Thread.run(Thread.java:833)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8517:4326,concurren,concurrent,4326,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517,3,['concurren'],['concurrent']
Performance,4.1.7.0 VariantAnnotator Performance issue,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6663:25,Perform,Performance,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6663,1,['Perform'],['Performance']
Performance,41.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:35:41.882 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 11:35:41.997 INFO ProgressMeter - Starting traversal; 11:35:41.997 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:35:42.019 DEBUG ReadsPathDataSource - Preparing readers for traversal; 11:35:42.470 DEBUG Mutect2 - Processing assembly region at chrM:1-300 isActive: false numReads: 0; 11:35:42.497 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.520 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:7415,cache,cache,7415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance,"4180M --driver-memory 10G /mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar CountReadsSpark -I hdfs://arlab174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.N",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1761,load,loadLibrary,1761,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['loadLibrary']
Performance,45 DEBUG GenomeLocParser - chrUn_GL000216v2 (176608 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_GL000218v1 (161147 bp); 23:44:42.545 DEBUG GenomeLocParser - chrEBV (171823 bp); 23:44:42.632 INFO FeatureManager - Using codec IntervalListCodec to read file file:///gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list; 23:44:42.739 DEBUG FeatureDataSource - Cache statistics for FeatureInput /gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list:/gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list:; 23:44:42.740 DEBUG FeatureCache - Cache hit rate was 0.00% (0 out of 0 total queries); 23:44:42.743 INFO IntervalArgumentCollection - Processing 1022379 bp from intervals; 23:44:42.756 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 23:44:43.119 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 23:44:43.160 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 23:44:43.160 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 23:44:43.160 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 23:44:43.160 DEBUG GenomeLocParser - chr1 (248956422 bp); 23:44:43.161 DEBUG GenomeLocParser - chr2 (242193529 bp); 23:44:43.161 DEBUG GenomeLocParser - chr3 (198295559 bp); 23:44:43.161 DEBUG GenomeLocParser - chr4 (190214555 bp); 23:44:43.161 DEBUG GenomeLocParser - chr5 (181538259 bp); 23:44:43.161 DEBUG GenomeLocParser - chr6 (170805979 bp); 23:44:43.161 DEBUG GenomeLocParser - chr7 (159345973 bp); 23:44:43.161 DEBUG GenomeLocParser - chr8 (145138636 bp); 23:44:43.161 DEBUG GenomeLocParser - chr9 (138394717 bp); 23:44:43.161 DEBUG GenomeLocParser - chr10 (133797422 bp); 23:44:43.161 DEBUG GenomeLocParser - chr11 (135086622 bp); 23:44:43.161 DEBUG GenomeLocParser - ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:20118,perform,performed,20118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['perform'],['performed']
Performance,45816 +473 ; Branches 16090 16107 +17 ; ===============================================; + Hits 126517 126891 +374 ; - Misses 12966 13048 +82 ; - Partials 5860 5877 +17; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5601?src=pr&el=tree) | Coverage  | Complexity  | |; |---|---|---|---|; | [...kers/variantutils/CalculateGenotypePosteriors.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9DYWxjdWxhdGVHZW5vdHlwZVBvc3RlcmlvcnMuamF2YQ==) | `92.857% <100%> ()` | `17 <0> ()` | :arrow_down: |; | [...walkers/genotyper/afcalc/AFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQUZDYWxjdWxhdG9yUHJvdmlkZXIuamF2YQ==) | `22.222% <0%> (-44.444%)` | `2% <0%> (-2%)` | |; | [...notyper/afcalc/ConcurrentAFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQ29uY3VycmVudEFGQ2FsY3VsYXRvclByb3ZpZGVyLmphdmE=) | `50% <0%> (-33.333%)` | `1% <0%> (-1%)` | |; | [...nder/utils/downsampling/PositionalDownsampler.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9kb3duc2FtcGxpbmcvUG9zaXRpb25hbERvd25zYW1wbGVyLmphdmE=) | `88.462% <0%> (-11.538%)` | `22% <0%> (+1%)` | |; | [...er/engine/spark/datasources/VariantsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvVmFyaWFudHNTcGFya1NpbmsuamF2YQ==) | `78.125% <0%> (-11.53%)` | `8% <0%> (-1%)` | |; | [...broadinstitute/hellbender/engine/FeatureInput.java](https://codecov.io/,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5601#issuecomment-456985343:1611,Concurren,ConcurrentAFCalculatorProvider,1611,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5601#issuecomment-456985343,1,['Concurren'],['ConcurrentAFCalculatorProvider']
Performance,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:18785,load,loaded,18785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['load'],"['load', 'loaded']"
Performance,"47:00.976 INFO HaplotypeCaller - HTSJDK Version: 2.19.0; 15:47:00.976 INFO HaplotypeCaller - Picard Version: 2.19.0; 15:47:00.979 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:47:00.979 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:47:00.979 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:47:00.979 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:00.980 INFO HaplotypeCaller - Deflater: JdkDeflater; 15:47:00.981 INFO HaplotypeCaller - Inflater: JdkInflater; 15:47:00.981 INFO HaplotypeCaller - GCS max retries/reopens: 20; 15:47:00.981 INFO HaplotypeCaller - Requester pays: disabled; 15:47:00.981 INFO HaplotypeCaller - Initializing engine; 15:47:15.632 INFO HaplotypeCaller - Done initializing engine; 15:47:20.372 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 15:47:20.380 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/glier_ubuntu/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 15:47:20.391 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/glier_ubuntu/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 15:47:20.423 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 15:47:20.423 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 15:47:20.423 INFO IntelPairHmm - Available threads: 40; 15:47:20.423 INFO IntelPairHmm - Requested threads: 4; 15:47:20.423 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 15:47:22.213 INFO ProgressMeter - Starting traversal; 15:47:22.213 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 15:47:22.231 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 15:47:22.231 INFO PairHMM - Total compute time in P",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6604:5724,Load,Loading,5724,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6604,1,['Load'],['Loading']
Performance,"4g; gatk Funcotator \; 	--variant ../relapse.filtered.snps.indels.vcf \; 	--reference $fa \; 	--ref-version hg38 \; 	--data-sources-path $func \; 	--output relapse.funcotated.maf \; 	--output-file-format MAF; ```; after running the script above, it stucked and show nothing anymore, is there something I ignored ?. ```; Using GATK jar /share/share/soft/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/share/soft/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar Funcotator --variant ../relapse.filtered.snps.indels.vcf --reference /share/share/data/NGS/ref_index/GATK_bundle/hg38/Homo_sapiens_assembly38.fasta --ref-version hg38 --data-sources-path /share/share/data/NGS/ref_index/GATK_bundle/funcotator_dataSources.v1.6.20190124g --output relapse.funcotated.maf --output-file-format MAF; 10:24:47.787 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/share/soft/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:50.558 INFO Funcotator - ------------------------------------------------------------; 10:24:50.559 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.0.0; 10:24:50.559 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:24:50.560 INFO Funcotator - Executing as javis@node4 on Linux v3.10.0-514.el7.x86_64 amd64; 10:24:50.560 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 10:24:50.560 INFO Funcotator - Start Date/Time: April 24, 2019 10:24:47 AM CST; 10:24:50.560 INFO Funcotator - ------------------------------------------------------------; 10:24:50.560 INFO Funcotator - ------------------------------------------------------------; 10:24:50.561 INFO Funcotator - HTSJDK Version: 2.18.2; 10:24:50.561 INFO Funcotator - Picard Version: 2.18.25; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5903:1311,Load,Loading,1311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5903,1,['Load'],['Loading']
Performance,5 from BlockManagerMaster.; 18/01/09 18:31:18 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1515493209401_0001_01_000007 on host: tele-2. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_1515493209401_0001_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:262); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 1. 18/01/09 18:31:18 INFO storage.BlockManagerMaster: Removal of executor 6 requested; 18/01/09 18:31:18 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 6 from BlockManagerMaster.; 18/01/09 18:31:18 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 6; 18/01/09 18:31:18 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1515493209401_0001_01_000008 on host: tele-6. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_1515493209401_0001_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:24514,concurren,concurrent,24514,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['concurren'],['concurrent']
Performance,5); at htsjdk.samtools.MemoryMappedFileBuffer.readBytes(MemoryMappedFileBuffer.java:34); at htsjdk.samtools.AbstractBAMFileIndex.readBytes(AbstractBAMFileIndex.java:439); at htsjdk.samtools.AbstractBAMFileIndex.verifyIndexMagicNumber(AbstractBAMFileIndex.java:376); at htsjdk.samtools.AbstractBAMFileIndex.<init>(AbstractBAMFileIndex.java:70); at htsjdk.samtools.AbstractBAMFileIndex.<init>(AbstractBAMFileIndex.java:64); at htsjdk.samtools.CachingBAMFileIndex.<init>(CachingBAMFileIndex.java:56); at htsjdk.samtools.BAMFileReader.getIndex(BAMFileReader.java:418); at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:952); at htsjdk.samtools.BAMFileReader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:125); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:66); at org.broadinstitute.hellbender.engine.ReadsDataSource.prepareIteratorsForTraversal(ReadsDataSource.java:416); at org.broadinstitute.hellbender.engine.ReadsDataSource.iterator(ReadsDataSource.java:342); at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-681608709:1367,load,loadNextIterator,1367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-681608709,1,['load'],['loadNextIterator']
Performance,"5.6; 10:25:54.821 INFO ProgressMeter - chr2:237512416 1.5 1069999 691712.8; 10:26:04.863 INFO ProgressMeter - chr3:54999378 1.7 1197525 698570.8; 10:26:09.642 INFO CalibrateDragstrModel - Shutting down engine; [January 2, 2023 at 10:26:09 AM GMT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 1.81 minutes.; Runtime.totalMemory()=47647293440; java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006); at org.broadinstitute.hellbender.utils.Utils.runInParallel(Utils.java:1479); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsParallel(CalibrateDragstrModel.java:551); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:202); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139:6079,concurren,concurrent,6079,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139,1,['concurren'],['concurrent']
Performance,5/NA24631-chr15_68578892_84670250-block-regions.bed --interval_set_rule INTERSECTION --sparkMaster local[16] --conf spark.local.dir=/mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2017-10-03-104521.457/root/variantcall/8/variantcall_batch_region/16/bcbiotx/tmp7exzBH --annotation ClippingRankSumTest --annotation DepthPerSampleHC --output /mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2017-10-03-104521.457/root/variantcall/8/variantcall_batch_region/16/bcbiotx/tmp7exzBH/NA24631-chr15_68578892_84670250-block.vcf.gz --emitRefConfidence GVCF -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 60 -GQB 80; ```; and the full traceback is:; ```; 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3659:2691,concurren,concurrent,2691,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3659,1,['concurren'],['concurrent']
Performance,50% speedup for ApplyBQSR by removing indel quals and other optimizations,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1099:60,optimiz,optimizations,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1099,1,['optimiz'],['optimizations']
Performance,"53 ERROR executor.Executor: Exception in task 0.0 in stage 1.0 (TID 1); java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected; at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getOutputFile(TemplatedTextOutputFormat.java:50); at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getDefaultWorkFile(TemplatedTextOutputFormat.java:46); at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:995); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 15/07/14 13:14:53 ERROR util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]; java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected; at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getOutputFile(TemplatedTextOutputFormat.java:50); at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getDefaultWorkFile(TemplatedTextOutputFormat.java:46); at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:995); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713:29488,concurren,concurrent,29488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713,1,['concurren'],['concurrent']
Performance,"538 samples. Chrom 11-22 don't have problems, but 1-11 don't work.; What should I do?; thanks. ```; java -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/solivehong/miniconda3/envs/bio_base/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar GenotypeGVCFs -R /data/reference/update_gatk_v0/Homo_sapiens_assembly38.fasta -V gendb://Genomicsdb.2 -O /storage/project/collaborators/UH_Burdentest/1.running/genotypeGvcf/UH_Burdentest2222.vcf --tmp-dir /storage/GenomesDbimport/Agilent_WES/tmp -L chr2 -G StandardAnnotation --only-output-calls-starting-in-intervals --use-new-qual-calculator -D /data/reference/update_gatk_v0//Homo_sapiens_assembly38.dbsnp138.vcf; 20:09:21.335 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 20:09:21.383 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/solivehong/miniconda3/envs/bio_base/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:09:21.521 INFO GenotypeGVCFs - ------------------------------------------------------------; 20:09:21.521 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.3.0.0; 20:09:21.521 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 20:09:21.521 INFO GenotypeGVCFs - Executing as solivehong@solivehong on Linux v5.10.0-25-amd64 amd64; 20:09:21.521 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.13+7-b1751.21; 20:09:21.522 INFO GenotypeGVCFs - Start Date/Time: September 23, 2023 at 8:09:21 PM CST; 20:09:21.522 INFO GenotypeGVCFs - ------------------------------------------------------------; 20:09:21.522 INFO GenotypeGVCFs - ------------------------------------------------------------; 20:09:21.522 INFO GenotypeGVCFs - HTSJDK Version:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8527:1074,Load,Loading,1074,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8527,1,['Load'],['Loading']
Performance,55v1 (36723 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270756v1 (79590 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270757v1 (71251 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_GL000214v1 (137718 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270742v1 (186739 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_GL000216v2 (176608 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_GL000218v1 (161147 bp); 23:44:42.545 DEBUG GenomeLocParser - chrEBV (171823 bp); 23:44:42.632 INFO FeatureManager - Using codec IntervalListCodec to read file file:///gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list; 23:44:42.739 DEBUG FeatureDataSource - Cache statistics for FeatureInput /gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list:/gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list:; 23:44:42.740 DEBUG FeatureCache - Cache hit rate was 0.00% (0 out of 0 total queries); 23:44:42.743 INFO IntervalArgumentCollection - Processing 1022379 bp from intervals; 23:44:42.756 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 23:44:43.119 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 23:44:43.160 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 23:44:43.160 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 23:44:43.160 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 23:44:43.160 DEBUG GenomeLocParser - chr1 (248956422 bp); 23:44:43.161 DEBUG GenomeLocParser - chr2 (242193529 bp); 23:44:43.161 DEBUG GenomeLocParser - chr3 (198295559 bp); 23:44:43.161 DEBUG GenomeLocParser - chr4 (190214555 bp); 23:44:43.161 DEBUG GenomeLocParser - chr5 (181538259 bp); 23:44:43.161 DEBUG GenomeLocParser - chr6 (170805979 bp); 23:44:43.161 DEBUG GenomeLocParser ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:19777,Cache,Cache,19777,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Cache'],['Cache']
Performance,"581.5; 06:47:23.111 INFO ProgressMeter - NC_038255.2:27050560 21.0 121742000 5793973.2; 06:47:33.116 INFO ProgressMeter - NC_038255.2:27256247 21.2 122736000 5795288.5; 06:47:42.432 INFO CombineGVCFs - Shutting down engine; [March 13, 2024 at 6:47:42 AM GMT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 21.46 minutes.; Runtime.totalMemory()=920649728; htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Transport endpoint is not connected; at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:48); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:205); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:149); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:24); at org.broadinstitute.hellbender.engine.MultiVariantDataSource$1.next(MultiVariantDataSource.java:408); at org.broadinstitute.hellbender.engine.MultiVariantDataSource$1.next(MultiVariantDataSource.java:393); at htsjdk.samtools.util.PeekableIterator.advance(PeekableIterator.java:71); at htsjdk.samtools.util.PeekableIterator.next(PeekableIterator.java:57); at htsjdk.samtools.util.MergingIterator.next(MergingIterator.java:101); at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1845); at java.base/java.util.stream.AbstractPipeline.co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8735:22683,load,loadNextFeature,22683,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735,1,['load'],['loadNextFeature']
Performance,"58875536 4.8 5592000 1176709.9; 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; [March 2, 2023 3:40:16 PM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 5.27 minutes.; Runtime.totalMemory()=3432513536; java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to java.lang.Comparable; 	at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.util.TimSort.sort(TimSort.java:234); 	at java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); 	at java.util.Arrays.parallelSort(Arrays.java:1180); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.lang.Thread.run(Thread.java:750); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 		at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); 		at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SAMFileGATK",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:6318,concurren,concurrent,6318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['concurren'],['concurrent']
Performance,"598-b0c5267647ee/call-ImportGVCFs/shard-9/execution/stderr.background. INFO:  Using cached SIF image. INFO:  Using cached SIF image. Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar. Running:.   java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms8000m -Xmx25000m -DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true -jar /gatk/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/inputs/-1806236336/0009-scattered.interval\_list \[...list of input gvcs\]--reader-threads 1 --merge-input-intervals true --consolidate false. Picked up \_JAVA\_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/tmp.bd1b0bc7. 20:38:55.819 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so. 20:38:56.233 INFO GenomicsDBImport - ------------------------------------------------------------. 20:38:56.233 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.6.1. 20:38:56.234 INFO GenomicsDBImport - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/). 20:38:56.237 INFO GenomicsDBImport - Executing as [lee04110@cn2006.mesabi.msi.umn.edu](mailto:lee04110@cn2006.mesabi.msi.umn.edu) on Linux v3.10.0-1160.76.1.el7.x86\_64 amd64. 20:38:56.237 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_242-8u242-b08-0ubuntu3~18.04-b08. 20:38:56.238 INFO GenomicsDBImport - Start Date/Time: October 18, 2022 8:38:55 PM GMT. 20:38:56.238 INFO GenomicsDBImport - ------------------------------------------------------------. 20:38:56.238 INFO GenomicsDBImport - ---",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076:15082,Load,Loading,15082,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076,1,['Load'],['Loading']
Performance,"6 . AC=2;AF=1.00;AN=2;DP=13;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.08;SOR=1.179 GT:AD:DP:GQ:PL 1/1:0,13:13:39:392,39,0; 13 32929232 . A G 168.64 . AC=1;AF=0.500;AN=2;BaseQRankSum=1.335;DP=16;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=15.33;ReadPosRankSum=-1.442;SOR=0.446 GT:AD:DP:GQ:PL 0/1:5,6:11:99:176,0,121; 13 32929387 . T C 209.02 . AC=2;AF=1.00;AN=2;DP=7;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.86;SOR=1.609 GT:AD:DP:GQ:PL 1/1:0,7:7:21:223,21,0; ```. Execution log:; ```; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller --input sample.bam --annotation OrientationBiasReadCounts --intervals b37.chr13.bed --reference hs37d5.fa --output sample.vcf.gz; 03:56:44.380 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2023 3:56:44 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 03:56:44.703 INFO HaplotypeCaller - ------------------------------------------------------------; 03:56:44.704 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.2.0; 03:56:44.704 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:56:44.705 INFO HaplotypeCaller - Executing as root@d2b0ea7e4079 on Linux v5.10.76-linuxkit amd64; 03:56:44.705 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 03:56:44.705 INFO HaplotypeCaller - Start Date/Time: January 6, 2023 3:56:44 AM GMT; 03:56:44.705 INFO HaplotypeCaller - ------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8149:2567,Load,Loading,2567,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149,1,['Load'],['Loading']
Performance,"6 INFO Mutect2 - Initializing engine; 14:13:46.660 INFO FeatureManager - Using codec VCFCodec to read file gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz; 14:13:48.823 INFO FeatureManager - Using codec VCFCodec to read file gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz; 14:13:54.570 INFO FeatureManager - Using codec IntervalListCodec to read file gs://fc-secure-76d1542e-1c49-4411-8268-e41e92f9f311/729d209c-0ef4-409f-b3af-2e84ff45ee36/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list; 14:13:55.076 INFO IntervalArgumentCollection - Processing 308828640 bp from intervals; 14:13:55.233 INFO Mutect2 - Done initializing engine; 14:13:56.023 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:13:56.039 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:13:56.116 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:13:56.122 INFO IntelPairHmm - Available threads: 1; 14:13:56.123 INFO IntelPairHmm - Requested threads: 4; 14:13:56.123 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:13:56.127 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:13:56.302 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 14:13:56.492 INFO ProgressMeter - Starting traversal; 14:13:56.493 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:14:08.796 INFO ProgressMeter - chr1:16085 0.2 60 292.6; 14:14:09.377 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.008674977; 14:14:09.3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849:3402,Load,Loading,3402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849,1,['Load'],['Loading']
Performance,6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); 	... 3 more; Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:602); 	... 8 more; Caused by: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:640); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$600(GenomicsDBImport.java:593); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	... 3 more; Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: ComputeEngineCredentials cannot find the metadata server. ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:2557,concurren,concurrent,2557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,"6/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list; 14:13:55.076 INFO IntervalArgumentCollection - Processing 308828640 bp from intervals; 14:13:55.233 INFO Mutect2 - Done initializing engine; 14:13:56.023 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:13:56.039 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:13:56.116 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:13:56.122 INFO IntelPairHmm - Available threads: 1; 14:13:56.123 INFO IntelPairHmm - Requested threads: 4; 14:13:56.123 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:13:56.127 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:13:56.302 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 14:13:56.492 INFO ProgressMeter - Starting traversal; 14:13:56.493 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:14:08.796 INFO ProgressMeter - chr1:16085 0.2 60 292.6; 14:14:09.377 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.008674977; 14:14:09.378 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.28976746200000003; 14:14:09.378 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 1.41 sec; 14:14:09.384 INFO Mutect2 - Shutting down engine; [May 13, 2022 2:14:09 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.49 minutes.; Runtime.totalMemory()=850644992; java.lang.ArrayIndexOutOfBoundsException: -1; at java.util.ArrayList.ele",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849:3851,multi-thread,multi-threaded,3851,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849,1,['multi-thread'],['multi-threaded']
Performance,603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:12444,cache,cache,12444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance,"617); at java.lang.Thread.run(Thread.java:745); 21/04/13 07:32:24 ERROR SparkHadoopWriter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:6317,concurren,concurrent,6317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['concurren'],['concurrent']
Performance,"625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-BenchmarkVCFControlSample/Benchmark/8cf95ec9-48a7-4e20-a8fe-816dc3e652ae/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""100.56416111111112"",; ""NIST evalHCsystemhours"": ""0.19999166666666665"",; ""NIST evalHCwallclockhours"": ""74.00048055555555"",; ""NIST evalHCwallclockmax"": ""4.007605555555555"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-EVALRuntimeTask/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-BenchmarkVCFTestSample/Benchmark/6b79227b-3ca8-4f5b-96b6-60d57760cc5b/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-CreateHTMLReport/cacheCopy/report.html""; },; ""errors"": null; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1533946590:22032,cache,cacheCopy,22032,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1533946590,1,['cache'],['cacheCopy']
Performance,"625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-BenchmarkVCFControlSample/Benchmark/9f6d4e85-981d-4607-8ff6-97495034807f/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""96.65376666666666"",; ""NIST evalHCsystemhours"": ""0.17881944444444442"",; ""NIST evalHCwallclockhours"": ""68.38394444444445"",; ""NIST evalHCwallclockmax"": ""3.8226138888888888"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-EVALRuntimeTask/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-BenchmarkVCFTestSample/Benchmark/e62b142c-c39c-4c1f-9a08-c41a96647879/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-CreateHTMLReport/cacheCopy/report.html""; },; ""errors"": null; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1182703672:21372,cache,cacheCopy,21372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1182703672,1,['cache'],['cacheCopy']
Performance,"64le system. When I use; > HaplotypeCaller, I see the following messages on the screen:; >; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; >; > INFO: Failed to detect whether we are running on Google Compute Engine.; >; > 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.843 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; >; > 16:17:05.843 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/ga",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:1970,Load,Loading,1970,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['Load'],['Loading']
Performance,"6:20:57.252 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:57.252 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:57.252 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:57.252 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:57.252 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:20:57.253 INFO GenomicsDBImport - Inflater: IntelInflater; 16:20:57.253 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:20:57.253 INFO GenomicsDBImport - Requester pays: disabled; 16:20:57.253 INFO GenomicsDBImport - Initializing engine; 16:20:57.921 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/PoN_gvcf/xgen_plus_spikein.b38.bed; 16:20:58.514 INFO IntervalArgumentCollection - Processing 38997831 bp from intervals; 16:20:58.591 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 16:20:58.594 INFO GenomicsDBImport - Done initializing engine; 16:20:58.829 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/PoN_gvcf/pon_db/vidmap.json; 16:20:58.829 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/PoN_gvcf/pon_db/callset.json; 16:20:58.829 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/PoN_gvcf/pon_db/vcfheader.vcf; 16:20:58.829 INFO GenomicsDBImport - Importing to array - /mnt/PoN_gvcf/pon_db/genomicsdb_array; 16:20:58.830 WARN GenomicsDBImport - GenomicsDBImport cannot use multiple VCF reader threads for initialization when the number of intervals is greater than 1. Falling back to serial VCF reader initialization.; 16:20:58.830 INFO ProgressMeter - Starting traversal; 16:20:58.830 INFO ProgressMeter - Current Loc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6158:3736,perform,performance,3736,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6158,1,['perform'],['performance']
Performance,7-10-03-104521.457/root/variantcall/8/variantcall_batch_region/16/gatk-haplotype/chr15/NA24631-chr15_68578892_84670250-block-regions.bed --interval_set_rule INTERSECTION --sparkMaster local[16] --conf spark.local.dir=/mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2017-10-03-104521.457/root/variantcall/8/variantcall_batch_region/16/bcbiotx/tmp7exzBH --annotation ClippingRankSumTest --annotation DepthPerSampleHC --output /mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2017-10-03-104521.457/root/variantcall/8/variantcall_batch_region/16/bcbiotx/tmp7exzBH/NA24631-chr15_68578892_84670250-block.vcf.gz --emitRefConfidence GVCF -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 60 -GQB 80; ```; and the full traceback is:; ```; 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3659:2606,concurren,concurrent,2606,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3659,1,['concurren'],['concurrent']
Performance,"7.131 INFO PostprocessGermlineCNVCalls - Shutting down engine ; ; \[August 30, 2021 11:04:37 AM HKT\] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.27 minutes. ; ; Runtime.totalMemory()=2463105024 ; ; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; ; python exited with 1 ; ; Command Line: python /tmp/segment\_gcnv\_calls.8152704641395924200.py --ploidy\_calls\_path /staging/wes/healthy\_bams\_for\_CNV/using\_v7\_probe/v7\_case\_ploidy/v7\_cases\_ploidy\_1\_sample\_20210615-calls --model\_shards /staging/wes/healthy\_bams\_for\_C ; ; Stdout: 11:04:36.532 INFO segment\_gcnv\_calls - THEANO\_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast\_run,compute\_test\_value=ignore,openmp=true,blas.ldflags=-lmkl\_rt,openmp\_elemwise\_minsize=10 ; ; 11:04:36.532 INFO segment\_gcnv\_calls - Loading ploidy calls... ; ; 11:04:36.533 INFO gcnvkernel.io.io\_metadata - Loading germline contig ploidy and global read depth metadata... ; ; 11:04:36.543 INFO segment\_gcnv\_calls - Instantiating the Viterbi segmentation engine... Stderr: Traceback (most recent call last): ; ; File ""/tmp/segment\_gcnv\_calls.8152704641395924200.py"", line 92, in <module> ; ; args.intervals\_vcf, args.clustered\_vcf) ; ; TypeError: \_\_init\_\_() takes 6 positional arguments but 8 were given. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75) ; ; at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:112) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:193) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:168) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:139) ; ; at org.broadinstitute.hellbender.tools.copynu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444:5460,Load,Loading,5460,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444,1,['Load'],['Loading']
Performance,7.246 INFO Mutect2 - HTSJDK Version: 2.20.3; 15:47:37.246 INFO Mutect2 - Picard Version: 2.21.1; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:37.247 INFO Mutect2 - Deflater: IntelDeflater; 15:47:37.247 INFO Mutect2 - Inflater: IntelInflater; 15:47:37.247 INFO Mutect2 - GCS max retries/reopens: 20; 15:47:37.247 INFO Mutect2 - Requester pays: disabled; 15:47:37.247 INFO Mutect2 - Initializing engine; 15:47:41.204 INFO Mutect2 - Done initializing engine; 15:47:42.352 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 15:47:42.423 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 15:47:42.482 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 15:47:42.483 INFO IntelPairHmm - Available threads: 8; 15:47:42.483 INFO IntelPairHmm - Requested threads: 4; 15:47:42.483 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 15:47:42.936 INFO ProgressMeter - Starting traversal; 15:47:42.936 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 15:47:53.565 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:19555 0.2 90 508.0; 15:48:05.962 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:136820 0.4 600 1563.5; 15:48:16.023 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:360783 0.6 1560 2828.9; 15:48:19.342 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.01,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:2278,Load,Loading,2278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['Load'],['Loading']
Performance,7/10/11 14:19:38 INFO storage.BlockManagerMaster: Removed 2 successfully in removeExecutor; 17/10/11 14:19:38 ERROR cluster.YarnScheduler: Lost executor 2 on com2: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:38 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:25566,concurren,concurrent,25566,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['concurren'],['concurrent']
Performance,"716.2 MB]; Executing build with daemon context: DefaultDaemonContext[uid=7e8a7a6d-190b-445f-9873-f0329477e561,javaHome=/usr/lib/jvm/java-8-oracle,daemonRegistryDir=/home/axverdier/.gradle/daemon,pid=32687,idleTimeout=10800000,daemonOpts=-XX:MaxPermSize=256m,-XX:+HeapDumpOnOutOfMemoryError,-Xmx1024m,-Dfile.encoding=US-ASCII,-Duser.country=US,-Duser.language=en,-Duser.variant]; Starting Build; Settings evaluated using settings file '/home/axverdier/Tools/GATK4/git/gatk/settings.gradle'.; Projects loaded. Root project using build file '/home/axverdier/Tools/GATK4/git/gatk/build.gradle'.; Included projects: [root project 'gatk']; Evaluating root project 'gatk' using build file '/home/axverdier/Tools/GATK4/git/gatk/build.gradle'.; build for version:4.0.0.0-32-gf700774-SNAPSHOT; All projects evaluated.; No tasks specified. Using project default tasks 'bundle'; Selected primary task 'bundle' from project :; Tasks to be executed: [task ':createPythonPackageArchive', task ':compileJava', task ':processResources', task ':classes', task ':gatkTabComplete', task ':shadowJar', task ':sparkJar', task ':bundle']; In-memory cache of /home/axverdier/Tools/GATK4/git/gatk/.gradle/3.1/taskArtifacts/fileHashes.bin: Size{2449}, CacheStats{hitCount=9796, missCount=2449, loadSuccessCount=0, loadExceptionCount=0, totalLoadTime=0, evictionCount=0}; In-memory cache of /home/axverdier/Tools/GATK4/git/gatk/.gradle/3.1/taskArtifacts/fileSnapshots.bin: Size{3}, CacheStats{hitCount=0, missCount=3, loadSuccessCount=0, loadExceptionCount=0, totalLoadTime=0, evictionCount=0}; In-memory cache of /home/axverdier/Tools/GATK4/git/gatk/.gradle/3.1/taskArtifacts/taskArtifacts.bin: Size{2}, CacheStats{hitCount=8, missCount=2, loadSuccessCount=0, loadExceptionCount=0, totalLoadTime=0, evictionCount=0}; :createPythonPackageArchive (Thread[Daemon worker Thread 2,5,main]) started.; :createPythonPackageArchive; Executing task ':createPythonPackageArchive' (up-to-date check took 0.003 secs) due to:; Output propert",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4248:3016,cache,cache,3016,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4248,1,['cache'],['cache']
Performance,"74aa3480b67b321dc66426b1c600a/src/main/java/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFsEngine.java#L377. and the `--genotype-assignment-method` thus has no effect:; for example (same data as in [#5727#issuecomment-1781017195](https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195)) :; ```; $ gatk GenotypeGVCFs -R chr19.fa -V output.g.vcf -O output.vcf --genotype-assignment-method SET_TO_NO_CALL; Using GATK jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar GenotypeGVCFs -R chr19.fa -V output.g.vcf -O output.vcf --genotype-assignment-method SET_TO_NO_CALL; Picked up JAVA_TOOL_OPTIONS: -Djava.net.useSystemProxies=true; 14:10:28.241 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:10:28.300 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:10:28.305 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.4.0.0; 14:10:28.305 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:10:28.305 INFO GenotypeGVCFs - Executing as gleixner@odcf-worker02 on Linux v3.10.0-1160.76.1.el7.x86_64 amd64; 14:10:28.305 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v17+35-2724; 14:10:28.306 INFO GenotypeGVCFs - Start Date/Time: October 29, 2023 at 2:10:28 PM CET; 14:10:28.306 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:10:28.306 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:10:28.307 INFO GenotypeGVCFs - HTSJDK Version: 3.0.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8569:1166,Load,Loading,1166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8569,1,['Load'],['Loading']
Performance,"756762-69756762 due to alternate allele: \* ; 07:33:14.575 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756763-69756763 due to alternate allele: \* ; 07:33:14.575 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756763-69756763 due to alternate allele: \* ; 07:33:14.580 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756764-69756764 due to alternate allele: \* ; 07:33:14.580 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756764-69756764 due to alternate allele: \* ; 07:33:16.681 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:70289137-70289137 due to alternate allele: \* ; 07:33:16.681 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:70289137-70289137 due to alternate allele: \* ; 07:33:17.957 INFO VcfFuncotationFactory - dbSNP 9606\_b150 cache hits/total: 521/453691 ; 07:33:18.138 INFO Funcotator - Shutting down engine ; [May 28, 2020 7:33:18 AM EDT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 34.35 minutes. ; Runtime.totalMemory()=3822059520 ; java.lang.StringIndexOutOfBoundsException: String index out of range: 545 ; at java.lang.String.substring(String.java:1963) ; at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.initializeForInsertion(ProteinChangeInfo.java:256) ; at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.<init>(ProteinChangeInfo.java:93) ; at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.create(ProteinChangeInfo.java:371) ; at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:2003) ; at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createCodingRegionFuncotationForProtei",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651:1935,cache,cache,1935,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651,1,['cache'],['cache']
Performance,"7:04:15.666 INFO FeatureManager - Using codec VCFCodec to read file file:///db_students1/gatk_out/db_raw_call_bbe_6largest.vcf; 17:04:15.912 INFO HaplotypeCaller - Done initializing engine; 17:04:15.914 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 17:04:15.948 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 17:04:15.948 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 17:04:15.960 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_utils.so; 17:04:15.962 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 17:04:16.002 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 17:04:16.003 INFO IntelPairHmm - Available threads: 40; 17:04:16.003 INFO IntelPairHmm - Requested threads: 4; 17:04:16.003 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 17:04:16.052 INFO ProgressMeter - Starting traversal; 17:04:16.052 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 17:04:16.589 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes; 17:04:17.126 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 17:04:17.126 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6260:4136,Load,Loading,4136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6260,1,['Load'],['Loading']
Performance,7]a7bb49eaece47a172e2d/TMP/jeter.vcf.gz ; 18:15:23.374 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.385 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.403 INFO IntervalArgumentCollection - Processing 1028 bp from intervals ; 18:15:23.411 INFO HaplotypeCaller - Done initializing engine ; 18:15:23.430 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so ; 18:15:23.475 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so ; 18:15:23.651 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; 18:15:23.651 INFO IntelPairHmm - Available threads: 4 ; 18:15:23.651 INFO IntelPairHmm - Requested threads: 4 ; 18:15:23.651 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; 18:15:23.671 INFO ProgressMeter - Starting traversal ; 18:15:23.671 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute ; 18:15:26.788 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr1:30191420 and possibly subsequent; at least 10 samples must have called genotypes ; 18:15:27.190 WARN DepthPerSampleHC - Annotation will not be calculated at position chr1:30477350 and possibly subsequent; genotype for sample B00I9EL is not called; 18:15:35.547 INFO ProgressMeter - chr1:32128426 0.2 40 202.1 ; 18:15:48.416 INFO ProgressMeter - chr1:36398656 0.4 80 194.0 ; 18:15:51.025 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.012874514 ; 18:15:51.026 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 5.818477527000001; 18:15:51.026 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 1.35 sec ; 18:15:51.027 INFO HaplotypeCaller - Shutting d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8106:5073,multi-thread,multi-threaded,5073,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8106,1,['multi-thread'],['multi-threaded']
Performance,"8 from BlockManagerMaster.; 18/01/09 18:31:21 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1515493209401_0001_01_000010 on host: tele-6. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_1515493209401_0001_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:262); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 1. 18/01/09 18:31:21 INFO storage.BlockManagerMaster: Removal of executor 9 requested; 18/01/09 18:31:21 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 9; 18/01/09 18:31:21 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 9 from BlockManagerMaster.; 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms); 18/01/09 18:31:26 INFO server.AbstractConnector: Stopped Spark@283ab206{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/09 18:31:26 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.1.4:4040; 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:29080,concurren,concurrent,29080,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['concurren'],['concurrent']
Performance,8); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.executeAttempt(RetryingFutureImpl.java:141); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.access$500(RetryingFutureImpl.java:59); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl$AttemptFutureCallback.onFailure(RetryingFutureImpl.java:177); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$6.run(Futures.java:1764); 	at shaded.cloud_nio.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:153); 	at shaded.cloud_nio.com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:47); 	at shaded.cloud_nio.com.google.api.gax.core.internal.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:53); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellb,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:4680,concurren,concurrent,4680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"8.2; 23:10:12.683 INFO CountReadsSpark - Picard Version: 2.18.25; 23:10:12.683 INFO CountReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 23:10:12.683 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:10:12.683 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 23:10:12.683 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:10:12.684 INFO CountReadsSpark - Deflater: IntelDeflater; 23:10:12.684 INFO CountReadsSpark - Inflater: IntelInflater; 23:10:12.684 INFO CountReadsSpark - GCS max retries/reopens: 20; 23:10:12.684 INFO CountReadsSpark - Requester pays: disabled; 23:10:12.684 WARN CountReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CountReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 23:10:12.685 INFO CountReadsSpark - Initializing engine; 23:10:12.685 INFO CountReadsSpark - Done initializing engine; 19/02/05 23:10:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 19/02/05 23:10:15 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 19/02/05 23:10:18 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 806177853; 19/02/05 23:11:51 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(6,WrappedArray()); 19/02/05 23:11:51 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(13,WrappedArray()); 23:11:51.429 INFO CountReadsSpark - Shutting down engine; [February 5, 2019 11:11:51 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 1.67 minutes.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912:5473,load,load,5473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912,2,['load'],"['load', 'loaded']"
Performance,8.627 INFO GermlineCNVCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:54:28.627 WARN GermlineCNVCaller - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: GermlineCNVCaller is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:54:28.627 INFO GermlineCNVCaller - Initializing engine; 21:54:31.994 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 21:54:33.457 INFO GermlineCNVCaller - Intervals specified...; 21:54:34.113 INFO IntervalArgumentCollection - Processing 10999816 bp from intervals; 21:54:34.145 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 21:54:34.217 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 21:54:34.217 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 21:54:34.241 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00096.lc.soohee1k.hdf5 (1 / 24); 21:54:36.539 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00268.lc.soohee1k.hdf5 (2 / 24); 21:54:37.967 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00419.lc.soohee1k.hdf5 (3 / 24); 21:54:40.147 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00759.lc.soohee1k.hdf5 (4 / 24); 21:54:41.782 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG01051.lc.soohee1k.hdf5 (5 / 24); 21:54:43.197 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4826:2783,perform,performed,2783,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4826,1,['perform'],['performed']
Performance,"826.FIN.exome.cram \; > -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; > -readIndex gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram.crai \; > -O HG00190_cram.bam \; > ; Using GATK jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar PrintReads -I gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa -readIndex gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram.crai -O HG00190_cram.bam; 14:59:57.284 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; [October 5, 2017 2:59:57 PM EDT] PrintReads --output HG00190_cram.bam --input gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram --readIndex gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram.crai --reference /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --clo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:13860,Load,Loading,13860,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['Load'],['Loading']
Performance,"89); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:24005,Perform,Performance,24005,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,"['Perform', 'load']","['Performance', 'loaded']"
Performance,"8:31:04 INFO yarn.Client: Application report for application_1515493209401_0001 (state: ACCEPTED); 18/01/09 18:31:05 INFO yarn.Client: Application report for application_1515493209401_0001 (state: ACCEPTED); 18/01/09 18:31:05 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 18/01/09 18:31:05 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> tele-1, PROXY_URI_BASES -> http://tele-1:8088/proxy/application_1515493209401_0001), /proxy/application_1515493209401_0001; 18/01/09 18:31:05 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 18/01/09 18:31:06 INFO yarn.Client: Application report for application_1515493209401_0001 (state: RUNNING); 18/01/09 18:31:06 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: 192.168.1.4; 	 ApplicationMaster RPC port: 0; 	 queue: root.users.sun; 	 start time: 1515493860237; 	 final status: UNDEFINED; 	 tracking URL: http://tele-1:8088/proxy/application_1515493209401_0001/; 	 user: sun; 18/01/09 18:31:06 INFO cluster.YarnClientSchedulerBackend: Application application_1515493209401_0001 has started running.; 18/01/09 18:31:06 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44190.; 18/01/09 18:31:06 INFO netty.NettyBlockTransferService: Server created on 192.168.1.4:44190; 18/01/09 18:31:06 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 18/01/09 18:31:06 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.4, 44190, None); 18/01/09 18:31:06 INFO storage.BlockManagerMasterEndpoint: Registering block manager 192.168.1.4:44190 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.1.4, 44190, None); 18/01/09 18:31:06 INFO storage.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:14360,queue,queue,14360,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['queue'],['queue']
Performance,"8v1:61596-61748), so it looks like an issue with the estimation of the number of repeats. This is not the most important location, but the error could affect more important calls for other people. The log is the following: ; ```gatk Mutect2 --java-options ""-Xmx15G"" -R /data/references/Homo_sapiens/GATK/hg38/Homo_sapiens_assembly38.fasta -I test1.bam -I test2.bam -O tests.vcf -L test_err.bed ; Using GATK jar /home/alcalan/.conda/mutect2-cd161e2f51ff2240ce6390abc942bbdd/share/gatk4-4.1.5.0-1/gatk-package-4.1.5.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15G -jar /home/alcalan/.conda/mutect2-cd161e2f51ff2240ce6390abc942bbdd/share/gatk4-4.1.5.0-1/gatk-package-4.1.5.0-local.jar Mutect2 -R /data/references/Homo_sapiens/GATK/hg38/Homo_sapiens_assembly38.fasta -I test1.bam -I test2.bam -O tests.vcf -L test_err.bed; 10:34:24.578 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/alcalan/.conda/mutect2-cd161e2f51ff2240ce6390abc942bbdd/share/gatk4-4.1.5.0-1/gatk-package-4.1.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 23, 2020 10:34:24 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:34:24.819 INFO Mutect2 - ------------------------------------------------------------; 10:34:24.820 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.5.0; 10:34:24.820 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:34:24.820 INFO Mutect2 - Executing as alcalan@hn.pioneerx on Linux v3.10.0-1062.4.3.el7.x86_64 amd64; 10:34:24.820 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 10:34:24.820 INFO Mutect2 - Start Date/Time: March 23, 2020 10:34:24 AM CET; 10:34:24.820 INFO Mutect2 - --------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6516:1525,Load,Loading,1525,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6516,1,['Load'],['Loading']
Performance,"9 INFO HaplotypeCaller - Initializing engine; 03:56:45.204 INFO FeatureManager - Using codec BEDCodec to read file file:///data/b37.chr13.bed; 03:56:45.276 INFO IntervalArgumentCollection - Processing 595907 bp from intervals; 03:56:45.305 INFO HaplotypeCaller - Done initializing engine; 03:56:45.324 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 03:56:45.349 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 03:56:45.351 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 03:56:45.373 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 03:56:45.375 INFO IntelPairHmm - Available threads: 8; 03:56:45.375 INFO IntelPairHmm - Requested threads: 4; 03:56:45.375 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 03:56:45.415 INFO ProgressMeter - Starting traversal; 03:56:45.416 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 03:56:46.180 WARN VariantAnnotatorEngine - Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given.; 03:56:46.210 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position 13:32911888 and possibly subsequent; at least 10 samples must have called genotypes; 03:56:46.621 INFO HaplotypeCaller - 1 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityAvailableReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 57 read(s) filtered by: NotDuplicateReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filtered b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8149:5453,multi-thread,multi-threaded,5453,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149,1,['multi-thread'],['multi-threaded']
Performance,"9.0/gatk-package-4.1.9.0-local.jar VariantRecalibrator -V temp/vatiant_germline/sites.only.vcf.gz -O temp/vatiant_germline/recaliberation.indel.vcf --tranches-file temp/vatiant_germline/tranches.indel.txt --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0 -an DP -an FS -an MQRankSum -an QD -an ReadPosRankSum -an SOR -mode INDEL --max-gaussians 4 -resource:mills,known=false,training=true,truth=true,prior=12 ~/db/mutect2_support/b37/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=2 ~/db/mutect2_support/b37/hg19_v0_dbsnp_138.b37.vcf.gz --use-allele-specific-annotations -resource:axiomPoly,known=false,training=true,truth=false,prior=10 ~/db/mutect2_support/b37/Axiom_Exome_Plus.genotypes.all_populations.poly.b37.vcf.gz. 14:58:10.389 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 12, 2020 2:58:10 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:58:10.555 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.555 INFO VariantRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.9.0; 14:58:10.555 INFO VariantRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:58:10.555 INFO VariantRecalibrator - Executing as y@c001 on Linux v3.10.0-957.el7.x86_64 amd64; 14:58:10.555 INFO VariantRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 14:58:10.556 INFO VariantRecalibrator - Start Date/Time: November 12, 2020 2:58:10 PM CST; 14:58:10.556 INFO VariantRecalibrator - -----------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6963:2970,Load,Loading,2970,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6963,1,['Load'],['Loading']
Performance,"9.0/gatk-package-4.1.9.0-local.jar VariantRecalibrator -V temp/vatiant_germline/sites.only.vcf.gz -O temp/vatiant_germline/recaliberation.indel.vcf --tranches-file temp/vatiant_germline/tranches.indel.txt --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0 -an DP -an FS -an MQRankSum -an QD -an ReadPosRankSum -an SOR -mode INDEL --max-gaussians 4 -resource:mills,known=false,training=true,truth=true,prior=12 ~/db/mutect2_support/b37/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=2 ~/db/mutect2_support/b37/hg19_v0_dbsnp_138.b37.vcf.gz --use-allele-specific-annotations -resource:axiomPoly,known=false,training=true,truth=false,prior=10 ~/db/mutect2_support/b37/Axiom_Exome_Plus.genotypes.all_populations.poly.b37.vcf.gz. 14:58:10.389 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 12, 2020 2:58:10 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:58:10.555 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.555 INFO VariantRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.9.0; 14:58:10.555 INFO VariantRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:58:10.555 INFO VariantRecalibrator - Executing as y@c001 on Linux v3.10.0-957.el7.x86_64 amd64; 14:58:10.555 INFO VariantRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 14:58:10.556 INFO VariantRecalibrator - Start Date/Time: November 12, 2020 2:58:10 PM CST; 14:58:10.556 INFO VariantRecalibrator - -----------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6701#issuecomment-726406532:1622,Load,Loading,1622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6701#issuecomment-726406532,1,['Load'],['Loading']
Performance,98); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Existing mirrorFile and resourceId don't match isDirectory status! '/hadoop_gcs_connector_metadata_cache/hellbender/test/output/gatk4-spark/recalibrated.bam' (dir: 'false') vs 'gs://hellbender/test/output/gatk4-spark/recalibrated.bam/' (dir: 'true'); 	at com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache.getCacheEntryInternal(FileSystemBackedDirectoryListCache.java:198); 	at com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache.putResourceId(FileSystemBackedDirectoryListCache.java:363); 	at com.google.cloud.hadoop.gcsio.CacheSupplementedGoogleCloudStorage.createEmptyObjects(CacheSupplementedGoogleCloudStorage.java:150); 	at com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:578); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.mkdirs(GoogleHadoopFileSystemBase.java:1372); 	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1881); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:313); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1150); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1078); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1078); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271419191:2418,Cache,CacheSupplementedGoogleCloudStorage,2418,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271419191,1,['Cache'],['CacheSupplementedGoogleCloudStorage']
Performance,"99027448764 2.1547132944470522E-5; CSCC_0007-M1 0.00135679065051943 2.0692791516445317E-5; CSCC_0008-M1 0.004394182081805844 3.748225078434626E-5; CSCC_0009-M1 0.0019614948575730397 2.4555494660711082E-5; CSCC_0010-M1 0.004122282756273677 3.6210336627748355E-5; CSCC_0010-P1 0.001888852796306713 3.040210329291157E-5; CSCC_0011-M1 0.004616869166852859 3.9987828482322895E-5; CSCC_0012-M1 0.0013866025034395032 2.1835070542119526E-5; CSCC_0012-P1 0.9856060967650699 0.0023006992152694522; CSCC_0013-M1 0.014792148767770472 9.498068474793835E-5; CSCC_0014-M1 0.0028227703351458118 4.122117743000552E-5; CSCC_0015-M1 0.01467099675552882 9.531218938517413E-5; CSCC_0016-P1 0.0014411085088999514 2.716291906758587E-5; CSCC_0017-P1 0.0015213899870480127 2.712650453920576E-5; CSCC_0018-P1 0.001694677662867099 2.913615483470931E-5; CSCC_0019-P1 0.0016654868517623346 2.8602851235266697E-5; CSCC_0020-P1 0.0015496166402163914 2.7824601469663656E-5; ```. The procedure done is. > Base quality score recalibration was performed with GATK 4.1.2.0 (Van der Auwera et al. 2013). GATK SplitIntervals was used to define 32 evenly-sized genomic intervals over which GATK BaseRecalibrator was run for each sample. The 32 recalibration tables per sample were merged into one table per sample with GATK GatherReports.; > ; > Base-recalibrated BAM files were produced with GATK ApplyBQSR in parallel over each of the 3,366 contigs in the hg 38 + alt reference genome, plus the unmapped reads. These were merged into a final BAM per sample with GATK GatherBamFiles.; > ; > SplitIntervals was used to define 3,200 evenly sized genomic intervals across hg38 + alt contigs, excluding telomeres, centromeres, unplaced contigs, unlocalized contigs, decoy and the Epstein-Barr viral sequence (Supplementary data 1). These intervals were used for scattering tasks in the Germline short variant calling and Somatic short variant calling workflows outlined below.; > ; > Germline short variant calling; > ; > HaplotypeCaller was ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656499472:1563,perform,performed,1563,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656499472,1,['perform'],['performed']
Performance,"9931-Funcotator-Information-and-Tutorial#1.1.2.2.1), if I expand both `gnomAD_exome.tar.gz` and `gnomAD_genome.tar.gz`, funcotator dies at startup with a `400 Bad Request` error. This also happens if I expand either one of the `gnomad_*.tar.gz` files individually. . #### Expected behavior; Funcotator annotates my VCF and includes gnomAD annotations in the output VCF. . #### Actual behavior. Crash with 400 Bad Request:. ```; Using GATK jar /opt/conda/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /opt/conda/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar Funcotator --variant cohort.vcf.gz --reference GRCh38.d1.vd1/GRCh38.d1.vd1.fa --ref-version hg38 --data-sources-path funcotator_dataSources.v1.7.20200521g --output cohort.funcotator.vcf.gz --output-file-format VCF; 14:24:33.589 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/conda/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:24:33.842 INFO Funcotator - ------------------------------------------------------------; 14:24:33.842 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.9.0; 14:24:33.842 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:24:33.843 INFO Funcotator - Executing as alanh@r820-2-0.local on Linux v3.10.0-1062.el7.x86_64 amd64; 14:24:33.843 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-b11; 14:24:33.843 INFO Funcotator - Start Date/Time: October 29, 2020 2:24:33 PM UTC; 14:24:33.843 INFO Funcotator - ------------------------------------------------------------; 14:24:33.843 INFO Funcotator - ------------------------------------------------------------; 14:24:33.844 INFO Funcotator - HTSJDK Version: 2.23.0; 14:24:33.844 INFO Funcotator - Picard Version: 2.23.3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926:1926,Load,Loading,1926,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926,1,['Load'],['Loading']
Performance,9scy9leG9tZS9Bbm5vdGF0ZVRhcmdldHMuamF2YQ==) | `78.049% <> ()` | `7 <0> ()` | :arrow_down: |; | [...llbender/tools/exome/plotting/PlotACNVResults.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9wbG90dGluZy9QbG90QUNOVlJlc3VsdHMuamF2YQ==) | `84.615% <> ()` | `22 <0> ()` | :arrow_down: |; | [...adinstitute/hellbender/tools/exome/PadTargets.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QYWRUYXJnZXRzLmphdmE=) | `100% <> ()` | `3 <0> ()` | :arrow_down: |; | [...hellbender/tools/genome/SparkGenomeReadCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWUvU3BhcmtHZW5vbWVSZWFkQ291bnRzLmphdmE=) | `91.089% <> ()` | `18 <0> ()` | :arrow_down: |; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <> ()` | `2 <0> ()` | :arrow_down: |; | [...llbender/tools/walkers/validation/Concordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2UuamF2YQ==) | `88.542% <> ()` | `28 <0> ()` | :arrow_down: |; | [...ute/hellbender/tools/exome/ConvertACNVResults.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9Db252ZXJ0QUNOVlJlc3VsdHMuamF2YQ==) | `87.805% <> ()` | `4 <0> ()` | :arrow_down: |; | [...idation/AnnotateVcfWithExpectedAlleleFraction.java](https://codecov.io/gh/broadinstitute/gatk/pull/313,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3135#issuecomment-309876624:2345,Perform,PerformAlleleFractionSegmentation,2345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3135#issuecomment-309876624,1,['Perform'],['PerformAlleleFractionSegmentation']
Performance,: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:602); ... 8 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:94); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:419); at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:222); at htsjdk.tribble.AbstractFeatureReader,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:2329,concurren,concurrent,2329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance,": false; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:19:40.101 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:19:40.101 INFO GenomicsDBImport - Inflater: IntelInflater; 11:19:40.101 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:19:40.102 INFO GenomicsDBImport - Requester pays: disabled; 11:19:40.102 INFO GenomicsDBImport - Initializing engine; 11:19:40.385 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/data/project/reseq/KPSNY042021067K/result/03.bwa_dup_gvcf/geno/chr33.bed; 11:19:40.390 INFO IntervalArgumentCollection - Processing 10664 bp from intervals; 11:19:40.391 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 11:19:40.429 INFO GenomicsDBImport - Done initializing engine; 11:19:40.624 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; 11:19:40.625 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/data/chr33.db/vidmap.json; 11:19:40.625 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/data/chr33.db/callset.json; 11:19:40.625 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/data/chr33.db/vcfheader.vcf; 11:19:40.625 INFO GenomicsDBImport - Importing to workspace - /mnt/data/chr33.db; 11:19:40.625 INFO ProgressMeter - Starting traversal; 11:19:40.625 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:19:49.073 INFO GenomicsDBImport - Importing batch 1 with 1115 samples; 11:20:12.073 INFO GenomicsDBImport - Importing batch 1 with 1115 samples; 11:20:32.582 INFO GenomicsDBImport - Importing batch 1 with 1115",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460:3383,perform,performance,3383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460,1,['perform'],['performance']
Performance,": note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:94:38: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]; if (isinf(small) == -1 || isinf(big) == -1); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/headers.h:27:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /usr/local/Cellar/gcc/5.3.0/include/c++/5.3.0/cmath:853:5: note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^. /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.cc: In function 'void get_time(timespec*)':; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.cc:214:17: error: 'CLOCK_REALTIME' was not declared in this scope; clock_gettime(CLOCK_REALTIME, store_struct);; ^; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.cc:214:45: error: 'clock_gettime' was not declared in this scope; clock_gettime(CLOCK_REALTIME, store_struct);; ^; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.cc: In function 'uint64_t diff_time(timespec&)':; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.cc:220:17: error: 'CLOCK_R",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343:14780,Load,LoadTimeInitializer,14780,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343,1,['Load'],['LoadTimeInitializer']
Performance,": note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:94:38: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]; if (isinf(small) == -1 || isinf(big) == -1); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/headers.h:27:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /usr/local/Cellar/gcc/5.3.0/include/c++/5.3.0/cmath:853:5: note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h: In instantiation of 'static NUMBER ContextBase<NUMBER>::approximateLog10SumLog10(NUMBER, NUMBER) [with NUMBER = double]':; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:75:53: required from 'static void ContextBase<NUMBER>::initializeMatchToMatchProb() [with NUMBER = double]'; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:47:35: re",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343:11845,Load,LoadTimeInitializer,11845,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343,1,['Load'],['LoadTimeInitializer']
Performance,:+1: Could you add a note explaining that it's a short circuit for performance reasons? Merge when ready.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1624#issuecomment-200977770:67,perform,performance,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1624#issuecomment-200977770,1,['perform'],['performance']
Performance,:+1: I don't see any problems although I'm skeptical about some of the optimization making a difference.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1623#issuecomment-200996224:71,optimiz,optimization,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1623#issuecomment-200996224,1,['optimiz'],['optimization']
Performance,":+1: merge after addressing comments. If the test I requested is too slow to be part of our test suite, should create a ticket ""Fix performance issues in `BAMInputFormat.setIntervals()` with large interval lists""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1516#issuecomment-188935087:132,perform,performance,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1516#issuecomment-188935087,1,['perform'],['performance']
Performance,":///home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed; 09:39:56.024 INFO IntervalArgumentCollection - Processing 74592 bp from intervals; 09:39:56.032 INFO Mutect2 - Done initializing engine; 09:39:56.044 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:39:56.077 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 09:39:56.139 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 09:39:56.139 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 09:39:56.139 INFO IntelPairHmm - Available threads: 36; 09:39:56.139 INFO IntelPairHmm - Requested threads: 4; 09:39:56.139 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 09:39:56.146 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 09:39:56.146 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 09:39:56.146 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.00 sec; 09:39:56.148 INFO Mutect2 - Shutting down engine; [July 3, 2020 9:39:56 AM CEST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2233991168; htsjdk.samtools.util.RuntimeIOException: File not found: mutect2/concatenated_ACC5611A1_XXXXXX_mutect2_unfiltered_ss_r2.vcf.gz; 	at htsjdk.variant.variantcontext.writer.VariantContextWriterBuilder.build(VariantContextWriterBuilder.java:451); 	at htsjdk.variant.variantcontext.writer.VariantContextWriterBuilder.build(VariantContextWriterBuilder.java:415); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.createVCFWriter(GATKVariantContextUtils.java:121); 	at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695:3880,multi-thread,multi-threaded,3880,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695,1,['multi-thread'],['multi-threaded']
Performance,":///home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed; 11:47:51.465 INFO IntervalArgumentCollection - Processing 74592 bp from intervals; 11:47:51.474 INFO Mutect2 - Done initializing engine; 11:47:51.487 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:47:51.489 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:47:51.534 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 11:47:51.534 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:47:51.534 INFO IntelPairHmm - Available threads: 16; 11:47:51.534 INFO IntelPairHmm - Requested threads: 4; 11:47:51.534 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:47:51.557 INFO ProgressMeter - Starting traversal; 11:47:51.557 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:47:52.683 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 11:47:52.683 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 11:47:52.683 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.24 sec; 11:47:52.684 INFO Mutect2 - Shutting down engine; [July 2, 2020 11:47:52 AM CEST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2511863808; java.lang.IllegalArgumentException: Read bases and read quality arrays aren't the same size: Bases: 38 vs Base Q's: 38 vs Insert Q's: 146 vs Delete Q's: 146.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:734); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEng",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482:4348,multi-thread,multi-threaded,4348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482,1,['multi-thread'],['multi-threaded']
Performance,":00.557 INFO SplitNCigarReads - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:31:00.558 INFO SplitNCigarReads - Deflater IntelDeflater; 15:31:00.558 INFO SplitNCigarReads - Initializing engine; 15:31:00.659 INFO SplitNCigarReads - Done initializing engine; 15:31:00.679 INFO ProgressMeter - Starting traversal; 15:31:00.679 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 15:31:05.088 INFO SplitNCigarReads - Shutting down engine; [July 20, 2016 3:31:05 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1011875840; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2026:4485,Load,LoadSnappy,4485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026,4,"['Load', 'load']","['LoadSnappy', 'loadClass']"
Performance,":01.494 INFO HaplotypeCaller - Deflater: IntelDeflater. 18:58:01.494 INFO HaplotypeCaller - Inflater: IntelInflater. 18:58:01.494 INFO HaplotypeCaller - GCS max retries/reopens: 20. 18:58:01.494 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes. 18:58:01.494 INFO HaplotypeCaller - Initializing engine. 18:58:02.043 INFO HaplotypeCaller - Done initializing engine. 18:58:02.053 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output. 18:58:02.053 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output. 18:58:02.886 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_utils.so. 18:58:02.888 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so. 18:58:02.932 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM. 18:58:02.933 INFO IntelPairHmm - Available threads: 8. 18:58:02.933 INFO IntelPairHmm - Requested threads: 4. 18:58:02.933 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. 18:58:02.986 INFO ProgressMeter - Starting traversal. 18:58:02.987 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute. 18:58:12.992 INFO ProgressMeter - chr1:6969901 0.2 23290 139712.1. 18:58:22.989 INFO ProgressMeter - chr1:13470130 0.3 44960 134866.5. 18:58:32.991 INFO ProgressMeter - chr1:21393130 0.5 71370 142721.0. INFO 18:58:37,986 HelpFormatter - ---------------------------------------------------------------------------------- . INFO 18:58:37,989 HelpForm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4788:3624,Load,Loading,3624,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4788,1,['Load'],['Loading']
Performance,":03:49.911 INFO ProgressMeter - D01:106344 16018.7 6806170 424.9; 12:04:23.685 INFO ProgressMeter - D01:108187 16019.3 6806180 424.9; 12:04:33.872 INFO ProgressMeter - D01:131768 16019.5 6806320 424.9; 12:04:44.080 INFO ProgressMeter - D01:142033 16019.6 6806390 424.9; 12:05:30.576 INFO ProgressMeter - D01:151103 16020.4 6806460 424.9; 12:05:40.721 INFO ProgressMeter - D01:171985 16020.6 6806600 424.9; 12:05:52.649 INFO ProgressMeter - D01:179627 16020.8 6806650 424.9; 12:06:05.001 INFO ProgressMeter - D01:191227 16021.0 6806730 424.9; 12:06:32.206 INFO ProgressMeter - D01:211153 16021.4 6806880 424.9; 12:06:46.312 INFO ProgressMeter - D01:220323 16021.7 6806940 424.9; 12:06:56.646 INFO ProgressMeter - D01:232399 16021.8 6807020 424.9; 12:07:10.267 INFO ProgressMeter - D01:235919 16022.1 6807050 424.9; 12:07:22.064 INFO ProgressMeter - D01:256207 16022.3 6807200 424.9. **_The later LOG_**. nohup: ignoring input and appending output to nohup.out; 09:13:38.398 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/home/chenwei/biosoft/gatk-4.0.10.1/gatk-package-4.0.10.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:13:57.227 INFO HaplotypeCaller - ------------------------------------------------------------; 09:13:57.228 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.10.1; 09:13:57.228 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:13:57.229 INFO HaplotypeCaller - Executing as chenwei@localhost.localdomain on Linux v3.10.0-1160.31.1.el7.x86_64 amd64; 09:13:57.229 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_292-b10; 09:13:57.230 INFO HaplotypeCaller - Start Date/Time: September 3, 2021 9:13:38 AM CST; 09:13:57.230 INFO HaplotypeCaller - ------------------------------------------------------------; 09:13:57.230 INFO HaplotypeCaller - ------------------------------------------------------------; 09:13:57.232 INFO HaplotypeCaller - HTSJDK",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7454:6927,Load,Loading,6927,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454,1,['Load'],['Loading']
Performance,":04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; > 21:13:04.231 INFO GenotypeGVCFs - Initializing engine; > 21:13:11.834 INFO GenotypeGVCFs - Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 21:14:17.635 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.858 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 13 > 12 expanding to 26; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 27 > 26 expanding to 54; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 55 > 54 expanding to 110; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 111 > 110 expanding to 222; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 223 > 222 expanding to 446; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 447 > 446 expanding to 894; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 895 > 894 expanding to 1790; > 21:14:17.874 DEBUG MathUtils$Log10Cache - ca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:5197,cache,cache,5197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['cache'],['cache']
Performance,":05:02.975 INFO HaplotypeCaller - Requester pays: disabled; 09:05:02.976 INFO HaplotypeCaller - Initializing engine; 09:05:05.609 INFO HaplotypeCaller - Done initializing engine; 09:05:05.616 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 09:05:06.026 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 09:05:06.026 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 09:05:06.055 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/home/chenwei/biosoft/gatk-4.0.10.1/gatk-package-4.0.10.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:05:06.083 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 09:05:06.083 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 09:05:06.411 INFO ProgressMeter - Starting traversal; 09:05:06.412 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:05:16.445 INFO ProgressMeter - A01:27883 0.2 110 657.9; 09:05:27.780 INFO ProgressMeter - A01:45980 0.4 240 673.9; 09:05:38.219 INFO ProgressMeter - A01:64498 0.5 360 679.1; .......; 11:56:31.165 INFO ProgressMeter - A13:109860605 16011.4 6803200 424.9; 11:56:41.437 INFO ProgressMeter - A13:109878042 16011.6 6803330 424.9; 11:56:51.882 INFO ProgressMeter - A13:109913861 16011.8 6803540 424.9; 11:57:02.097 INFO ProgressMeter - A13:109924924 16011.9 6803620 424.9; 11:57:14.212 INFO ProgressMeter - A13:109930687 16012.1 6803650 424.9; 11:57:26.561 INFO ProgressMeter - A13:109956963 16012.3 6803",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7454:3170,multi-thread,multi-threaded,3170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454,1,['multi-thread'],['multi-threaded']
Performance,":118752077 4.6 5530000 1207397.2; 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; [March 2, 2023 3:40:16 PM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 5.27 minutes.; Runtime.totalMemory()=3432513536; java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to java.lang.Comparable; 	at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.util.TimSort.sort(TimSort.java:234); 	at java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); 	at java.util.Arrays.parallelSort(Arrays.java:1180); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.lang.Thread.run(Thread.java:750); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 		at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); 		at org.broadin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:6247,concurren,concurrent,6247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['concurren'],['concurrent']
Performance,":160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:21931,concurren,concurrent,21931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['concurren'],['concurrent']
Performance,":22.413 INFO HaplotypeCaller - Deflater: IntelDeflater; 22:42:22.413 INFO HaplotypeCaller - Inflater: IntelInflater; 22:42:22.413 INFO HaplotypeCaller - GCS max retries/reopens: 20; 22:42:22.413 INFO HaplotypeCaller - Requester pays: disabled; 22:42:22.413 INFO HaplotypeCaller - Initializing engine; 22:42:22.705 INFO IntervalArgumentCollection - Processing 2001 bp from intervals; 22:42:22.710 INFO HaplotypeCaller - Done initializing engine; 22:42:22.712 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 22:42:22.719 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 22:42:22.720 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.dylib; 22:42:22.722 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 22:42:22.724 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 22:42:22.724 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 22:42:22.734 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 22:42:22.734 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 22:42:22.734 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 22:42:22.748 INFO IntelPairHmm",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:2851,Load,Loading,2851,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['Load'],['Loading']
Performance,:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Generated by:. java -Xmx16g -jar /dsde/working/slee/CNV-1.5_HCC1143/resources/gatk-package-4.beta.5-97-g066c0b4-SNAPSHOT-local.jar SparkGenomeReadCounts \; --input /dsde/working/slee/CNV-1.5_HCC1143/run/cromwell-executions/CNVSomaticPairWorkflow/859bdb60-ba9c-4cf8-98c7-0fb9847a7ee0/call-CollectReadCountsNormal/inputs/seq/picard_aggregation/G15511/HCC1143_BL/current/HCC1143_BL.bam \; --reference /dsde/working/slee/CNV-1.5_HCC1143/run/cromwell-executions/CNVSomaticPairWorkflow/859bdb60-ba9c-4cf8-98c7-0fb9847a7ee0/call-CollectReadCountsNormal/inputs/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta \; --binLength 250 \; --keepXYMT false \; --disableToolDefaultReadFilters false \; --disableSequenceDictionaryValidation true \; --disableReadFilter NotDuplicateReadFilter \; --output HCC1143_BL.readCounts.tsv \; --writeHdf5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3679:12006,concurren,concurrent,12006,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3679,1,['concurren'],['concurrent']
Performance,:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 21/04/13 07:32:24 ERROR SparkHadoopWriter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:5173,concurren,concurrent,5173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['concurren'],['concurrent']
Performance,:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engine; 11:35:41.748 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:35:41.775 DEBUG NativeLibraryLoader - Extracting libgkl_utils.so to /tmp/libgkl_utils9151568277466250840.so; 11:35:41.777 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:35:41.802 DEBUG NativeLibraryLoader - Extracting libgkl_pairhmm_omp.so to /tmp/libgkl_pairhmm_omp8179002917276126697.so; 11:35:41.847 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:35:41.848 INFO IntelPairHmm - Available threads: 64; 11:35:41.848 INFO IntelPairHmm - Requested threads: 4; 11:35:41.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:35:41.882 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 11:35:41.997 INFO ProgressMeter - Starting traversal; 11:35:41.997 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:35:42.019 DEBUG ReadsPathDataSource - Preparing readers for traversal; 11:35:42.470 DEBUG ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:5946,Load,Loading,5946,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Load'],['Loading']
Performance,:409); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:130); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.samtools.SAMException: Fasta index file could not be opened: /private/var/folders/5s/v5t08tmd42z_2m2c30vqf6kc0000gn/T/spark-556aa7a2-4d88-4bae-ad16-36d5af920fa9/userFiles-aeb68992-3215-4897-8f8a-040396296185/Homo_sapiens_assembly18.fasta.fai; at htsjdk.samtools.reference.FastaSequenceIndex.<init>(FastaSequenceIndex.java:74); at htsjdk.samtools.reference.IndexedFastaSequenceFile.<init>(IndexedFastaSequenceFile.java:98); at htsjdk.samtools.reference.ReferenceSequenceFileFactory.getReferenceSequenceFile(ReferenceSequenceFileFactory.java:139); at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:148); ... 24 more; Caused by: java.nio.file.FileSystemException: /private/var/folders/5s/v5t08tmd42z_2m2c30vqf6kc0000gn/T/sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6642:3609,concurren,concurrent,3609,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6642,1,['concurren'],['concurrent']
Performance,:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:55:47.796 DEBUG Mutect2Engine - Ref haplotype coords chrM:9202-9684; 11:55:47.800 DEBUG Mutect2Engine - Haplotype count 128; 11:55:47.803 DEBUG Mutect2Engine - Kmer sizes count 0; 11:55:47.807 DEBUG Mutect2Engine - Kmer sizes values []; 12:05:48.002 DEBUG Mutect2 - Processing assembly region at chrM:9585-9884 isActive: false numReads: 125080; 12:05:51.435 DEBUG Mutect2 - Processing assembly region at chrM:9885-10184 isActive: false numReads: 0; 12:05:51.44,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:16950,cache,cache,16950,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance,":42.093 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:18:42.093 INFO HaplotypeCaller - Deflater: IntelDeflater; 12:18:42.093 INFO HaplotypeCaller - Inflater: IntelInflater; 12:18:42.093 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:18:42.093 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:18:42.093 INFO HaplotypeCaller - Initializing engine; 12:18:42.597 INFO FeatureManager - Using codec VCFCodec to read file file:///beegfs/work/zxmai83/Reference/dbs/b37/dbsnp_138.b37.vcf; 12:18:42.723 INFO HaplotypeCaller - Done initializing engine; 12:18:42.732 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 12:18:42.732 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 12:18:43.546 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:18:43.549 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:18:43.599 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002b5f92e39fab, pid=85482, tid=0x00002b5f56e60ae8; #; # JRE version: OpenJDK Runtime Environment (8.0_151-b12) (build 1.8.0_151-b12); # Java VM: OpenJDK 64-Bit Server VM (25.151-b12 mixed mode linux-amd64 compressed oops); # Derivative: IcedTea 3.6.0; # Distribution: Custom build (Tue Nov 21 11:22:36 GMT 2017); # Problematic frame:; # C [libgomp.so.1+0x7fab] omp_get_max_threads+0xb; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java aga",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:3440,Load,Loading,3440,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['Load'],['Loading']
Performance,":49.226 INFO GenomicsDBImport - Done importing batch 2/5; 23 Feb 2022 18:26:19,107 DEBUG: 	18:26:19.105 INFO GenomicsDBImport - Done importing batch 3/5; 23 Feb 2022 19:20:18,500 DEBUG: 	19:20:18.478 INFO GenomicsDBImport - Done importing batch 4/5; 24 Feb 2022 16:51:19,017 DEBUG: 	[TileDB::utils] Error: (gzip_handle_error) Cannot decompress with GZIP: inflateInit error: Z_MEM_ERROR; 24 Feb 2022 16:51:19,048 DEBUG: 	[TileDB::Codec] Error: Could not decompress with GZIP.; 24 Feb 2022 16:51:19,056 DEBUG: 	[TileDB::ReadState] Error: Cannot decompress tile for /home/exacloud/gscratch/prime-seq/workDir/0950f56b-7565-103a-a738-f8f3fc8675d2/Job2.work/WGS_1852_consolidated.gdb/2$1$196197964/__e7217c9e-767d-4295-b75a-9162c22c6996139785909643008_1613563029631/END.tdb.; 24 Feb 2022 16:51:51,388 DEBUG: 	16:51:51.388 erro NativeGenomicsDB - pid=225263 tid=225739 VariantStorageManagerException exception : Error while consolidating TileDB array 2$1$196197964; 24 Feb 2022 16:51:51,405 DEBUG: 	TileDB error message : ; 24 Feb 2022 16:51:51,412 DEBUG: 	terminate called after throwing an instance of 'VariantStorageManagerException'; 24 Feb 2022 16:51:51,419 DEBUG: 	 what(): VariantStorageManagerException exception : Error while consolidating TileDB array 2$1$196197964; 24 Feb 2022 16:51:51,427 DEBUG: 	TileDB error message : ; 24 Feb 2022 16:52:27,478 WARN : 	process exited with non-zero value: 134; ```. Does that give anything to suggest troubleshooting steps?. The full command is:; ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	Xmx497g -Xms497g -Xss2m \; 	-jar /home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar \; 	GenomicsDBImport \; 	-V 25780.g.vcf.gz \; 	-V <total of 92 gVCFs> \; 	--genomicsdb-update-workspace-path WGS_1852_consolidated.gdb \; 	--batch-size 10 \; 	--reader-threads 12 \; 	--consolidate \; 	--genomicsdb-shared-posixfs-optimizations \; 	--bypass-feature-reader \; 	-R 128_Mmul_10.fasta; ```. this is GATK v4.2.5.0. Thanks i advance for any ideas.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1050434022:2931,optimiz,optimizations,2931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1050434022,1,['optimiz'],['optimizations']
Performance,":50:16.818 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:50:16.818 INFO HaplotypeCaller - Inflater: IntelInflater; 14:50:16.818 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:50:16.818 INFO HaplotypeCaller - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:50:16.819 INFO HaplotypeCaller - Initializing engine; 14:50:18.950 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 14:50:18.965 INFO HaplotypeCaller - Done initializing engine; 14:50:19.021 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:50:19.280 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.481 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.776 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:50:19.795 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:50:19.847 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:50:19.848 INFO IntelPairHmm - Available threads: 48; 14:50:19.848 INFO IntelPairHmm - Requested threads: 4; 14:50:19.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:50:19.926 INFO ProgressMeter - Starting traversal; 14:50:19.926 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:50:30.309 INFO ProgressMeter - chr17:740224 0.2 3010 17395.5; 14:50:41.016 INFO ProgressMeter - chr17:1675683 0.4 7020 19973.4; 14:50:51.041 INFO ProgressMeter - chr17:24",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678:6625,Load,Loading,6625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678,1,['Load'],['Loading']
Performance,:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at htsjdk.tribble.readers.AsynchronousLineReader.checkAndThrowIfWorkerException(AsynchronousLineReader.java:61); at htsjdk.tribble.readers.AsynchronousLineReader.readLine(AsynchronousLineReader.java:43); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:24); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:11); at htsjdk.samtools.util.AbstractIterator.hasNext(AbstractIterator.java:44); at htsjdk.tribble.AsciiFeatureCodec.isDone(AsciiFeatureCodec.java:53); at htsjdk.tribble.AsciiFeatureCodec.isDone(AsciiFeatureCodec.java:41); at htsjdk.tribble.TribbleIndexedFeatureReader$QueryIterator.readNextRecord(TribbleIndexedFeatureReader.java:447); at htsjdk.tribble.TribbleIndexedFeatureReader$QueryIterator.<init>(TribbleIndexedFeatureReader.java:390); at htsjdk.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1638:4455,concurren,concurrent,4455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638,1,['concurren'],['concurrent']
Performance,"; 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:28 ERROR cluster.YarnScheduler: Lost executor 1 on com2: Container marked as failed: container_1507683879816_0006_01_000002 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000002; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:28 WARN scheduler.TaskSetManager: Lost task 0.1 in stage 1.0 (TID 2, com2, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_1507683879816_0006_01_000002 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000002; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultCont",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:19767,concurren,concurrent,19767,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['concurren'],['concurrent']
Performance,; 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); 	at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.GATKException: Expected message of length 3 but only found 0 bytes; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.waitForAck(StreamingProcessController.java:228); 	... 26 more; Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Expected message of length 3 but only found 0 bytes; 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getBytesFromStream(StreamingProcessController.java:261); 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.lambda$waitForAck$0(StreamingProcessController.java:208); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). but if I change memory as below: it works. ; qlogin -l s_vmem=20G -l mem_req=20G,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-895854147:3601,concurren,concurrent,3601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-895854147,5,['concurren'],['concurrent']
Performance,; 	at org.broadinstitute.hellbender.tools.GatherVcfsCloud.doWork(GatherVcfsCloud.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); 	at org.broadinstitute.hellbender.Main.main(Main.java:239); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: 403 Forbidden; 443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to broad-jg-dev-11k-call-set/JointGenotyping/0cb36821-b8bf-4e6d-a352-07b101f6b7d1/call-ApplyRecalibration/shard-1734/GMKF_Seidman_CHD_WGS_904.filtered.1734.vcf.gz.; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 10 more; Caused by: com.google.cloud.storage.StorageException: 403 Forbidden; 443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to broad-jg-dev-11k-call-set/JointGenotyping/0cb36821-b8bf-4e6d-a352-07b101f6b7d1/call-ApplyRecalibration/shard-1734/GMKF_Seidman_CHD_WGS_904.filtered.1734.vcf.gz.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:526); 	at com.goog,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3735:2823,concurren,concurrent,2823,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3735,1,['concurren'],['concurrent']
Performance,"; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:274); at org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:259); at org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:175); at org.a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2183:1349,load,loadClass,1349,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183,1,['load'],['loadClass']
Performance,"; 04:37:30.680 INFO ProgressMeter - Starting traversal; 04:37:30.680 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 04:37:33.253 INFO GenomicsDBImport - Starting batch input file preload; 04:37:35.079 INFO GenomicsDBImport - Finished batch preload; 04:37:35.079 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:37.079 INFO GenomicsDBImport - Starting batch input file preload; 04:37:38.712 INFO GenomicsDBImport - Finished batch preload; 04:37:38.712 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:39.162 INFO GenomicsDBImport - Shutting down engine; [October 8, 2018 4:37:39 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=4116185088; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:769); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:165); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:604); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFutu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5300:4351,concurren,concurrent,4351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300,1,['concurren'],['concurrent']
Performance,"; 06:50:14.229 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 427.5 minutes.; 06:50:14.236 INFO GenomicsDBImport - Import completed!; 06:50:14.236 INFO GenomicsDBImport - Shutting down engine; [January 27, 2019 6:50:14 AM CST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 428.57 minutes.; Runtime.totalMemory()=8988393472; Tool returned:; true; Using GATK jar /home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx24g -jar /home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar GenotypeGVCFs -R /home/WangBS/Reference/Qrobur/Qrob_PM1N.fa -V gendb:///home/WangBS/Analyses/vcf/test/chr02 -all-sites -O /home/WangBS/Analyses/vcf/test/chr02.vcf; 06:50:19.236 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 06:51:21.116 INFO GenotypeGVCFs - ------------------------------------------------------------; 06:51:21.116 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.11.0-56-g2c0e9b0-SNAPSHOT; 06:51:21.116 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 06:51:21.117 INFO GenotypeGVCFs - Executing as WangBS@cu53 on Linux v3.10.0-693.el7.x86_64 amd64; 06:51:21.117 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 06:51:21.117 INFO GenotypeGVCFs - Start Date/Time: January 27, 2019 6:50:19 AM CST; 06:51:21.117 INFO GenotypeGVCFs - ------------------------------------------------------------; 06:51:21.117 INFO GenotypeGVCFs - ------------------------------------------------------------; 06:51:21.118 INFO GenotypeGVCFs - HTSJDK",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5865:4993,Load,Loading,4993,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865,1,['Load'],['Loading']
Performance,; Caused by: com.google.cloud.storage.StorageException: Read timed out; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.executeAttempt(RetryingFutureImpl.java:141); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.access$500(RetryingFutureImpl.java:59); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl$AttemptFutureCallback.onFailure(RetryingFutureImpl.java:177); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$6.run(Futures.java:1764); 	at shaded.cloud_nio.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:153); 	at shaded.cloud_nio.com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:47); 	at shaded.cloud_nio.com.google.api.gax.core.internal.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:53); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:4364,concurren,concurrent,4364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,; Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); 	... 3 more; Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:602); 	... 8 more; Caused by: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReade,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:2151,concurren,concurrent,2151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,"; ```. example usage:. ```; @Test; public void testApplyToString(){; Pipeline p = GATKTestPipeline.create();; PCollection<Integer> pints = p.apply(Create.of(Arrays.asList(1, 2, 3)));. PCollection<String> presults = DataflowUtils.apply( pints, Object::toString).setCoder(StringUtf8Coder.of());. DataflowAssert.that(presults).containsInAnyOrder(""1"",""2"",""3"");; p.run();; }; ```. note the `setCoder` call, if you don't include that you get. ```; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-jdk14/1.7.7/25d160723ea37a6cb84e87cd70773ff02997e857/slf4j-jdk14-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.10/b3eeae7d1765f988a1f45ea81517191315c69c9e/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.5/6edffc576ce104ec769d954618764f39f0f0f10d/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.JDK14LoggerFactory]; java.lang.IllegalStateException: Unable to infer a default Coder for AnonymousParDo.out [PCollection]; either correct the root cause below or use setCoder() to specify one explicitly. ; at com.google.cloud.dataflow.sdk.values.TypedPValue.getCoder(TypedPValue.java:48); at com.google.cloud.dataflow.sdk.values.PCollection.getCoder(PCollection.java:137); at com.google.cloud.dataflow.sdk.transforms.windowing.Window$Bound.getDefaultOutputCoder(Window.java:286); at com.google.cloud.dataflow.sdk.transforms.windowing.Window$Bound.getDefaultOutputCoder(Window.java:221); at com.google.cloud.dataflow.sdk.transforms.PTransform.getDefaultOutputCoder(PTransform.java:334); at com.google.cloud.datafl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122314248:1545,cache,caches,1545,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122314248,1,['cache'],['caches']
Performance,; at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.grad,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:15874,cache,cache,15874,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['cache']
Performance,"; at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:122); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:33); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Then I check HCC1954.sam, and found . @RG ID:HCC1954 LB:HCC1954 SM:HCC1954. Since it complains no Platform information, I manually add PL:illumina to this line. then step3 works. . My question is do you also manually add PL, platform information to the generated sam file from bwa mem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1949:2806,concurren,concurrent,2806,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1949,2,['concurren'],['concurrent']
Performance,; callable 1.0; ```; results in FilterMutectCalls exception; ```; java.lang.IllegalArgumentException: logValues must be non-infinite and non-NAN; at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeFromLogToLinearSpace(NaturalLogUtils.java:27); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.posteriorProbabilityOfError(Mutect2FilteringEngine.java:93); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:140); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSomaticVariant(SomaticClusteringModel.java:146); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.performEMIteration(SomaticClusteringModel.java:345); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:330); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:153); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:165); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLin,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7276#issuecomment-1293969047:1769,perform,performEMIteration,1769,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7276#issuecomment-1293969047,1,['perform'],['performEMIteration']
Performance,"; gatk MarkDuplicates I=WA02_i5-537_i7-98_S11819_L004.bam O=test.dup.bam M=marked_dup_metrics.txt; Using GATK jar /opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar MarkDuplicates I=WA02_i5-537_i7-98_S11819_L004.bam O=test.dup.bam M=marked_dup_metrics.txt; INFO 2024-07-03 15:26:21 MarkDuplicates. ********** NOTE: Picard's command line syntax is changing.; **********; ********** For more information, please see:; **********; https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition); **********; ********** The command line looks like this in the new syntax:; **********; ********** MarkDuplicates -I WA02_i5-537_i7-98_S11819_L004.bam -O test.dup.bam -M marked_dup_metrics.txt; **********. 15:26:21.393 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Wed Jul 03 15:26:21 CEST 2024] MarkDuplicates INPUT=[WA02_i5-537_i7-98_S11819_L004.bam] OUTPUT=test.dup.bam METRICS_FILE=marked_dup_metrics.txt MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 TAG_DUPLICATE_SET_MEMBERS=false REMOVE_SEQUENCING_DUPLICATES=false TAGGING_POLICY=DontTag CLEAR_DT=true DUPLEX_UMI=false FLOW_MODE=false FLOW_DUP_STRATEGY=FLOW_QUALITY_SUM_STRATEGY USE_END_IN_UNPAIRED_READS=false USE_UNPAIRED_CLIPPED_END=false UNPAIRED_END_UNCERTAINTY=0 UNPAIRED_START_UNCERTAINTY=0 FLOW_SKIP_FIRST_N_FLOWS=0 FLOW_Q_IS_KNOWN_END=false FLOW_EFFECTIVE_QUALITY_THRESHOLD=15 ADD_PG_TAG_TO_READS=true REMOVE_DUPLICATES=false ASSUME_SORTED=false DUPLICATE_SCORING_STRATEGY=SUM_OF_BASE_QUALITIES PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates READ_NAME_REGEX=<optimiz",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8904:6119,Load,Loading,6119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8904,1,['Load'],['Loading']
Performance,; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/uksza_2022_Nature -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/uksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:36:33.528 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.547 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.550 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.551 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.669 INFO BaseRecalibrator - ------------------------------------------------------------; 13:36:33.670 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:36:33.670 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:36:33.670 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86_64 amd64; 13:36:33.670 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087; 13:36:33.671 INFO BaseRecalibrator - Start Dat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:6708,load,load,6708,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['load'],['load']
Performance,"; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar CombineGVCFs -R /db_students1/cc/genetic_map/snp_calling/bbv18h27rm.fa --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_1to128.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_1280to18.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_180to25.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_250to35.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_350to5.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_50to69.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_690to999.g.vcf.gz -O /db_students1/cc/gatk_out/raw_new52_off_xL4_70.g.vcf.gz; 17:52:22.080 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 11, 2020 5:52:23 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:52:23.841 INFO CombineGVCFs - ------------------------------------------------------------; 17:52:23.841 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.3.0-25-g8d88f6e-SNAPSHOT; 17:52:23.841 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:52:23.842 INFO CombineGVCFs - Executing as cc@hr18b on Linux v3.10.0-957.el7.x86_64 amd64; 17:52:23.842 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_212-b04; 17:52:23.842 INFO CombineGVCFs - Start Date/Time: January 11, 2020 5:52:22 PM CST; 17:52:23.842 INFO CombineGVCFs - ----------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6368:1395,Load,Loading,1395,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6368,1,['Load'],['Loading']
Performance,;)I> (0x000000067b20f3e0) thrown at [/home/buildozer/aports/community/openjdk8/s; Event: 3.490 Thread 0x00005648765c2000 Exception <a 'java/lang/NoSuchMethodError': java.lang.Object.lambda$comparingInt$7b0bb60$1(Ljava/util/function/ToIntFunction;Ljava/lang/Object;Ljava/lang/Object;)I> (0x000000067b219168) thrown at [/home/buildozer/aports/community/openjdk8/src/icedtea-3.6.; Event: 3.491 Thread 0x00005648765c2000 Exception <a 'java/lang/NoSuchMethodError': java.lang.Object.lambda$thenComparing$36697e65$1(Ljava/util/Comparator;Ljava/lang/Object;Ljava/lang/Object;)I> (0x000000067b220588) thrown at [/home/buildozer/aports/community/openjdk8/src/icedtea-3.6.0/openjdk/. Events (10 events):; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmmOMP; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmmOMP done; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmm; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmm done; Event: 4.326 loading class com/intel/gkl/IntelGKLUtils; Event: 4.326 loading class com/intel/gkl/IntelGKLUtils done; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder done; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder done. Dynamic libraries:; 3c0000000-41b600000 rw-p 00000000 00:00 0 ; 41b600000-66ab00000 ---p 00000000 00:00 0 ; 66ab00000-6aef00000 rw-p 00000000 00:00 0 ; 6aef00000-7c0000000 ---p 00000000 00:00 0 ; 7c0000000-7c0520000 rw-p 00000000 00:00 0 ; 7c0520000-800000000 ---p 00000000 00:00 0 ; 2b5f56cd5000-2b5f56d5e000 r-xp 00000000 07:00 565 /lib/ld-musl-x86_64.so.1; 2b5f56d5e000-2b5f56d60000 ---p 00000000 00:00 0 ; 2b5f56d60000-2b5f56d63000 ---p 00000000 00:00 0 ; 2b5f56d63000-2b5f56e61000 rw-p 00000000 00:00 0 [stack:85483]; 2b5f56e61000-2b5f,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:29287,load,loading,29287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['load'],['loading']
Performance,"=0, nn=gs://hellbender-test-logs, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:; java.io.IOException: Must supply a value for configuration setting: fs.gs.project.id; 	at com.google.cloud.hadoop.util.ConfigurationUtil.getMandatoryConfig(ConfigurationUtil.java:39); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createOptionsBuilderFromConfig(GoogleHadoopFileSystemBase.java:2185); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1832); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1013); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:976); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2812); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:100); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2849); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2831); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:171); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:168); 	at java.base/java.security.AccessController.doPrivileged(Native Method); 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:168); 	at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.generateHeadersForQuery(GenomicsDBFeatureReader.java:176); 	at org.genomicsdb.reader.Gen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6522:1311,Cache,Cache,1311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6522,1,['Cache'],['Cache']
Performance,===========; Files 1139 1146 +7 ; Lines 60902 62029 +1127 ; Branches 9437 9684 +247 ; ===============================================; + Hits 48705 49752 +1047 ; - Misses 8401 8435 +34 ; - Partials 3796 3842 +46; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree) | Coverage  | Complexity  | |; |---|---|---|---|; | [...ellbender/tools/exome/FilterByOrientationBias.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9GaWx0ZXJCeU9yaWVudGF0aW9uQmlhcy5qYXZh) | `83.019% <> ()` | `14 <0> ()` | :arrow_down: |; | [...ls/walkers/mutect/CreateSomaticPanelOfNormals.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9DcmVhdGVTb21hdGljUGFuZWxPZk5vcm1hbHMuamF2YQ==) | `100% <0%> ()` | `10% <0%> (+3%)` | :arrow_up: |; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <0%> ()` | `4% <0%> (+2%)` | :arrow_up: |; | [...itute/hellbender/tools/walkers/mutect/Mutect2.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyLmphdmE=) | `92.593% <0%> ()` | `32% <0%> (+16%)` | :arrow_up: |; | [...s/spark/pathseq/PSBuildReferenceTaxonomyUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BTQnVpbGRSZWZlcmVuY2VUYXhvbm9teVV0aWxzLmphdmE=) | `88.961% <0%> ()` | `39% <0%> (?)` | |; | [.../hellbender/tools/spark/utils/LongBloomFilter.java](https://codecov.io/gh/broadinstitute/gat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3031#issuecomment-306370974:1560,Perform,PerformAlleleFractionSegmentation,1560,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3031#issuecomment-306370974,1,['Perform'],['PerformAlleleFractionSegmentation']
Performance,====================; Files 1138 1142 +4 ; Lines 62637 62823 +186 ; Branches 9521 9548 +27 ; ===============================================; + Hits 49731 49871 +140 ; - Misses 9110 9142 +32 ; - Partials 3796 3810 +14; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree) | Coverage  | Complexity  | |; |---|---|---|---|; | [...ber/coverage/readcount/ReadCountFileHeaderKey.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2NvdmVyYWdlL3JlYWRjb3VudC9SZWFkQ291bnRGaWxlSGVhZGVyS2V5LmphdmE=) | `0% <0%> ()` | `0 <0> (?)` | |; | [...der/utils/test/IntegerReadCountFileComparator.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0ludGVnZXJSZWFkQ291bnRGaWxlQ29tcGFyYXRvci5qYXZh) | `70.732% <70.732%> ()` | `6 <6> (?)` | |; | [...pynumber/utils/CachedBinarySearchIntervalList.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL0NhY2hlZEJpbmFyeVNlYXJjaEludGVydmFsTGlzdC5qYXZh) | `74.603% <74.603%> ()` | `18 <18> (?)` | |; | [...bender/tools/copynumber/CollectFragmentCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL0NvbGxlY3RGcmFnbWVudENvdW50cy5qYXZh) | `88.406% <88.406%> ()` | `11 <11> (?)` | |; | [...er/tools/spark/sv/discovery/AlignmentInterval.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvQWxpZ25tZW50SW50ZXJ2YWwuamF2YQ==) | `90.517% <0%> (-0.431%)` | `63% <0%> (-1%)` | |; | [...ute/hellbender/utils/read/ArtificialReadUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?sr,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3690#issuecomment-335961649:1557,Cache,CachedBinarySearchIntervalList,1557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3690#issuecomment-335961649,1,['Cache'],['CachedBinarySearchIntervalList']
Performance,========================; + Coverage 86.362% 86.626% +0.264% ; + Complexity 39551 38919 -632 ; ===============================================; Files 2362 2336 -26 ; Lines 186121 182603 -3518 ; Branches 20305 20062 -243 ; ===============================================; - Hits 160738 158181 -2557 ; + Misses 18236 17379 -857 ; + Partials 7147 7043 -104 ; ```. | [Files Changed](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage |; |---|---|; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `` |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `0.000%` |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `` |; | [...scalable/modeling/BGMMVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstit,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153:1950,scalab,scalable,1950,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153,1,['scalab'],['scalable']
Performance,=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvU2lkZVJlYWRJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `81.481% <81.481%> ()` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `91.667% <84.615%> (+33.333%)` | :arrow_up: |; | [...ellbender/engine/VariantWalkerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvVmFyaWFudFdhbGtlckludGVncmF0aW9uVGVzdC5qYXZh) | `87.288% <86.667%> ()` | |; | [...engine/cache/DrivingFeatureInputCacheStrategy.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvRHJpdmluZ0ZlYXR1cmVJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `88.000% <88.000%> ()` | |; | [...ellbender/engine/cache/LocatableCacheUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvTG9jYXRhYmxlQ2FjaGVVbml0VGVzdC5qYXZh) | `96.471% <96.471%> ()` | |; | [...gumentcollections/ReadInputArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#di,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741:3501,cache,cache,3501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741,1,['cache'],['cache']
Performance,"> 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; > 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; > 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; > [March 2, 2023 3:40:16 PM EST]; > org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done.; > Elapsed time: 5.27 minutes.; > Runtime.totalMemory()=3432513536; > java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to; > java.lang.Comparable; > at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); > at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); > at java.util.TimSort.sort(TimSort.java:234); > at; > java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); > at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); > at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); > at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); > at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); > at java.util.Arrays.parallelSort(Arrays.java:1180); > at; > htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); > at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); > at; > htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); > at; > htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); > at java.lang.Thread.run(Thread.java:750); > Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record; > to closed writer.; > at; > htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); > at; > htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); > at; > org.broadinstitute.hellbender.utils.read.SAMFi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:6761,concurren,concurrent,6761,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['concurren'],['concurrent']
Performance,"> > Hi, Meng,; > > I also meet this issue, have you solve it?; > > Thank you; > ; > Hello,; > Actually, I didn't solve it completely.; > When I change to another server, it can run 2.14GB's data well.; > I think it's because the data is too large, and the server can't perform it normally.; > If it's not necessary, you can choose other tools.; > ; > Best wish; > Meng. Hi @MengZhang2019 Meng, Thank you; I also didn't solve this problem.; Do you have any tools recommend? I have tried other methods, but the running time was too long, they do not suit large sample data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-625218636:269,perform,perform,269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-625218636,1,['perform'],['perform']
Performance,"> > In the latest filtering paradigm, how would somebody who only wanted variants with really high quality bases change the default parameters?; > ; > You could decrease `f-score-beta` (default 1.0) to bias the threshold optimization in favor of precision versus sensitivity. Okay. Rip it out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5827#issuecomment-475756509:221,optimiz,optimization,221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5827#issuecomment-475756509,1,['optimiz'],['optimization']
Performance,"> > Not to mention, in theory one could have some job trying to read the original workspace, which might get hosed if some other job is trying to edit that workspace in place.; > ; > This is not possible as new GenomicsDB workspace fragments are created for the incremental updates. During the actual finalization of the fragments, the array in the workspace is locked using Posix file locks for concurrency. As @nalinigans says, individual arrays/intervals in a GenomicsDB workspace will be consistent during incremental import. One caveat though, since each array is independently updated, different arrays/intervals will finish adding samples at different times while incremental import is in progress. So, querying a workspace that is being incrementally imported into isn't recommended.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6558#issuecomment-617405801:396,concurren,concurrency,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6558#issuecomment-617405801,1,['concurren'],['concurrency']
Performance,> @Bowen1992 Could you please try running with the latest GATK release (`4.2.6.1`) and reporting whether the issue persists?. Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR7_gvcf_database -G StandardAnnotation --genomicsdb-shared-posixfs-optimizations true -O new_ALL_MATERIALS_chr7.g.vcf.gz; 17:49:50.404 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 17:49:50.653 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:49:51.271 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.273 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.6.1; 17:49:51.273 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:49:51.273 INFO GenotypeGVCFs - Executing as gaoshibin@comput6 on Linux v3.10.0-693.el7.x86_64 amd64; 17:49:51.274 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 17:49:51.274 INFO GenotypeGVCFs - Start Date/Time: 2022522 054950; 17:49:51.274 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.275 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.276 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 17:49:51.276 INFO GenotypeGVCFs - Picard Version: 2.27.1; 17:49:51.276 INFO GenotypeGVCFs - Built for Spark Version: 2.4.5; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:49:,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135302097:574,optimiz,optimizations,574,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135302097,2,"['Load', 'optimiz']","['Loading', 'optimizations']"
Performance,"> At the very least we should add a unit test that generates the evidenceIndexBySampleIndex cache, then calls marginalize() (both types) and asserts that we have emptied the cache. I would do the same for appendEvidence() and addMissingAlleles(). It's simpler than this because allele operations such as `marginalize()` and `addMissingAlleles` don't modify the evidence list. While they require care with the likelihoods arrays they don't require anything at all from the evidence-to-index caches. As I mentioned above, I left the cache updating in `appendEvidence` as it was because it was so simple. I will try to write the test for removing evidence tomorrow. Tempting to try tonight, but I'm trying to accept the reality that working until 2 am is a bad idea.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6593#issuecomment-633180869:92,cache,cache,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6593#issuecomment-633180869,4,['cache'],"['cache', 'caches']"
Performance,"> Do you have numbers for the performance here? How big is this code in the profiler before or after? I'm curious. The unit tests are about 5% faster. In practice, this won't affect HaplotyeCaller because the overwhelming majority of the CPU cost of those tests comes from the ploidy = 20, allele count = 6 cases. For anything else the genotyping likelihoods calculation is not only not a bottleneck, it's completely negligible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-580337113:30,perform,performance,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-580337113,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"> Hi, I am trying to generate vcf using GATK pipeline from bam file, but everytime, I am getting the following exception: 01:13:15.801 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/ngs/programs/gatk-4.0.0.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so 01:13:16.075 INFO HaplotypeCaller - ------------------------------------------------------------ 01:13:16.075 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.0.0 01:13:16.075 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/ 01:13:16.076 INFO HaplotypeCaller - Executing as shashank@grande on Linux v3.13.0-79-generic amd64 01:13:16.076 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_72-internal-b15 01:13:16.076 INFO HaplotypeCaller - Start Date/Time: January 18, 2020 1:13:15 AM IST 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------ 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------ 01:13:16.077 INFO HaplotypeCaller - HTSJDK Version: 2.13.2 01:13:16.077 INFO HaplotypeCaller - Picard Version: 2.17.2 01:13:16.077 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false 01:13:16.078 INFO HaplotypeCaller - Deflater: IntelDeflater 01:13:16.078 INFO HaplotypeCaller - Inflater: IntelInflater 01:13:16.078 INFO HaplotypeCaller - GCS max retries/reopens: 20 01:13:16.078 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes 01:13:16.078 INFO HaplotypeCaller - Initializing engine 01:13:17.087 INFO HaplotypeCaller - ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-1605272955:162,Load,Loading,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-1605272955,1,['Load'],['Loading']
Performance,"> Hi, Meng,; > I also meet this issue, have you solve it?; > Thank you. Hello,; Actually, I didn't solve it completely.; When I change to another server, it can run 2.14GB's data well.; I think it's because the data is too large, and the server can't perform it normally. ; If it's not necessary, you can choose other tools. Best wish; Meng",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-623968600:251,perform,perform,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-623968600,1,['perform'],['perform']
Performance,> I also encounter this error when most samples have been imported. I ran importing in batches '--batch-size 50 --consolidate '. The error occured at the last batch. Can I reuse some of the imported data files or have to rerun the whole importing again?. ...; 13:13:26.069 INFO GenomicsDBImport - Done importing batch 21/22; 13:13:26.069 INFO GenomicsDBImport - Starting batch input file preload; 13:13:27.440 INFO GenomicsDBImport - Finished batch preload; 13:13:27.440 INFO GenomicsDBImport - Importing batch 22 with 22 samples; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading tile offsets failed.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : Error while consolidating TileDB array chrY$1$57227415; TileDB error message : [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading tile offsets failed,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6519#issuecomment-641091886:640,load,load,640,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6519#issuecomment-641091886,2,['load'],['load']
Performance,"> I think we've seen similar issues before. Libgomp needs to be installed and findable. I think it typically is installed when you install gcc. See #6012 for more discussion. Thank you, I checked my system and found gcc module not loaded. I reloaded my gcc by typing; `module load gcc/5.4.0`; and it works now. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8194#issuecomment-1425155969:231,load,loaded,231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194#issuecomment-1425155969,2,['load'],"['load', 'loaded']"
Performance,"> If one thinks about a genomicsDB workspace more like a database than single file, are there defrag/shrink-like tasks that need to be performed on the workspace for efficiency?. Did you use the `--consolidate` option with GenomicsDBImport? This option consolidates fragments to help with query performance later. Usually not needed for very small batch sizes.; Also, available from 4.1.7.0 is a `--genomicsdb-shared-posixfs-optimizations` option. Can you try GenotypeGVCFs with this knob turned on if your workspace is on NFS/Lustre and let us know?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669344356:135,perform,performed,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669344356,3,"['optimiz', 'perform']","['optimizations', 'performance', 'performed']"
Performance,"> It would appear this does what it says on the tin, though I'm curious what the scenario was where the absence of the timestamp caused a call caching issue. I reuse a dataset for development (rsa_gvs_quickstart_dev). One past run was did not use compressed references, so that is always used when call caching is turned on, even though the dataset has reingested compressed references since then. This is the exact scenario that `GetBQTableLastModifiedDatetime` was created for  database-based tasks that we want to be able to call cache accurately.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8667#issuecomment-1917045421:534,cache,cache,534,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8667#issuecomment-1917045421,1,['cache'],['cache']
Performance,"> Just curious, why no last modified checks? Was it to keep the code simpler?. Mostly because I couldn't readily think of a scenario where I would actually want this to call cache, but I could easily imagine call caching leading to undesired clobbering of previously generated results. We can certainly revisit this decision if it turns out we're using the script in ways where we really would want call caching.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8002#issuecomment-1227739990:174,cache,cache,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8002#issuecomment-1227739990,1,['cache'],['cache']
Performance,"> One final thing: i'm happy to try to debug this, and was going to write a test case based on the existing GenomicsDB integration tests. However, when I try to run any integration test involving genomicsdb, I get an exception like the following. I am on windows, so perhaps this is the issue?; > ; > 09:03:37.460 FATAL GenomicsDBLibLoader -; > java.io.FileNotFoundException: File /tiledbgenomicsdb.dll was not found inside JAR.; > at org.genomicsdb.GenomicsDBLibLoader.loadLibraryFromJar(GenomicsDBLibLoader.java:118) ~[genomicsdb-1.3.2.jar:?]; > at org.genomicsdb.GenomicsDBLibLoader.loadLibrary(GenomicsDBLibLoader.java:55) [genomicsdb-1.3.2.jar:?]; > at org.genomicsdb.GenomicsDBUtilsJni.(GenomicsDBUtilsJni.java:30) [genomicsdb-1.3.2.jar:?]; > at org.genomicsdb.GenomicsDBUtils.createTileDBWorkspace(GenomicsDBUtils.java:46) [genomicsdb-1.3.2.jar:?]; > at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.overwriteCreateOrCheckWorkspace(GenomicsDBImport.java:1005) [classes/:?]; > at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onTraversalStart(GenomicsDBImport.java:661) [classes/:?]; > at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1056) [classes/:?]. Yes, Windows is not supported by GenomicsDB. This is mentioned obliquely in the requirements for gatk too -; ```; Operating system. The GATK runs natively on most if not all flavors of UNIX, which includes MacOSX, Linux and BSD. It is possible to get it ; running on some recent versions of Windows, but we don't provide any support nor instructions for that. If you need to run on; a Windows machine, consider using Docker.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-754106359:470,load,loadLibraryFromJar,470,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-754106359,2,['load'],"['loadLibrary', 'loadLibraryFromJar']"
Performance,"> Running with default arguments locally the runtime (for a WGS full chr15) drops from ~8.9 minutes to ~4.7 minutes after this patch. If I had to peg something else to optimize it would be replacing CSVWriter which seems to be somewhat slow but I can be contented that this tool is reasonably fast when nothing pathological is being triggered. Hello, you mentioned that running DepthOfCoverage for a WGS full chr15 only takes ~4.7 minutes. Would you mind letting me know what is the coverage for the BAM you use? It took days for me to run DepthOfCoverage on a 80X WGS. And, will there be a Spark implementation for DepthOfCoverage in the near future? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6740#issuecomment-723391687:168,optimiz,optimize,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6740#issuecomment-723391687,1,['optimiz'],['optimize']
Performance,"> Therefore we don't actually have to take the log of the probability and normalize it, we can just take the probability straight from HypergeometricDistribution. The problem is mostly inside `HypergeometricDistribution`. A better implementation of this class should cache the last value, such that computing `hypergeo(i)` and then `hypergeo(i+1)` consecutively does not unnecessarily trigger full computation, which is quite expensive. Anyway, the current implementation is fine given that exact test is unlikely to be a bottleneck in its current use.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266857573:267,cache,cache,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266857573,2,"['bottleneck', 'cache']","['bottleneck', 'cache']"
Performance,"> What happen to the bundling performance improvement changes by the way?. The large 2D file array can be handled by the latest Cromwell versions, so we do not need to bundle. It is much more elegant and readable this way and should actually improve performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-672068385:30,perform,performance,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-672068385,2,['perform'],['performance']
Performance,> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/hgnc/hg19/hgnc_download_Nov302017.tsv; 12:11:28.270 INFO DataSourceUtils - Resolved data source file path: file:///gatk/hg19_All_20180423.vcf.gz -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.270 INFO DataSourceUtils - Setting lookahead cache for data source: dbSNP : 100000; 12:11:28.277 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.426 INFO DataSourceUtils - Resolved data source file path: file:///gatk/hg19_All_20180423.vcf.gz -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.771 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.877 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; 12:11:28.882 INFO DataSourceUtils - Resolved data source file path: file:///gatk/oreganno.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; 12:11:28.883 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.config; 12:11:28.905 INFO DataSourceUtils - Resolved data source file path: file:///gatk/oreganno.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; 12:11:28.906 INFO DataSourceUtils - Resolved data source file path: file:///gatk/oreganno.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; WARNING 2021-03-24 12:11:28 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 12:11:28.910 INFO DataSourceUtils - Re,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:11521,cache,cache,11521,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,1,['cache'],['cache']
Performance,"> ls -l genome.*; -rw-rw---- 1 kh3 kh3 784809415 Sep 16 10:16 genome.2bit; -rw-rw---- 1 kh3 kh3 3168829906 Feb 4 2014 genome.fa; -rw-r----- 1 kh3 kh3 106669 Sep 16 11:32 genome.fa.amb; -rw-r----- 1 kh3 kh3 3276 Sep 16 11:32 genome.fa.ann; -rw-r----- 1 kh3 kh3 3137454592 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 3:47:28 PM EDT] Executing ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171:1154,load,load,1154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171,1,['load'],['load']
Performance,>10% performance improvement seems worthwhile,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7869:5,perform,performance,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7869,1,['perform'],['performance']
Performance,">In the latest filtering paradigm, how would somebody who only wanted variants with really high quality bases change the default parameters?. You could decrease `f-score-beta` (default 1.0) to bias the threshold optimization in favor of precision versus sensitivity.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5827#issuecomment-475728493:212,optimiz,optimization,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5827#issuecomment-475728493,1,['optimiz'],['optimization']
Performance,"@Aqoolare Hello. There are a a few things going on here. The `unrecognized runtime attribute keys` warning is coming from cromwell. It's telling you that the cromwell **local** backend doesn't understand those keys, which is true. That means it's just ignoring them. I think the actual problem is different though. You're running the spark tool in spark local mode, which in this case isn't configured to use the correct amount of cores or memory. I think the intent of this wdl script was that it would be run in a container on a cluster and the container would restrict the cores and memory options. In any case, it's not configured correctly for what you need. I would skip running cromwell and just invoke gatk directly since this wdl only executes a single job. Since this is going to run spark in local mode you need to specify the number of cores using the `--spark-master` argument, and set the memory using the `--java-options ""-Xmx""` arguments. For example:. ```; gatk ReadsPipelineSpark ; --java-options ""-Xmx16G"" ; --spark-master 'local[8]'; --I yourbam.; ... etc; ```. The above command is specifying to use 8 (that's what the local[**8**] means) cores for spark and give it 16G of memory. Your job was accidentally using 200 cores so it doesn't surprise me that it would run into memory issues. Using spark with more than 16ish cores in a single process is going to bog down a lot. I think 8 is a good starting place to try. If you want to go wider you should really look into running a proper cluster (or using dataproc), but there's pretty heavy diminishing returns. Try 8 or 16 and tune the memory from there.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1108913771:1599,tune,tune,1599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1108913771,1,['tune'],['tune']
Performance,"@Aqoolare You might, but there are scaling issues when running within a single java process which is what you're doing. There are issues with lock contention and garbage collection which cause more cores to not be utilized very well. (Lots of cores waiting while garbage collection stops the world, that sort of thing.). . You could definitely test it. We found that 8-16 cores was optimal for our use cases for running in spark local mode, but spark performance is extremely finicky and it's very possible your system might do better than what I'm used to. If you wantt to go very parallel it works better to run an actual spark cluster. You should be able to utilize cores more efficiently that way, but it's more complicated to set up and operatte and there are still bottlenecks that keep it from being infinitely scaleable. (IO bandwidth and network traffic being important ones). . There are lots of articles on the internet about how to set up a local yarn cluster that can help walk you through it if you want to try. . I'm going to close this ticket since it seems like the problem is solved. Feel free to reopen or open a new one if you have other issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1111473992:451,perform,performance,451,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1111473992,2,"['bottleneck', 'perform']","['bottlenecks', 'performance']"
Performance,"@Bowen1992 **I got the same error, do you have a solution now?**. Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx76800m -jar /home/zwc1988/miniconda3/envs/gatk/share/gatk4-4.2.6.1-0/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R data/ref/CL200105941_L02.fa -V gendb://results/genotype/genodb/group2 -O results/genotype/vcfs/group2.vcf.gz --tmp-dir ./tmp; 18:24:10.205 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/zwc1988/miniconda3/envs/gatk/share/gatk4-4.2.6.1-0/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:24:10.451 INFO GenotypeGVCFs - ------------------------------------------------------------; 18:24:10.452 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.6.1; 18:24:10.452 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:24:10.452 INFO GenotypeGVCFs - Executing as zwc1988@fat01 on Linux v3.10.0-957.el7.x86_64 amd64; 18:24:10.453 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_312-b07; 18:24:10.453 INFO GenotypeGVCFs - Start Date/Time: June 19, 2022 6:24:10 PM CST; 18:24:10.453 INFO GenotypeGVCFs - ------------------------------------------------------------; 18:24:10.453 INFO GenotypeGVCFs - ------------------------------------------------------------; 18:24:10.454 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 18:24:10.454 INFO GenotypeGVCFs - Picard Version: 2.27.1; 18:24:10.454 INFO GenotypeGVCFs - Built for Spark Version: 2.4.5; 18:24:10.454 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 18:24:10.455 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 18:24:10.455 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 18:24:10.455 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:24:10.455 INFO Genotype",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1159695894:523,Load,Loading,523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1159695894,1,['Load'],['Loading']
Performance,"@Cashalow Can you include the full command line? 50 human genomes (ploidy 2) would need less than 7GB memory in my experience, even for highly multi-allelic indel sites. I would expect ploidy 1 calls (as most users run microbial genomes) would require even less, but we have some diploid-specific optimizations. Are you using the `new-qual` argument? That QUAL calculation algorithm is less computationally intensive and I believe it is applicable to all ploidies.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4467#issuecomment-370462977:297,optimiz,optimizations,297,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4467#issuecomment-370462977,1,['optimiz'],['optimizations']
Performance,"@DarioS I wonder if you're looking at the JVM garbage collector threads -- by default, Java uses a multi-threaded garbage collector. You can control the number of threads it uses via the `-XX:ParallelGCThreads=N` argument, where N is the number of garbage collector threads. To pass this option into GATK, use the `--java-options` argument. Eg., `./gatk --java-options '-XX:ParallelGCThreads=4'`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7156#issuecomment-804282075:99,multi-thread,multi-threaded,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7156#issuecomment-804282075,1,['multi-thread'],['multi-threaded']
Performance,"@DonFreed, I agree with @magicDGS's assessment about it. This feels like a fix that was applied to Gatk3 but doesn't translate to 4? Of course, there could be implementations of GATKRead that don't obey the given contract about copying, but it's worth fixing those since we were more careful to think about copy/no copy when we wrote the new interface. . Of note: if you haven't seen it, `GATKRead` provides a set of unsafe `getBaseQualitiesNoCopy()` methods for times when the copy is a performance bottleneck and you can guarantee safe use of the underlying array. . I'm going to close this. Feel free to reopen if you disagree / can provide a unit test that demonstrates the issue still exists.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562:488,perform,performance,488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"@EdwardDixon I did not know that! In that case master does already require AVX. If it only impacts this tool and we provide sufficient warning and instructions, I think the single intel-optimized conda environment will be so much easier to test and maintain. Users who don't have AVX can simply install an older tensorflow in their environment, but GATK doesn't need to worry about it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429142837:186,optimiz,optimized,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429142837,1,['optimiz'],['optimized']
Performance,"@EdwardDixon Thanks for this! I think the AVX check in CNNScoreVariants is good. As @droazen points out, we still want the split environments, though now with the check in place I think we can make intel optimized the default. Other thoughts @droazen, @cmnbroad ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-428272123:204,optimiz,optimized,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-428272123,1,['optimiz'],['optimized']
Performance,"@EdwardDixon Well, you'd be surprised at some of the hardware we have to deal with. Even some machines here at the Broad don't have AVX. In general, our policy with hardware-dependent optimizations in GATK has been to insist on having a transparent fallback mechanism when the required hardware isn't present -- I'd really prefer not to start making exceptions to that rule. Could the Intel-optimized Tensorflow be patched to fall back to vanilla tensorflow when AVX is not present? Is that an option? Or could it at least be patched to not actually crash in that case?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5142#issuecomment-417073151:184,optimiz,optimizations,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5142#issuecomment-417073151,2,['optimiz'],"['optimizations', 'optimized']"
Performance,"@EvanTheB crai index is supported. This ticket is quite old, and quite a few changes have been made to CRAM in the interim (especially to index queries, some of which affected both .crai and .bai). The TL;DR version is that while there are still open tickets in htsjdk relating to CRAM performance, and some to index queries, I don't see any that would have an obvious connection to the HC discrepancies reported in the forum post.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2850#issuecomment-405930781:286,perform,performance,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2850#issuecomment-405930781,1,['perform'],['performance']
Performance,@Fruit-loops this happens for many-allelic sites. SelectVariants historically assumed every input was a germline site with PL values (genotype likelihoods) for every possible combination of alleles and ploidy and generated an expensive cache of these indices. For somatic sites this concept is meaningless and for many-allelic sites it's combinatorially expensive. The PR mentioned above skips the expensive calculation for somatic sites that lack PLs.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6254#issuecomment-583801876:236,cache,cache,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6254#issuecomment-583801876,1,['cache'],['cache']
Performance,"@Horneth: what @lbergelson said. Java will use all of the memory you give it, regardless of how much the program asks for. The only difference is how long it takes until the memory's used up and the garbage collector needs to kick in. Thus lowering the memory requirements of a Java program tends to increase its performance, up to the point where we cut memory that it needed. . I would suggest either measuring the memory immediately after a garbage collection or, more directly, measure the performance of the program as you vary buffer sizes. Make sure to control for cache effects since the program's doing I/O.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298973266:313,perform,performance,313,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298973266,3,"['cache', 'perform']","['cache', 'performance']"
Performance,"@Horneth:. - With 100 samples, is the tool always running out of memory, or is it only running out of memory with certain buffer sizes?. - If I'm interpreting your document correctly, you found that with a buffer size of 0 or 1, performance degrades by 10%, is that right?. - Can you post the file sizes involved here? Both the size of the original GVCF inputs, *and* the size of the data from those inputs that overlaps your interval (you can find out the latter by running GATK4 SelectVariants on the GVCF using the same interval, and recording the resulting file size). I'd also like to know the sizes of the index files. - It would be good if you could post your profiling results directly in this ticket, rather than in a Google doc, so that @jean-philippe-martin (who has implemented the GCS support in GATK) can easily see them. @jean-philippe-martin Could you chime in here to remind us of your profiling results with NIO buffer sizes and the use case of a single query interval? Didn't you find that there was a very large performance difference between running with and without buffering?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298927579:229,perform,performance,229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298927579,2,['perform'],['performance']
Performance,"@LeeTL1220 - I addressed in the next commit the documentation issues; let me know if you have more suggestions on that. If you have any suggestion for an already implemented tool in GATK for the performance and some data for profiling, I can do a couple of runs in my local computer for look at them. If not, I can implement an small example tool for profiling when splitting by sample is needed (I can also modify `ExampleLocusWalker` for that in a different PR to being able to profile this after this PR too).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332227961:195,perform,performance,195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332227961,1,['perform'],['performance']
Performance,"@LeeTL1220 A few minor remaining comments. Do what you will. How much of a performance impact does the change have? You said it slows it down, is it significant? It might be faster if you make it a long instead of an atomic long which should be safe it it's single threaded and you don't use parallel streams anywhere.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316490330:75,perform,performance,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316490330,1,['perform'],['performance']
Performance,@LeeTL1220 Ready for review. I will concurrently test in Firecloud.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4710:36,concurren,concurrently,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4710,1,['concurren'],['concurrently']
Performance,"@LeeTL1220 The first commit makes all concordance tools and MC3/M2 vcf merge much faster in Firecloud -- tasks that have been taking an hour will take a few minutes. The second commit makes `GetPileupSummaries`, hence the contamination task much faster. Previously that tool has cached all reads for the 100,000 bases around each site, so basically it had to do a whole bam's worth of I/O just to get ~60,000 pileups.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5073:279,cache,cached,279,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5073,1,['cache'],['cached']
Performance,"@LeeTL1220 commented on [Mon May 23 2016](https://github.com/broadinstitute/gatk-protected/issues/524). - [ ] File issues for evaluations (and make sure each evaluation is described, including ground truth). Make sure that there is a milestone and assignee for each. Need to show some measure of performance for the WGS evaluation (GATK CNV only. Not ACNV)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858:296,perform,performance,296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858,1,['perform'],['performance']
Performance,@LeeTL1220 commented on [Thu Jan 21 2016](https://github.com/broadinstitute/gatk-protected/issues/316). How does the performance look?; This may also include having to troubleshoot adding a `sparkJar` artifact to `build.gradle`. ---. @LeeTL1220 commented on [Fri Jan 22 2016](https://github.com/broadinstitute/gatk-protected/issues/316#issuecomment-173938780). Dependent on issue https://github.com/broadinstitute/gatk-protected/issues/317,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2834:117,perform,performance,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2834,1,['perform'],['performance']
Performance,"@LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/937). By grabbing the gatk-protected docker image (or whichever is being used for M2), this task commits to a ~2GB download. However, the task does basic bash commands, which could easily be performed using one of the ``ubuntu:14.04`` images or maybe even one smaller.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2952:289,perform,performed,289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2952,1,['perform'],['performed']
Performance,"@LeeTL1220 commented on [Tue May 17 2016](https://github.com/broadinstitute/gatk-protected/issues/508). Currently, the tumor only het pulldown assumes purity of < 1.0. However, cell lines do have a purity of 1, so we would expect poor performance in some regions. ---. @samuelklee commented on [Thu Aug 18 2016](https://github.com/broadinstitute/gatk-protected/issues/508#issuecomment-240759250). @LeeTL1220 @mbabadi can you close if appropriate?. ---. @mbabadi commented on [Thu Aug 18 2016](https://github.com/broadinstitute/gatk-protected/issues/508#issuecomment-240890316). We haven't addressed this issue yet; we haven't thought much about it; either. I'd say leave it open until we decide whether or not we want to do; something about it. @LeeTL1220 what do you think?. On Thu, Aug 18, 2016 at 11:26 AM, samuelklee notifications@github.com; wrote:. > @LeeTL1220 https://github.com/LeeTL1220 @mbabadi; > https://github.com/mbabadi can you close if appropriate?; > ; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk-protected/issues/508#issuecomment-240759250,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/AOmMjWPbLQa_lDeVIwY-bP7_9lmaXLRVks5qhHmRgaJpZM4IgpaN; > .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2856:235,perform,performance,235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2856,1,['perform'],['performance']
Performance,@LeeTL1220 commented on [Wed Mar 02 2016](https://github.com/broadinstitute/gatk-protected/issues/396). We would like performance for WGS. ; Metrics could be: ; - sensitivity and precision; - concordance between multiple WGS replicates; - RMSE against HAPSEG results. Feel free to finalize this list...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2840:118,perform,performance,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2840,1,['perform'],['performance']
Performance,"@LeeTL1220 has already made substantial progress on this front, but I think we can improve ground truth sets and expand the number and type of evaluation metrics calculated. To start, this will include:. - [ ] TCGA WGS; - [ ] HCC1143 WES purity series; - [ ] HCC1143 reproducibility. We should decide on a small set of tools to compare against as well, including GATK CNV/ACNV. We should look for other gold-standard somatic callsets and build the evaluation infrastructure so that these can be easily added as sources of GT. We should also evaluate performance on germline and compare with gCNV (see #4123). @MartonKN will need this evaluation (and may need to help design some aspects of it) for #4115. See also #2881.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4122:550,perform,performance,550,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4122,1,['perform'],['performance']
Performance,"@MartonKN I've labeled the update of the caller as a ""reach"", so I'm not expecting that it gets done before release. However, I expect that the tutorial data should be updated well before release. The tutorial data runs quickly (~1 hr for coverage collection, which is mostly limited by the slowest samples or cloud preemptions, and then ~minutes once collection has been call cached), so we should have plenty of time. Whether or not the actual tutorial itself will be ready depends on whether @sooheelee has available bandwidth and if it is a high priority for comms.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3826#issuecomment-353730988:377,cache,cached,377,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3826#issuecomment-353730988,1,['cache'],['cached']
Performance,"@Neato-Nick ~~No, it's very same.~~ Sorry, typing too fast. I tried it again and it crashes:. Using GATK jar /home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar GenotypeGVCFs -O rad34test.comb2.raw.g.vcf.gz -R ../../../jic_reference/alygenomes.fasta -V rad34test.comb2.raw.vcf.gz -new-qual; 21:10:39.975 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:10:43.152 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:10:43.153 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.5.2; 21:10:43.153 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:10:44.334 INFO GenotypeGVCFs - Initializing engine; 21:10:44.849 INFO FeatureManager - Using codec VCFCodec to read file file:///home/vojta/dokumenty/fakulta/botanika/arabidopsis/samples/lib_2018_06/4_joined/rad34test.comb2.raw.vcf.gz; 21:10:44.979 INFO GenotypeGVCFs - Done initializing engine; 21:10:45.057 INFO ProgressMeter - Starting traversal; 21:10:45.057 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:10:45.344 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 21:10:47.780 INFO GenotypeGVCFs - Shutting down engine; [2. ervence 2018 21:10:47 CEST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=501219328; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 6.911788849595091E-17,NaN; at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); at org.broadinstitute.hellb",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-401904928:559,Load,Loading,559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-401904928,1,['Load'],['Loading']
Performance,"@RWilton The `HaplotypeCaller` performs two separate filtering passes on the read mapping qualities:. The first is in the `MappingQualityReadFilter`, and occurs before the `HaplotypeCaller` even sees the reads. This filtering pass can be controlled via the `--minimum-mapping-quality` argument you're using (and I can confirm that this argument is hooked up and does work as intended). The second pass is in `HaplotypeCallerEngine.filterNonPassingReads()`. This happens later (just before genotyping), and is hardcoded to filter out reads with mapping quality < 20. There is no way to control the mapping quality threshold in this second pass, as allowing reads with low mapping qualities into the genotyper would result in large numbers of false positive variant calls. You can confirm that the `--minimum-mapping-quality` argument for `MappingQualityReadFilter` works as intended by setting it to a value higher than 20, and inspecting the differences in the output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6854#issuecomment-701446008:31,perform,performs,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6854#issuecomment-701446008,1,['perform'],['performs']
Performance,"@SHuang-Broad . Nice hack using the cluster name. I don't see any other way to pass an arg to an initialization action. I have one suggestion to consider, but if you think it's too much work or not worth it feel free to skip: what if we separated out the reference bundle to copy from the data by specifying them both in the cluster name? That way we could, say, load either the hg19 or hg38 reference depending on the data we might be working with. So you could say ""cluster-hg38"" or ""cluster-hg19"" or ""cluster-hg19-na12878"". . Carrying it further, if we had a special convention for specifying data, like ""data-$SAMPLE"", we could just map $SAMPLE to a subdirectory on the bucket. That would provide a ton of flexibility. One minor note while you are messing with these scripts: the createCluster.sh script comment says ""This script deletes a Google Dataproc cluster used for running the GATK-SV pipeline."" Could you change to say it creates rather than deletes a cluster?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2456#issuecomment-286876038:363,load,load,363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2456#issuecomment-286876038,1,['load'],['load']
Performance,@SHuang-Broad where does this optimization around soft clipping come from? How does it get turned on or off?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294157699:30,optimiz,optimization,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294157699,1,['optimiz'],['optimization']
Performance,"@SaintBacchus Yes, this work is in progress. The first step was to add a ""strict"" mode to `HaplotypeCallerSpark` that causes it to match the output of the regular `HaplotypeCaller` very closely (this was done in https://github.com/broadinstitute/gatk/pull/5416). The next step will be to improve the performance of the Spark version.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5265#issuecomment-451206406:300,perform,performance,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5265#issuecomment-451206406,1,['perform'],['performance']
Performance,"@SusieX . Marking duplicates: I do recommend removing duplicates (we run MarkDuplicates from Picard). . BQSR: The pipeline we're developing is for Whole Genome data, so our bams have gone through BQSR in the whole genome pipeline. We're using those recalibrated base qualities. I haven't tested running BQSR only on the mitochondria so I don't know how well that would work. . If you do need to run BQSR only on the mitochondria I'd start by using the phylotree sites as `--known-sites`, but you'd need to have those sites in vcf format. Again, I haven't tested this so I don't know how well it will perform. If you end up using BQSR I think you're pipeline (BAM -> remove dup -> BQ recalibrate -> Mutect2 call -> FilterMutectCalls) is correct. Good luck!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-431852996:600,perform,perform,600,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-431852996,1,['perform'],['perform']
Performance,"@achevali commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gatk-protected/issues/902). It should state that the wrong file was given instead of trying to use it. ```12:11:34.836 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/dsde/working/aaronc/testing/LUAD/HMM_eval/gatk-protected/build/libs/gatk-protected-all-c17c8ed-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 12:11:34.894 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [February 15, 2017 12:11:34 PM EST] org.broadinstitute.hellbender.tools.exome.plotting.PlotACNVResults --hets /dsde/working/aaronc/testing/LUAD/wgs/pulldown/TCGA-55-6972-01A-11D-1945-08.hets.tsv --tangentNormalized /dsde/working/aaronc/testing/LUAD/wgs/tumor_pcov/TCGA-55-6972-01A-11D-1945-08-gc-corrected.tn.tsv --segments out/wes/TCGA-55-6972-01A-11D-1945-08.seg --sequenceDictionaryFile /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta --outputPrefix HMM_eval.TCGA-55-6972-01A-11D-1945-08. --output out/wgs/ --minimumContigLength 1000000 --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [February 15, 2017 12:11:34 PM EST] Executing as aaronc@gsa5.broadinstitute.org on Linux 2.6.32-642.11.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; 12:11:34.899 INFO PlotACNVResults - Defaults.BUFFER_SIZE : 131072; 12:11:34.899 INFO PlotACNVResults - Defaults.COMPRESSION_LEVEL : 5; 12:11:34.899 INFO PlotACNVResults - Defaults.CREATE_INDEX : false; 12:11:34.899 INFO PlotACNVResults - Defaults.CREATE_MD5 : false; 12:11:34.899 INFO PlotACNVResults - Defaults.CUSTOM_READER_FACTORY : ; 12:11:34.899 INFO PlotACNVResults - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 12:11:34.899 INFO PlotACNVResults - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 12:11:34.899 INFO PlotACNVResults - Defaults.REFERENCE_FASTA : null; 12:11:34.899 INFO PlotACNVResults - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2941:225,load,load,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941,2,['load'],"['load', 'loaded']"
Performance,"@akiezun ; I agree with separation of a native code repo from gatk. But can we have a single source tree for the native code? It will be useful for code maintenance to keep the common files in a single repository since 75% of the code are common between AVX and PPC. The source tree originally had the file set for the 128-bit vector ISA in GATK3, and the PPC porting adds only one file to the file set. We can split the files into the common and cpu-specific file sets. You propose two repos for the native library. Can you create a single source tree without code duplication (below) by using the two repos?. ```; common/ (~1500 lines); baseline.cc; common_data_structure.h; headers.h; jni_common.h; LoadTimeInitializer.{h,cc}; org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.{h,cc}; pairhmm-template-kernel.cc; template.h; utils.{h,cc}; avx/ (~450 lines, including the file set for the 256-bit vector ISA; enabled for AVX); define-double.h; define-float.h; function_instantiations.cc (old name: avx_function_instantiations.cc); headers_md.h; shift_template.cc; template_md.h; utils_md.{h,cc}; power8/ (~490 lines, including the file set for the 128-bit vector ISA; enabled for POWER8 (optional: SSE)); define-double.h (old name: define-sse-double.h); define-float.h (old name: define-sse-float.h); function_instantiations.cc (old name: sse_function_instantiations.cc); headers_md.h; power8.h; shift_template.cc; template_md.h; utils_md.{h,cc}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-215007739:702,Load,LoadTimeInitializer,702,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-215007739,1,['Load'],['LoadTimeInitializer']
Performance,"@akiezun @droazen @lbergelson Found the issue with FTZ being cleared. The FTZ setting only applies to the thread where FTZ is set. When running tests in gradle/testng, each test is run in a new thread. However, the pairHMM native library is only loaded for the first HaplotypeCaller test, since code in `VectorLoglessPairHMM.java` prevents the library from being loaded more than once in the same JVM. This means only the first test uses FTZ. A code change that loads the pairHMM native library for each test resolves the issue, and all `HaplotypeCallerIntegrationTest` tests pass. Another option to explore is setting FTZ in `main`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701:246,load,loaded,246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701,3,['load'],"['loaded', 'loads']"
Performance,@akiezun @droazen Here are my performance results for the latest commit using `-ERC GVCF` and `--activeRegionsOnly`. **Original Code**. ```; Elapsed time: 2.54 minutes.; Time in Smith-Waterman search = 0.0037741310000000004 sec; Time in Smith-Waterman full = 19.315050559 sec; Total Smith-Waterman calls = 84948 (0 substring matches = 0.00 %); ```. **Code with lastIndexOf for array**. ```; Elapsed time: 2.43 minutes.; Time in Smith-Waterman search = 0.020596938000000002 sec; Time in Smith-Waterman full = 12.461222691000001 sec; Total Smith-Waterman calls = 84948 (69338 substring matches = 81.62 %); ```. ~5% improvement for this test case. The Smith-Waterman profiling is currently enabled by default. There is also code to dump every Smith-Waterman call and result (currently disabled by default).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1677#issuecomment-204507562:30,perform,performance,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1677#issuecomment-204507562,1,['perform'],['performance']
Performance,"@akiezun @lindenb I can help. It's probably going to be a slightly complicated process though, especially actually building the cross platform jar. I assume we're targeting OSX and x86-64 to start with, and then hopefully expanding to POWER8 in the future? . The general idea is to prebuild the c code for whatever platforms we want to support. Then package that in a structured way into a jar, and write some java code which will detect the platform at runtime and extract the correct executable into a temporary location. Then we can publish that jar as a maven artifact. We have an example of how to do the extraction in the `VectorLoglessPairHMM` constructor. It's not perfect and probably needs a bit of refactoring to make it more general but it's the right idea. Other libraries that package native code have similar examples. I.e. Snappy-java https://github.com/xerial/snappy-java. We're going to be performing similar packaging for other native dependencies that we have, so standardizing it is a good idea.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-215820135:908,perform,performing,908,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-215820135,1,['perform'],['performing']
Performance,@akiezun A few documentation issues. I think the default methods should be have the contract that they must match the behavior of the default implementations but may be implemented differently for performance reasons. . Either remove `getBaseCounts` or replace `getLength` with it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1477#issuecomment-182546660:197,perform,performance,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1477#issuecomment-182546660,1,['perform'],['performance']
Performance,"@akiezun Can you determine whether you're using the HDFS -> GCS adapter in your test case? The adapter historically did have performance problems of this magnitude. As @jean-philippe-martin mentioned, we should benchmark the new NIO -> GCS support as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213097025:125,perform,performance,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213097025,1,['perform'],['performance']
Performance,"@akiezun Can you double-check that assumption? I am concerned because caching is disabled on the second `FeatureDataSource` for the driving variants source (the one we add to the `FeatureManager` -- see `VariantWalker.initializeDrivingVariants()`), and I know that `VariantFiltration` does queries on that cacheless data source.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155563487:306,cache,cacheless,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155563487,1,['cache'],['cacheless']
Performance,"@akiezun Change the cloud tests to broadcast a 2bit reference stored in our test GCS bucket for now? Should work fine. We need to benchmark the performance of broadcasting a 2bit reference in the cloud vs. reading it directly from a bucket from every worker, but this will require some code changes before we can do the comparison.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1756#issuecomment-213150052:144,perform,performance,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1756#issuecomment-213150052,1,['perform'],['performance']
Performance,"@akiezun Description of the steps to be performed:; 1. Create a ~100 GB slice of `/humgen/gsa-hpprojects/GATK/bundle/current/b37/CEUTrio.HiSeq.WGS.b37.NA12878.bam` using `PrintReads` (will need to experiment with intervals a bit to get the right size -- for reference, the first 130 megabases of chromosome 1 produces a ~14 GB bam).; 2. Create an index on the bam slice above using `samtools index`; 3. Create a slice of `/humgen/gsa-hpprojects/GATK/bundle/current/b37/dbsnp_138.b37.vcf` using `SelectVariants` and the same interval used to create the bam above.; 4. Create an index on the vcf slice above using `IndexFeatureFile`; 5. For the reference, use the existing complete `.2bit` reference `/local/dev/droazen/spark_inputs/human_g1k_v37.2bit` on `dataflow01`; 6. Put all inputs into both hdfs (for the BROADCAST implementation) and a GCS bucket (for the SHARDED implementation).; 7. Create an up-to-date hellbender spark jar from the latest master. Run the BROADCAST implementation on `dataflow01` using the unix `time` command and a script like the one below:. ```; spark-submit \; --master yarn-client \; --driver-memory 8G \; --num-executors 16 \; --executor-cores 4 \; --executor-memory 10G \; --conf spark.driver.maxResultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; $JAR BaseRecalibratorSpark \; --input hdfs:///path/to/your.bam \; --output bqsr_out_broadcast.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --knownSites hdfs:///path/to/your.vcf \; --joinStrategy BROADCAST \; --apiKey $API_KEY \; --sparkMaster yarn-client; ```. I recommend running BROADCAST with 4 cores per executor and 10 GB/memory per executor (though this may need to be increased if you see any swapping). 16 executors (ie., 64 cores total) seems like a good size for our cluster as it would allow you to increase memory per core if necessary. All inpu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/995#issuecomment-160722098:40,perform,performed,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/995#issuecomment-160722098,1,['perform'],['performed']
Performance,"@akiezun Hate to interrupt your performance work, but could we get this and https://github.com/broadinstitute/gatk/pull/1424 into a mergeable state this week? They are blocking one of @cmnbroad's tickets.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1425#issuecomment-203968000:32,perform,performance,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1425#issuecomment-203968000,1,['perform'],['performance']
Performance,@akiezun Have we captured all of the performance suggestions from Intel in github tickets? (I know this was one of them),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1872#issuecomment-223036925:37,perform,performance,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1872#issuecomment-223036925,1,['perform'],['performance']
Performance,"@akiezun Here is a case study for your reference:; **Scalable Genomics Data Processing Pipeline with Alluxio, Mesos, and Minio**; https://alluxio.com/blog/scalable-genomics-data-processing-pipeline-with-alluxio-mesos-and-minio",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1508#issuecomment-288377411:53,Scalab,Scalable,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1508#issuecomment-288377411,2,"['Scalab', 'scalab']","['Scalable', 'scalable-genomics-data-processing-pipeline-with-alluxio-mesos-and-minio']"
Performance,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:79,cache,cached,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307,2,"['cache', 'perform']","['cached', 'performance']"
Performance,"@akiezun Keep it open as an alpha-3 ticket, but un-assign yourself. We'll continue to work on improving performance in the next quarter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1608#issuecomment-234342209:104,perform,performance,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1608#issuecomment-234342209,1,['perform'],['performance']
Performance,"@akiezun My only concern now is that someone takes log10 of a very large number triggering a massive and slow cache expansion. This caching scheme is good for clustered queries of small values, but terrible for sparse large queries. Is that a case we need to consider?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1957#issuecomment-230863916:110,cache,cache,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1957#issuecomment-230863916,1,['cache'],['cache']
Performance,"@akiezun That's why I was suggesting invalidating all cached values on every call to any setter -- that way we don't have to think about the nuances of when it's necessary to recalculate, and greatly reduce the risks that normally come with caching while still getting most of the performance benefit in typical usage.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235136126:54,cache,cached,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235136126,2,"['cache', 'perform']","['cached', 'performance']"
Performance,"@akiezun it's really important to make sure that the results match between the original and newly optimized versions (and if they don't, to make sure we understand why). Some of the ""indel stuff"" should probably stay, e.g. the masking of sites using known indels. Since we aren't running Indel Realigner anymore, there may be some ""errors"" that we do want to mask because they are alignment artifacts and not sequencing errors.; Pinging @yfarjoun for his thoughts. We can discuss this at a future methods meeting if you like.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152598082:98,optimiz,optimized,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152598082,1,['optimiz'],['optimized']
Performance,"@akiezun sorry, I apologize for didn't describe it properly. Basically, I need to obtain the base/deletion counts (and insertions after the position) from the reads that overlaps each position in the reference genome. Previously I did that with a `LocusWalker` like this:. ``` java; public Integer map(RefMetaDataTracker refMetaDataTracker, ReferenceContext referenceContext, AlignmentContext alignmentContext) {; ReadBackedPileup pileup = alignmentContext.getBasePileup();; int[] baseCounts = pileup.getBaseCounts();; int delCounts = pileup.getNumberOfDeletions();; int insCounts = pileup.getNumberOfInsertionsAfterThisElement();; // other operations ...; }; ```. I don't know exactly how could be done this using the `ReadWalker` (my first idea is to accumulate the reads in a queue with positions, but it does not seem very efficient). On the other hand, with the `IntervalWalker` I imagine that could be done iterating over each position in the interval (in the apply implementation), but for an efficient position by position analysis will be interesting to pass them in a base per base basis. What is the best approach? How are the GATK team planning to do it for their tools?. Thank you very much.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1463#issuecomment-178659953:779,queue,queue,779,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1463#issuecomment-178659953,1,['queue'],['queue']
Performance,"@akiezun thanks. I think this is ready for review now. It would be good to merge something that works, even if there are future performance and usability improvements we can do later.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219460978:128,perform,performance,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219460978,1,['perform'],['performance']
Performance,"@akiezun understanding how to optimize a Spark job can apply to multiple projects, so it doesn't seem right to put this in the readme or wiki of this project. Ideally we'd write an article for a blog or a technical presentation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/284#issuecomment-147513499:30,optimiz,optimize,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/284#issuecomment-147513499,1,['optimiz'],['optimize']
Performance,@akiezun we still have the limitation that refs in CRAM are treated as a local file. We need a release of https://github.com/damiencarol/jsr203-hadoop so that HDFS works as a java.nio.file.Path implementation. Then we can include that JAR on the hellbender classpath and hdfs URIs will be loaded correctly.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1326#issuecomment-163879054:289,load,loaded,289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1326#issuecomment-163879054,1,['load'],['loaded']
Performance,"@akiezun... because you are working in the performance of LIBS (#2032), could you have a look?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235542188:43,perform,performance,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235542188,1,['perform'],['performance']
Performance,"@ashwini06 . This bam appears to be malformed and it fails Picard ValidateSamFile. I think you'll need to examine the earlier stages of your pipeline that produce your bam to ensure you get a correctly formed bam. I'm going to close this ticket now since this doesn't appear to be an issue with Mutect2. (base) wm462-624:Downloads fleharty$ java -jar $PICARD ValidateSamFile I=concatenated_ACC5611A1_XXXXXX_consensusalign_ds.bam ; INFO	2020-07-14 11:25:52	ValidateSamFile	. ********** NOTE: Picard's command line syntax is changing.; **********; ********** For more information, please see:; ********** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition); **********; ********** The command line looks like this in the new syntax:; **********; ********** ValidateSamFile -I concatenated_ACC5611A1_XXXXXX_consensusalign_ds.bam; **********. 11:25:52.673 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/fleharty/resources/picard.jar!/com/intel/gkl/native/libgkl_compression.dylib; [Tue Jul 14 11:25:52 EDT 2020] ValidateSamFile INPUT=concatenated_ACC5611A1_XXXXXX_consensusalign_ds.bam MODE=VERBOSE MAX_OUTPUT=100 IGNORE_WARNINGS=false VALIDATE_INDEX=true INDEX_VALIDATION_STRINGENCY=EXHAUSTIVE IS_BISULFITE_SEQUENCED=false MAX_OPEN_TEMP_FILES=8000 SKIP_MATE_VALIDATION=false VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false; [Tue Jul 14 11:25:52 EDT 2020] Executing as fleharty@wm462-624 on Mac OS X 10.15.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_191-b12; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.20.4-SNAPSHOT; WARNING	2020-07-14 11:25:52	ValidateSamFile	NM validation cannot be performed without the reference. All other validations will still occur.; ERROR: Record 18321, Read name U",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:931,Load,Loading,931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['Load'],['Loading']
Performance,"@asmirnov239 I made some additions to your code to add HDF5 writing and make TSV writing consistent. I used an OverlapDetector instead of the CachedBinarySearchIntervalList and I think there was no performance hit. I also went ahead and added HDF5Utils, which has some methods that will be used in later PRs. . Let's consider your first commit reviewed by me; could you take a look at my changes in the second commit?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3775:142,Cache,CachedBinarySearchIntervalList,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3775,2,"['Cache', 'perform']","['CachedBinarySearchIntervalList', 'performance']"
Performance,"@asmirnov239 I think that some of the optimizations that @vruano made to the postprocessing step concern the config JSONs, gCNV version, and interval list output added in #5176. These take a lot of time to localize when the number of shards is large but, aside from the interval list, aren't really used for anything, correct?. Were these just added for debugging purposes, or for reproducibility/provenance? Let's revisit whether it's necessary to pass these files on when we merge @vruano's changes into the canonical WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-472862393:38,optimiz,optimizations,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-472862393,1,['optimiz'],['optimizations']
Performance,"@bbimber @mlathara Here is a pretty good article for optimizing the GenomicsDBImport [https://gatk.broadinstitute.org/hc/en-us/articles/360056138571-GDBI-usage-and-performance-guidelines] There is some advice about handling many small contigs that may be useful. . To troubleshoot the GenomicsDBImport high memory issue my script have, I reran the script on chr1 to narrow down the source of the high memory issue. These are running on reblocked gvcfs. . 1. Without --bypass-feature-reader and -consolidate; 2. With --bypass-feature-reader; 3. With --consolidate without --bypass-feature-reader (This ended up on a node with 384gb.) The other ran on 256GB nodes. . Test 2 ran the fastest with the lowest memory requirements (Wall clock 76 hours); Test 1 ran slower and required more memory 40-50% of 256GB (Wall Clock 94 hours); Test 3 ran initially faster with less memory than test 1 but by batch 65 it was using 75% of 384 GB. This job has not finished and appears stuck on importing batch 65. So the consolidate option appears to have a memory leak or using just requiring too much memory. The -consolidate option was the culprit. So rerunning chr1-3 with just the --bypass-feature-reader option (test2) ran fine without lots of memory being used. Below is the time output from chr1. The output shows the Maximum resident set size (kbytes): **2630440**. Using GATK jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar defined in environment variable GATK_LOCAL_JAR; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx200g -Xms16g -jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reade",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687:53,optimiz,optimizing,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687,2,"['optimiz', 'perform']","['optimizing', 'performance-guidelines']"
Performance,"@bbimber Let me see if I can dig up the old branches where I started this work. I'm not sure what state they're in, and I'm sure they're very out of date wrt/master, but they're probably at least worth looking at. I recall thinking that https://github.com/broadinstitute/gatk/issues/5441 would be quite a lot of work. Have you done any profiling to see where the performance issues actually are for your use case? I'd highly recommend doing that first before embarking on any big changes like this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5439#issuecomment-720514553:363,perform,performance,363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5439#issuecomment-720514553,1,['perform'],['performance']
Performance,"@bbimber The -Xmx option controls only the maximum size of the *Java* heap. It does not limit the size of the C heap used by the GenomicsDB library. This is why we always advise leaving extra memory available for C when using GenomicsDB by selecting an -Xmx value that is comfortably under the amount of physical memory available. Even though your GenotypeGVCFs command is not importing into a GenomicsDB, it is reading out of a GenomicsDB and performing an on-the-fly merge, which will cause the native library to use nontrivial amounts of memory. Having said that, ~80 GB for the native heap does seem like a lot. Do you have lots of highly multi-allelic records in your callset? @nalinigans @mlathara Thoughts on this one?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-963504342:444,perform,performing,444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-963504342,1,['perform'],['performing']
Performance,"@bbimber There are still a lot of unaddressed comments from my review. Eg.,. https://github.com/broadinstitute/gatk/pull/4495#discussion_r189690850; https://github.com/broadinstitute/gatk/pull/4495#discussion_r189693827; https://github.com/broadinstitute/gatk/pull/4495#discussion_r189694685; https://github.com/broadinstitute/gatk/pull/4495#discussion_r189697161; https://github.com/broadinstitute/gatk/pull/4495#discussion_r193207051; https://github.com/broadinstitute/gatk/pull/4495#discussion_r195236794; https://github.com/broadinstitute/gatk/pull/4495#discussion_r195242052; https://github.com/broadinstitute/gatk/pull/4495#discussion_r195242436; https://github.com/broadinstitute/gatk/pull/4495#discussion_r195243668; https://github.com/broadinstitute/gatk/pull/4495#discussion_r195243837. You also missed a bunch of @cmnbroad 's comments from his earlier review. I'm guessing what's going on here is that you might not be expanding the `N Hidden Conversations...Load More` box that github inserts into the middle of the reviews to hide comments when there are more than a certain number of them?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-397359678:970,Load,Load,970,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-397359678,1,['Load'],['Load']
Performance,"@bbimber There is a defrag operation that you can perform on the DB. My understanding is that it's only necessary if you have >100ish input batches. . When you say similar size combine gVcfs do you mean similar size of the input data, or similar size of the stored database compared to the merged gvcf? If you mean the later then that's expected because a genomicsdb will have much more data than an equivalent gvcf. I assume you mean the former though. I believe it is also expected that running from a genomicsdb should be slower than from a combined gvcf (up to a certain size of gvcf) because the combination operation is done at at read-time so it slows down the genotyping operation. The thing to look at is the sum of GenomicsDBImport + GenotypeGvcfs vs CombineGVCFS + GenotypeGvcfs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669343616:50,perform,perform,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669343616,1,['perform'],['perform']
Performance,"@bbimber You may not want to restart at this point, but the latest release (https://github.com/broadinstitute/gatk/releases/tag/4.1.8.0) has some optimizations targeted towards shared posix filesystems -- a knob called `--genomicsdb-shared-posixfs-optimizations`. It also reduces the storage space requirements for the genomicsdb workspace substantially. You could also try to check on the size of the workspace where that scatter job is writing. The import should be writing to an ""invisible"" directory (i.e., one starting with a ""."") so it may not be visible under the contig directory. But if the process is making progress, the size should increase over time.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-655897920:146,optimiz,optimizations,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-655897920,2,['optimiz'],['optimizations']
Performance,"@bbimber question for you - what is the primary motivation for wanting to merge the scattered workspaces back into a single workspace here? I'm assuming the scatter is because you have a large enough dataset that you need multiple nodes to run the import in parallel. (side note: we're planning on enabling reading vcfs through native htslib in GenomicsDBImport which should drive down memory usage for cases that are able to take advantage of that route. This might make it more feasible to use `--max-intervals-to-import-in-parallel` for multiple threads on a single node ). If the large dataset is the primary reason, wouldn't you want the benefits of distributed processing on the query side as well? You mentioned in the previous thread that you saw the fact that a single workspace is a valid GenomicsDB workspace as a benefit...and that's certainly true - but if performance is the driving factor then it might be worth it to keep the workspace separate. @droazen Could you elaborate on what you envision we should do here? This approach should work as long as the same command line is used for each import with a different/unique interval list each time. Are you asking for a test case to be run just for sanity? Or add test cases to GATK? Or add a tool to do this...?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-635081015:870,perform,performance,870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-635081015,1,['perform'],['performance']
Performance,"@bbimber, I have placed another version of consolidate_genomicsdb_array [here](https://github.com/GenomicsDB/GenomicsDB/releases/download/v1.4.3/consolidate_genomicsdb_array). This allows for batch-wise consolidation with the `--batch-size` or `-b` option. The tool also does better reuse of the consolidation buffers between reads, so might work a little better. Be aware that the total time to consolidate increases with the batch option and the final batches require almost as much memory as consolidating all the fragments at once. Please try consolidating any one GenomicsDB array to see how it functions as we are still working at making this scalable in a better way. This is just a draft version and if you can share some of the resulting logs, that will be very helpful. Please do let me know the total size of all the `__book_keeping.tdb.gz` files in your fragments. Just a back-of-envelope calculation, you probably need about 40 times that total size of memory to successfully consolidate.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1081029205:649,scalab,scalable,649,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1081029205,1,['scalab'],['scalable']
Performance,"@bbimber, did you use `GenomicsDBImport` from v4.2.5.0 as well? How large is your /home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb/1$1$223616942/__9b9a9e96-139c-4105-81ec-ab1455d1f01d140490873108224_1597099702436/__book_keeping.tdb.gz file? Anyway, you can share it with us?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042028821:116,cache,cachedData,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042028821,1,['cache'],['cachedData']
Performance,"@bbimber, sorry that the import with consolidate did not complete. If you are amenable to using a native tool, please download the tool from [here](https://github.com/GenomicsDB/GenomicsDB/releases/download/v1.4.3/consolidate_genomicsdb_array) for consolidation. This executable will consolidate a given array in a GenomicsDB workspace, it has been instrumented to output memory stats to help tune the segment size. Note that the executable is for Centos 7, if you find any unresolved shared library dependencies during usage, please let me know and I will work on getting another one to you. For usage from a bash shell:; ```; ~/GenomicsDB: ./consolidate_genomicsdb_array; Usage: consolidate_genomicsdb_array [options]; where options include:; 	 --help, -h Print a usage message summarizing options available and exit; 	 --workspace=<GenomicsDB workspace URI>, -w <GenomicsDB workspace URI>; 		 Specify path to GenomicsDB workspace; 	 --array-name=<Array Name>, -a <Array Name>; 		 Specify name of array that requires consolidation; 	 --segment-size=<Segment Size>, -z <Segment Size>; 		 Optional, default is 10M. Specify a buffer size for consolidation; 	 --shared-posixfs-optimizations, -p; 		 Optional, default is false. If specified, the array folder is not locked for read/write and file handles are kept open until a final close for write; 	 --version Print version and exit; ```. ```; ~/GenomicsDB.: ./consolidate_genomicsdb_array -w /Users/xxx/WGS.gdb/ -a ""1\$1\$249250621"" -z 1048576 -p; 21:09:47.100 info consolidate_genomicsdb_array - pid=30881 tid=30881 Starting consolidation of 1$1$249250621 in ws; Using buffer_size=1048576 for consolidation; 21:9:47 Memory stats(pages) beginning consolidation size=45821 resident=18998 share=1824 text=3530 lib=0 data=16810 dt=0; 21:9:47 Memory stats(pages) after alloc for attribute=END size=45821 resident=19009 share=1835 text=3530 lib=0 data=16810 dt=0; 21:9:48 Memory stats(pages) after alloc for attribute=REF size=46788 resident=19743 share=18",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1057680354:393,tune,tune,393,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1057680354,1,['tune'],['tune']
Performance,"@bbimber, thanks to @mlathara and @kgururaj, here is a suggestion. With `~36 attributes+offsets` and `~80 fragments` in GenomicsDB parlance getting stored, with `--genomicsdb-segment-size=1048576(1M default)`, we are looking at memory requirements in the range of 10G of memory for reading during consolidation. Just wondering, if you could try GenomicsDBImport with the following options to see if it helps.; ```; java heap options of say -Xmx100g -Xms 100g; --genomicsdb-update-workspace-path WGS_1852_consolidated.gdb \; --batch-size 10 \; --consolidate \; --genomicsdb-shared-posixfs-optimizations \; --bypass-feature-reader \; --genomicsdb-segment-size 32768 \; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1051157333:588,optimiz,optimizations,588,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1051157333,1,['optimiz'],['optimizations']
Performance,"@bbimber, there was an issue with `max-alternate-alleles` getting passed to the GenomicsDB layer from `GenotypeGVCFs`. That has been fixed in this [branch](https://github.com/broadinstitute/gatk/tree/ng_genomicsdb_args) if you would like to try. See this related [GenomicsDB Issue](https://github.com/GenomicsDB/GenomicsDB/commit/3c7d1ead0110fec26d56ff85a5871d8df673504d). This will hopefully help with memory usage. On another note, there is a [performance/bug fix ](https://github.com/broadinstitute/gatk/pull/7520) while reading/writing GenomicDB bookkeeping artifacts in the latest release and in this branch as well. This may help with memory while incrementally adding new batches and using the `--consolidate` option with import.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-974487707:446,perform,performance,446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-974487707,1,['perform'],['performance']
Performance,"@bbimber, we are investigating some scalable solutions for you. Meanwhile, can you provide the following information?; 1. What is the total and free amount of memory available to say consolidate_genomicsdb_array on your individual nodes?; 2. Sizes of all files under any one fragment say in 9$1$134124166?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1062349294:36,scalab,scalable,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1062349294,1,['scalab'],['scalable']
Performance,"@bbimber, your approach should mostly work, this is exactly what I am going to allow with the standalone tool, a new arg for `batch-wise consolidation`. The tool is also better optimized with memory allocations and you can specify the batch size to the tool for batch-wise consolidation which should clamp down the memory use. Still testing out the tool, hopefully I can get some version to you over the weekend or on Monday.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1073040553:177,optimiz,optimized,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1073040553,1,['optimiz'],['optimized']
Performance,"@bhanugandham As a side note, you shouldn't be running GATK4 using `java -jar` directly. You should use the included `gatk` launcher script, which sets a lot of important configuration settings, some of which have a major effect on tool performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5900#issuecomment-485873403:237,perform,performance,237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5900#issuecomment-485873403,1,['perform'],['performance']
Performance,"@bhanugandham It looks like the user is not using our provided resource file for `GetPileupSummaries`. It seems that the user selected only biallelic sites from gnomAD by hand, without removing all the extraneous info fields and restricting the exonic variants as we do in our resource. This means that the tool needs to load a huge amount of gnomAD into memory at any given time and is probably causing the crash. . If the user follows our best practices I expect the problem to go away.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5918#issuecomment-495288994:321,load,load,321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5918#issuecomment-495288994,1,['load'],['load']
Performance,"@ccastane9, looks like a memory issue. Some questions -. 1. What are the sizes of the book-keeping files in your GenomicsDB workspace? Try running `find /ECA3_GenomicsDB_260 -name __book_keeping.tdb.gz -ls`.; 2. Is /ECA3_GenomicsDB_260 on NFS or another shared Posix FS? Can you try running GenotypeGVCFs with `--genomicsdb-shared-posixfs-optimizations` turned on?; 3. What does your hardware configuration look like, memory wise?; 3. What are your `-Xmx` and `-Xms` java options?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-754244432:339,optimiz,optimizations,339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-754244432,1,['optimiz'],['optimizations']
Performance,"@chandrans Can you please ask the user to test with the `4.0.0.0` (or `4.0.1.0`) release, as there were major performance improvements to the `HaplotypeCaller` just before the release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-361988531:110,perform,performance,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-361988531,1,['perform'],['performance']
Performance,"@chandrans Hmn, I don't really know what's happening. We wouldn't expect gatk4 haplotype caller to be that much slower. . It looks like they're running beta2 which is kind of old as well. Can you ask them what exact version they're using?. Can you ask if they have the log (stdout + stderr) for the gatk4 non-spark run? I can't tell what pairhmm they're actually running with and the logs would help with that. . Can you also find out what sort of hardware they're running on? Specifically, is it an intel machine with support for AVX?. A good setting for` --nativePairHmmThreads` is probably 4-8, you won't see any improvement after that. I also noticed that they're setting -XX:+UseParallelGC -XX:ParallelGCThreads=32 for the gatk3. They would be better off setting it to 2-4 threads. Performance gets worse beyond that typically from what I've seen. They can set the same thing for gatk4 using`--javaOptions ' -XX:+UseParallelGC -XX:ParallelGCThreads=4'`. Their spark configuration looks wrong in a number of ways which is probably a big part of why they're not seeing any improvement. In general you want executors with ~4-8 cores and at least 4g of memory per core. I don't know how much memory their nodes have, and I don't know if they're running with autoscaling turned on, but I suspect they're only allocating 1 executor on 1 node and then it's thrashing memory because it's trying to run 32 threads at once. Spark tuning for haplotype caller is going to be complicated though and I don't know how to do it will yet, we will be revisiting it in the next quarter probably. They're also running withs spark 2.1.0, we currently require spark 2.0.2 which is an unfortunately specific version, we're planning on upgrading to spark 2.2.+ in the next quarter. . You should make it clear to them that the results will not be the same between 3, 4, and 4-spark yet and that 4 is in rapid state of flux and has known performance issues that we're planning on working soon. Even so though, that slowdow",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332879964:787,Perform,Performance,787,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332879964,1,['Perform'],['Performance']
Performance,"@chandrans Please ask the user to repeat the test, with the following adjustments:. * Run both GATK 3.7 and GATK 4.0.1.1 (latest release) on the same machine, one right after another, on chromosome 20 only (using `-L` in both cases), and ensure that there are no other expensive processes running on this machine during the tests. Run each version 3 times, and take the average of the results.; * Add `-pairHMM AVX_LOGLESS_CACHING` to both the GATK3 and GATK4 command lines, to guarantee that the native PairHMM will be used in both cases.; * Get rid of the `--native-pair-hmm-threads 32` in the GATK 4 command line. Too many threads can sometimes make performance worse by introducing too much contention.; * Check both the GATK3 and GATK4 output to ensure that the Intel inflater and deflater were used in both cases.; * Check both the GATK3 and GATK4 command lines to be sure they are equivalent (eg., if one is running with -ERC GVCF, the other one should as well).; * Compute wall-clock time by running the Unix `time` command, if the user is not already doing so (eg., `time ./gatk HaplotypeCaller....`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4361#issuecomment-363583464:653,perform,performance,653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4361#issuecomment-363583464,1,['perform'],['performance']
Performance,"@chandrans commented on [Mon Jan 29 2018](https://github.com/broadinstitute/dsde-docs/issues/2881). User reports BaseRecalibratorSpark in gatk-4.beta.6-17 took 3.79 minutes vs 40 minutes in official release. ----; User Report; ----. Dear GATK_team, I'd like to run Spark-enabled GATK tools on a Spark cluster. Precisely I am launching a Spark cluster in the standalone mode submitting the `BaseRecalibratorSpark` application via Slurm. Before the official release, I was running the `gatk-4.beta.6-17` version, with the following allocated resources, and the following command line for the Spark arguments: `./gatk-launch BaseRecalibratorSpark \ --sparkRunner SPARK --sparkMaster spark://${MASTER} --driver-memory 80g --num-executors 16 --executor-memory 8g`. The speed-up achieved was 3.79 min. However, with the official release GATK-4.0.0.0, with the same datafiles and the same Spark arguments I don't see the same nice speed-up anymore (~ 40 min). Am I missing something with the new version? Or with the invoking command line? Thanks in advance for your time and kind answer. Best, Giuseppe. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11260/gatk-4-0-0-0-baserecalibratorspark-low-performance/p1. ---. @chandrans commented on [Mon Jan 29 2018](https://github.com/broadinstitute/dsde-docs/issues/2881#issuecomment-361324925). @droazen @lbergelson Hi David and Louis. Do you have any comments? I was supposed to put this in gatk but put it in dsde-docs. Thanks. ---. @droazen commented on [Mon Jan 29 2018](https://github.com/broadinstitute/dsde-docs/issues/2881#issuecomment-361342262). @chandrans Could you move this ticket to the gatk repo so that we can remember to have a look at the tool? Someone will have to re-profile.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4300:1253,perform,performance,1253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4300,1,['perform'],['performance']
Performance,"@chandrans commented on [Wed Sep 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1476). ## Feature request; ### Tool(s) involved. PrintReads; ### Description. Right now, we have a flag that outputs reads with a MQ greater than the set threshold. Users may want to only output reads that have a MQ below a certain threshold. There should be a flag (something like -maximumMappingQuality) that only emits reads that have less than the set Mapping Quality. . For example, a user needs this to compare low mapQ reads from different samples, blast search and/or perform de novo assembly. ---. This Issue was generated from your [forums](http://gatkforums.broadinstitute.org/gatk/discussion/comment/32943#Comment_32943) . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1476#issuecomment-260498511). Might end up being a feature request for bidirectional read filtering functionality in GATK4. . ---. @chandrans commented on [Tue Feb 28 2017](https://github.com/broadinstitute/gsa-unstable/issues/1476#issuecomment-283141618). Okay. Should I keep this open or close it/move to GATK4?. ---. @vdauwera commented on [Tue Feb 28 2017](https://github.com/broadinstitute/gsa-unstable/issues/1476#issuecomment-283150007). Yeah move to 4 please",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2432:572,perform,perform,572,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2432,1,['perform'],['perform']
Performance,"@chapmanb We were able to reproduce a failure with your command line. This looks like an issue related to JNI and garbage collection that is exposed by setting `-Xmx46965m` and `-XX:+UseSerialGC`, but it needs further debugging. To confirm, can you please try running without specifying these javaOptions? Something like this:; ```; ./gatk-launch --javaOptions '-Djava.io.tmpdir=$TEMP_DIR' \; ApplyBQSRSpark \; --sparkMaster local[16] \; --input $BAM_IN \; --output $BAM_OUT \; --bqsr_recal_file $BQSR_RECAL \; -- \; --conf spark.local.dir=$SPARK_LOCAL_DIR; ```. FYI, we see better performance from Spark when using an SSD for spark.local.dir. The `--conf ` option above shows how to set the spark.local.dir.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3605#issuecomment-332370070:582,perform,performance,582,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3605#issuecomment-332370070,1,['perform'],['performance']
Performance,"@cmnbroad . The non-spark version of CountReads runs fine...... ```; gatk-4.0.12.0/gatk CountReads --input /restricted/projectnb/adsp/wgs.hg38/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; Using GATK jar /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar CountReads --input /restricted/projectnb/adsp/wgs.hg38/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; 15:18:43.541 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:18:45.267 INFO CountReads - ------------------------------------------------------------; 15:18:45.267 INFO CountReads - The Genome Analysis Toolkit (GATK) v4.0.12.0; 15:18:45.268 INFO CountReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:18:45.268 INFO CountReads - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 15:18:45.268 INFO CountReads - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 15:18:45.269 INFO CountReads - Start Date/Time: January 2, 2019 3:18:43 PM EST; 15:18:45.269 INFO CountReads - ------------------------------------------------------------; 15:18:45.269 INFO CountReads - ------------------------------------------------------------; 15:18:45.270 INFO CountReads - HTSJDK Version: 2.18.1; 15:18:45.270 INFO CountReads - Picard Version: 2.18.16; 15:18:45.270 INFO CountReads ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-450983210:925,Load,Loading,925,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-450983210,1,['Load'],['Loading']
Performance,"@cmnbroad @lbergelson Looks like `SparkContextFactory.DEFAULT_TEST_PROPERTIES` is currently initialized statically at class-loading time, resulting in a call to `getGcsHadoopAdapterTestProperties()` even when we're not running the test suite.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330902481:124,load,loading,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330902481,1,['load'],['loading']
Performance,"@cmnbroad Thanks for pointing to the conda environment file. I tried to install load it but the first error it gave me was ; ```; NoPackagesFoundError: Package missing in current linux-64 channels:; - intel-openmp 2018.0.0*; ```; so I installed this package by hand with conda install -c anaconda intel-openmp and afterwards tried to install the gatkcondaenv.yml again with conda env create -n gatk -f gatkcondaenv.yml . Unfortunately I run into the next error which says:. ```; root@k-hg-srv1:/BioinfSoftware# conda env create -n gatk -f gatkcondaenv.yml; Using Anaconda API: https://api.anaconda.org; Solving environment: done. Downloading and Extracting Packages; mkl-service 1.1.2: ################################################################################################################################################################################################################## | 100%; libgcc-ng 7.2.0: #################################################################################################################################################################################################################### | 100%; mkl 2018.0.1: ####################################################################################################################################################################################################################### | 100%; intel-openmp 2018.0.0: ############################################################################################################################################################################################################## | 100%; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Requirement 'build/gatkPythonPackageArchive.zip' looks like a filename, but the file does not exist; Processing ./build/gatkPythonPackageArchive.zip; Exception:; Traceback (most recent call last):; File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/basecommand.py"", lin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357188460:80,load,load,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357188460,1,['load'],['load']
Performance,"@cmnbroad The results look good to me. Ideally though, shouldn't we be downloading and baking these images into our bundle instead of fetching them at page load? It seems bad to rely on an external webservice for documentation to render.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6606#issuecomment-631708551:156,load,load,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6606#issuecomment-631708551,1,['load'],['load']
Performance,"@cmnbroad This one is unblocked now, and should be a very quick change. Since changing `copy()` into a deep copy method might hurt the performance of things like the read clipper, recommend adding a separate method `deepCopy()` to `GATKRead`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/623#issuecomment-157452737:135,perform,performance,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/623#issuecomment-157452737,1,['perform'],['performance']
Performance,"@cmnbroad on a related note -- it might be worthwhile to setup the Docker to include a dynamic BLAS library and pass it to theano. I will test how it affects the performance. NumPy is usually either linked against MKL or OpenBLAS. If theano has no dynamic BLAS lib available to link the compiled graph against, it will fall back to NumPy for linalg ops. It is not too bad since the only cost is the c++/python communication overhead.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350264808:162,perform,performance,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350264808,1,['perform'],['performance']
Performance,"@cmnbroad right, at the time when we impemented the pedigree annotations we made the decision that we didn't really expect the `PossibleDeNovo` to be used in the same way as the other pedigree annotations, as it seemed to be a specific case. It would be possible to add the ability for `PossibleDeNovo` to perform generate a SampleDB from its pedigree file at construction so it will be compatible. We still couldn't use the variant annotator for `CalculateGenotypePosteriors` though as we still can't have the same argument name on the path in multiple places",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-403538495:306,perform,perform,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-403538495,1,['perform'],['perform']
Performance,"@cwhelan . The soft-clip -> hard-clip optimization is in lines 47-63 in `BwaMemAlignmentUtils.applyAlignment()`, which was called by `AlignedAssemblyOrExcuse.writeSAMFile()` in our discovery pipeline (BwaSparkEngine calls it as well but not affecting what we are talking about here).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294160235:38,optimiz,optimization,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294160235,1,['optimiz'],['optimization']
Performance,"@cwhelan @tedsharpe @vruano please review.; It is intended for copying the right data for the ""correctly""-named cluster for performing intended analysis.; Cluster provisioning time is now ~25 minutes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2456:124,perform,performing,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2456,1,['perform'],['performing']
Performance,"@cwhelan @tedsharpe please review. There are 4 new classes here:. 1. LongHopscotchSet - based on HopscotchCollection/Set but adapted to store primitive longs instead of objects. The most significant bit is used to tell if a bucket is null or not, so the longs being stored must be non-negative. This works for use with k-mers, which we are assume are odd-length up to 31 and thus consume up to 62 bits.; 2. LargeLongHopscotchSet - for sets of longs greater than ~2 billion (the max Java array size) using a List of LongHopscotchSets.; 3. LongBloomFilter - Bloom filter for long's; 4. LargeLongBloomFilter - Bloom filter when the filter index size exceeds 2GB using a List of LongBloomFilters. - LongIterator and QueryableLongSet interfaces for convenience.; - Minor change to HopscotchSet max legal size, which was higher than the actual allowed Java array size. PS I just had a thought that the Bloom filters could use long instead of byte buckets to expand the max index size 8-fold. Could maybe be done for the Hopscotch sets as well, but with considerably more difficulty. Thoughts? On the other hand, the performance is already adequate so perhaps I'll save this idea for later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2729:1110,perform,performance,1110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2729,1,['perform'],['performance']
Performance,@cwhelan Can you please provide some of these calls? I am considering filtering the mapping/alignments and want to evaluate the filter's performance. Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3225#issuecomment-313876090:137,perform,performance,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3225#issuecomment-313876090,1,['perform'],['performance']
Performance,"@cwhelan I've having problems with the non-Spark JAR though:. ``` bash; $ gradle clean installDist; $ java -jar build/libs/gatk-4.pre-alpha-*-SNAPSHOT.jar; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/logging/log4j/LogManager; at org.broadinstitute.hellbender.cmdline.ClassFinder.<clinit>(ClassFinder.java:29); at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:108); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:86); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.LogManager; at java.net.URLClassLoader$1.run(URLClassLoader.java:372); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:360); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 4 more; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1213#issuecomment-162013287:891,load,loadClass,891,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1213#issuecomment-162013287,3,['load'],['loadClass']
Performance,"@cwhelan is seeing performance issues now with large interval lists + reads + spark. This might be a result of the way intervals are currently being sent to Hadoop-BAM in `ReadsSparkSource` (marshaled into a giant String, set as a Java system property, then re-parsed on the other end). @akiezun we might want to bump up the priority of this ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-208969908:19,perform,performance,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-208969908,1,['perform'],['performance']
Performance,"@david-wb Interesting... what version of spark / spark submit are you using? I was pretty sure that at the point where setMaster() was called by gatk the spark context was already created by yarn and setup with the appropriate master. I wonder if it changed from a previous version of spark... Alternatively, I may be misremembering and relying on the fact that our wrapper script supplies `--sparkMaster yarn` as a gatk argument which would probably override what's being set. Could you try:; ```; spark-submit \; --deploy-mode client \; --class org.broadinstitute.hellbender.Main \; --master yarn \; /home/hadoop/gatk-package-4.alpha.2-269-gdce8abc-SNAPSHOT-spark.jar BwaSpark \; --bwamemIndexImage /var/tmp/hs38DH-V.fasta.img \; -I hdfs:///unaligned.bam \; -O hdfs:///aligned.bam \; -R hdfs:///hg38/hs38DH-V.fasta \; --disableSequenceDictionaryValidation true \; --sparkMaster yarn; ```. or with the wrapper: ; ```; GATK_SPARK_JAR_ENV_VARIABLE=/home/hadoop/gatk-package-4.alpha.2-269-gdce8abc-SNAPSHOT-spark.jar. ./gatk-launch \; --bwamemIndexImage /var/tmp/hs38DH-V.fasta.img \; -I hdfs:///unaligned.bam \; -O hdfs:///aligned.bam \; -R hdfs:///hg38/hs38DH-V.fasta \; --disableSequenceDictionaryValidation true \; -- \; --sparkRunner SPARK \; --sparkMaster yarn; ```. Our wrapper script sets a number of properties which we think are important for running our spark tools. If running directly on spark you might want to set them explicitly. Things like `-Dsamjdk.compression_level=1` have MAJOR performance implications. . Also, be aware the BwaSpark is not in the best health at the moment may have issues. If you're running it you may want to run the version that's in this branch which is currently under review https://github.com/broadinstitute/gatk/pull/2494.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2718#issuecomment-301925138:1498,perform,performance,1498,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2718#issuecomment-301925138,1,['perform'],['performance']
Performance,"@davidangb Based on your experience, would you be able to give an upper limit for magnitude of number of datapoints/dimensions beyond which performance is a blocking issue, on eg a reasonable personal laptop?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/248#issuecomment-77250988:140,perform,performance,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/248#issuecomment-77250988,1,['perform'],['performance']
Performance,"@davidbenjamin . - Downsampling is definitely a concern for us, but as you have suggested, I have utilized this parameter to not limit ourselves to 50 reads per alignment start. After running a range of values, I landed on 500 as not being too much of a burden computationally, but still allowing us to fully digest alignments at each start site in a given region. For us, I have estimated a value of 500 would cover us to a depth of about 2000 or so, since we expect to see a bias at the projected amplicon start site.; - The read filters you list don't have a huge effect on the majority of our regions. Generally, I would not expect to see more than a 5% loss based on the mapping quality filter. The read size filter should never be triggered, as we input only reads larger than 30 bases to M2. We perform duplicate removal, as we are working with UMIs, so this also should not be an issue. ; - I wouldn't expect the realignment stage to cause the type of effect I see, and as you say, it really just corrects the data anyways. **Example:**; The following was called:; ```; 1	12919623	.	C	T	.	.	DP=741;ECNT=4;POP_AF=1.000e-03;P_GERMLINE=-2.169e-04;TLOD=743.86; 	GT:AD:AF:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:PGT:PID:SA_MAP_AF:SA_POST_PROB	0/1:520,210:0.291:0,0:520,210:36:153,132:57:47:0; |1:12919623_C_T:0.232,0.283,0.288:0.835,2.995e-04,0.165; ```; Also called in a different variant caller:; ```; 1	12919623	.	CC	TG	6989.54	.	AB=0.308968;ABP=423.639;AC=1;AF=0.5;AN=2;AO=410;CIGAR=2X;DP=1327;DPB=1327;DPRA=0;EPP=55.973;EPPR=139.678;GTI=0;LEN=2;MEANALT=8;MQM=50.4732;MQMR=56.3933;NS=1;NUMALT=1;ODDS=1609.4;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=13156;QR=30163;RO=900;RPL=282;RPP=128.617;RPPR=256.291;RPR=128;RUN=1;SAF=177;SAP=19.6194;SAR=233;SRF=377;SRP=54.4404;SRR=523;TYPE=mnp;technology.ILLUMINA=1	GT:DP:AD:RO:QR:AO:QA:GL	0/1:1327:900,410:900:30163:410:13156:-725.008,0,-2308.15; ```; Yes, M2 also calls the neighboring C>G substitution, these are just being represented differently between the ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3808#issuecomment-344740595:802,perform,perform,802,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3808#issuecomment-344740595,1,['perform'],['perform']
Performance,"@davidbenjamin @droazen unfortunately the new PON does not make up for the precision loss introduced in v4.1.9.0.; In v4.4.0.0 we get just 2 fewer FP SNVs in our performance evaluation, compared to the old PON.; Benchmarking results in WES tumor-normal mode, HCC1395 benchmark, and:. - v4.1.8.1 (last release with high SNV precision), v4.1.9.0 (first release affected by precision drop), v4.4.0.0 (current release); - oldPON: 1000g_pon.hg38.vcf.gz, newPON: mutect2-hg38-pon.vcf.gz; ![FD_TN_4181_FD_TN_4181_oldPON_FD_TN_4181_newPON_FD_TN_4190_FD_TN_4190_oldPON_FD_TN_4190_newPON_FD_TN_4400_FD_TN_4400_oldPON_FD_TN_4400_newPON](https://user-images.githubusercontent.com/15612230/236126940-9fc26627-260a-43c2-b409-69fbcec6ad47.png). Any chance to get this issue fixed? With Mutect3 not being available and v4.1.8.1 being affected by the log4j vulnerability, it is quite regrettable to be stuck with inferior precision. Extended methods, code, and data to reproduce the issue are here: ; [https://github.com/ddrichel/Mutect2_calling_performance_bug](https://github.com/ddrichel/Mutect2_calling_performance_bug)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1534177043:162,perform,performance,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1534177043,1,['perform'],['performance']
Performance,"@davidbenjamin I feel like we need to put good, continuous *performance* regression tests in place for the `HaplotypeCaller` so that we can make changes of this nature without fear. Testing for a performance regression in the `HaplotypeCaller` is currently very non-trivial -- you have to run with and without intervals, with and without -ERC GVCF, on both exome and genome to be confident that you haven't killed performance in a certain mode.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3184#issuecomment-358777173:60,perform,performance,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3184#issuecomment-358777173,3,['perform'],['performance']
Performance,"@davidbenjamin I tried and this time its a different error. ; ```; 14:55:53.232 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/shollizeck/clustering.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 09, 2020 2:55:53 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:55:53.432 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.433 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.4.1-6-g6bb31a7-SNAPSHOT; 14:55:53.433 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:55:53.433 INFO FilterMutectCalls - Executing as shollizeck@stpr-res-compute02.unix.petermac.org.au on Linux v3.10.0-1062.4.3.el7.x86_64 amd64; 14:55:53.433 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_232-b09; 14:55:53.434 INFO FilterMutectCalls - Start Date/Time: 9 January 2020 2:55:53 PM; 14:55:53.434 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.434 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.434 INFO FilterMutectCalls - HTSJDK Version: 2.21.0; 14:55:53.435 INFO FilterMutectCalls - Picard Version: 2.21.2; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:55:53.435 INFO FilterMutectCalls - Deflater: IntelDeflater; 14:55:53.435 INFO FilterMutectCalls - Inflater: IntelInflater; 14:55:53.435 INFO FilterMutectCalls - GCS max retries/reopens: 20; 14:55:53.435 INFO FilterMutectCalls - Requester pays: disabled; 14:55:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341:107,Load,Loading,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341,1,['Load'],['Loading']
Performance,"@davidbenjamin Intellij pointed out this if statement to me as suspicious and I think it is. There are two arms of the second if statement that are guarded by `includeNonVariants`. However the second one can never be hit because if `includeNonVariants` you will already have chosen the first clause. Seems suspicious...; ```; if (regenotypedVC == null || (!GATKVariantContextUtils.isProperlyPolymorphic(regenotypedVC) && !includeNonVariants)) {; return null;; }; if (GATKVariantContextUtils.isProperlyPolymorphic(regenotypedVC) || includeNonVariants) {; // Note that reversetrimAlleles must be performed after the annotations are finalized because the reducible annotation data maps; // were generated and keyed on the un reverseTrimmed alleles from the starting VariantContexts. Thus reversing the order will make; // it difficult to recover the data mapping due to the keyed alleles no longer being present in the variant context.; final VariantContext withGenotypingAnnotations = addGenotypingAnnotations(originalVC.getAttributes(), regenotypedVC);; final VariantContext withAnnotations = annotationEngine.finalizeAnnotations(withGenotypingAnnotations, originalVC);; final int[] relevantIndices = regenotypedVC.getAlleles().stream().mapToInt(a -> originalVC.getAlleles().indexOf(a)).toArray();; final VariantContext trimmed = GATKVariantContextUtils.reverseTrimAlleles(withAnnotations);; final GenotypesContext updatedGTs = subsetAlleleSpecificFormatFields(outputHeader, trimmed.getGenotypes(), relevantIndices);; result = new VariantContextBuilder(trimmed).genotypes(updatedGTs).make();; } else if (includeNonVariants) {; result = originalVC;; } else {; return null;; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6109:594,perform,performed,594,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6109,1,['perform'],['performed']
Performance,"@davidbenjamin Let me do a bit of profiling/optimization on the new code path to see if I can narrow the gap. The patch was intended to address memory use rather than runtime, and could use a profiling pass.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330324547:44,optimiz,optimization,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330324547,1,['optimiz'],['optimization']
Performance,"@davidbenjamin The bug here is that we misinterpreted what a TreeSet in java does. The actual behavior for this method was that it only took the FRIST variant at each start position that it saw from the ordering of the haplotypes it saw. This meant if a SNP and INDEL started at the same position then there was a chance that site only looks like a SNP to the subsequent trimming code and we trim incorrectly. . See the TreeSet docs:; ```; <p>Note that the ordering maintained by a set (whether or not an explicit; * comparator is provided) must be <i>consistent with equals</i> if it is to; * correctly implement the {@code Set} interface. (See {@code Comparable}; * or {@code Comparator} for a precise definition of <i>consistent with; * equals</i>.) This is so because the {@code Set} interface is defined in; * terms of the {@code equals} operation, but a {@code TreeSet} instance; * performs all element comparisons using its {@code compareTo} (or; * {@code compare}) method, so two elements that are deemed equal by this method; * are, from the standpoint of the set, equal. The behavior of a set; * <i>is</i> well-defined even if its ordering is inconsistent with equals; it; * just fails to obey the general contract of the {@code Set} interface.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6661#issuecomment-645528965:888,perform,performs,888,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6661#issuecomment-645528965,1,['perform'],['performs']
Performance,@davidbenjamin Was familiarizing myself with `KBestHaplotypeFinder` and decided to take a crack at this issue. . I have no idea how much of a performance hit this will end up being at extreme sites. At worst it involves adding more paths into the priority queues than existed before which could slow down the whole search algorithm. . Fixes #5907,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5952:142,perform,performance,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5952,2,"['perform', 'queue']","['performance', 'queues']"
Performance,@davidbenjamin You like minor optimizations...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6439#issuecomment-581622363:30,optimiz,optimizations,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6439#issuecomment-581622363,1,['optimiz'],['optimizations']
Performance,"@davidbenjamin commented on [Fri Sep 09 2016](https://github.com/broadinstitute/gatk-protected/issues/700). Currently, `Mutect2` hard filters a candidate somatic variant if any event occurs at the same locus in the panel of normal samples. The idea is to avoid false positive calls at inherently noisy sites. This approach is reasonable but perhaps we can improve it. Some thoughts:; - Asymptotically, as the size of the PoN goes to infinity eventually every site will have some event and we will filter out every variant. Obviously this is an unrealistic limit, but a model should always perform better with more data.; - It might be good to use the PoN to learn a probabilistic model of error at each site, similar to the tool EBCall which has been noted to perform quite well on indels.; - regardless of our model, we should consider alternatives to hard filtering, such as perhaps using the PoN to penalize a somatic quality score.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2891:589,perform,perform,589,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2891,2,['perform'],['perform']
Performance,"@davidbenjamin commented on [Sun May 28 2017](https://github.com/broadinstitute/gatk-protected/issues/1113). This sets the minimum base quality to consider a base in the assembly graph. By default it's 10, but in practice anything under 20 is usually junk. We could probably get a performance boost just by running Mutect with a higher value than the default.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3023:281,perform,performance,281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3023,1,['perform'],['performance']
Performance,"@davidbenjamin commented on [Thu Feb 16 2017](https://github.com/broadinstitute/gatk-protected/issues/909). It might be sufficient, especially for SNP calling, to run `PairHMM` over s small number of bases, say 20 or so, surrounding a variant. This might make sense for `HaplotypeCaller` as well. . . ---. @ldgauthier commented on [Fri Feb 24 2017](https://github.com/broadinstitute/gatk-protected/issues/909#issuecomment-282432230). I think @yfarjoun and I discussed this a long time ago. In theory it would speed up the really simple cases. I think the main blocker in implementing it would be the complexity of the existing code, though I don't know how much you cleaned up the GATK4 version. We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. In cases where the haplotype has multiple SNPs and the phasing is poor, this could artificially inflate the likelihoods. Although we've seen that the graph traversal frequently breaks phasing then generating haplotypes anyway, so maybe I overestimate our current likelihood accuracy. Anyway, take my advice with a grain of salt. It's just some musings from a bored and somewhat sleep-deprived mom with a sleeping baby on her lap. ---. @davidbenjamin commented on [Fri Mar 03 2017](https://github.com/broadinstitute/gatk-protected/issues/909#issuecomment-284024760). > I think the main blocker in implementing it would be the complexity of the existing code, though I don't know how much you cleaned up the GATK4 version. We refactored all the engine stuff shared with `HaplotypeCaller` to be very distinct from the somatic genotyping logic, so the only complexity would be in local assembly and PairHMM. Which could be significant, of course. > We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. . .Although we've seen that the graph traversal frequently breaks phasing. The specific case I had in mind is when you hav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2945:756,optimiz,optimization,756,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945,1,['optimiz'],['optimization']
Performance,"@davidbenjamin commented on [Thu Mar 31 2016](https://github.com/broadinstitute/gatk-protected/issues/432). Before integrating this with probabilistic segmentation, which is more complicated for somatic than for germline, we can simply replace the current tangent normalization step with the mode of the likelihood (as a function of copy ratio) resulting from the generative coverage model. This requires issues https://github.com/broadinstitute/gatk-protected/issues/429 and https://github.com/broadinstitute/gatk-protected/issues/430 to be completed. ---. @davidbenjamin commented on [Thu Mar 31 2016](https://github.com/broadinstitute/gatk-protected/issues/432#issuecomment-204015562). Also, this should only be attempted if we get good results from doing the equivalent in the germline code, issue https://github.com/broadinstitute/gatk-protected/issues/431. After it is done somatic and germline will share a PoN and all associated code, including tangent normalization. ---. @LeeTL1220 commented on [Mon Jun 06 2016](https://github.com/broadinstitute/gatk-protected/issues/432#issuecomment-223992660). This only needs to be done if we stick with CBS for segmentation. . At the very least, we need to implement this for checking performance against HMM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2842:1234,perform,performance,1234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2842,1,['perform'],['performance']
Performance,"@davidbenjamin commented on [Tue Jun 07 2016](https://github.com/broadinstitute/gatk-protected/issues/542). Calculating log gammas is an expensive part of the allele fraction model. We could speed this with negligible loss of accuracy by caching a few tens of thousands of values from 0 to 100 or 1000 and using linear interpolation. This issue can be closed by implementing such caching or by showing that it doesn't significantly improve performance. ---. @davidbenjamin commented on [Tue Jun 07 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224329788). The JVM running on my laptop does 10 million log gammas per second, which is about three times as expensive as logarithms. The allele fraction model needs to calculate 4 log gammas per het, so if you have 25,000 hets all the log gammas in the model likelihood take 1/100 of a second. . To get MLEs for each parameter (minor allele fractions, outlier probability etc) might require 100 evaluations each, so we're probably dealing with 10 seconds of log gammas per iteration to find the posterior mode. Getting a few hundred MCMC samples is probably more expensive but roughly comparable. These numbers are manageable but get expensive when we relearn the model at every iteration of segment merging. In my opinion it makes sense to come back to this issue after we have a new segmentation strategy. We'll see how pressing it is then. ---. @samuelklee commented on [Wed Jun 08 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224786950). To clarify, I think this is primarily an issue for WGS, where we have ~1.5 million hets. From the logs in /dsde/working/lichtens/wgs/out_case_chip_wgs/acnv/*out it looks like finding the MLE takes ~10 minutes (which is roughly consistent with your estimate), but 200 MCMC iterations takes ~1 hr. Naive profiling of the AlleleFractionModeller tests suggests that around ~60% CPU is going toward log gammas, so if we can improve on this I think it ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2860:440,perform,performance,440,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860,1,['perform'],['performance']
Performance,"@davidbenjamin has requested that we look into whether there are `PairHMM` optimizations we could do that would benefit Mutect2 specifically -- the performance profile may be different than for the `HaplotypeCaller`. . Intel has agreed to have a look, but they'll need a Mutect2 test case that they can run. @davidbenjamin would you be able to provide one?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2562:75,optimiz,optimizations,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2562,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"@davidbenjamin looks that it works for CombineVariants now (not on the full set of data). However, I am getting a lot of random errors for HaplotypeCallerSpark (4.1.8.1):. > 20/09/15 21:46:45 ERROR Executor: Exception in task 14.0 in stage 5.0 (TID 464); > java.util.ConcurrentModificationException; > 	at java.util.ArrayList.sort(ArrayList.java:1456). after rerunning with the same parameters for some runs problems disappeared, for some doesn't, and I must rerun them once again. There was no this kind of issue when I was using 4.1.3.0 HaplotypeCallerSpark O_o. I am confused O_o for what version tools works and for what doesn't. [H1_1.2.gatk.spark.HaplotypeCaller.gvcf.log](https://github.com/broadinstitute/gatk/files/5229907/H1_1.2.gatk.spark.HaplotypeCaller.gvcf.log); [H1_2.5.gatk.spark.HaplotypeCaller.gvcf.log](https://github.com/broadinstitute/gatk/files/5229908/H1_2.5.gatk.spark.HaplotypeCaller.gvcf.log). in contrast to the working processes; [H1_2.3.gatk.spark.HaplotypeCaller.gvcf.log](https://github.com/broadinstitute/gatk/files/5229912/H1_2.3.gatk.spark.HaplotypeCaller.gvcf.log). exactly same HPC infrastructure O_o. ________________; I think I have stuck once again somewhere in #5680 and #6730",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-693185541:267,Concurren,ConcurrentModificationException,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-693185541,1,['Concurren'],['ConcurrentModificationException']
Performance,"@davidbenjamin ready for re-review. I'm going to do a little performance benchmarking in the meantime. It takes ~40min to call the whole contig for my 4000X bams, which isn't terrible, but it isn't great. Anecdotally it seems like the AF thresholding slowed things down, but I'll collect some numbers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-449458286:61,perform,performance,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-449458286,1,['perform'],['performance']
Performance,"@davidbenjamin take note: the changes to `AssemblyRegionWalker` here have a major effect on `HaplotypeCaller` performance when run with a large (eg., whole-exome) interval list. I would expect there to also be a dramatic effect on `Mutect2` performance when run with such an interval list -- would you have time to do a quick check on this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4031#issuecomment-354902432:110,perform,performance,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4031#issuecomment-354902432,2,['perform'],['performance']
Performance,"@davidbenjamin the benchmark data is still not ours, it is the current somatic ""gold standard"" HCC1395 from [https://www.nature.com/articles/s41587-021-00993-6](https://www.nature.com/articles/s41587-021-00993-6), based on data from multiple WGS sequencing runs with combined 1,500x coverage, etc.... Apart from that, we are on the same page here. Now that the change leading to the apparent differences in performance is found, and holds up to scrutiny so far (thanks for looking into it), it is a more real possibility than ever that this is not a bug but a case of Mutect2 outperforming the ""gold standard"". Let's keep the issue open for now, I am still investigating and would like to have a place to report my progress.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1543422454:407,perform,performance,407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1543422454,1,['perform'],['performance']
Performance,"@davidbenjamin, @lbergelson reminded me about this bug. Unlikely, but any chance the sign error in the digamma implementation (which kicks in for x >= 49) affects M2 or anything else that uses the Dirichlet class? Looks like there are also a few calls in VQSR. I'm still primarily interested in any possible performance/runtime improvements that could be gained by updating to a more recent package, perhaps one more actively developed than Apache Commons Math (e.g. Apache Commons Numbers). Seems to be some complications regarding release policies across those projects that preclude them from being updated frequently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6133#issuecomment-547988573:308,perform,performance,308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6133#issuecomment-547988573,1,['perform'],['performance']
Performance,"@davidbenjamin, et al. I have two recommendations:. 1) Though I prefer to work symbolically and through proofs, it might be nice to first expand on the validation by proof in the JavaDoc - including for the specific function's header - and anywhere else where necessary across the GATK code, just for sanity's sake, and for tying things together neatly and properly. This process of always going through the mathematical steps alerts me when I code that I have not missed anything. . 2) When dealing with multiple levels of transformations, it probably would be good to formulate a collection of complete set of simple tests. Since like you mentioned {phased} is a subset () of {unphased}, then the paths of phased genotypes one works with would also be ideal to test on. Does this function have any validation tests confirming the correct likelihoods, which would be performed for both phased and unphased genotypes? These can be generated tests, if original files do not exists. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221:869,perform,performed,869,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221,1,['perform'],['performed']
Performance,"@davidbernick Thanks for looking into this. ; 1. It looks like we have a big 200G called /app disk but we're only writing to the root for some reason. Can we write there instead? ; 2.  to removing most of the old jobs. It would be nice if we could save the job status logs but delete the artifacts and temp files. If that's hard to do though it's fine to just rotate out the old ones. Can we keep two months of jobs? That would be enough buffer so we can look back and see when something went wrong. For the performance tests you wrote, if there's any way we can keep their historical run times that would be best since they're mostly useful to see trends against the repositories history.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2180#issuecomment-248319308:509,perform,performance,509,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2180#issuecomment-248319308,1,['perform'],['performance']
Performance,"@ddrichel It is regrettable that the tradeoff between precision and recall wasn't beneficial in your data, but in other data it has been. I can't call it a bug because mathematically speaking it is an improvement. At worst it can be considered an arbitrary movement along the ROC curve. Regardless, we have always developed Mutect2 and its successor under the principle that theoretical soundness is the best long-term policy. That is not to deny or disregard that some changes harm performance on some data. There is a chance that adjusting the `-f-score-beta`, which controls the relative contribution of recall and precision to the weighted F score that FilterMutectCalls seeks to optimize, will be able to reverse this shift and give up the gained recall in exchange for the lost precision. I can't promise that this will help, but fortunately you have the patched 4.1.8.1 that @droazen has kindly produced.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1538858085:483,perform,performance,483,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1538858085,2,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,"@dpmccabe commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1008). (Very low-priority enhancement request). Allow GetBayesianHetCoverage's matched tumor-normal mode to run on multiple tumor samples matched to a single normal. The normal coverage pulldown and likelihood calculations really only need to be calculated and written to a file once. Alternatively, allow the user to specify a `normalHets` file instead of a BAM if one has already been generated. Thanks!. ---. @samuelklee commented on [Thu Apr 27 2017](https://github.com/broadinstitute/gatk-protected/issues/1008#issuecomment-297704915). We're slowly rebuilding the entire somatic pipeline. One change on the allelic side will be to simply collect allelic counts at all specified sites, rather than performing genotyping on all sites in matched normals and then collecting the corresponding tumor counts at het sites. . The CLI tool to do this (CollectAllelicCounts) is already merged, if you'd like to start using it. You'd only have to run this once on each BAM. The ultimate idea is that resulting allelic count files, along with the corresponding coverage files, could then be passed to a SomaticCNVCaller tool, along with the necessary annotations denoting whether they are tumor or normal. For now, you could probably insert a simple script that performs the genotyping step if you still want to use the rest of the old pipeline but avoid pulling down the normal multiple times.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2977:799,perform,performing,799,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2977,2,['perform'],"['performing', 'performs']"
Performance,"@droazen +1 for being affected by this issue in production. As this is in production (same as @schelhorn , with big pharma which have very strict security requirements), and as 4.1.8.0 contains critical security vulnerabilities that were mitigated in subsequent releases, we are in a serious pickle here. @jhl667 how did you conclude that 4.1.8.0 performs better than newer versions? What we see is that it emits more variants, but after filtering and intersecting with other callers (i.e. Strelka), we get more variants and a ""better"" result (we can't really define ""better"" - it's merely an observation) with 4.2.4.1.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1407439000:347,perform,performs,347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1407439000,1,['perform'],['performs']
Performance,"@droazen - That won't be solved by the current #3447, because there is no way of fine-tune the codecs: I require to being able to add/remove concrete classes, and exclude codecs from a concrete package. An example is a custom codec implementation for some feature, to provide extra-validation for the downstream toolkit. This would be even more useful if HTSJDK is moving to an interface-based library...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-337622596:86,tune,tune,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-337622596,1,['tune'],['tune']
Performance,"@droazen - sorry for the compilation error, it was just an early optimization. Can you have a look to it? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384300799:65,optimiz,optimization,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384300799,1,['optimiz'],['optimization']
Performance,"@droazen @cmnbroad @mbabadi I generally agree with the sentiments expressed in #4127, except that I think it's OK to require a conda environment (or even use of the Docker) for these particular tools. How we should validate this requirement is another question. We can discuss more with @vdauwera. @stefandiederich Hopefully once you get the conda environment set up you will be able to run the tools. We would definitely appreciate any feedback you might be able to provide. Note that the gCNV model is relatively sophisticated, so there may be some parameters (which control the priors for the model as well as how inference is performed) that you will need to adjust for your data. Depending on the number of intervals/bins you are using and your memory constraints, you may also need to scatter across multiple GermlineCNVCaller runs; see how things are done in the WDLs here: https://github.com/broadinstitute/gatk/tree/master/scripts/cnv_wdl/germline. As you noted, this pipeline is still in beta. We are currently running several evaluations and hope to soon release some Best Practices recommendations for the aforementioned parameter values that should work well for various data types generated at the Broad. We will also have some blog or forum posts that explain the new CNV pipelines in more detail coming soon---stay tuned!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357034364:630,perform,performed,630,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357034364,2,"['perform', 'tune']","['performed', 'tuned']"
Performance,"@droazen I didn't really look at runtime scientifically, so I can't comment on that. I pasted this table and explanation into the picard issue, but I think it had already been closed at that point, so I'm not sure how many people saw it:. > I have some _small_ test BAMs that are constructed by extracting reads overlapping a few hundred kb of genome from WGS samples. I made the table below using such a BAM made from the 1KG PCR-free WGS data from NA19625. Not ideal, but I would _think_ would perform fairly similar to a full WGS bam for compression purposes. What I see is that at compression level 1 the intel deflator produces a significantly larger BAM that the JDK deflator at level=1. . Compression Level | Intel Deflater File Size | Intel Deflater % of JDK l=5 | JDK Deflater File Size | JDK Delfater % of JDK l=5; ---|------------|----------|---------------|---------; 1 | 54,840,445 | 175.23% | 38,543,684 | 123.16%; 2 | 35,782,642 | 114.33% | 36,745,494 | 117.41%; 3 | 34,989,899 | 111.80% | 35,262,326 | 112.67%; 4 | 31,815,698 | 101.66% | 32,549,560 | 104.00%; 5 | 31,240,892 | 99.82% | 31,296,433 | 100.00%; 6 | 30,675,174 | 98.01% | 30,577,906 | 97.70%; 7 | 30,379,699 | 97.07% | 30,380,325 | 97.07%; 8 | 30,124,200 | 96.25% | 30,124,375 | 96.25%; 9 | 30,064,322 | 96.06% | 30,064,325 | 96.06%. That does seem to suggest that the intel deflator at `level=2` produces a BAM that is smaller than the JDK deflator at either level 2 or 1, and if it is also faster in your testing, that sounds pretty good for intermediate files. It's still ~15% bigger than a `level=5` BAM though, so unless the vast majority of users are switching over to CRAM for storage, I'd hesitate to change the default compression level in any of the toolkits.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-323840661:496,perform,perform,496,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-323840661,1,['perform'],['perform']
Performance,"@droazen I hacked one of the TrainVariantAnnotationsModelIntegrationTest cases to run in your Docker (only necessary because it seems like `gradlew test --tests *TrainVariantAnnotationsModelIntegrationTest` doesn't recognize tests that use a `DataProvider`, but perhaps I did something wrong). Here are the differences:. ```; (gatk) root@a87e0994889e:/repo# h5diff -v /repo/src/test/resources/large/org/broadinstitute/hellbender/tools/walkers/vqsr/scalable/train/expected/extract.nonAS.snpIndel.posUn.train.snpIndel.posOnly.IF.snp.trainingScores.hdf5 /repo/extract.nonAS.snpIndel.posUn.train.snpIndel.posOnly.IF.snp.trainingScores.hdf5. file1 file2; ---------------------------------------; x x / ; x x /data ; x x /data/scores . group : </> and </>; 0 differences found; group : </data> and </data>; 0 differences found; dataset: </data/scores> and </data/scores>; size: [445] [445]; position scores scores difference ; ------------------------------------------------------------; [ 60 ] -0.419202 -0.419202 5.55112e-17 ; 1 differences found; ```. Looks pretty negligible to me! :stuck_out_tongue_closed_eyes: Probably a result of the native code being called by the python/ML packages used in these tools; even minor changes in the compilers across Ubuntu versions might introduce differences like these. A quick fix might be to replace all system calls to `h5diff` in these tests with `h5diff --use-system-epsilon`; seems to do the trick here. But if that doesn't fix all test cases, then perhaps you can relax things with `h5diff -p EPSILON`, where `EPSILON` is a relative threshold. Probably OK to pick something like `1E-6`. OK if I leave it to you to try this or otherwise check the rest of the cases?. Sorry for the inconvenience! I think the exact-match test worked as intended here, but I probably could've put in better messaging originally. Unfortunately, it's a bit awkward to grab the output of system commands. And thanks for dealing with conda again (a necessary evil, unless we want ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848796931:448,scalab,scalable,448,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848796931,1,['scalab'],['scalable']
Performance,@droazen I have pushed the cache removal step down to a more testable point in the code and added the assertion to the existing testing infrastructure. Can you take a quick look at this branch so it can go in at some point?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5911#issuecomment-491914930:27,cache,cache,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5911#issuecomment-491914930,1,['cache'],['cache']
Performance,"@droazen I posted the complete command line I used (the version is above). I posted a test.sam that reproducibly fails on my machine (OSX). And below is the log from my machine:. ```; 22:42:22.298 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; Aug 01, 2020 10:42:22 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 22:42:22.412 INFO HaplotypeCaller - ------------------------------------------------------------; 22:42:22.412 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; 22:42:22.412 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:42:22.412 INFO HaplotypeCaller - Executing as nhomer@ip-192-168-7-102.ec2.internal on Mac OS X v10.14.6 x86_64; 22:42:22.412 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 22:42:22.412 INFO HaplotypeCaller - Start Date/Time: August 1, 2020 10:42:22 PM MST; 22:42:22.412 INFO HaplotypeCaller - ------------------------------------------------------------; 22:42:22.412 INFO HaplotypeCaller - ------------------------------------------------------------; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Version: 2.23.0; 22:42:22.413 INFO HaplotypeCaller - Picard Version: 2.22.8; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:42:22.413 INFO HaplotypeCaller - Deflater: IntelDeflater; 22:42:22.413 INFO HaplotypeCaller - Inflater: IntelInflater; 22:42:22.413 INFO HaplotypeCaller ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:224,Load,Loading,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['Load'],['Loading']
Performance,"@droazen I ran #4314 and it did not solve the problem. When I reverted the ADAM patch from the #4314 branch I got the normal performance. @fnothaft I wish I knew. @lbergelson said he saw more logs being produced. Another (untested) theory is that the Kryo registrations changed somehow. GATK only uses the 2bit code from ADAM, so it is surprising that it is having such an effect. I'm not sure how to track down the problem at this point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366294101:125,perform,performance,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366294101,1,['perform'],['performance']
Performance,"@droazen I ran the latest version but the message about google is still there!. 14:08:05.607 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cm/shared/unil/software/8.3/GATK/4.1; .9.0-GCCcore-8.3.0-Java-8/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 14, 2020 2:08:06 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-708360241:120,Load,Loading,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-708360241,1,['Load'],['Loading']
Performance,@droazen I think the BCF code is broken here too. The problem is fundamental to htsjdk. CombineVariants almost certainly has the same or similar problems because it's fundamental to combining vcfs and the fact that htsjdk doesn't handle partially empty lists. Bcftools likely has similar issues. Or loading the correct output from bcftools will recreate the issuue. What about fixing the combine operation so it can substitute default missing values with a per attribute configuration for what value to substitute?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-677814646:299,load,loading,299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-677814646,1,['load'],['loading']
Performance,"@droazen I thought you might say that. We might be able to come up with an intermediate caching solution where not the entire index is cached. Although the index isn't really that big, so I'm not sure if it's a big deal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2366#issuecomment-275425244:135,cache,cached,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2366#issuecomment-275425244,1,['cache'],['cached']
Performance,"@droazen I'm not sure this is an improvement. We want the fundamental unit of spark tool to be the transform, not the cli wrapper around it. If we do this then we're pushing more of the contract of the transform outside of itself, i.e. see the newly duplicated bqsr code. I think that it was a deliberate decision to lift all reads into the initial rdd and then filter them in the transforms to what was needed by that transform. This is paying some performance cost in multi-stage pipelines which will potentially apply the same filters over and over again, but it simplifies the code because the filters can be baked into the transform and the pipeline writer doesn't have to think about them. It would be nice if we had a mechanism for adding metadata to an RDD so we can say ""this is a sorted RDD filtered with X,Y,and Z filters"", so we could intelligently avoid re-filtering.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856:450,perform,performance,450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856,1,['perform'],['performance']
Performance,"@droazen It's just a bit strange that it goes from 20s on local machines to 5 minutes on travis. Must be memory. Or some critical spark performance bug we haven't seen before ""_""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1342#issuecomment-163781324:136,perform,performance,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1342#issuecomment-163781324,1,['perform'],['performance']
Performance,"@droazen Looks good. I'm so glad to see that repetitive loading code gone. Just a few minor comments. Feel free to merge when tests pass. (providing that they do pass... I think you've changed the behavior of a few tools to now pull in unmapped reads, which is an improvement, but it's possible the tests are assuming that unmapped reads are ignored)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/955#issuecomment-146009870:56,load,loading,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/955#issuecomment-146009870,1,['load'],['loading']
Performance,"@droazen Off the top of my head, we. * cache `log10(n)` and `log10(n!)` up to some large value.; * have a fast version of `log10SumLog10(double a, double a)` that works as follows: we want to compute `log10(10^a + 10^b)`. WLOG `a < b`, so this comes out to `a + log10(1 + 10^(a - b))`. I believe we cache the values of `log10(1 + 10^(x)` over a finely-spaced grid and round `a-b` to the nearest cached `x`. . There might not be anything else. There's a lot of stuff to keep calculations in log space for numerical stability but those don't avoid `log10()` and `Math.pow()`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292584322:39,cache,cache,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292584322,3,['cache'],"['cache', 'cached']"
Performance,"@droazen Responded to comments, note that i added a further escape condition where getMatchingPriors is avoided altogether if the VCpriors list is empty. Remember that this method is in a performance sensitive part of the code so every little bit of speed counts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5616#issuecomment-461597381:188,perform,performance,188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5616#issuecomment-461597381,1,['perform'],['performance']
Performance,"@droazen Thank you for the confirmation that HaplotypeCaller performs separate filtering passes on the read mapping qualities, and that the code on line 729 of HaplotypeCallerEngine.java (method ```filterNonPassingReads()``` ) is indeed executing subsequent to the ```MappingQualityReadFilter```. May I suggest, however, that MAPQ values less than 20 might not necessarily lead to an increase in FP variant calls? My understanding is that HaplotypeCaller uses MAPQ values only in a nonparametric rank sum test, in which case MAPQ is treated as an ordinal. This seems appropriate since the magnitude of a MAPQ value depends both on the data and on the computational model the read aligner uses to calculate it. With this in mind, a set of mappings with MAPQ in a lower range (e.g., ```--minimum-mapping-quality 10``` and a correspondingly lower ```--maximum-mapping-quality``` as well) might very well be appropriate for variant calling. So changing the semantics of ```MappingQualityReadFilter``` or parameterizing the currently-hardwired MAPQ range would enable additional control without affecting performance. @jamesemery I will watch for the HaplotypeCaller update that implements that functionality. And if you have a moment, could you please point me to the code that might be adversely affected by decreasing the low-end MAPQ threshold? I might have some ideas about that (or not!)... Thanks again!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6854#issuecomment-701512278:61,perform,performs,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6854#issuecomment-701512278,2,['perform'],"['performance', 'performs']"
Performance,"@droazen The thought was that we would fail at the point we try to load bases which is usually very close to the start. It moves the error back a bit which isn't great, but would allow for crams with embedded references.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6665#issuecomment-645474983:67,load,load,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6665#issuecomment-645474983,1,['load'],['load']
Performance,"@droazen Yes, it's tied for next in my queue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2067#issuecomment-256740267:39,queue,queue,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2067#issuecomment-256740267,1,['queue'],['queue']
Performance,"@droazen Yes, this is because the native PairHMM is using single precision floating point and Flush To Zero (FTZ), while the Java PairHMM is using double precision and not using FTZ. I planned to address this when we integrate native PairHMM into HaplotypeCaller. It looks like the time is here. For now, you can configure native PairHMM to use double precision and not use FTZ. With the diff below, the VCFs from native and Java PairHMM are exactly the same. In the future, we need to enable FTZ in the Java PairHMM and provide the option to use single precision or double precision in native PairHMM. ``` diff; --- i/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc; +++ w/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc; @@ -23,7 +23,7 @@ LoadTimeInitializer::LoadTimeInitializer() //will be called when library is loa; //Very important to get good performance on Intel processors; //Function: enabling FTZ converts denormals to 0 in hardware; //Denormals cause microcode to insert uops into the core causing big slowdown; - _MM_SET_FLUSH_ZERO_MODE(_MM_FLUSH_ZERO_ON);; + // _MM_SET_FLUSH_ZERO_MODE(_MM_FLUSH_ZERO_ON);. //Profiling: times for compute and transfer (either bytes copied or pointers copied); m_compute_time = 0;; diff --git i/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc w/src/main/cpp/VectorLoglessPairH; index f45153e..70cf54f 100644; --- i/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc; +++ w/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc; @@ -6,7 +6,7 @@. using namespace std;. -bool use_double = false;; +bool use_double = true;. //Should be called only once for the whole Java process - initializes field ids for the classes JNIReadDataHolderClass; //and JNIHaplotypeDataHolderClass; diff --git i/src/main/java/org/broadinstitute/hellbender/utils/pairhmm/VectorLoglessPairHMM.java w/src/main/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1572#issuecomment-195496083:653,Load,LoadTimeInitializer,653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1572#issuecomment-195496083,5,"['Load', 'perform']","['LoadTimeInitializer', 'performance']"
Performance,"@droazen correct. ; Generally, one issue is that this slows down the docker image creation in a somewhat substantial way. Around 10 minutes currently. Half of this is unzipping the bundled jar, and another piece is some redundant gradle downloading that can be alleviated with cache shenanigans.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666:277,cache,cache,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666,1,['cache'],['cache']
Performance,"@droazen here are the error messages with gatk4.1.8.1 and gatk4.1.4.1:. 15:01:44.424 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cm/shared/unil/software/8.3/GATK/4.1.4.1-GCCcore-8.3.0-Java-8/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 09, 2020 3:01:45 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine. 14:28:22.786 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cm/shared/unil/software/8.3/GATK/4.1.8.1-GCCcore-8.3.0-Java-8/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 09, 2020 2:28:23 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-707085229:112,Load,Loading,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-707085229,2,['Load'],['Loading']
Performance,"@droazen ideally, the executor must be able to fire up one (or more) python kernels and keep it (them) alive as long as the user decides to keep it (or the GATK session terminates). Here's an example why this is desirable: the compilation of a complicated theano computational graph can take a significant portion of the total computation time. The compiled graph is a function of data dimensions, which in my use case, varies from loci to loci. My current solution to this is to pre-compile and cache a number of theano computational graphs with different sizes, pad the data to fit it to the closest matching computational graph, and re-use the same compiled graph(s) as required. To this end, one needs to keep the python kernel w/ the compiled graphs alive.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3501#issuecomment-325040324:496,cache,cache,496,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3501#issuecomment-325040324,1,['cache'],['cache']
Performance,"@droazen thanks for noticing that PR. Looks like theres quite a bit more going on there than is covered by this issue, although I didnt take a close look. We should certainly make sure that any assumptions on SW parameters, etc. there are checked as well, if it does end up going inseems quite stale, no?. Also, hope you dont mind if I unassign myself from thiswhats the point of punting on something if it just gets reassigned to you? ;) Happy to review a PR if someone else is convinced that the original optimization is worth restoring, though!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7441#issuecomment-908616477:513,optimiz,optimization,513,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7441#issuecomment-908616477,1,['optimiz'],['optimization']
Performance,"@droazen this behavior hasn't changed in the most recent GenomicsDB release. . Short recap: this happens because bcf codec doesn't support the 64 bit values that GenomicsDB is returning. Running with `--genomicsdb-use-vcf-codec` will resolve it. From our discussions in the office hours, I thought we had decided to change the behavior in htsjdk so that it doesn't try to decode the type if it doesn't recognize it. (maybe I should have filed https://github.com/broadinstitute/gatk/issues/6548 in htsjdk instead? I thought I was told to do in GATK, but its been long enough that I can't remember). Another possibility is to make `--genomicsdb-use-vcf-codec` the default - though I recall you had some potential performance concerns about that. Lastly, we could change GenomicsDB to throw out a warning if a 64 bit value is needed and we're using bcf codec. Of course, this would still require the user to (re)run with `--genomicsdb-use-vcf-codec` to avoid hitting the NPE (or whatever other failure would be hit if the NPE was changed to something a bit more meaningful).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6667#issuecomment-646167430:711,perform,performance,711,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6667#issuecomment-646167430,1,['perform'],['performance']
Performance,"@droazen this branch wasn't STRICTLY dependent on #5607, so I removed it from this branch to make reviewing easier. Its worth noting that the performance numbers and observed speedup were seen when this branch did hang off of #5607.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5616#issuecomment-460423597:142,perform,performance,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5616#issuecomment-460423597,1,['perform'],['performance']
Performance,@droazen what do you want to do here? We're matching the performance but not beating it at this point. We can either keep this open and continue to investigate or close as 'matching not beating',MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1608#issuecomment-234341641:57,perform,performance,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1608#issuecomment-234341641,1,['perform'],['performance']
Performance,"@droazen would you mind reviewing/assigning? Or perhaps @jamesemery can take a look, since he expressed interest in doing similar Bayesian optimizations for an HMM? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-904536278:139,optimiz,optimizations,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-904536278,1,['optimiz'],['optimizations']
Performance,"@droazen you were asking for a check of the performance. I ran the following two commands:. ```; $ ./gatk-launch PrintReads -I gs://$MYBUCKET/CEUTrio.HiSeq.WGS.b37.ch20.1m-2m.NA12878.bam -O gs://$MYBUCKET/pr.bam; $ ./gatk-launch PrintReads -I gs://$MYBUCKET/CEUTrio.HiSeq.WGS.b37.ch20.1m-2m.NA12878.bam -O /tmp/pr.bam; ```. output to: | local disk | GCS; --|--|--; run 1 | 0.12 min | 0.68 min; run 2 | 0.12 min | 0.29 min; run 3 | 0.12 min | 0.28 min; **median** |**0.12 min** | **0.29 min**. So it looks like there's a significant performance difference. For what it's worth, copying the output file to GCS from my desktop takes 3.5s. The log when running PrintReads indicates:. ```; 11:06:13.011 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 11:06:13.011 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:06:13.011 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:06:13.011 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-336978915:44,perform,performance,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-336978915,2,['perform'],['performance']
Performance,"@droazen's [measurements in #995](https://github.com/broadinstitute/gatk/issues/995#issuecomment-152310985) show that for the 7GB input the optimized version (""sharding"") is **9x faster** than the unoptimized version (""shuffle""). The code is fast as intended, I'm closing this bug.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1006#issuecomment-155863239:140,optimiz,optimized,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1006#issuecomment-155863239,1,['optimiz'],['optimized']
Performance,"@droazen, I will address this in #2041. As you suggested this when I implemented `LocusWalker`, I would like to have some idea about why `DownsamplingMethod` is used as a parameter in the constructor. I think that this is misleading, because independently of the method for downsampling the one that is used by `SamplePartitioner` is a `ReservoirDownsampler` (if downsampling is performed), so API users could think that they are performing a different downsampling in LIBS that the actual one. I will keep the constructor in the PR, but I would like some feedback for this either here or in #2041.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006:379,perform,performed,379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006,2,['perform'],"['performed', 'performing']"
Performance,"@droazen, the SAMRecord interface exposes getReferenceIndex and getMateReferenceIndex, so there is no question that the index has to be there. . What I was talking about what the optimization where when serializing, the [BAMRecordCodec only saves the index](https://github.com/samtools/htsjdk/blob/master/src/java/htsjdk/samtools/BAMRecordCodec.java#L131), and not the name. That is because if we have the header then we can go from the index to the name. If there is no header, then we can't do that optimization anymore and instead have to save those two fields for every read.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141251365:179,optimiz,optimization,179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141251365,2,['optimiz'],['optimization']
Performance,"@droazen, yes for luster, we should pass the `--genomicsdb-shared-posixfs-optimizations` especially with GenomicsDBImport.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1032004698:74,optimiz,optimizations,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1032004698,1,['optimiz'],['optimizations']
Performance,"@dwuab, we are making some performance improvements with GenomicsDB and still are in the testing stage. Just wondering if you could try gatk from this branch `https://github.com/broadinstitute/gatk/tree/genomicsdb_142` to import large intervals(the ones that were problematic before) and let us know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-930343647:27,perform,performance,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-930343647,1,['perform'],['performance']
Performance,"@erniebrau That's the performance of the pairhmm, but not the entire haplotype caller, there's definitely diminishing returns at some point. I could be a bit off on the core count before it starts leveling out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332917726:22,perform,performance,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332917726,1,['perform'],['performance']
Performance,"@fleharty sorry, each curve is a different set of parameters run on the same sample, and I'm plotting all curves generated over the course of optimizing those parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712333444:142,optimiz,optimizing,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712333444,1,['optimiz'],['optimizing']
Performance,@fnothaft I tried reverting 1eed8e8 and the performance was back to normal! So it would be worth reverting in ADAM for the next release if possible.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-368564267:44,perform,performance,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-368564267,1,['perform'],['performance']
Performance,"@gspowley I'm having trouble building on mac still. I changed from clang to gcc and I'm getting different errors now. ```; :compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/baseline.cc:4:; /Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/jre/../include/jni.h:45:20: fatal error: jni_md.h: No such file or directory; compilation terminated. In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:2:; /Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/jre/../include/jni.h:45:20: fatal error: jni_md.h: No such file or directory; compilation terminated. In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/jni_common.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc:2:; /Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/jre/../include/jni.h:45:20: fatal error: jni_md.h: No such file or directory; compilation terminated. In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.cc:3:; /Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/jre/../include/jni.h:45:20: fatal error: jni_md.h: No such file or directory; compilation terminated. In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/template.h:86:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/avx_function_instantiations.cc:3:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h: In instanti",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187417081:274,Load,LoadTimeInitializer,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187417081,3,['Load'],['LoadTimeInitializer']
Performance,"@gspowley Yes, defining the env. var. avoided crash. It just says unable to load GKL. thx!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265880530:76,load,load,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265880530,1,['load'],['load']
Performance,"@gudeqing I think you are referrring to the calls to `GetPileupSummaries`, where we have both `-L` and `-V` arguments with the same variable. This is actually not redundant, though I admit it is clumsy. This is a consequence of `GetPileupSummaries` being written as a GATK `LocusWalker`, which is necessary for optimal performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7731#issuecomment-1154211085:319,perform,performance,319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7731#issuecomment-1154211085,1,['perform'],['performance']
Performance,"@jamesemery Could you review this? I think you may appreciate it. It took several tries, but I was finally able to write a stripped-down version of the code that actually slightly outperforms the old version. What I realized after a lot of profiling the old code and various failed rewrites was that cache-friendliness is the critical thing here. It turns out that this can be achieved without too many buffers, without precomputing the log frequencies, and without storing 2D and 3D arrays as flattened 1D arrays.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351:300,cache,cache-friendliness,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351,1,['cache'],['cache-friendliness']
Performance,"@jamesemery Great, thanks for checking. Could you do a review pass on this when you get a chance? It's not clear that the approach taken here of sending the owner config file around is what we want....it seems like instead we need a way to load the owner config from the launcher script itself.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188:240,load,load,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188,1,['load'],['load']
Performance,"@jamesemery I agree - all access (read and write) to `GenotypeLikelihoodCalculators` instance variables needs to be synchronized to make it safe. I think it would be sufficient to make `getInstance()` and `calculateGenotypeCountUsingTables()` synchronized. @droazen, are you concerned about performance for the Spark case? For the walker version, presumably the access is single-threaded, and hence [uncontended, which is very cheap](https://books.google.co.uk/books?id=mzgFCAAAQBAJ&pg=PA230&lpg=PA230&dq=java+uncontended+synchronization+goetz&source=bl&ots=7W4J807faW&sig=YALE1qdWoAUELPqLRhIedz-bZ20&hl=en&sa=X&ved=2ahUKEwj4jJeko8zdAhXVFsAKHazkBrcQ6AEwB3oECAIQAQ#v=onepage&q=java%20uncontended%20synchronization%20goetz&f=false). Another option would be to maintain a separate instance of `GenotypeLikelihoodCalculators` per genotyping engine. The size of the table is ploidy * alleles, so not too large?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-423546586:291,perform,performance,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-423546586,1,['perform'],['performance']
Performance,"@jamesemery While you're in the `HaplotypeCallerEngine` doing optimizations, you should profile peak memory usage as well and see if we can get it down to < 3 GB. This would reduce costs by allowing us to use cheaper instances on the cloud.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2591#issuecomment-460407699:62,optimiz,optimizations,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2591#issuecomment-460407699,1,['optimiz'],['optimizations']
Performance,@jberghout If you post your actual output we might be able to track down your variant of the problem (the error message in the original post looks to me somewhat like a corrupt gradle cache: error reading /vsc-hard-mounts/leuven-user/304/vsc30484/.gradle/caches/modules-2/files-2.1/org.spire-math/spire_2.11/0.11.0/998b1c1d841baf4fc5d1b119ea55f165f6684ef5/spire_2.11-0.11.0.jar; error in opening zip file). Is it the `gatkTabComplete` task that is failing for you as well ?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566793345:184,cache,cache,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566793345,2,['cache'],"['cache', 'caches']"
Performance,@jean-philippe-martin Any thoughts on the cause of the failure to load class `org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos` in travis? Do tests pass for you if you run them locally using `./gradlew test`?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1994#issuecomment-232406786:66,load,load,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1994#issuecomment-232406786,1,['load'],['load']
Performance,"@jean-philippe-martin Given the issues identified above with the zero-shuffle implementation, should this PR be closed while you re-think your approach to optimizing this tool?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/863#issuecomment-139650193:155,optimiz,optimizing,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/863#issuecomment-139650193,1,['optimiz'],['optimizing']
Performance,"@jean-philippe-martin I disagree that just because there are getters for the index, the index has to be there. It is already allowed to be absent for various reasons (via the special value `NO_ALIGNMENT_REFERENCE_INDEX`). Since use of the index instead of the name is mostly a performance optimization, I think we can get away with allowing records that have the name filled in but not the index. Relying on contig indices in general is a bad idea, as they are quite brittle, particularly when querying data from multiple sources. We purposefully moved away from using the indices in hellbender in favor of names when we migrated from `GenomeLoc` to `SimpleInterval`, and are, I think, willing to pay the extra cost of string parsing to avoid the subtle bugs that historically resulted from relying on the indices. This is a micro-optimization that probably isn't worth pursuing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141264817:277,perform,performance,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141264817,3,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"@jean-philippe-martin I think we should do the comparison in https://github.com/broadinstitute/gatk/issues/995 before porting the ApplyBQSR optimizations, actually. If it turns out that we decide to go with the simpler broadcast approach we'd then need to figure out how the dataflow ApplyBQSR changes fit in. So it probably makes sense to spin out a separate ticket for the ApplyBQSR changes and close this one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/970#issuecomment-148758446:140,optimiz,optimizations,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/970#issuecomment-148758446,1,['optimiz'],['optimizations']
Performance,"@jean-philippe-martin In our initial tests with the latest gatk, we're still getting errors like this at a rate of ~2%:. ```; com.google.cloud.storage.StorageException: 503 Service Unavailable; java.lang.IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment. Please set a project ID using the builder.; com.google.cloud.storage.StorageException: 503 Service Unavailable; com.google.cloud.storage.StorageException: 503 Service Unavailable; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; com.google.cloud.storage.StorageException: 503 Service Unavailable; com.google.cloud.storage.StorageException: 503 Service Unavailable; com.google.cloud.storage.StorageException: 503 Service Unavailable; com.google.cloud.storage.StorageException: 503 Service Unavailable; com.google.cloud.storage.StorageException: 503 Service Unavailable; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: Remote host closed connection during handshake, for input source:; ```. Now, I know that you put in an explicit retry for 503's, so I'm wondering what could be going on. I've asked the person running the tests to check that they're using an up-to-date GATK jar, but I'm wondering if we're setting all the right retry options on the GATK side. Eg., your PR https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2083 says ""but only when OptionMaxChannelReopens is set"" -- are we setting this properly? Any other thoughts on things we could try?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306944607:555,concurren,concurrent,555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306944607,2,['concurren'],['concurrent']
Performance,"@jean-philippe-martin Thank you for putting this together, but performing authentication and reading the first byte might be too small a test for running on the Cloud. Could you run a profile test using `gcloud-java-nio` with 100 GB, 500 GB, 1 TB, 10 TB of data and process it?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2013#issuecomment-233417619:63,perform,performing,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2013#issuecomment-233417619,1,['perform'],['performing']
Performance,"@jean-philippe-martin Yup, reading hundreds of different files in the same tool. We're trying to optimize the buffer size for that case, that's what we're looking at in that other thread you were commenting on. It seems like we may have some memory leak somewhere else, you seem to need much more memory than we would expect if everything is working as expected. . This branch should fix the thread leak we were seeing.  Merge when tests pass.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2643#issuecomment-299038867:97,optimiz,optimize,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2643#issuecomment-299038867,1,['optimiz'],['optimize']
Performance,"@jean-philippe-martin do you think it would be possible to fix this? The workaround is to set `GOOGLE_APPLICATION_CREDENTIALS`, but it would be preferable to have gcloud-java-nio fail when a `gs://` path is specified and the credentials are not set, rather than when the filesystem providers are loaded.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2110#issuecomment-241670337:296,load,loaded,296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2110#issuecomment-241670337,1,['load'],['loaded']
Performance,"@jean-philippe-martin noticed that the performance of his optimized version of spark BQSR took a nosedive during one of the rebases of his branch. Since he's on leave, one of us will have to profile it in order to find out what the bottleneck is and submit a patch. This is a prerequisite to being able to do the broadcast vs. manual sharding comparison called for in https://github.com/broadinstitute/gatk/issues/995",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1006:39,perform,performance,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1006,3,"['bottleneck', 'optimiz', 'perform']","['bottleneck', 'optimized', 'performance']"
Performance,"@jean-philippe-martin will be porting his dataflow optimizations to spark as part of https://github.com/broadinstitute/gatk/issues/970, and then going on leave for several weeks. It would be good if @tomwhite and @laserson could advise him over the next week or so as he does this, and then take over the process of optimization after he leaves. I've created this ticket as a place for @jean-philippe-martin, @tomwhite, and @laserson to sync up and spawn additional tickets as necessary.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/986:51,optimiz,optimizations,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/986,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"@jean-philippe-martin, this change changes `testBQSRLocal` to be truly local, so that it reads the reference from the local filesystem (which was added in https://github.com/broadinstitute/hellbender/pull/827). So, for that test no API key is needed. . The `testBQSRRefCloud` test still loads the reference from the cloud, so we're still testing that case too.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/862#issuecomment-136760832:287,load,loads,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/862#issuecomment-136760832,1,['load'],['loads']
Performance,"@jean-philippe-martin, we want to expose a walker-like interface, but we also care about the general ease of writing programs using the tools we and the user wrote (in native Dataflow/Spark). This is a point that @droazen, has emphasized to me several times. I'll let him add more detail on this if needed. I agree that the static approach won't work for Dataflow when workers are added, but I think I have a solution for Spark that works even when workers are added.; We'd create a new class (like I suggested above), but this would have a `Broadcast<SAMFileHeader>`, which is basically a lazy-loader for headers. We'd only load the header when needed.; I think this may be the best of all solution as it could also support several headers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141177047:595,load,loader,595,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141177047,2,['load'],"['load', 'loader']"
Performance,"@jhl667 I'm looking into this. It looks like I neglected to set the sqlite connection to read only mode when connecting to the db file. I'm going to update it to do so. I'm not sure this applies when a read-only connection is created, but it looks like sqlite has some issues with NFS / distributed file systems:; - https://stackoverflow.com/questions/9907429/locking-sqlite-file-on-nfs-filesystem-possible ; - https://github.com/CGATOxford/CGATPipelines/issues/. One post in the github thread above mentions using `-o flock` when mounting Lustre partitions so that they all have concurrent locks. This _may_ be a workaround in the meantime. . I'll try to look at it on our NFS mounts - I don't have access to a Lustre fs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4413#issuecomment-366009015:580,concurren,concurrent,580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4413#issuecomment-366009015,1,['concurren'],['concurrent']
Performance,"@jjfarrell . After talking with @cmnbroad this afternoon, we'd like to ask you to perform an experiment to limit the scope where hunt down the issue. Is it possible for you to run `PrintReadsSpark` on the same cluster? That is, something similar to . ```bash; gatk --java-options ""-Djava.io.tmpdir=tmp"" \; PrintReadsSpark \; -R $REF \; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.cram \; -- \; --spark-runner SPARK \; --spark-master yarn \; --deploy-mode client \; --executor-memory 85G \; --driver-memory 30g \; --num-executors 40 \; --executor-cores 4 \; --conf spark.yarn.submit.waitAppCompletion=false \; --name ""$SAMPLE"" \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-495357208:82,perform,perform,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-495357208,1,['perform'],['perform']
Performance,"@jjfarrell Glad you found that article useful!. In general, `--consolidate` will be memory and time intensive. It's not intuitive, but as you already figured out if `--consolidate` is enabled, we do it on the very last batch. If you only have on the order of a few hundred batches total, not having specified consolidate shouldn't affect read performance much. The only other thing that would help scale here would be to break up your intervals so that larger contigs are split up into multiple regions. Less memory required and you can throw more cores at it (if you have them). What sort of performance did you see on `GenotypeGVCFs` or `SelectVariants`? That could be the other issue with these large intervals.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252834003:343,perform,performance,343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252834003,2,['perform'],['performance']
Performance,"@jjfarrell You don't need splitting index for cram. The index works around a bam specific problem which makes it hard to find good split points in the file. Cram is designed in a way that makes it easier to find the split points so the index is unnecessary. . I don't have good numbers for how long it takes to find the split points for bam. It depends on your filesystem. If you have a low latency file system like a local disk or hdfs setup than finding split points takes very little time (~seconds), but if you have a high latency file system like something backed by google object store then finding split points may take a long time (on the order of minutes to tens of minutes depending on latency and file size).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371224015:391,latency,latency,391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371224015,3,['latency'],['latency']
Performance,"@jjfarrell and @mlathara: thanks for running these tests and sorry I havent been able to do anything yet with our data. I'm under some grant deadlines until into Oct, but I do hope to add to this. A couple comments:. 1) that broadly matches my experience. 2) My sense is that we were caught between and rock and a hard place with GenomicsDb and GenotypeGVCFs. Our workflow until this summer involved creating a workspace (running per-contig), which involved importing >1500 animals at first. This would execute OK when using a reasonable --batch-size on GenomicsDbImport. However, when we had large workspaces that were imported in lots of batches, GenotypeGVCFs (which we execute scattered, where each job works on a small interval) tended to perform badly and was a bottleneck (i.e. would effectively stall). Therefore we began to --consolidate the workspaces using GenomicsDBImport during the append process. Initially --consolidate worked; however, as @jjfarrell noted, that's memory intensive and once our workspace was a certain size, this basically died again. Therefore we even worked with @nalinigans to their the standalone GenomicsDB consolidate tool. This was a viable way to consolidate the workspaces and we successfully aggregated and consolidated all our data (which took a while). However, these massive, consolidated workspaces seem to choke GenotypeGVCFs. Therefore this process is still basically dead. 3) As I noted above, I'm currently giving up on trying to maintain permanent data in genomicsDB. There's so many advantages to not doing so, and letting the gVCFs exist as the permanent store. Notably, there are many reasons we would want/need to remake a gVCF (like the introduction of reblocking). Whenever any one of the source gVCFs changes, the workspace is basically worthless anyway (which is a massive waste of computation time). We've had great success running each GenotypeGVCFs scattered, where each job runs GenomicsDbImport on-the-fly, to make a transient workspace",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1256246852:744,perform,perform,744,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1256246852,2,"['bottleneck', 'perform']","['bottleneck', 'perform']"
Performance,"@kdatta @kgururaj It seems like we're losing rsID's in the input gvcf when we load them into genomics db. Is this deliberate to save space? Is it a bug? Is it a configuration option that isn't exposed by `GenomicsDBImport`? . I don't think it's important for production because they pass in a dbSNP at genotyping time so that can be recomputed, but it's causing issues in some of my tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2636:78,load,load,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2636,1,['load'],['load']
Performance,"@kdatta @kgururaj We have some questions about this pull request. It looks like maybe you forgot some commits, (and included some extra ones that you didn't intend to). . 1. The tests are a direct copy and paste of `PrintReadsIntegrationTests` and couldn't possibly run. Did you forget to push the commit with the actual test code? We really need some tests especially because it's not totally obvious how to use this tool and what needs to be in the jsons. . 2. The tool currently takes a partition index and a json with stream ids. The way we had envisioned this working was that it would take a `-V` argument with a list of vcfs and `-L` argument specifying what chunk of the genome to load. Can you explain why it is designed this way and if it is possible to change to use the more idiomatic input style? . 3. Can you let us know when 0.4.0 is available on maven? . We have additional review comments, but it seems premature to get into those details until the above are answered.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277310677:689,load,load,689,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277310677,1,['load'],['load']
Performance,"@kdatta I think the reason its failing is because the dylib has an unresolved transitive dependency on openssl (and possibly other things). When I debug locally, I can see that it has streamed the dylib out to a local folder and its trying to load it, but then I get this (see the highlighted text):. /private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb2535884808429708562.dylib: dlopen(/private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb2535884808429708562.dylib, 1): Library not loaded: **/usr/local/opt/openssl/lib/libssl.1.0.0.dylib**; Referenced from: /private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb2535884808429708562.dylib; Reason: image not found. It looks TileDB does have such a dependency ([here](https://github.com/Intel-HLS/TileDB/blob/master/CMakeLists.txt#L79)). Do you know if thats new ? I think we need to figure out how many of these there are and resolve them somehow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294230131:243,load,load,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294230131,2,['load'],"['load', 'loaded']"
Performance,"@kdatta Looks like the integration tests passed on travis after clearing the cache! Once you address comments, squash, and rebase onto the latest gatk master the unit tests should pass as well, since you just need the TestNG fix that got merged into master. This means we can merge this today in all likelihood!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829:77,cache,cache,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829,1,['cache'],['cache']
Performance,"@kdatta Note that a requirement of this feature is that the batch size should limit the number of simultaneous `FeatureReaders` open at any given time. Each `FeatureReader` has an NIO buffer around its byte stream to make queries over GCS performant, and maintaining too many of such buffers at once would likely exceed any reasonable memory limits.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2613#issuecomment-296683686:239,perform,performant,239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2613#issuecomment-296683686,1,['perform'],['performant']
Performance,"@kdatta The .so doesn't seem to load on travis, any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296267896:32,load,load,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296267896,1,['load'],['load']
Performance,"@kdatta Won't using a `VCFCodec` instead of a `BCF2Codec` hurt performance? Can we try to understand why the problem arises when using a `BCF2Codec` by having a look in a debugger?. Also, can you explain why those particular INFO fields are not supported? Many of them are default annotations of the `HaplotypeCaller`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293593226:63,perform,performance,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293593226,1,['perform'],['performance']
Performance,"@kgururaj As I start to think about upgrading exome joint calling to use GenomicsDBImport the 100 interval threshold seems like it might be problematic. I've been working with WGS data, so I don't have much intuition for benchmarking with missing data. Is there any performance downside to running over larger intervals that include missing data? For example, if we want to scatter the exome 50 ways, each subset of the exome interval list will have ~4000 intervals, but the GVCFs won't have data outside those intervals. Does it make sense to pass to GenomicsDBImport a single interval encompassing all of those?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-409956462:266,perform,performance,266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-409956462,1,['perform'],['performance']
Performance,"@kgururaj I ran the commands you suggested. . > [user@cedar5 bin]$ bash -x TestGenomicsDBJar/run_checks.sh; > + [[ hB != hxB ]]; > + XTRACE_STATE=-x; > + [[ hxB != hxB ]]; > + VERBOSE_STATE=+v; > + set +xv; > + unset XTRACE_STATE VERBOSE_STATE; > ++ uname -s; > + osname=Linux; > + jar xf genomicsdb--jar-with-dependencies.jar libtiledbgenomicsdb.so; > java.io.FileNotFoundException: genomicsdb--jar-with-dependencies.jar (No such file or directory); > at java.util.zip.ZipFile.open(Native Method); > at java.util.zip.ZipFile.<init>(ZipFile.java:219); > at java.util.zip.ZipFile.<init>(ZipFile.java:149); > at java.util.zip.ZipFile.<init>(ZipFile.java:120); > at sun.tools.jar.Main.extract(Main.java:1004); > at sun.tools.jar.Main.run(Main.java:305); > at sun.tools.jar.Main.main(Main.java:1288); > + jar xf genomicsdb--jar-with-dependencies.jar libtiledbgenomicsdb.dylib; > java.io.FileNotFoundException: genomicsdb--jar-with-dependencies.jar (No such file or directory); > at java.util.zip.ZipFile.open(Native Method); > at java.util.zip.ZipFile.<init>(ZipFile.java:219); > at java.util.zip.ZipFile.<init>(ZipFile.java:149); > at java.util.zip.ZipFile.<init>(ZipFile.java:120); > at sun.tools.jar.Main.extract(Main.java:1004); > at sun.tools.jar.Main.run(Main.java:305); > at sun.tools.jar.Main.main(Main.java:1288); > + '[' Linux == Darwin ']'; > + LIBRARY_SUFFIX=so; > + ldd libtiledbgenomicsdb.so; > ldd: ./libtiledbgenomicsdb.so: No such file or directory; > + md5sum libtiledbgenomicsdb.so; > md5sum: libtiledbgenomicsdb.so: No such file or directory. I'm using a compute canada server, so I don't have root access. The version of gatk4 I'm using was installed by their support team, and I load it using 'module load gatk'. I had that module loaded when I ran this test.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-357005071:1697,load,load,1697,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-357005071,3,['load'],"['load', 'loaded']"
Performance,"@kgururaj I ran with batch size 50, using a sample map, 5 reader threads:. `java -jar GATK_GDBfork.jar GenomicsDBImport --genomicsdb-workspace-path forkTest --batch-size 50 -L chr20:45840744-45870555 --sample-name-map gnarly_reblocked_all.sample_map --reader-threads 5`. That sample map has 80K genomes because that's the project I'm working on now. Log was as follows:; ```; 16:27:48.831 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/humgen/gsa-hpprojects/dev/gauthier/reblockGVCF/GATK_GDBfork.jar!/com/intel/gkl/nati; ve/libgkl_compression.so; 16:27:48.947 INFO GenomicsDBImport - ------------------------------------------------------------; 16:27:48.948 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.3.0-24-g8804e16-SNAPSHOT; 16:27:48.948 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:27:48.949 INFO GenomicsDBImport - Executing as gauthier@gsa5.broadinstitute.org on Linux v2.6.32-642.15.1.el6.x86_64 amd64; 16:27:48.949 INFO GenomicsDBImport - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 16:27:48.950 INFO GenomicsDBImport - Start Date/Time: May 4, 2018 4:27:48 PM EDT; 16:27:48.950 INFO GenomicsDBImport - ------------------------------------------------------------; 16:27:48.950 INFO GenomicsDBImport - ------------------------------------------------------------; 16:27:48.950 INFO GenomicsDBImport - HTSJDK Version: 2.14.3; 16:27:48.951 INFO GenomicsDBImport - Picard Version: 2.18.1; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:27:48.951 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:27:48.951 INFO GenomicsDBImport - Inflater: IntelInflater; 16",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:416,Load,Loading,416,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['Load'],['Loading']
Performance,"@kgururaj Right, but we want the extra performance provided by using the BCF2Codec, if at all possible..",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2632#issuecomment-297985092:39,perform,performance,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2632#issuecomment-297985092,1,['perform'],['performance']
Performance,"@kgururaj Thanks for adding the test. Running it locally on my laptop (without your fix) succeeds though - I have to bump it up from 1000 intervals to 9000 to reproduce the stack overflow. But if I do that, it takes a long time to run, since it appears to be creating lots of small partitions. Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?. Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:. `[GenomicsDB::VariantStorageManager] INFO: ignore message ""[TileDB::StorageManager] Error: Cannot list TileDB directory; Directory buffer overflow."" in the previous line`. followed by a failure that ends like this:. `[TileDB::StorageManager] Error: Cannot store schema; Too many open files in system.; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Could not define TileDB array; TileDB error message : [TileDB::StorageManager] Error: Cannot store schema; Too many open files in system`. Can you reproduce that ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031:905,Load,LoadOperatorException,905,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031,2,['Load'],['LoadOperatorException']
Performance,"@kgururaj We got this issue report in the forum, could you please look into it? Thanks!. https://gatkforums.broadinstitute.org/gatk/discussion/12388/how-to-use-multi-interval-in-genomicsdbimport-with-gatk-4-0-6-0. ----. I used the GenomicsDBImport with a interval list file and got a error like below.; So what is the correct way to use Multi-interval in GenomicsDBImport?. gatk version: 4.0.6.0. Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -Xms4g -jar /mnt/gatk/gatk-4.0.6.0/gatk-package-4.0.6.0-local.jar GenomicsDBImport -L test.intervals --genomicsdb-workspace-path ../RAW_VCF/my_database -V file1 -V file2 -V file3. 02:57:15.591 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/workshop/xinchen.pan/test/gatk/gatk-4.0.6.0/gatk-package-4.0.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 02:57:15.772 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.772 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.6.0; 02:57:15.772 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:57:15.772 INFO GenomicsDBImport - Executing as on Linux v3.10.0-514.6.1.el7.x86_64 amd64; 02:57:15.772 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b13; 02:57:15.773 INFO GenomicsDBImport - Start Date/Time: July 10, 2018 2:57:15 AM EDT; 02:57:15.773 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.773 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Version: 2.16.0; 02:57:15.773 INFO GenomicsDBImport - Picard Version: 2.18.7; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_S",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4994:794,Load,Loading,794,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4994,1,['Load'],['Loading']
Performance,"@kvn95ss Can you clarify whether the performance was **faster** in the newer version (4.2.5) compared to the older version? We did do some performance work in `SelectVariants` between 4.1.8 and 4.2.5. Also, there are reports that `SelectVariants` is **much** slower if the sample names in the header are not sorted, because the tool has to reorder the genotypes on output if they're not sorted. Can you check whether your sample names are sorted?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7671#issuecomment-1074271066:37,perform,performance,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7671#issuecomment-1074271066,2,['perform'],['performance']
Performance,"@kvn95ss It looks like you're using `--exclude-non-variants` with SelectVariants, which can negatively impact performance. It would be interesting to know how much leaving that out changes things, if thats an option for you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7671#issuecomment-1075132292:110,perform,performance,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7671#issuecomment-1075132292,1,['perform'],['performance']
Performance,"@laserson One use case would be that after running your Spark pipeline you wanted to run a file-based tool on the resulting reads. The vast majority of our tools are still file-based, and for tasks like performing QC on the output of a spark pipeline it's easiest to materialize a single file and run existing tools on it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1015#issuecomment-153081201:203,perform,performing,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1015#issuecomment-153081201,1,['perform'],['performing']
Performance,"@laserson When you get a chance, would you mind adding an update here with the status of this, and whether you think it'll be possible to get any additional optimizations for `MarkDuplicatesSpark` in by alpha (Dec 4)? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1100#issuecomment-158539873:157,optimiz,optimizations,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1100#issuecomment-158539873,1,['optimiz'],['optimizations']
Performance,"@laserson the `SAMRecord` vs. Google `Read` is a loooooong story.; The super-short version:; We had a bunch of utilities written for `SAMRecord` that @droazen refactored over months to take the GATKRead interface. As it happens, the SAM spec and the GA4GH spec are not 100% compatible. So, it's not possible to losslessly convert from A -> B -> A (where A is `SAMRecord` or Google `Read`). The cases where it doesn't work are edge cases, but they exist. Second, @jean-philippe-martin found that converting to Google `Read` was fairly expensive. Between those two points, I think we're probably better off with SAM-backed reads. (Also, right now the Google `Read` is serialized via JSON, so it's not that small anyway.). @tomwhite and @jean-philippe-martin, I think adding the header back will be fine for us engineers working on the engine, but it will make for a poorer user experience for newcomers and Comp Bios to burden them with having to care about what happens with shuffles (when they just want to prototype something). . That said, I think this is probably the best approach we have at our disposal. If we do, we need to do an excellent job of throwing errors if users try to perform actions that would require the header. The error message should explain what really happened and ideally point to some documentation we write explaining the stripping of the header and how to fix it. If this error occurs, it needs to be simple for anyone to fix it. @droazen @lbergelson, what do you two think? (also @laserson, do you have any ideas or thoughts on the header since we're probably stuck with `SAMRecord`?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025:1186,perform,perform,1186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025,1,['perform'],['perform']
Performance,"@lbergelson , as we discussed, I made the mistake of not using the returned RDD when calling `cache()`, after I fixed it, [here](http://dataflow01.broadinstitute.org:18088/history/application_1464285223085_0460/jobs/) is the runtime, running the following code:. ```; 148 final JavaPairRDD<Long, SGAAssemblyResult> cachedResults = results.cache(); // cache because Spark doesn't have an efficient RDD.split(predicate) yet; 149 // results.count(); // ugly hack to make the actual computation happen, so later filtering step will be based on what has been actually computed; 150 ; 151 // save fasta file contents or failure message; 152 final JavaPairRDD<Long, SGAAssemblyResult> success = cachedResults.filter(entry -> entry._2().assembledContigs!=null);; 153 final JavaPairRDD<Long, SGAAssemblyResult> failure = cachedResults.filter(entry -> entry._2().assembledContigs==null);; 154 ; 155 if(!success.isEmpty()){; 156 success.map(entry -> entry._1().toString() + ""\n"" + entry._2().assembledContigs.toString()); 157 .saveAsTextFile(outputDir+""_0"");; 158 }; 159 ; 160 if(!failure.isEmpty()){; 161 failure.map(entry -> entry._1().toString() + ""\n"" + entry._2().collectiveRuntimeInfo.toString()); 162 .saveAsTextFile(outputDir+""_1"");; 163 }; ```. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1877#issuecomment-225641452:94,cache,cache,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1877#issuecomment-225641452,6,['cache'],"['cache', 'cachedResults']"
Performance,"@lbergelson Are you interested in exploring this proposal as part of your performance work on `MarkDuplicatesSpark` this quarter (https://github.com/broadinstitute/gatk/issues/3706)? If not, feel free to remove from the 4.0 milestone.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1873#issuecomment-337351994:74,perform,performance,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1873#issuecomment-337351994,1,['perform'],['performance']
Performance,"@lbergelson Can you add a summary of the performance improvements (eg., runtime % speedup) introduced in this branch as a comment to the master Funcotator performance ticket (https://github.com/broadinstitute/gatk/issues/4586)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4740#issuecomment-387475240:41,perform,performance,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4740#issuecomment-387475240,2,['perform'],['performance']
Performance,"@lbergelson Do you have an opinion on the best way to pip install the gcnvkernel python package and dependencies for Travis testing? I've verified that the pip install works within a basic conda environment with python=3.6. We'll need to load this environment both for unit/integration tests as well as WDL tests. As long as this is the only python environment we need, I think we can simply use the base environment in the Docker. If more environments are required (e.g., for @lucidtronix), then maybe we'll need to be more clever for unit/integration tests, but we can still load them manually in the scripts that kick off the WDL tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-348073948:238,load,load,238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-348073948,2,['load'],['load']
Performance,"@lbergelson I am trying latest release, I use following command:. ```; ./gatk-4.1.3.0/gatk --java-options ""-Xmx4g"" FilterMutectCalls -O Filtered.vcf -V Try.vcf.gz -R ~/human.fa/ucsc.hg19.fasta; ```. and got following Info:. ```; Using GATK jar /mnt/md0/DataProcess/Ranshi/Mutect2/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /mnt/md0/DataProcess/Ranshi/Mutect2/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar FilterMutectCalls -O Filtered.vcf -V Try.vcf.gz -R /home/imp/human.fa/ucsc.hg19.fasta; 09:44:27.763 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/md0/DataProcess/Ranshi/Mutect2/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 21, 2019 9:44:29 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 09:44:29.499 INFO FilterMutectCalls - ------------------------------------------------------------; 09:44:29.500 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.3.0; 09:44:29.500 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:44:29.500 INFO FilterMutectCalls - Executing as imp@imp-WorkStation on Linux v4.15.0-55-generic amd64; 09:44:29.500 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_222-8u222-b10-1ubuntu1~16.04.1-b10; 09:44:29.501 INFO FilterMutectCalls - Start Date/Time: 2019821 094427; 09:44:29.501 INFO FilterMutectCalls - ------------------------------------------------------------; 09:44:29.501 INFO FilterMutectCalls - ------------------------------------------------------------; 09:44:29.502 INFO FilterMutectCalls - HTSJDK Version: 2.20.1; 09:44:29.502 INFO FilterMutectCalls - Picard Version: 2.20.5; 09:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6102#issuecomment-523262338:714,Load,Loading,714,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6102#issuecomment-523262338,1,['Load'],['Loading']
Performance,"@lbergelson I completely agree. GenomicsDB has been a big step up from CombineGVCFs, but it's been a struggle. Here are some notable characteristics of our data, in case anything seems relevant:. - The core data is ~2K WGS (not WXS) rhesus macaque datasets. WXS tends to perform better.; - We imported them into a workspace using GenomicsDB. ; - We regularly append new batches. As the data has grown, we found we need to lower the batch size (maybe 50-100) to make GenomicsDbImport practically work. It's possible a --consolidate type step might help us; however, our earlier efforts to run this resulted in hung jobs.; - We execute these jobs scattered across a cluster that is using a lustre filesystem. We use the non-posix optimization flag.; - We include the unplaced contigs, so our genome is ~2,900 contigs.; - We currently ask GenotypeGVCFs to call sites genome-wide. I've been pondering whether we should mask repetitive regions. This might also have the effect of removing sites with incredibly high numbers of distinct alleles . I'd appreciate any ideas you or the GATK team has.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964343707:271,perform,perform,271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964343707,2,"['optimiz', 'perform']","['optimization', 'perform']"
Performance,"@lbergelson I think it doesn't actually need to implement max and min at all because those are only used in unit tests. Furthermore, the `kmerCounts` in `AssemblyResultSet` themselves are only used in such methods i.e. they don't need to be members at all. You could delete `CountSet` entirely without replacing it. And even if you keep it there it is definitely not crucial for performance and could easily be any old `SortedSet`. So, the options are 1) replace with `TreeSet` or whatever; 2) delete entirely. Which would you like?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2890#issuecomment-397398169:379,perform,performance,379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2890#issuecomment-397398169,1,['perform'],['performance']
Performance,@lbergelson I updated this branch with the new key representation. After some performance runs it appears that these lead to a slightly faster mapping operation and approximately 15% less serialization for the step where they are used. Can you take a look?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396730222:78,perform,performance,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396730222,1,['perform'],['performance']
Performance,"@lbergelson I was referring more to the middle part of the StackOverflow by Daniel Chapman - specifically the [4.1 The ObjectStreamClass Class](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a5082) and [4.6 Stream Unique Identifiers](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a4100):. _If not specified by the class, the value returned is a hash computed from the class's name, interfaces, methods, and fields using the Secure Hash Algorithm (SHA) as defined by the National Institute of Standards._. Now when I look at the `java.io.ObjectStreamClass.java` file for 64-bit JDK7 and JDK8 - from src.zip - both have the same code for the following parts after performing a `diff` - I didn't list all of the lines of code since they are quite long:. ```; public long getSerialVersionUID() {; // REMIND: synchronize instead of relying on volatile?; if (suid == null) {; suid = AccessController.doPrivileged(; new PrivilegedAction<Long>() {; public Long run() {; return computeDefaultSUID(cl);; }; }; );; }; return suid.longValue();; }; ... private static long computeDefaultSUID(Class<?> cl) {; ...very long code which can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:721,perform,performing,721,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['perform'],['performing']
Performance,"@lbergelson I'm not convinced we want to pre-calculate it at construction time, though. The null check is no big deal, and only needed in one place for each field (the method that retrieves or recalulates the cached value).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235127503:209,cache,cached,209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235127503,1,['cache'],['cached']
Performance,"@lbergelson If you query for output after you've terminated the process, the query will fail immediately because the Futures will have been completed with a CancellationException when the pipes were broken by the termination. But I think even that might be subject to a race condition. Previously we were dependent on stdout/stderr for synchronization and error detection, but with the ack fifo and the python exception handler installed, we really aren't anymore. We do need to fix https://github.com/broadinstitute/gatk/issues/5100, and have a better logging integration strategy, but in general I think we should seek to eliminate all use of stdout/stderr except for advisory purposes. On a separate tangent, what I'd really like to do is unify the two PythonExecutors into a single one. All of these features I'm adding like profiling, version checking, logging integration etc., will have to be done in both of them otherwise.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5097#issuecomment-413575698:270,race condition,race condition,270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5097#issuecomment-413575698,1,['race condition'],['race condition']
Performance,"@lbergelson In your opinion, how likely is this feature to cause problems? We do still call `QueryInterval.optimizeIntervals()` to merge intervals in `ReadsDataSource` before starting an iteration, and I think that's the main example of an HTSJDK query interface that can't handle overlapping intervals.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5887#issuecomment-567634113:107,optimiz,optimizeIntervals,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5887#issuecomment-567634113,1,['optimiz'],['optimizeIntervals']
Performance,@lbergelson Performance hit is not enough to worry about.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316495018:12,Perform,Performance,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316495018,1,['Perform'],['Performance']
Performance,"@lbergelson The purpose of `HomogeneousPloidyModel` is so that `getPloidy(int sample)` could return `ploidy` instead of `ploidies[sample]`. The speed difference ought to be negligible, I think. I suppose there's an O(# samples) memory cost, but that can't conceivably matter, can it?. Do you think it's okay to have a single `PloidyModel`, implemented as `HeterogeneousPloidyModel` is currently? That is, do you see any point in the ""optimizations"" in `HomogeneousPloidyModel`?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6082#issuecomment-519354634:434,optimiz,optimizations,434,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6082#issuecomment-519354634,1,['optimiz'],['optimizations']
Performance,"@lbergelson These changes are dependent on a Barclay [PR](https://github.com/broadinstitute/barclay/pull/17) that is not merged yet, so we should at least wait for that snapshot. Ideally, the other two (tiny) Barclay PRs in the queue should also be reviewed/merged, then we could do a release and upgrade to that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2327#issuecomment-271991891:228,queue,queue,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2327#issuecomment-271991891,1,['queue'],['queue']
Performance,"@lbergelson did you get to the bottom of why the `hdfs` provider is loaded, but the `gs` one isn't?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-267992182:68,load,loaded,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-267992182,1,['load'],['loaded']
Performance,"@lbergelson everything I know I learned from there:; http://stackoverflow.com/questions/28939166/error-submitting-a-cloud-dataflow-job. Mine was also in the 4MB range, I switched to loading that file at the worker instead of the client and it worked. So the size limit is probably somewhere between 3 and 4MB.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/595#issuecomment-114594863:182,load,loading,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/595#issuecomment-114594863,1,['load'],['loading']
Performance,"@lbergelson sorry for my late response. I'm currently on vacations but I will try to respond (with some delay) any question.; So, what I'm seeing here (using Github web without having a proper dev env) is that for each interval, it's going to call (in parallel) sample reader function from the configuration =>; ```; final Map<String, FeatureReader<VariantContext>> sampleToReaderMap =; this.config.sampleToReaderMapCreator().apply(; this.config.getSampleNameToVcfPath(), updatedBatchSize, index); ; ```; That's is the first difference from previous implementation. If whatever you have in that function consume lots of memory, that's an issue.; Regarding the thread pool, I'm not seeing it's being starved by chromosome parallel import but it might use extra memory to execute since there is a high load of threads use due to the number of parallel imports.; Worker threads can execute only one task at the time, but the ForkJoinPool doesnt create a separate thread for every single subtask. Instead, each thread in the pool has its own double-ended queue (or deque, pronounced deck) **which stores tasks**. Those are the two things I'm seeing right now without having the chance to debug :(.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542:800,load,load,800,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542,2,"['load', 'queue']","['load', 'queue']"
Performance,@lbergelson thank you for the comment and sorry for my bit late response. I excluded the dependency to the jsr203-s3a and tested that both local- and spark-gatk can access s3a files by dynamically loading it. I also added a new directory `scripts/s3a` for documentation and simple tests for s3a demonstration.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-665484597:197,load,loading,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-665484597,1,['load'],['loading']
Performance,"@lbergelson yes, i meant 2 concurrent tools making indices for the same **input** file. They will both try to write the same index file, with bad consequences.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/365#issuecomment-93539172:27,concurren,concurrent,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/365#issuecomment-93539172,1,['concurren'],['concurrent']
Performance,"@lbergelson yes, i meant the same number of WES samples as input (either merged into one gVCF or imported into one workspace), not size of data on the disk. I did not use --consolidate because your docs seemed to recommend against it. For WES, we performed the GenomicsDB import with default options, which I assume means no batching. We are doing batching on our WGS data, but that is not complete yet and we havent yet tried this for GenotypeGVCFs. I didnt realize --genomicsdb-shared-posixfs-optimizations was an option for GenotypeGVCFs, but will try this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669369551:247,perform,performed,247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669369551,2,"['optimiz', 'perform']","['optimizations', 'performed']"
Performance,"@lbergelson, I don't think that this solution will help in this case, because another error when trying to use `CommandLineProgramTest`is that it extends `BaseTest`, which loads directly a `GenomeLocParser` for a reference that is not present and it blows up in every test. Regarding the `Main` class, because you point it out here, I would like to have some control over `Main` and how it manages things like errors or logging header. Basically all the things that I'm facing at the moment are, apart of this error using the testing framework, is that the framework have tons of mentions to the GATK itself (error messages pointing to the GATK manual page or bundle tools), and little control over which of them should be expose to the final user. Only as an example, I would like to output a line with the name and version of my software and a short notice about the usage of the GATK framework and which version I'm using (for easier maintenance, and contribution if a bug is found).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242802278:172,load,loads,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242802278,1,['load'],['loads']
Performance,"@ldgauthier ; My thinking on not doing the larger tests was that the native code hasn't changed for this support, so performance and functionality shouldn't see anything unexpected. Additionally, we technically do ""incremental import"" whenever the import is batched currently. We're just extending that same paradigm to extend beyond the case where the initial GenomicsDBImport command is used. Of course, all of this is not to say I don't want to do the larger tests...just wondering if we could capture that in a separate issue? @droazen mentioned that there's a tentative plan for a new GATK release this week and we would like to have this feature in there, if you agree. We'll work in parallel on the performance testing you requested. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-518327043:117,perform,performance,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-518327043,2,['perform'],['performance']
Performance,"@ldgauthier ; Performance for single import is the same, and we have CI tests on the genomicsdb side as well. Admittedly we haven't done a lot of large scale performance testing using GenomicsDBImport. Can you elaborate on what you mean by accuracy for large scale imports?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-517850985:14,Perform,Performance,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-517850985,2,"['Perform', 'perform']","['Performance', 'performance']"
Performance,"@ldgauthier @mbabadi Revisiting this in detail in preparation for a Methods meeting presentation, I discovered that much of the initial poor model performance and the issue with the strange GQs on chr1 was actually fixed in #4335 via a seemingly innocuous change. . Prior to the change, the ploidy model used *lexicographical* sorting to determine contig order (see #4374), but the contig order of the per-contig counts matrix was determined by the *sequence dictionary*. This understandably leads to shenanigans, since the model needs to know things like the number of intervals on each contig. After the change, the sequence-dictionary order is properly used everywhere and most of the bad behavior seems to be resolved (not all of the unusual sex genotypes are resolved, but this may be partly due to mosaicism in those samples) . (@mbabadi, not sure if we actually realized this before? I don't recall nor do I see it discussed elsewhere, but if so, then consider this a note of it!). So the problems with this model are not as severe as we initially thought, and thus we can keep this issue at relatively low priority. Nevertheless, the improved model should still yield additional benefits, such as better depth estimates and ploidy qualities, as well as more robustness to unusual sex genotypes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-414852311:147,perform,performance,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-414852311,1,['perform'],['performance']
Performance,"@ldgauthier I think it's probably best to try and get to the bottom of why this variant's qual isn't being adjusted as it's reduced from 7 alleles in the gVCF to 2 alleles in the called VCF. This is, as you guessed, all single-sample. I've attached a reduced gVCF that just includes the variant in question, and the resulting genotyped VCF from running the command line below (and their indices) ; [here](https://github.com/broadinstitute/gatk/files/3059414/gatk-5793-testcase.tar.gz). ```; gatk GenotypeGVCFs \; -R /Work/refseq/hg19/hg19.fasta \; -V HG02568.g.vcf \; -O HG02568.vcf \; -L chr11:6637700-6637800 \; -stand-call-conf 30; ```. A few observations from running the above command but varying the `-stand-call-conf` at that locus, all performed with GATK 4.1.1.0:; - Running with `-stand-call-conf 30` results in a reduction from 7->2 alleles but no change at all in QUAL; - Running with `-stand-call-conf 0` results in the same genotype and QUAL, but another allele squeaks through even though it's not referenced in the genotype; - Running with `-stand-call-conf 100` results in no variants being emitted. Circling back to one of my original statements, I believe the least confusing way for this to work would be to think of it this way:; - If you run with `-stand-call-conf 0` you should see all variants; - If you run with `-stand-call-conf n` you should only lose variants that were previously emitted with `-stand-call-conf 0` that had QUAL < n. That said, it sounds like maybe the problem is less with the filtering on QUAL and more to do with the calculation of the final QUAL that ends up in the VCF?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5793#issuecomment-481273026:744,perform,performed,744,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5793#issuecomment-481273026,1,['perform'],['performed']
Performance,"@ldgauthier I'm in complete agreement, I don't think we should optimize outside the high confidence regions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714549510:63,optimiz,optimize,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714549510,1,['optimiz'],['optimize']
Performance,"@ldgauthier Indexing inputs on the fly is prone to race conditions, so we decided early on to only index outputs on-the-fly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3837#issuecomment-345064459:51,race condition,race conditions,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3837#issuecomment-345064459,1,['race condition'],['race conditions']
Performance,"@ldgauthier It actually looks like the PairHMM one might be a bug. It should be skipping the subset of ones that aren't loadable, but it seems like it think's it's succesfully loading the library but then failing when it actually tries to compute it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5339#issuecomment-592720481:120,load,loadable,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5339#issuecomment-592720481,2,['load'],"['loadable', 'loading']"
Performance,"@ldgauthier It does, but you should always run via the launch script and `--java-options`, since the script sets a number of important system properties, some of which affect performance. Do you know whether the GATK3 HC is able to run on the same bams without running out of memory in 4G/2G/1G?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4272#issuecomment-385489993:175,perform,performance,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4272#issuecomment-385489993,1,['perform'],['performance']
Performance,"@ldgauthier This is why the `--disable-sequence-dictionary-validation` argument exists in `GATKTool`. If you're confident in the compatibility of your inputs, and the checks are too expensive, you can run with that option and (optionally) perform some less strict validation of your own in your `onTraversalStart()` method.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625366653:239,perform,perform,239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625366653,1,['perform'],['perform']
Performance,@ldgauthier This optimization is in the old `DiploidExactAFCalculator`. Can we close the issue?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1437#issuecomment-397359485:17,optimiz,optimization,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1437#issuecomment-397359485,1,['optimiz'],['optimization']
Performance,@ldgauthier gradle caches things in ~/.gradle by default. This is tiny on our systems so it's likely you're exceeding your quota. . set GRADLE_USER_HOME in your bashrc to somewhere with sufficient space. ```; export GRADLE_USER_HOME=<somewherewithspace>; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364#issuecomment-164529488:19,cache,caches,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364#issuecomment-164529488,1,['cache'],['caches']
Performance,"@ldgauthier so I think this is a different issue with overlapping deletions. The original bug was with queries, this is manifesting on loading. This seems to be an issue specific to dealing with the * allele and the fact that the deletion spans the interval specified. We're working on a fix.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5449#issuecomment-484951791:135,load,loading,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5449#issuecomment-484951791,1,['load'],['loading']
Performance,"@ldgauthier's points are well-taken. I'm not quite ready to close this issue, but I believe there are other optimizations that don't have the downsides of this one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2945#issuecomment-310475504:108,optimiz,optimizations,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945#issuecomment-310475504,1,['optimiz'],['optimizations']
Performance,"@ldgauthier, I've performed a pass. I think the _Caveat_ section of the AS_StrandOddsRatio doc could use some attention. Here it is currently (I did not touch it):. ![screenshot 2019-03-07 15 37 39](https://user-images.githubusercontent.com/11543866/53987567-60fc0000-40ef-11e9-8415-d52403f01f83.png). Here is what the rendered javadocs look like now. Let me know what you think.; ![screenshot 2019-03-07 15 37 21](https://user-images.githubusercontent.com/11543866/53987525-475ab880-40ef-11e9-8f0e-24a57a84586a.png). ![screenshot 2019-03-07 15 37 32](https://user-images.githubusercontent.com/11543866/53987535-4d509980-40ef-11e9-835b-65ceee2cdc06.png)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5703#issuecomment-470685949:18,perform,performed,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5703#issuecomment-470685949,1,['perform'],['performed']
Performance,"@lucidtronix @mbabadi @samuelklee I think the best solution would be to establish a single, common Python environment, with a single set of dependencies, that all GATK Python tools depend on. We would establish a single docker image that has all of these dependencies pip installed, and could also include a conda env for the GATK environment for users who don't want to use the docker image. If we could do that, it would eliminate the need load per-tool conda environments. From what I've seen so far based on existing branches, the two environments we need (gCNV and CNN-VQSR) don't look that far apart in terms of dependencies. gCNV is using Theano, and CNN Tensorflow, but the rest looks [pretty close](https://docs.google.com/a/broadinstitute.org/spreadsheets/d/1RV7--uBQ0ctlXzMH09cmr0VimpZYIU68DdxJzE60y-c/edit?usp=sharing). So a strawman proposal for the main components for a common environment would be:. Python 3.6; Numpy >= 1.13.1; Scipy 1.0.0; Theano .0.9.0; Tensorflow 1.4.0; Pymc3 3.1; Keras 2.1.1. Can you all chime on on whether you think we can converge in a single environment ? If so, it would greatly simplify things, and we can start with getting a docker image built for running travis tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3692#issuecomment-348188451:442,load,load,442,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3692#issuecomment-348188451,1,['load'],['load']
Performance,"@lucidtronix Any suggestions on this? I don't recall ever seeing this before, but we've hit it about 5 times in tests in the last week or so. It looks like its coming from this [jit assembler](https://github.com/intel/caffe/blob/a3d5b022fe026e9092fc7abc7654b1162ab9940d/xbyak/xbyak.h#L229) code used by the intel-optimized tensorflow ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6307#issuecomment-566108578:313,optimiz,optimized,313,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6307#issuecomment-566108578,1,['optimiz'],['optimized']
Performance,"@magicDGS . I am afraid this is not easy. I didn't write the binding (@tedsharpe did), but I would asseme the limitation comes from bwa mem itself, not the binding, as the binding is a thin wrapper that delegates the loading of the index files (or the image that combines all 5 index files in this case) to bwa. . The SV team here have a script (`scripts/sv/default_init.sh`) that when the Spark cluster is created and initialized, the image file is distributed to all walker nodes. Spark clusters other than Google's Dataproc would probably allow you to provide scripts as initialization actions as well. On the other hand, there seem to be a `--files` argument that you can append to your cmd line arguments which yarn will parse and distribute the provided local file to all nodes, though in this case it will be very inefficient considering the image file's size.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312362074:217,load,loading,217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312362074,1,['load'],['loading']
Performance,"@magicDGS @heuermh It sounds like there are definite advantages to switching to SLF4j. Our fear is that the switch will end up resulting in confusing class loading issues in different spark environments. We're just beginning a round of spark performance testing and evaluation under a pretty tight deadline, and we don't want to introduce any surprises. . We'd be happy to look at a pull request, but we might delay until the end of quarter to fully test / merge it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-261547892:156,load,loading,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-261547892,2,"['load', 'perform']","['loading', 'performance']"
Performance,@magicDGS Doesn't the static block run when a subclass of Main is loaded as well?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324388854:66,load,loaded,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324388854,1,['load'],['loaded']
Performance,@magicDGS I provide some example data for a tutorial at <https://gatkforums.broadinstitute.org/gatk/discussion/7156/howto-perform-local-realignment-around-indels>. Search the page for ` tutorial_7156.tar.gz`. I showcase illustrative sites within the tutorial and also in <https://software.broadinstitute.org/gatk/blog?id=7847>. I'm actually new to test data so what cases are you hoping to test with the data? The snippet in the tutorial data is much larger than you need so it would be good narrow down the test case.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-314886000:122,perform,perform-local-realignment-around-indels,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-314886000,1,['perform'],['perform-local-realignment-around-indels']
Performance,"@magicDGS I'm still missing why this wouldn't work for downstream projects (as long as they load Main or some Main-derived class). I think the owner config issue is different; for locale, we need to always force US. Can you verify this, or maybe provide more details about what case doesn't work ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324622945:92,load,load,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324622945,1,['load'],['load']
Performance,"@magicDGS Overall, the documentation changes are good. A few more minor comments: ; - If these are classes that are really pegged to the usage of LIBS, then you may want to mention that in the javadocs for the class. A comment along the lines of ""these classes are fairly low-level, developers should probably confirm that their changes do not belong in a higher-level calss such as LIBS"". ; - I am okay with an informal test of the speed, even if you just look at some logs. From the review, it looks like behavior of higher-level API calls will be unchanged. >I think that because the LocusIteratorByState is already splitting by sample, that can improve even more performance, because it will come directly in the state where it can be used by-sample in an efficient way. And maybe, if the tool does not require to split by sample at all, we can add an option to disable that behavior while creating the tracker. Agreed. I do not think you need to worry about the flag, for now. If you'd like, file an issue, but I think it is low priority for us. I tend to be worried about new developers or contributors getting lost in the codebase. And many do not have experience w/ GATK3. Hence, the documentation about when to use the class and why it exists. A couple additional things:; - I found a typo (see my comment); - Can you document the `presorted` parameter? Make sure to mention that if a developer specifies false, and sorting is needed, it will be done under-the-hood.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332215187:667,perform,performance,667,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332215187,1,['perform'],['performance']
Performance,"@magicDGS Sorry for the delayed reply, I had to see what direction the `HaplotypeCaller` branch would take before I could answer your post above. In order to get the `HaplotypeCaller` performance up to acceptable levels we've had to make some changes to the traversal that have caused it to diverge quite a bit from the idea of a `SlidingWindowWalker` in this branch. Also, the way `SlidingWindowWalker` handles the `intervalsForTraversal` (using them to select fixed-size windows) is not compatible with what the `HaplotypeCaller` currently requires. As a result, I recommend that we merge your `SlidingWindowWalker` in as a separate traversal rather than trying to reconcile it with the `HaplotypeCaller` branch and mutate it into something that might not be as useful to you. Fortunately, walkers in GATK4 are simple enough that it's perfectly fine to have several similar-but-subtly-different walker types, provided they all serve actual use cases. I'll",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-204134447:184,perform,performance,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-204134447,1,['perform'],['performance']
Performance,"@magicDGS The HaplotypeCaller traversal has undergone some changes in the past few weeks to improve performance and bring the output of the tool closer to GATK3. There is now an `AssemblyRegionWalker` that divides the intervals into active and inactive regions, in a greatly simplified version of the GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:100,perform,performance,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,1,['perform'],['performance']
Performance,"@mbabadi Ah, well file-based I/O would be the simplest option of all, of course, and should definitely be considered as a candidate solution to this ticket, particularly if you've already tried it and found the performance penalty to be minimal for your use case (@cmnbroad take note). The division of labor you describe between Java and Python sounds great, by the way -- exactly the sort of approach I was hoping you'd implement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3698#issuecomment-337319853:211,perform,performance,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3698#issuecomment-337319853,1,['perform'],['performance']
Performance,"@mbabadi I've updated my PR to use miniconda3. @mbabadi @lucidtronix @samuelklee I think we should aim for tools that at least run out-of-box, without depending on any out-of-band configuration other than the conda env. On top of that we can provide guidance/configs for users on how to enable further optimizations, like g++. Does that sound like an achievable goal ?. As for the docker, we're going to have strike the right balance between image bloat and performance(including test performance). I think we're around 4+ gig now, and counting. Before the Python integration we were at 1.9G, and trying to find ways to reduce it. So lets see where we wind up but keep that in mind. Finally, we need to find a way to install the (GATK) python package(s) without depending on access to the GATK repo. Right now I think the gCNV branch has a ""pip install from source"" added to the conda env .yml. That will work on the docker at the moment (and thus on travis), but that won't work for non-docker users how don't have source/repo access. Also, one of the proposals to reduce the size of the docker is to remove the repo clone that is currently there. My proposal is that we change the gradle build to create an archive/zip of the python source (this would include the VQSR-CNN package code as well as gCNV kernel). We can then copy that on to the docker image, and pip-install it from the copy. That would retain the ability to always run travis tests based on the code in the repo, and also keep the nightly docker image in sync. We'll also have deliver the archive as an artifact somehow (perhaps including PyPi) for non-docker users.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277:302,optimiz,optimizations,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277,3,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1069). This is a long shot, but the idea is to be able to learn biases from mixed N/T cohorts. In a way, this is similar to semisupervised learning where the _stiff_ integer-state HMM on normal samples lead the way of learning biases (as a matter of imposing a strong copy-neutrality prior), and tumor samples along with a _loose_ infinite HMM provide additional (though generalically less) statistical power. Weak tumor-in-normal contamination can be handled using an adaptive integer-state HMM where the quantizied copy ratio states are chosen uniformly, though, adaptively. In the future, we must move toward a generic CLI tool called something like FancySchmancyCNVCaller that can perform the following tasks in its idealized form:. - create PoN and make calls from normals; - create PoN and make calls from tumors (possible with iHMM); - create PoN and make calls from mixed normals and tumors (possible with iHMM); - make calls from a given model on normals; - make calls from a given model on tumors; - make calls from a given model on mixed normals and tumors. The tool would then additionally take a sample annotation table (normal, tumor) and perform its job. For the first release, all samples have be annotated as normal; otherwise, an UnsupportedFeatureException is thrown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3004:781,perform,perform,781,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3004,2,['perform'],['perform']
Performance,@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1071). It is desirable to perform an explicit bookkeeping of ICG function evaluation to ensure that gCNV is not performing unnecessary EM evaluation in each cycle.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3007:122,perform,perform,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3007,2,['perform'],"['perform', 'performing']"
Performance,@mbabadi commented on [Thu Jan 12 2017](https://github.com/broadinstitute/gatk-protected/issues/853). - [ ] thread capping in `SynchronizedUnivariateSolver`; - [ ] faster calculation of prior copy number contribution to log likelihood in `IntegerCopyNumberExpectationsCalculator`; - [ ] optimize `CoverageModelEMWorkspace.replaceMaskedEntries` using Nd4j native ops,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2931:287,optimiz,optimize,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2931,1,['optimiz'],['optimize']
Performance,"@mbabadi commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1097). Devin McCabe discovered a _bug_ (read: bad model behavior) in TargetCoverageSexGenotyper. The bug was discovered by feeding the tool with coverage data on autosomes + X chromosome (no Y chromosome). Since the X chr in XX samples has 2x ploidy of X in XY samples, one expects the tool to be able to make the correct inference. However, the tool genotyped all samples as XX (see the attached figure -- left: autosome+X+Y, right:autosome+X). ![unnamed](https://cloud.githubusercontent.com/assets/15305869/26426249/ce3ffb68-40a5-11e7-8002-6ea4f8513eea.png). A naive calculation of the relative X ploidy, i.e. calculating X_pcov = (X_total_read_counts / autosome_total_read_count) for all samples, performing a 2-mean clustering, and dividing the X_pcov by the lower ploidy cluster mean reveals that indeed, the X conting has twice more coverage on _average_ in XX samples:; ![image](https://cloud.githubusercontent.com/assets/15305869/26426348/2b2d6982-40a6-11e7-8eca-e93916bfc80c.png). Further investigation shows that the wrong behavior of TargetCoverageSexGenotyper stems from the lack of robustness of Poisson regression to outliers: there are a number of targets in the X contig with anomalously high coverage (200x median!). In the absence of Y coverage data (and bias adjustment), higher ploidy genotypes are always favored (in this case, XX). Solution: either filter read counts for outliers before calculating Poisson log likelihoods, or simply use the naive median-based ploidy estimates and perform genotyping on the estimated ploidies (rather than target-resolved read counts). The latter is proven to be robust to outliers. Update: it turns out that the issue can be fixed by simply taking into account bait count as a multiplicative bias. Otherwise, the distribution of raw read counts is multimodal and far from Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/2651",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3015:796,perform,performing,796,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3015,1,['perform'],['performing']
Performance,@meganshand - FYI a small bug fix and there is another one (in t0 parsing) to follow. This one has a very tiny positive effect on indel performance,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8171#issuecomment-1403552050:136,perform,performance,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8171#issuecomment-1403552050,1,['perform'],['performance']
Performance,"@meganshand Here's a quick example:. ![image](https://user-images.githubusercontent.com/11076296/158385742-20a3303b-d8ce-4335-b42f-622da9bfa8d3.png); ![image](https://user-images.githubusercontent.com/11076296/158385777-6174f8b8-7abb-4b31-92d1-11cc8064854b.png). Note that the malaria data used was pretty small: chr1-2 training (~20k positive training/truth variants, ~50k negative training variants; note also that the threshold for determining negative training was not tuned---a threshold corresponding to a 98% truth sensitivity was arbitrarily chosen), chr3 validation (~50k variants), and chr4-6 test (~150k variants). The LL score is calculated from a validation set held out from the training/truth positives used to train the model, while the F1 score is calculated using ""orthogonal truth"" positives/negatives determined using 3 families of ~30 trios each. However, there's some arbitrariness in how we define the boundary for the latter positives/negatives, and hence some arbitrariness in the F1 score itself. But I'd expect using gold-standard GIAB truth would be more straightforward. Not sure how much we can conclude, but that the validation and test F1s are similar and that the validation LL score isn't *too* far off are encouraging. That said, there is a pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian con",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:473,tune,tuned---a,473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,1,['tune'],['tuned---a']
Performance,"@meganshand commented on [Thu Feb 16 2017](https://github.com/broadinstitute/gatk-protected/issues/907). Using a tiny bam file that I typically use for testing while running the CNV wdl on the cloud, I got the following errors (the tiny file is here: `gs://broad-dsde-methods/takuto/test_files/small_NA12878_hg19.bam`):. 1. The output tsv from TumorNormalizeSomaticReadCounts contained NaNs; 2. TumorPerformSeg threw the following error:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; [February 16, 2017 3:23:02 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation --tangentNormalized /cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output small_NA12878.seg --log2Input true --alpha 0.01 --nperm 10000 --pmethod hybrid --minWidth 2 --kmax 25 --nmin 200 --eta 0.05 --trim 0.025 --undoSplits none --undoPrune 0.05 --undoSD 3 --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [February 16, 2017 3:23:02 PM UTC] Executing as root@3addd2d7b373 on Linux 3.16.0-0.bpo.4-amd64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; [February 16, 2017 3:23:04 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=185597952; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/cromwell_root/tmp/root/Rlib.5210694187065743072';source('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2944:583,Perform,PerformSegmentation,583,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944,1,['Perform'],['PerformSegmentation']
Performance,@mishaploid and @spikebike. 4.1.8.0 has a new option - `--genomicsdb-shared-posixfs-optimizations` for GenomicsDBImport that disable file locking and minimize writes to NFS. We are interested to know if this option helps your use case even as we continue making performance improvements. Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-651343677:84,optimiz,optimizations,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-651343677,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"@mishaploid, I am assuming the 295 to be single sample vcfs. What do the `data/processed/scattered_intervals/0004-scattered.intervals` look like? Are the intervals very small? Have you tried larger intervals?. @spikebike, thanks for the systemtap output. We do have large internal buffers to help with this type of usage, but will revisit the code to figure out the behavior you are seeing. We do have some experimental optimizations not rolled out yet for writing minimally to shared filesystems. Would you be able to run your tests if we create a gatk branch next week with those changes?. @ldgauthier, I think the Hail team used multi sample vcfs as well. We do have some optimizations(work-in-progress) for importing multi sample vcfs that will get rolled out in the next GenomicsDB release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-595405471:420,optimiz,optimizations,420,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-595405471,2,['optimiz'],['optimizations']
Performance,"@mlathara As suggested, I removed all MNPs from normal vcf files and changed the bed file format and it worked. But it is not going beyond chromosome 1. Here are the stack trace:. 15:44:02.495 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/akansha/vivekruhela/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 16, 2021 3:44:02 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:44:02.750 INFO GenomicsDBImport - ------------------------------------------------------------; 15:44:02.750 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.9.0; 15:44:02.750 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:44:02.750 INFO GenomicsDBImport - Executing as akansha@sbilab on Linux v4.4.0-169-generic amd64; 15:44:02.751 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-8u265-b01-0ubuntu2~16.04-b01; 15:44:02.751 INFO GenomicsDBImport - Start Date/Time: January 16, 2021 3:44:02 PM IST; 15:44:02.751 INFO GenomicsDBImport - ------------------------------------------------------------; 15:44:02.751 INFO GenomicsDBImport - ------------------------------------------------------------; 15:44:02.751 INFO GenomicsDBImport - HTSJDK Version: 2.23.0; 15:44:02.751 INFO GenomicsDBImport - Picard Version: 2.23.3; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2 ; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:44:02.752 INFO GenomicsDBImport - Deflater: IntelDeflater; 15:44:02.752 INFO GenomicsDBImport - Inflater: IntelInflater; 15:44:02.752 INFO GenomicsDBImport ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811:220,Load,Loading,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811,1,['Load'],['Loading']
Performance,"@mlathara I think we're talking a bit in circles. The main use case I foresee for a generic split/merge tool would be to allow parallelized processing. I cant say there wouldnt be other uses I'm not seeing now (in the VCF world, SelectVariants is an extremely useful tool), but i dont have a specific use-case for GenomicsDB subsetting today beyond this. . I would point out this rapidly gets into specifics and quirks of any one user's infrastructure. I dont actually mind copying the GenomicsDB workspace prior to appending to it, because processing occurs on shared lustre space, while our permanent data lives on other disk space. Therefore we would probably do a copy no matter what. I agree you dont want to develop our one person's infrastructure. . The only aspect that gives me pause on your plan regarding split jobs is that GATK doesnt provide the scheduler. Sure there used to be queue and I gather GATK pushes WIDL/Cromwell (unless this changed), but we never used these. If GATK is not trying to provide the scheduler (which is better), does this really just look like: . 1) kick off X independent jobs for GenomicsDB/append; 2) each job specifies the interval(s) on which to operate; 3) Each job has no knowledge of the other jobs; 4) each job writes it's output to the same workspace; 5) Presumably there is something in place so jobs can run concurrently. This must be the new feature?. I imagine this could work. It does obligate one to have/use some kind of shared disk space, which we can handle, but could be a negative for some.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641469405:892,queue,queue,892,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641469405,2,"['concurren', 'queue']","['concurrently', 'queue']"
Performance,"@mlathara Let's discuss our options at our next meeting...if the problem is common enough, and a proper fix is not coming in the short term, we might want to consider making the VCF codec the default despite the performance hit.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6667#issuecomment-646232210:212,perform,performance,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6667#issuecomment-646232210,1,['perform'],['performance']
Performance,"@mlathara The nodes have considerably more (256 or so). is there any rule or thumb or guidance on expected memory needs based on number of gVCFs and/or type of input (WES vs WGS)?. I do think you might be onto something though. Out default cluster submission code takes our slurm job memory request, subtracts only a few GB and passes the remainder to -Xmx/Xms. I will update to leave more buffer as you suggest. Our cluster happens to be undergoing maintenance this week, so this particular job was killed. I'll update the GATK version, add --genomicsdb-shared-posixfs-optimizations, and adjust the memory. One other thing: i noticed GenomicsDBImport is not nearly as verbose in logging as typical GATK tools. Is that expected, or a symptom of whatever problem we're having?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-656259475:570,optimiz,optimizations,570,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-656259475,1,['optimiz'],['optimizations']
Performance,@mmorgantaylor was seeing . | Caused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: INVALID_ARGUMENT: Rows must be specified. Due to flush() trying to write an empty buffer,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7627:51,concurren,concurrent,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7627,1,['concurren'],['concurrent']
Performance,"@mwalker174's idea (my interpretation, might be slightly off):; SV pipeline performs local assembly at active regions where seemingly a structural variant is present. Pathogen integration into (human) host generates similar signal and it makes sense for the SV pipeline to help identify such sites.; A tool for extracting the locally assembled contigs and their alignments (if any) that potentially useful for this purpose is desired.; And since we output VCF for SV, the potential location of integration would be helpful too.; But for this feature we need #3192 dealt with first, which is now being handled by PR #3457 .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3458:76,perform,performs,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3458,1,['perform'],['performs']
Performance,"@nalinigans Mixed results so far. I'm running the new consolidate tool, per chromosome as before. I ran it with defaults (no custom arguments). It is running longer than previously, but chr 1, the largest, died after consolidating 2 attributes. This job had 248G of RAM allocated. Are there optimizations you'd suggest?. ```; 02 Apr 2022 16:34:31,433 DEBUG: 	[April 2, 2022 4:34:31 PM PDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 5,993.06 minutes.; 02 Apr 2022 16:34:31,438 DEBUG: 	Runtime.totalMemory()=178017796096; 02 Apr 2022 16:34:31,443 DEBUG: 	Tool returned:; 02 Apr 2022 16:34:31,448 DEBUG: 	true; 02 Apr 2022 16:34:34,663 INFO : Will consolidate the workspace using consolidate_genomicsdb_array; 02 Apr 2022 16:34:34,723 INFO : Consolidating contig folder: /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb/1$1$223616942; 02 Apr 2022 16:34:34,748 INFO : 	/home/exacloud/gscratch/prime-seq/bin/consolidate_genomicsdb_array -w /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb --shared-posixfs-optimizations -a 1$1$223616942; 02 Apr 2022 16:34:34,754 DEBUG: using path: /home/exacloud/gscratch/prime-seq/bin:/home/exacloud/gscratch/prime-seq/bin/:/home/exacloud/gscratch/prime-seq/java/current/bin/:/home/exacloud/gscratch/prime-seq/bin/:/usr/local/bin:/usr/bin; 02 Apr 2022 16:34:35,059 DEBUG: 	16:34:35.059 info consolidate_genomicsdb_array - pid=34848 tid=34848 Starting consolidation of 1$1$223616942 in /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb; 02 Apr 2022 16:34:36,091 DEBUG: 	Using buffer_size=10485760 for consolidation; 02 Apr 2022 16:34:36,097 DEBUG: 	Number of fragments to consolidate=26; 02 Apr 2022 16:34:36,101 DEBUG: 	Sat Apr 2 16:34:36 2022 Memory stats beginning consolidation size=483MB resident=379MB share=6MB text=13MB lib=0 data=",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1087750975:291,optimiz,optimizations,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1087750975,1,['optimiz'],['optimizations']
Performance,"@nalinigans OK, so most of these jobs are still going (we run per contig); however, one just died as follows:; ```; 03 Mar 2022 12:49:28,390 INFO : 	/home/exacloud/gscratch/prime-seq/bin/consolidate_genomicsdb_array -w /home/exacloud/gscratch/prime-seq/workDir/0950f56c-7565-103a-a738-f8f3fc8675d2/Job3.work/WGS_1852_consolidated.gdb --shared-posixfs-optimizations --segment-size 32768 -a 3$1$185288947; 03 Mar 2022 12:49:28,444 DEBUG: 	12:49:28.444 info consolidate_genomicsdb_array - pid=147768 tid=147768 Starting consolidation of 3$1$185288947 in /home/exacloud/gscratch/prime-seq/workDir/0950f56c-7565-103a-a738-f8f3fc8675d2/Job3.work/WGS_1852_consolidated.gdb; 03 Mar 2022 12:58:29,209 DEBUG: 	Using buffer_size=32768 for consolidation; 03 Mar 2022 12:58:29,222 DEBUG: 	12:58:29 Memory stats(pages) beginning consolidation size=16404102 resident=16377455 share=1814 text=3530 lib=0 data=16375282 dt=0; 03 Mar 2022 12:58:29,228 DEBUG: 	12:58:29 Memory stats(pages) after alloc for attribute=END size=16404138 resident=16377465 share=1821 text=3530 lib=0 data=16375318 dt=0; 04 Mar 2022 16:03:57,025 WARN : 	process exited with non-zero value: 137; ```. Is there any information from this, or information i could gather, that's helpful here?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1059616021:351,optimiz,optimizations,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1059616021,1,['optimiz'],['optimizations']
Performance,"@nalinigans Tests passed, but it's worth pointing out that the new `testWriteToAndQueryFromGCS()` test took 9 minutes to complete, which seems very slow. Possible performance issue?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-430760216:163,perform,performance,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-430760216,1,['perform'],['performance']
Performance,"@nalinigans This iteration is on a smaller input (~600 WGS samples). Based on the info below, do you suggest changing --buffer-size or --batch-size?. To your questions:. In chromosome 1's folder (1$1$223616942), there are 26 fragments (the GUID-named folders). The sizes of book_keeping files are:. 168M; 131M; 155M; 149M; 136M; 142M; 216M; 147M; 150M; 134M; 127M; 75M; 172M; 122M; 207M; 122M; 581M; 149M; 150M; 141M; 149M; 143M; 143M; 165M; 160M; 163M. The last job failed an OOM error (the job requested 256GB and the slurm controller killed it). This is the command and output (with timestamps):. ```; 10 Apr 2022 01:56:21,275 INFO : 	/home/exacloud/gscratch/prime-seq/bin/consolidate_genomicsdb_array -w /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb --shared-posixfs-optimizations -a 1$1$223616942. 10 Apr 2022 01:56:21,556 DEBUG: 	01:56:21.556 info consolidate_genomicsdb_array - pid=146087 tid=146087 Starting consolidation of 1$1$223616942 in /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb; 10 Apr 2022 01:56:22,385 DEBUG: 	Using buffer_size=10485760 for consolidation; 10 Apr 2022 01:56:22,391 DEBUG: 	Number of fragments to consolidate=26; 10 Apr 2022 01:56:22,396 DEBUG: 	Sun Apr 10 01:56:22 2022 Memory stats beginning consolidation size=483MB resident=379MB share=6MB text=13MB lib=0 data=371MB dt=0; 10 Apr 2022 01:56:22,400 DEBUG: 	Sun Apr 10 01:56:22 2022 Memory stats Start: batch 1/1 size=503MB resident=379MB share=6MB text=13MB lib=0 data=391MB dt=0; 10 Apr 2022 01:59:22,970 DEBUG: 	Sun Apr 10 01:59:22 2022 Memory stats after alloc for attribute=END size=25GB resident=25GB share=7MB text=13MB lib=0 data=25GB dt=0; 11 Apr 2022 03:36:27,065 WARN : 	process exited with non-zero value: 137; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1095228878:838,optimiz,optimizations,838,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1095228878,1,['optimiz'],['optimizations']
Performance,"@nalinigans We have a very large WGS dataset (<2K subjects). We incrementally add to it over time. After each new batch of data is added, we typically run GenotypesGVCFs. We have run GenotypesGVCFs on prior iterations of this GenomicsDB workspace; however, we have never run it on this particular workspace, after the addition of new samples. You can get those files here: . https://prime-seq.ohsu.edu/_webdav/Labs/Bimber/Collaborations/GATK/%40files/Issue7674/. I think I was mistaken above. According to the job logs, we did run GATK GenomicsDBImport v4.2.5.0 when we did our last append. However, prior append operations would have used earlier GATK versions. I believe we have 79 fragments. We rarely do --consolidate, primarily because those jobs essentially never finish. We've had a lot of issues getting GATK/GenomicsDB to run effectively on this sample set. We have settled on doing the GenomicsDBImport/append operation with a moderate batch size. I realize newer GATK/GenomicsDB versions have been addressing performance, and it is possible we should re-evaluate this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042101804:1020,perform,performance,1020,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042101804,1,['perform'],['performance']
Performance,"@nalinigans, to answer your questions as best as I can (sorry, I'm a bit of a novice); 1. the size of the book keeping file is: 20,779,823bytes (~21Mb); 2. I believe it is a shared Posix FS; Running this option created a similar error except this time there was: ""cannot load book-keeping: Reading MBR failed"" (output below) ; 3. available memory ~89Gb; 4. I am running -Xmx16g java option. Newest output:; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 16:26:34.912 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2021 4:26:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:26:35.417 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:26:35.418 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 16:26:35.418 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:26:35.420 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 16:26:35.421 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_275-b01; 16:26:35.421 INFO GenotypeGVCFs - Start Date/Time: January 6, 2021 4:26:34 PM CST; 16:26:35.421 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:26:35",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:271,load,load,271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,3,"['Load', 'load', 'optimiz']","['Loading', 'load', 'optimizations']"
Performance,@nh13 I wrote a test for your branch (its very simple it just reruns the gvcf mode tests with --disable-optimizations enabled) that should work for your branch. Its in the branch je_addTestForDisableOptimizations. Since you submitted this PR from your own clone of the GATK I cannot push this onto the branch as it stands. Would you be able to copy it into this branch?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7125#issuecomment-793077846:104,optimiz,optimizations,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7125#issuecomment-793077846,1,['optimiz'],['optimizations']
Performance,"@nvnieuwk, have you tried the following to circumvent the inode limit issue?. - use of the `merge-contigs-into-num-partitions` option with GenomicsDBImport; ```; --merge-contigs-into-num-partitions <Integer>; Number of GenomicsDB arrays to merge input intervals into. Defaults to 0, which disables; this merging. This option can only be used if entire contigs are specified as intervals.; The tool will not split up a contig into multiple arrays, which means the actual number of; partitions may be less than what is specified for this argument. This can improve; performance in the case where the user is trying to import a very large number of contigs; ```; Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8297#issuecomment-1557405072:564,perform,performance,564,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8297#issuecomment-1557405072,1,['perform'],['performance']
Performance,"@owensgl Sorry you're running into problems. We typically speed up the process by running multiple GenotypeGVCF processes in parallel, subsetting by genomic intervals. GenotypeGVCFs isn't really multicore, you'll probably be best off giving each process 1 or 2 cores. (You'll see better performance with 2 since java has parallel garbage collection, but it might be more cost effective to run twice as many slower processes...) If you do that you'll want to run with `--only-output-calls-starting-in-intervals` enabled in order to avoid problems on the edges of intervals. . Things tend to bog down with many highly multi-allelic sites. If you have a population with very high diversity you may be hitting lots of sites like that. I'm not sure why it's as slow as you say it is though. It should be faster than 800bp / 30 minutes even with old qual. If you could provide a subset of your data we might be able to profile and see if there's some pathological case we're not handling well. I believe new-qual handles multi-allelic sites more efficiently which I suspect is why it's going faster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161#issuecomment-358054994:287,perform,performance,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161#issuecomment-358054994,1,['perform'],['performance']
Performance,@personalis commented on [Thu Jun 23 2016](https://github.com/broadinstitute/gatk-protected/issues/587). We got the following error when running gatk-launch FastqToSam:. java.lang.IllegalArgumentException: Self-suppression not permitted; at java.lang.Throwable.addSuppressed(Throwable.java:1043); at org.broadinstitute.hellbender.tools.picard.sam.FastqToSam.doWork(FastqToSam.java:163); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:61); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); Caused by: java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.l,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2868:900,Load,LoadSnappy,900,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868,1,['Load'],['LoadSnappy']
Performance,"@pgrosu Here's how I did it, although this was using the old mapred Hadoop 1.0 API so would be deprecated now. This class adds a bunch of BWA index files to the distributed cache:. https://github.com/cwhelan/cloudbreak/blob/master/src/main/java/edu/ohsu/sonmezsysbio/cloudbreak/command/CommandBWAPairedEnds.java. By calling into the addDistributedCacheFile method here:. https://github.com/cwhelan/cloudbreak/blob/master/src/main/java/edu/ohsu/sonmezsysbio/cloudbreak/command/BaseCloudbreakCommand.java. This mapper class can then just reference them as local files on the executing node:. https://github.com/cwhelan/cloudbreak/blob/master/src/main/java/edu/ohsu/sonmezsysbio/cloudbreak/mapper/BWAPairedEndMapper.java. Feel free to poke around that repo if you find it interesting.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/567#issuecomment-112864194:173,cache,cache,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-112864194,1,['cache'],['cache']
Performance,"@pgrosu I'm not sure that actually does explain what's happening. If I read it correctly it's saying that some objects were serialized, then the class was changed, and the old saved objects were no longer loadable. . Our current situation is that we serialized some objects, and deserializing them with the exact same class failed. The first situation is expected, the second one should not happen. . Is it possible that we are using different jvms on our local machine vs on the google cluster? So classes are serialized locally and then a jvm dependent hashcode is different at the other end?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107699331:205,load,loadable,205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107699331,1,['load'],['loadable']
Performance,"@rhysnewell The default settings for the GATK downsampler are optimized for germline calling on typical (ie., Illumina) short reads data. The goal of the downsampler is to control memory usage at dubious sites where the aligner happens to place huge numbers of low-mapping-quality reads, without harming the quality of the variant calls in any meaningful way. By default, if there are more than 50 reads starting at the exact same alignment start position, GATK will start to randomly eliminate reads -- this gives a maximum total coverage for each locus of `50 * read_length`, which is more than sufficient for typical whole-exome or genome germline calling. For other applications, such as amplicon sequencing (where the reads inherently all tend to start at the same positions) or microbial calling, these defaults will not work well, and you are likely better off disabling the built-in downsampler completely, and using a different downsampling approach if memory use is an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7873#issuecomment-1139176006:62,optimiz,optimized,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7873#issuecomment-1139176006,1,['optimiz'],['optimized']
Performance,"@rsasch my thought was to keep it around for now, but in the future we could add a step that removes it once we know the samples are safely loaded. It could also be that we get rid of the ""is_loaded' flag in sample_info instead since this data is more detailed",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7573#issuecomment-983938451:140,load,loaded,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7573#issuecomment-983938451,1,['load'],['loaded']
Performance,"@samuelklee DRAGEN STRE model doesn't actually make any alterations to the smith waterman parameters or how they work, it just works by adjusting the indel gap penalties that are used for the PairHMM. At one point we were concerned about SW parameters being different with dragen but as it turns out the biggest visible effect of the SW parameters on the output (the alignment we perform after haplotypes discovery) is irrelevant since they don't realign their reads internally. We kept the default gatk alignment behavior and thus the SW parameters that are used (for dangling head recovery which I believe are the old arguments) still match. As far as unifying the parameters I suspect it could be done though one wonders if there aren't risks where the different contexts in which we use the parameters will not perform as well with a unified set. Speculation on my part though. I agree with David that we should be cautious about making changes that will affect the HaplotypeCaller before November. . I support including an argument in any case (possibly multiple) to include the SW parameters. I would actually advocate we read these files in as tables of parameters where you simply point to on the command line to configure new parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993:380,perform,perform,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993,2,['perform'],['perform']
Performance,"@samuelklee I wrote some benchmarks for the exact combinatorics and you were right, my optimization was pointless. Although the `CombinatoricsUtils` method does explicitly multiply out instead of using cached factorials 1) the number of multiplications is only min(ploidy, (allele count - 1)), and 2) it actually takes quite a while (much larger than reasonable ploidy and allele count) for multiplication to take longer than the memory access of stored factorials. . I have removed this error in judgment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066873168:87,optimiz,optimization,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066873168,2,"['cache', 'optimiz']","['cached', 'optimization']"
Performance,"@samuelklee I'd say lets leave 2.1 base image up there for now, and yes on the cache clearing. Once tests pass with the cache cleared it should be good to merge. Feel free to squash and rebase if you like.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408453109:79,cache,cache,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408453109,2,['cache'],['cache']
Performance,"@samuelklee It wasn't just a rebase, it was a complete rewrite because the old code had since become completely entangled with DRAGEN code. But I did it! Everything is passing, the code is dramatically simpler, and it's even a bit faster. I have done my best to make a coherent commit history. I would recommend reviewing one commit at a time in side-by-side diff mode. Note that some commits rip out old code and replace it with pseudocode, deferring the new code to a later commit. Other commits tell a story of what all the different caches meant in order to motivate the simpification of later commits. The baroqueness of the old code was motivated by three considerations:; * cache-friendliness -- traversing all arrays by incrementing the innermost index, reads. This is absolutely essential.; * flattening 3D arrays into 1D arrays. This was a premature optimization.; * Precomputing addition operations -- this was misguided. The DRAGEN code relied on these caches in a rather complex way, which fortunately turned out not to be necessary and which could be dramatically simplified. My notes on tracking all the variables from the parent genotype calculator down to the DRAGEN calculator are in this google doc: https://docs.google.com/document/d/1v6s57mUAwfj38nL3VdktjA059kYBkJfokq18IDy79E8/edit?usp=sharing. Good luck and don't hesitate to ask me to explain anything.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1023647476:537,cache,caches,537,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1023647476,4,"['cache', 'optimiz']","['cache-friendliness', 'caches', 'optimization']"
Performance,"@samuelklee My plan for the deprecation was to do it concurrently with (or just before) the merge of this PR. As this gets closer to a final merge, we can coordinate on that aspect.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1939712690:53,concurren,concurrently,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1939712690,1,['concurren'],['concurrently']
Performance,"@samuelklee Now it's back to you. I agreed with and implemented all of your suggestions. `GenotypeIndexCalculator`, `GenotypeAlleleCounts`, `GenotypeLikelihoodsCalculator` and `GenotypeLikelihoodsCalculator` (renamed to `GenotypesCache`) now have clearly-defined roles. A lot of premature optimization is gone.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1068695779:289,optimiz,optimization,289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1068695779,1,['optimiz'],['optimization']
Performance,"@samuelklee Performance was comparable on one exome bam using the 5M sites file. Each took ~15 minutes. Sample size of 1, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3203#issuecomment-313156153:12,Perform,Performance,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3203#issuecomment-313156153,1,['Perform'],['Performance']
Performance,"@samuelklee The module can now save and load everything, including the state of the optimizer. This allows to making interesting inference pipelines. Here's a decent strategy for obtaining the global optimum (it works flawlessly on simulated data every time):. - In the first pass, one disables annealing and obtains the variational parameters in a thermal state. The temperature needs to be _high enough_ to allow most/all local minima to merge, though, not too high to allow copy numbers to travel too far away from baseline copy numbers. If this occurs, one must anneal very slowly in the next stage (see below). The results are checkpointed once converged. - In the second pass, one makes another call to the CLI tool, this time w/ annealing enabled (starting from the same temperature) and starting from the checkpointed thermal results (model params, posteriors, adam(ax) state). The annealing rate must be slow enough to prevent thermal fluctuations from getting quenched (i.e. the evolution must be quasi-isothermal). One must look for a steady and linear rise of ELBO, such that when the annealing protocol ends, SNR quickly drops to values below 1. In both runs, the learning rate must be very small (in the rate 0.01-0.05) such that we wouldn't have to worry about controlling stochastic noise. Adam(ax) quickly adjusts its moment estimates and compensates for the small learning rate, so this doesn't increase the training time significantly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-347369020:40,load,load,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-347369020,2,"['load', 'optimiz']","['load', 'optimizer']"
Performance,"@samuelklee The performance issues were a mirage. It turned out that there was a misconfiguration in the Carrot tests so it was pegging the ""control"" version to an older GATK release from a year ago. There was a slight regression in the past year the we haven't caught but that is a separate discussion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1551819210:16,perform,performance,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1551819210,1,['perform'],['performance']
Performance,"@samuelklee This is a very dangerous/misleading check! Most of the files in your list are actually in use in the test suite, and their presence is inferred from the path to the file(s) they are associated with. For example, vcf index files are loading indirectly by our readers, using the path to the vcf to infer the path to the index.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905#issuecomment-348591768:244,load,loading,244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905#issuecomment-348591768,1,['load'],['loading']
Performance,"@samuelklee Yeah, you did [see this issue there](https://github.com/broadinstitute/gatk/issues/6513#issuecomment-602077546) (as well as the concurrent modification exception).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6649#issuecomment-640866423:140,concurren,concurrent,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6649#issuecomment-640866423,1,['concurren'],['concurrent']
Performance,"@samuelklee commented on [Fri May 12 2017](https://github.com/broadinstitute/gatk-protected/issues/1038). - [x] PyMC3 (theano) implementation; - [x] PyStan implementation; - [x] investigate GPU performance; - [x] investigate batch/minibatch performance. See https://github.com/broadinstitute/dsde-methods-prototyping/tree/sl_advi for notebook on Bayesian GMM and instructions to set up the appropriate python environment with conda. ---. @samuelklee commented on [Sun May 14 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-301189833). Coverage model with ARD and full batch now in https://github.com/broadinstitute/dsde-methods-prototyping/blob/sl_advi/cnv-th/advi-prototypes/coverage.ipynb in the sl_advi branch. Toy number of targets (100) and samples (50) seems to work well and converge quickly with random initialization. ---. @samuelklee commented on [Sun May 14 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-301321417). T = 10^5 and N = 100 took 15 minutes running on one core of an i5-3570K with 32GB memory, which includes theano graph compilation. This includes 350 iterations of ADVI, but note that convergence to 1% was achieved after about 250 iterations. I also did not initialize with PCA. However, upping to T = 10^6 causes out of memory. Not sure if this could be naively alleviated by setting theano flags appropriately, but I think we will probably want to minibatch in T instead. Note also that this model uses the exact Poisson likelihood. Composing with an HMM segmentation step, perhaps alternating for a few iterations, would give the gCNV PoN without the Gaussian approximation we use. ---. @samuelklee commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-302234920). The same run of T = 10^5 and N = 100 took <4 minutes on the gsa5 Tesla K40c GPU---about a 3x speedup over my home CPU. A slightly larger run of T = 1.5 * 10^5 and N = 200 took 10 minutes and",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2984:194,perform,performance,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2984,2,['perform'],['performance']
Performance,@samuelklee commented on [Fri May 12 2017](https://github.com/broadinstitute/gatk-protected/issues/1039). Hooking up a siHMM (or some variation) to the coverage model from https://github.com/broadinstitute/gatk-protected/issues/1038 will be our first cut at GATK CNV 2.0. Performance and convergence on WGS-size data should be a priority. The end goal will be a joint siHMM once we also have the 2.0 allele-fraction model prototyped.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2985:272,Perform,Performance,272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2985,1,['Perform'],['Performance']
Performance,"@schelhorn Sorry for the long wait on this issue, which we do take seriously! Unfortunately as mentioned above we have extremely limited resources for Mutect2 maintenance at the moment due to our focus on Mutect3, which we see as the long-term solution to this and other issues. You should definitely continue to run 4.1.8.0 until this is resolved, if at all possible. Having said that, we are generating a new M2 PON now, and should know soon whether it resolves this issue as @davidbenjamin hopes. Stay tuned for an update on the outcome of this evaluation within the next week or two.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1405524738:505,tune,tuned,505,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1405524738,1,['tune'],['tuned']
Performance,"@schelhorn We have a large clinical ""truth"" set we utilize during workflow validations. We also utilize spike-in samples from SeraCare and perform dilutions using a couple of the common Coriell cell lines. We noticed the Mutect2 calling inconsistency while validating a small targeted panel.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171672027:139,perform,perform,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171672027,1,['perform'],['perform']
Performance,"@shengqh ; Looking at it, it seems that `test.pbs` is basically a shell script. So at the top of that add:. ```; source activate gatk; ```. Or... whatever it is you need to do to activate the environment. What the `exec` command is going to do is launch whatever is passed in. So... something like:. ```; singularity exec gatk.simg cat /etc/os-release; ```; Would show you the `/etc/os-release` file from within the container. `exec` isn't going to load a shell environment, but launch the application directly. In a `shell` look at: `/.singularity.d/actions/exec`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433549054:449,load,load,449,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433549054,1,['load'],['load']
Performance,"@shisheng-1 Hello, thank you for your contribution! . I tried running with and with-out your change enabled and I didn't see any performance improvement when compiling. I didn't do a very scientific test, but running the following a few times on master vs your branch didn't show any performance advantage for using fork = true.; ```; ./gradlew clean compileTestJava; ```. | fork = | false | true |; | --- | ----- | ---- |; | run 1 | 77 | 93 |; | run 2 | 67 | 70 |. From my quick tests it looked like the forking version might actually be a bit slower but I suspect that's just noise. . Do you see different results?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7561#issuecomment-967263911:129,perform,performance,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7561#issuecomment-967263911,2,['perform'],['performance']
Performance,"@sooheelee ""Flush to zero"" or FTZ is a CPU flag that causes extremely small floating-point values to be treated as 0. Enabling this flag improves performance and consistency across PairHMM implementations, without causing precision loss of any significance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1572#issuecomment-422175512:146,perform,performance,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1572#issuecomment-422175512,1,['perform'],['performance']
Performance,"@sooheelee Are you sure that smoothing doesn't always converge before we can see different behavior for your test data? How many iterations of smoothing do you get when you set this parameter to 0, and how many do you get when you set it to 1? (It might be helpful to post a condensed version of the stdout.). This parameter is not meant to be a boolean. If it is set to N > 0, then at most N smoothing iterations will be attempted before the next MCMC fit. However, if convergence is reached before N iterations, then the final MCMC fit is performed. If the parameter is set to 0, then no MCMC refits are performed. In both cases, the total number of allowed smoothing iterations is set by a separate parameter, `maximum-number-of-smoothing-iterations`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382781387:541,perform,performed,541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382781387,2,['perform'],['performed']
Performance,"@sooheelee I found that on FC, it was only necessary to make the File -> String change at the task level (i.e., in CollectCounts). Not sure if this works due to the particular version of Cromwell on FC. In any case, I don't think you should stress too much about getting NIO working on your VM. Again, I'd expect the current non-NIO gCNV WDL to only take a few hours on FC, and then less than that once coverage collection is call cached. No use spending more time getting NIO working than it would save you, after all!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437362263:431,cache,cached,431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437362263,1,['cache'],['cached']
Performance,"@sooheelee We've decided to delete the old tools. I will issue a PR soon. Hopefully this lightens the load on everyone a bit!. Incidentally, I would be fine with adding the tools `CollectFragmentCounts` and `CollectAllelicCounts` to the `CoverageAnalysis` category, as they are performing relatively generic tasks that fall in that category. (with the caveat that the output formats contain column headers that are specific to the new CNV workflows).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349145382:102,load,load,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349145382,2,"['load', 'perform']","['load', 'performing']"
Performance,@spatel-gfb Can you try running `GenomicsDBImport` with the `--genomicsdb-shared-posixfs-optimizations` argument and see if that helps? . @nalinigans @mlathara any other suggestions?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1031830101:89,optimiz,optimizations,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1031830101,1,['optimiz'],['optimizations']
Performance,"@spatel-gfb another option that should improve performance is to run the GATK GVCF postprocessing tool ReblockGVCF on all of your input GVCFs. That tool merges reference blocks (which BAF calculation doesn't need) and throws out uncalled alleles (which BAF calculation doesn't need), both of which should speed up your import. For the purposes of GATK-SV you can use the default GQ bands ([0,20], and (20, 99]).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1088703926:47,perform,performance,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1088703926,1,['perform'],['performance']
Performance,"@spikebike, just started looking at this issue again. We are benchmarking operations with NFS and will put out an optimized library soon. But, GenomicsDB does use filesystem locking to allow for simultaneous reads/writes. - Did you try `export TILEDB_DISABLE_FILE_LOCKING=1` ?; - Are you using the `--consolidate true` option with GenomicsDBImport explicitly? If yes, would it be possible to set that option to false which is also the default.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-616657603:114,optimiz,optimized,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-616657603,1,['optimiz'],['optimized']
Performance,"@stefandiederich We are finalizing some hyperparameter optimizations and will have some results and recommendations to share within ~1 month. You may find some preliminary recommendations for WES in this forum thread: https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. Once our optimizations are finalized, @vdauwera and her team will put together a tutorial and other workshop materials---stay tuned!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382777741:55,optimiz,optimizations,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382777741,3,"['optimiz', 'tune']","['optimizations', 'tuned']"
Performance,"@takeshi-yoshimura One more thing to note. You should be able to use this library with an existing version of gatk by including this in your classpath since filesystem providers should be dynamically loaded. ( something along the lines of`--java-options ""-cp <pathtothisjar>' should work.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-662035091:200,load,loaded,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-662035091,1,['load'],['loaded']
Performance,"@takutosato had a good suggestion: to stratify to low-complexity regions in the high-confidence regions. Not sure how many variants are there, but will take a look. EDIT: looks like it's ~5k / ~54k on chr22 in CHM. More generally, I think that defining the appropriate loss function for optimization to set ""default"" parameter values obviously has no unique answer. The problem is also made a little more complicated by our current strategy of sensitive calling + non-trivial filtering. But it would be great to come up with some hard constraints (e.g., we never want runtime/cost to exceed X, we always want to maintain Y metrics in these regions on these samples) and general procedures, then apply them as equitably as possible across all method/parameter changes. Also generally, I'm a bit wary of focusing too hard on the high-confidence regions, as this might lead to overfitting or could understate the potential of method/parameter changes in more difficult regions. But probably we'll have to downweight the loss or do more manual checks in low-confidence regions until we improve truth resources there. One naive question, just want to double check: is it correct that the overall scaling of each set of SW parameters is inconsequential? E.g., if I multiply each by a constant, should I expect the same results? I would expect this to be the case (unless my hazy recollection of the details of SW scoring is off) and simple experiments bear this out, but I'm not sure if there are some edge cases or idiosyncrasies in our implementation or use of the scores that I might be missing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714570055:287,optimiz,optimization,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714570055,1,['optimiz'],['optimization']
Performance,@tedsharpe ; not sure if the args checking would significantly affect performance.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5157:70,perform,performance,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5157,1,['perform'],['performance']
Performance,@tedsharpe How's the performance of this new binding? I'm assuming it's better and less broken than the old one. Do you have any numbers or even just anecdotes?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2367#issuecomment-275511690:21,perform,performance,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2367#issuecomment-275511690,1,['perform'],['performance']
Performance,"@tedsharpe I've addressed some of your comments here -- all the simpler stuff plus:. - I now only make distal targets for split reads with one supplementary alignment. We can make a ticket to handle more complex cases at some point.; - I renamed the concept of strand in the `EvidenceTargetLink` and related classes -- I'm now calling it `evidenceUpstreamOfBreakpoint`.; - I canonicalize `EvidenceTargetLinks` and only create them when the source is upstream of the target. This allowed me to get rid of the de-duplication code, so thanks for the suggestion. It seemed tricky to me to try to cluster these links during the initial pass over the reads while at the same time keeping track of coherent evidence. In my testing it doesn't seem like it is slow to run over the `EvidenceRDD` again to do this, but we could think about trying to change this sometime if we're looking for optimizations. . Want to take another look?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328300806:881,optimiz,optimizations,881,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328300806,1,['optimiz'],['optimizations']
Performance,"@tedsharpe So we've addressed a bunch of the issues you've mentioned in the new htsjdk.beta reader apis. In particular we now optimize the decoding checks so that we only scan the beginning of the files exactly once and the give the necessary bytes to all available codecs. That should work in combination with your optimization to push down the filtering which makes a lot of sense as well. Lazily loading the indexes when you need to query for them is a good idea. I think the thought behind aggressively loading them was probably to handle potential errors up front instead of waiting until later, and it's confounded by the bad choice of automatically discovering indexes by default so the engine can't tell if there SHOULD be an index until it tries and fails to load it. We've also addressed that in the new APIs to give the client more control over how indexes are discovered which should allow for cleaner lazy loading and things like that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1364325181:126,optimiz,optimize,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1364325181,6,"['load', 'optimiz']","['load', 'loading', 'optimization', 'optimize']"
Performance,"@tedsharpe Thanks for checking. In general I've seen the CPB tends to help a lot when reading through long contiguous stretches of BAM file and less when doing anything on smaller or fragmented data. I'm surprised it didn't make any difference here, but seems like it doesn't so that's fine. I've seen catastrophic interactions between insufficiently buffered index inputs with it disabled where it ended up performing an http request for every byte, but hopefully that's avoided just by using a buffered reader for it. . I have a plan to someday enable the more intelligent -L aware prefetcher that will use the list of actual positions of interest to buffer more intelligently, but that's not happening on any specific schedule.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1358245320:408,perform,performing,408,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1358245320,1,['perform'],['performing']
Performance,"@tedsharpe This looks good to me. In general partition sizes can be much larger than 100kb without problems, so I suspect it's is something funny to do with the task serialization of ctx.paralellize(). . If this is a performance critical tool it would probably be better to rewrite it in a way that it can load the reference in parallel. Since I assume this is something you run essentially once per reference it may not be worth it. . If you're worried about small machines running out of memory, I would expose the parameter that lets you configure how much memory each partition gets. I would expect in any spark configuration each core will have no less than ~1gb to work with and likely 2 -4 on any machines used for biology work.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1505#issuecomment-187387150:217,perform,performance,217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1505#issuecomment-187387150,2,"['load', 'perform']","['load', 'performance']"
Performance,"@tedsharpe, I have changed the implementation to use a lazy non-cached recreation of the read mapping information. . I have not added the additional mapping information in your branch like the MQ value. I would do that in a separate commit. . Please take a look and perhaps run your benchmark.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3133#issuecomment-311404369:64,cache,cached,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3133#issuecomment-311404369,1,['cache'],['cached']
Performance,@tfenne The gatk3 behavior when using intervals was that it only loaded data that started within the interval. We change that in gatk4 so queries get all overlapping data. This means that for GenoytpeGVCFs at least you should be getting all upstream deletions that overlap. If that's *NOT* the case then it's a bug we should fix. The side effect was getting duplicate calls in adjacent shards which is why we added `--only-output-calls-starting-in-interval`. . The HaplotypeCaller behavior is probably different and I don't know for sure what it is. ![HaplotypeCaller leading deletion behavior](https://user-images.githubusercontent.com/4700332/72105530-b569a080-32fb-11ea-9fc5-1adaee2ac039.png). It seems like there are 3 possible outcomes for a deletion that starts outside the interval in HaplotypeCaller. @davidbenjamin Do you know which we do? James said you were re-writing related code recently. A toggle to change between behavior 1 and 2 would be idea like you're suggesting @tfenne. I'm a bit afraid that we do 3 and emit nothing in these cases though.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6339#issuecomment-572762496:65,load,loaded,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6339#issuecomment-572762496,1,['load'],['loaded']
Performance,"@tomwhite #1847 is in, and there's now a `JBWAIntegrationTest` showing how to load and use jbwa in a way that will work on both Linux and Mac. You should be all set -- let me know if there are problems.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-220759191:78,load,load,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-220759191,1,['load'],['load']
Performance,"@tomwhite , #1701 is not a JNI binding (I'm working on providing one), but a really naive way of calling bwa from cmdline. Right now if you want to bring in the ""correct"" branch of BWA, I think messing with @lindenb 's MAKEFILE (bottom lines) and clone only the Apache2 branch would be enough. Per discussion in #1517 , the only difference seem to be `bwa index` performance, which isn't a concern here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-212961920:363,perform,performance,363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-212961920,1,['perform'],['performance']
Performance,@tomwhite From what I saw the hdfs provider wasn't loaded (at least on my local cluster). Only the default filesystem and the jar file system.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-268009239:51,load,loaded,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-268009239,1,['load'],['loaded']
Performance,"@tomwhite I think the general goal of unifying the Spark/non-Spark tool hierarchies is worthwhile, but I don't like the idea of all tools having `if (sparkArgs.useSpark) {} else {}` boilerplate. If we do this, we should have separate abstract methods that get called automatically in the Spark/non-Spark cases. I also think we should wait to perform this refactoring until later in the quarter, after the Spark evaluation, and after we've standardized more on `Path` for inputs/outputs, as it will break a lot of downstream code in gatk-protected -- and we don't want to break all the tools more than once if we can help it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254228946:342,perform,perform,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254228946,1,['perform'],['perform']
Performance,"@tomwhite I'm looking into the performance issues now with the new code path -- it brings the output much closer to GATK3, but clearly needs some profiling work. Can you tell me what kind of difference you saw in the runtime on Spark? Eg., was it on the order of 20-30%, or was it worse than that?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3533#issuecomment-330905564:31,perform,performance,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3533#issuecomment-330905564,1,['perform'],['performance']
Performance,"@tomwhite If there are a million reads mapped to the same position, my hope was that we could avoid loading them into an RDD at all, if possible (ie., eliminate as they are first read in), rather than load them into an RDD and then downsample. This is why I proposed doing this at the Hadoop-BAM layer. Do you think this is possible?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1644#issuecomment-205861300:100,load,loading,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1644#issuecomment-205861300,2,['load'],"['load', 'loading']"
Performance,"@tomwhite Ignore my previous message, sorry -- I see that you commented above that you tested https://github.com/broadinstitute/gatk/pull/4314 and it had no effect. I've opened https://github.com/broadinstitute/gatk/pull/4428 to revert the ADAM upgrade for now until we understand the underlying cause of the performance regression. @fnothaft It seems to me that the serializer registrations in ADAM could in theory affect the GATK, since we both register serializers for core classes in htsjdk. It seems worth investigating as a possibility, at least, as it's the only candidate mentioned so far that seems to have the potential to cause such a massive performance difference.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367069808:309,perform,performance,309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367069808,2,['perform'],['performance']
Performance,"@tomwhite In your `--files` branch, you might try enabling feature caching on the `FeatureDataSource` (assuming your sequential queries are forward-only, increasing). Without that it will execute a separate index query for each `query(interval)` to find the overlaps. If you pass a non-zero `queryLookaheadBases` value (we usually use 100000) to the FeatureDataSource constructor in `BroadcastJoinReadsWithVariants`, it will use lookahead and the feature cache to satisfy queries - I would expect it to speed it up a lot.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-412917472:455,cache,cache,455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-412917472,1,['cache'],['cache']
Performance,"@tomwhite Like we discussed this morning, we can and should get rid of the broadcast code but we should ideally first get some sort of plot we can point to in order to justify the change. This will also be useful for future presentations of our performance improvements. The plot would ideally be a comparison between the new distribution strategy and broadcasting compared across a variable number of cores, so the performance improvement can be better understood.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-416327226:245,perform,performance,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-416327226,2,['perform'],['performance']
Performance,"@tomwhite OOC, do you have any comment as to why bumping the ADAM version caused a performance regression? I'm not aware of any changes we've made in ADAM that would have impacted either BQSR or HC.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366072491:83,perform,performance,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366072491,1,['perform'],['performance']
Performance,"@tomwhite On the GATK side, I think the steps are:; 1. Rev our Hadoop-BAM dependency once https://github.com/HadoopGenomics/Hadoop-BAM/pull/49 is in.; 2. Take one of our existing test inputs and create a copy of it sorted by query name using the hellbender tool `SortSam`. I think a good file to use would be `src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.bam`, as that file hasn't had either MD or BQSR run on it, and it's the primary input in the `ReadsPipelineSparkIntegrationTest`.; 3. Before making any actual changes to MarkDuplicatesSpark, write a new integration test proving that MarkDuplicatesSpark can read in the queryname-sorted bam above, and produces the same result as when run on the coordinate-sorted version of the bam. The output in both cases when running with `--shardedOutput false` should be a coordinate-sorted bam.; 4. Now the potentially tricky part: in `MarkDuplicatesSpark`, we want to detect queryname-sorted reads using the `SAMFileHeader` SO attribute (`SAMFileHeader.getSortOrder()`), and if we have them, avoid performing the first `groupByKey()` operation in `MarkDuplicatesSparkUtils.transformReads()`, instead relying on the natural ordering of the reads to create the `PairedEnds` objects.; 5. Make sure that the test you wrote in step 3 doesn't break after doing step 4 :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1411#issuecomment-170043162:1114,perform,performing,1114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1411#issuecomment-170043162,1,['perform'],['performing']
Performance,"@tomwhite Please review, and confirm that this resolves the performance issues you encountered.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4428#issuecomment-367067878:60,perform,performance,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4428#issuecomment-367067878,1,['perform'],['performance']
Performance,"@tomwhite That's interesting -- @lbergelson 's profiling on `HaplotypeCallerSpark` in isolation did not show any noticeable slowdown as a result of the switch to the new code path (in addition to the output being much more correct than before), and the walker version which uses the same codepath is as fast or faster than GATK3 at this point in all modes. We should try to see whether we can replicate your results on our end. Can you provide more details on how you ran the tools?. The new `HaplotypeCaller` code path does result in the tool doing more work in some cases as compared to the early betas, particularly for genomic locations not covered by any reads -- but this work is necessary in order to produce correct output without boundary effects, and the fact that the early betas didn't do it was an oversight. I wonder if that could explain what you're seeing -- see the full discussion in https://github.com/broadinstitute/gatk/issues/4169. There are also several ways in which `HaplotypeCallerSpark` currently makes suboptimal use of the new code path introduced in https://github.com/broadinstitute/gatk/pull/4278:. 1. It uses only a single interval per shard, instead of many intervals as the walker version does (https://github.com/broadinstitute/gatk/issues/4299). 2. It materializes all of the assembly regions in a shard at once (https://github.com/broadinstitute/gatk/issues/4301), as opposed to the walker version which materializes the regions in a shard as lazily as possible. 3. The default read shard size may need adjusting (https://github.com/broadinstitute/gatk/issues/4298). The walker version creates one shard per contig, and lazily creates the assembly regions within that shard. That's obviously not possible for the Spark version, but I don't think that the effect of the shard size on performance has been measured yet for the Spark version.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-364179033:1821,perform,performance,1821,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-364179033,1,['perform'],['performance']
Performance,@tomwhite This seems like an easy way to get a big performance boost in `HaplotypeCallerSpark` -- what do you think?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4296#issuecomment-363201577:51,perform,performance,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4296#issuecomment-363201577,1,['perform'],['performance']
Performance,"@tomwhite To clarify, I think that the caller of `ensureCapacity()`, namely `GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables()`, also needs to be synchronized in order to avoid some unlikely but still-possible races. Given this, I think that we should consider whether `ThreadLocal` might be a better option here. It's not 100% clear to me whether a `ThreadLocal` `get()` call is cheaper than a synchronized method call, but some casual googling suggests that it might be. If we're going to end up entering a synchronized method on every single call to `GenotypeLikelihoodCalculators.getInstance()`, we might want to do some research into whether `ThreadLocal` + no synchronization would be faster, since I believe that this is a performance-sensitive section of code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422171244:746,perform,performance-sensitive,746,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422171244,1,['perform'],['performance-sensitive']
Performance,"@tomwhite Well, the idea is to downsample **before** we pay the cost of holding all the reads in memory -- with `sample` I think you'd be downsampling after we've paid that cost. The `ReservoirDownsampler` in GATK4 is able to perform a fair elimination on a stream of items as they come in, without having to store all the items in memory at once, and guaranteeing that every item has an equal probability of ending up in the final set. Also, I think we might want to downsample on a per-alignment-start basis, rather than per-spark-partition. We want protection against the situation where mapping artifacts have caused an insane number of reads to be aligned to the same position, without eliminating coverage at positions that do not pose a problem for us.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1644#issuecomment-203986193:226,perform,perform,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1644#issuecomment-203986193,1,['perform'],['perform']
Performance,"@tomwhite When you get a chance, could you please test this branch to check whether the performance regression you reported earlier is resolved?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376283397:88,perform,performance,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376283397,1,['perform'],['performance']
Performance,@tomwhite and/or @laserson Could we please get your expert opinions as to whether it's safe to remove the protective copying operations from the spark BQSR like this? (these were necessary in dataflow due to sibling fusion optimization).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/925#issuecomment-143236364:223,optimiz,optimization,223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/925#issuecomment-143236364,1,['optimiz'],['optimization']
Performance,@tomwhite and/or @laserson should have a look at this and give JP high-level feedback on his approach to optimization.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/987#issuecomment-146905889:105,optimiz,optimization,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987#issuecomment-146905889,1,['optimiz'],['optimization']
Performance,"@tomwhite before we close, how does this improved performance compare to non-spark writing of the same file (just use PrintReads vs PrintReadsSpark or something)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1015#issuecomment-158954274:50,perform,performance,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1015#issuecomment-158954274,1,['perform'],['performance']
Performance,"@tomwhite found performance issues with large (eg., exome target) interval lists on Spark in the course of our quarterly Spark evaluation. Specifically this test case was very slow:. ```; ./gatk-launch CountReadsSpark ; -I hdfs:///WGS-G94982-NA12878.bam ; -L Broad.human.exome.b37.interval_list; -- --sparkRunner GCS --cluster YOUR_CLUSTER --num-executors NUM_EXECUTORS --executor-cores 4 --executor-memory 16g; ```. Where the bam was `gs://hellbender/q4_spark_eval/WGS-G94982-NA12878.bam` pre-staged into HDFS and the interval list was a localized copy of `gs://hellbender/q4_spark_eval/Broad.human.exome.b37.interval_list`. A proposed fix is https://github.com/HadoopGenomics/Hadoop-BAM/pull/121",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2313:16,perform,performance,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2313,1,['perform'],['performance']
Performance,@tomwhite is this ticket related [Get Hadoop reference loading code into htsjdk](https://github.com/broadinstitute/gatk/issues/831) ?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1326#issuecomment-165812238:55,load,loading,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1326#issuecomment-165812238,1,['load'],['loading']
Performance,"@ury the performance evaluation is based on the HCC1395 benchmark, as described in my original report (see above). More variants in newer versions is in line with our analysis, but the new variants are likely to be mostly false positives.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1407452410:9,perform,performance,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1407452410,1,['perform'],['performance']
Performance,"@vdauwera JS performance is highly dependent, as you can imagine, on client hardware, browser version, what else is running on the machine, etc. It's also dependent on the type of plot - if the plot involves animation, shadows, etc. All that said, I'd put the start-to-worry line in the thousands, not hundreds. YMMV.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/248#issuecomment-77364832:13,perform,performance,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/248#issuecomment-77364832,1,['perform'],['performance']
Performance,"@vdauwera The tools are categorized and listed in the Google Spreadsheet above. It is waiting for you to assign tech leads to tools for documentation. One thing that @chandrans brought to my attention is that for BaseRecalibrator one of the parameters (`-bqsr`) actually causes an error. One can no longer generate the 2nd recalibration table with correction on the fly and instead must use the recalibrated BAM through BaseRecalibrator to generate the 2nd recal table for plotting. **This type of information is missing from the tool docs.** Furthermore, updates I made to the BQSR slidedeck (that showcase this `-bqsr` parameter) are based on information from a developer and this information turns out to be incorrect now (perhaps correct at some point in development?). Soooo, I think it may be prudent that those responsible for tool docs test the commands on data. - [4] Make sure the doc content enables Best Practices, e.g. plotting BQSR recalibration, and ; - [5] Test example commands to ensure they work. If they do not, make corrections and notate the change in application in the documentation.; - [6] Remember @vdauwera's plan to change the representation of parameters from camel to KEBAB case. Issue is <https://github.com/broadinstitute/gatk/issues/2596>. Geraldine would like your help to do this for the tools you are responsible for. Remember to change the integration tests too. . ### What the gatkDocs look like as of commit of `Mon Nov 20 17:30:46 2017 -0500` where we upgraded htsjdk to 2.13.1: . # [gatkdoc.zip](https://github.com/broadinstitute/gatk/files/1492593/gatkdoc.zip). Download and load the `index.html` into a web browser to click through the docs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346060583:1617,load,load,1617,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346060583,1,['load'],['load']
Performance,"@vdauwera reports getting this error when running `GenomicsDBImport` with a large interval list as the `-L` input:. ```; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; ```. Looking at the code that produces this error, this seems like a ""should never happen"" type of error that would likely only be produced by a race condition of some kind. Full stderr log follows:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.876625c8; 04:37:28.801 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 04:37:29.319 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.319 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.9.0; 04:37:29.320 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 04:37:29.320 INFO GenomicsDBImport - Executing as root@7a7880aef99b on Linux v4.9.0-0.bpo.6-amd64 amd64; 04:37:29.321 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-0ubuntu0.16.04.1-b11; 04:37:29.321 INFO GenomicsDBImport - Start Date/Time: October 8, 2018 4:37:28 AM UTC; 04:37:29.321 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - ------------------------------------------------------------; 04:37:29.322 INFO GenomicsDBImport - HTSJDK Version: 2.16.1; 04:37:29.323 INFO GenomicsDBImport - Picard Version: 2.18.13; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 04:37:29.323 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 04:37:29.324 INFO GenomicsDBImport - HTSJDK Defaults.US",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5300:131,concurren,concurrent,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300,3,"['Load', 'concurren', 'race condition']","['Loading', 'concurrent', 'race condition']"
Performance,"@vruano @mwalker174 Any estimates on cost that could help determine the priority of issue 1? Specifically, is the disk cost required to localize the entire count file for all samples a determining factor? If we can drastically reduce this cost, then we can dedicate more to increasing resolution, etc. Here is a minimal set of fixes that could enable the querying of intervals for GermlineCNVCaller (and also for DetermineGermlineContigPloidy without too much extra work, since we also subset intervals there) *only in the gCNV WGS pipeline*, without disrupting other interfaces:. 1) Write a Tribble SimpleCountCodec for the `counts.tsv` extension. I've already done this in a branch.; 2) Change GermlineCNVCaller and DetermineGermlineContigPloidy tools to accept paths.; 3) If an index is present for each count path, create a FeatureDataSource, merge the requested -L/-XL intervals, and query to perform the subset. We will also need to stream the SAM header metadata. It should not require much code to extract all this to a temporary IndexedSimpleCountCollection class. (Caveat: for now, this will work with the current gCNV convention of providing bins via -L/-XL. Technically, it will also work with the more conventional use of -L/-XL to denote contiguous regions, but we may have to perform checks that bins are not duplicated in adjacent shards if they overlap both, since querying a FeatureDataSource will return any bins that overlap the interval---rather than only those that are completely contained within it.); 4) Index read-count TSVs in the gCNV WGS pipeline after collection and modify the DetermineGermlineContigPloidy and GermlineCNVCaller tasks to take read-count paths and indices, if necessary. These changes could be confined in the gCNV WGS WDL for now. I think that should do the trick. If this is high priority, I can implement now. In the future, we might be able to promote all Locatable CNV Records to Features and write code to automatically pass the columns/encoders/de",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5716#issuecomment-468360082:898,perform,perform,898,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5716#issuecomment-468360082,1,['perform'],['perform']
Performance,"@vruano The HGDP crams also trigger this error. . ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/HGDP/data/Brahui/HGDP00001/alignment/HGDP00001.alt_bwamem_GRCh38DH.20181023.Brahui.cram. ```; 14:32:34.745 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 17, 2021 2:32:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:32:34.877 INFO CalibrateDragstrModel - ------------------------------------------------------------; 14:32:34.877 INFO CalibrateDragstrModel - The Genome Analysis Toolkit (GATK) v4.2.0.0; 14:32:34.877 INFO CalibrateDragstrModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:32:34.877 INFO CalibrateDragstrModel - Executing as farrell@scc-gh3.scc.bu.edu on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 14:32:34.877 INFO CalibrateDragstrModel - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_172-b11; 14:32:34.877 INFO CalibrateDragstrModel - Start Date/Time: April 17, 2021 2:32:34 PM EDT; 14:32:34.878 INFO CalibrateDragstrModel - ------------------------------------------------------------; 14:32:34.878 INFO CalibrateDragstrModel - ------------------------------------------------------------; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Version: 2.24.0; 14:32:34.878 INFO CalibrateDragstrModel - Picard Version: 2.25.0; 14:32:34.878 INFO CalibrateDragstrModel - Built for Spark Version: 2.4.5; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182#issuecomment-821876394:261,Load,Loading,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182#issuecomment-821876394,1,['Load'],['Loading']
Performance,@wujh2017 Great! Let us know if you have any more feedback. Please be aware that both DetermineGermlineContigPloidy and GermlineCNVCaller are still in beta. There are some parameters that may need to be tuned appropriately for your data. We are currently running evaluations and will release some recommendations that we find suitable for data generated at the Broad.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4457#issuecomment-369254401:203,tune,tuned,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4457#issuecomment-369254401,1,['tune'],['tuned']
Performance,"@wujh2017 thank you for trying out this beta tool! As @samuelklee mentioned above, your issue stems from not having the proper python conda env. I guess you have a python 2.x interpreter in your system environment which fails to parse GATK's python code (which requires python 3.6+). Please either follow the instructions in the README.md file, or use the official Docker image instead. Also, I would like to add that the default parameters for running the Germline CNV calling pipeline are currently being fine-tuned separately for WES and WGS data. The default parameters shipped with GATK 4.0.0.0 are preliminary and are not expected to yield optimal results.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4389#issuecomment-365117170:512,tune,tuned,512,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4389#issuecomment-365117170,1,['tune'],['tuned']
Performance,"@yfarjoun I've built docs from your branch to see what the changes look like. Here's the PicardDoc bundle I've zipped for you: [picarddoc_yossi_test.zip](https://github.com/broadinstitute/gatk/files/1535680/picarddoc_yossi_test.zip). View the rest by loading `index.html` into a browser. Here is one of the tool docs where you can see your changes:; <img width=""1099"" alt=""screenshot 2017-12-06 10 18 34"" src=""https://user-images.githubusercontent.com/11543866/33668948-1c286b90-da6f-11e7-91d6-3b368375cbab.png"">. ---; # Steps to build docs to see what changes look like:; [1] Download and switch to branch with changes, e.g. ; ```; git branch yf_documentation_update origin/yf_documentation_update; git checkout yf_documentation_update; ```; [2] Generate the docs. GATK and Picard do this differently:; ```; ./gradlew clean gatkDoc; ```; or; ```; ./gradlew clean picardDoc; ```. [3] Open `index.html` from a browser. Again, GATK and Picard have different locations for their respective docs:; > build/docs/gatkdoc; > build/docs/picarddoc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349675765:251,load,loading,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349675765,1,['load'],['loading']
Performance,"@yfarjoun for small arrays, `FastMath.log` is pretty much as fast as it gets. It is highly optimized. If you are dealing with giant arrays, however, an alternative is to cast the Java arrays into [NDArray](www.nd4j.org) and execute Nd4j native ops. Here's how the timings look:; ```N = 1, Apache = 0.001625 ms, Nd4j = 0.113038 ms; N = 10, Apache = 0.002112 ms, Nd4j = 0.029833 ms; N = 100, Apache = 0.011660 ms, Nd4j = 0.028464 ms; N = 1000, Apache = 0.049915 ms, Nd4j = 0.052455 ms; N = 10000, Apache = 0.348786 ms, Nd4j = 0.430606 ms; N = 100000, Apache = 3.572483 ms, Nd4j = 1.810641 ms; N = 10000000, Apache = 323.021844 ms, Nd4j = 175.421305 ms; ```; The nd4j times include the overhead of creating NDArrays and pulling back the results to JVM heap. The break even point is around N ~ 1000. If accuracy is not a concern, (1) a native implementation of log using half-precision floats or (2) caching, tabulation, and linear interpolation could help. ps> I just realized that the Nd4j call was using 4 threads. if you replace for loops in Java with parallel streams in Java with the same number of threads, Apache always beats Nd4j in this specific test:; ```N = 1, Apache = 0.073910 ms, Nd4j = 0.122488 ms; N = 10, Apache = 0.086508 ms, Nd4j = 0.056924 ms; N = 100, Apache = 0.067022 ms, Nd4j = 0.051674 ms; N = 1000, Apache = 0.081751 ms, Nd4j = 0.075098 ms; N = 10000, Apache = 0.202142 ms, Nd4j = 0.514030 ms; N = 100000, Apache = 1.190085 ms, Nd4j = 1.990945 ms; N = 10000000, Apache = 96.536308 ms, Nd4j = 210.331251 ms; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292602609:91,optimiz,optimized,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292602609,1,['optimiz'],['optimized']
Performance,@yfarjoun this is a memory optimization but I have confirmed it uses less memory and runs at the same speed,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3445#issuecomment-323204475:27,optimiz,optimization,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3445#issuecomment-323204475,1,['optimiz'],['optimization']
Performance,A Java API for loading both vcfs stored as files (single and multi-sample) and streams of `VariantContext` objects into TileDB.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2052:15,load,loading,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2052,1,['load'],['loading']
Performance,A couple more minor comments. I will take your word that your changes improve performance!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4846#issuecomment-414776060:78,perform,performance,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4846#issuecomment-414776060,1,['perform'],['performance']
Performance,"A couple of thoughts after doing a little more reading on this. Depending on the source it would appear that each arena will allocate either 64MB or 128MB of virtual memory (i.e. address space). So it would probably also be fine to set this limit a bit higher. Secondly, while there's lots of discussion online that setting this doesn't negatively impact Java code running on the JVM, it is possible that native code invoked using JNI could see a modest reduction in performance _if_ a) it's highly multithreaded and b) it's doing lots of heap allocations. I don't know enough about the native pair-HMM and other native code, but it would be helpful if someone who knows more about that could weigh in.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5849#issuecomment-478574401:467,perform,performance,467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5849#issuecomment-478574401,1,['perform'],['performance']
Performance,"A couple of users have reported errors with --disable-tool-default-read-filters. Is this expected? This user says the tool runs without that flag. ----; User Report; ----. I just encountered this error in Mutect2: . Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar Mutect2 -R /Users/loeblabm11/bioinformatics/reference/human/hg19/hg19.fa -I 20171027_BN31_python.dcs.filt.no_overlap.bam -tumor BN31 -O 20171027_BN31_python.dcs.MuTect2.vcf -bamout 20171027_BN31_python.dcs.MuTect2.bam --max-reads-per-alignment-start 0 --max-population-af 1 --disable-tool-default-read-filters; 12:18:10.900 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; 12:18:11.387 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.0.3.0; 12:18:11.388 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:18:11.388 INFO Mutect2 - Executing as loeblabm11@LoeblabM11s-iMac.local on Mac OS X v10.12.6 x86_64; 12:18:11.388 INFO Mutect2 - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 12:18:11.388 INFO Mutect2 - Start Date/Time: April 11, 2018 12:18:10 PM PDT; 12:18:11.388 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - HTSJDK Version: 2.14.3; 12:18:11.388 INFO Mutect2 - Picard Version: 2.17.2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665:840,Load,Loading,840,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665,1,['Load'],['Loading']
Performance,A double query is being performed to smooth over the chr/non-chr b/w b37 and hg19 and GENCODE,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4798:24,perform,performed,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4798,1,['perform'],['performed']
Performance,"A few interrelated issues:. -The install_R_packages.R script is copied and installed in the base Docker image. However, it is currently also copied (but not installed) in the non-base Docker image for some reason. @jamesemery may be able to comment (#4251).; -Different R packages are installed in that script in different ways. Some are pegged to older versions sourced from http://cran.r-project.org/src/contrib/Archive URLs; this is to prevent the http://cran.r-project.org/src/contrib URLs for the most recent versions from breaking out under us, which has happened frequently in the past. Other packages are simply installed using `dependencies = ...`; -We should perhaps consider moving the R dependencies into the conda environment, see discussion in #4209.; -R dependencies are cached in a `site-library` folder in the Travis build to avoid intermittent connection issues with the aforementioned URLs. This can cause tests to break after the fact if the cache is not cleared every time a dependency is removed. If we decide to cache pip installs similarly, we will also run into this issue.; -Requiring the base Docker image to be updated every time an R dependency is changed is also fragile. If it is accidentally not updated when dependencies are removed, tests can continue to pass.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4250:786,cache,cached,786,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4250,3,['cache'],"['cache', 'cached']"
Performance,"A few minor issues:. - [x] Change `--resource <blah>` to `--resource:<blah>` in tool-level documentation. EDIT: Added to the sl_lite_overlap branch mentioned below.; - [x] The VCF writer in VariantRecalibrator has a few conditionals to allow for VCF headers without contig lines, we could do the same for the writer in LabeledVariantAnnotationsWalker. EDIT: Added to the sl_lite_overlap branch mentioned below.; - [ ] Double check whether we should worry about any differences in extraction on test data (provided via email) from https://gatk.broadinstitute.org/hc/en-us/community/posts/7974912707099-VariantRecalibrator-IndexOutOfBoundsException. Probably nothing to worry about, and at least the error messaging in the new tools is more informative.; - [x] We could change the strategy for checking for resource overlaps to require allele-level matching (rather than only matching on start position, as was inherited from VQSR). A quick test on malaria shows that this can reduce the number of overlaps by O(10%), but performance doesn't really change too much. Branch is already open at https://github.com/broadinstitute/gatk/tree/sl_lite_overlap; - [ ] Expand the exact-match tests to cover some of these strategies, which were added separately in #8049 and merged to make a release deadline.; - [x] Catch the exception in https://github.com/broadinstitute/gatk/blob/fd782504d18b56dbc266c2b3bb4eb32f21916776/src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/scalable/LabeledVariantAnnotationsWalker.java#L389 and throw the same message that is thrown in AS mode. Added in #8074.; - [x] Add message to the score tool that the scores HDF5 file will not be out when the input VCF is empty (such a message is already emitted about the annotations HDF5 file). Added in #8074.; - [ ] Megan suggested in the review of #8074 that dynamic disk sizing could be added to the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1222787946:1020,perform,performance,1020,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1222787946,2,"['perform', 'scalab']","['performance', 'scalable']"
Performance,A handful of simple optimizations for VariantRecalibrator:; - Preallocate arrays when the size is known; - Eliminate unnecessary boxing of doubles; - Lift some loop invariants with unnecessary allocations (this eliminates millions of array allocations on the full SNP test used by GATK3). The current GATK4 (multi-variant walker) implementation is about 3% faster than GATK3 without these; these bring it to about 6% faster.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2186:20,optimiz,optimizations,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2186,1,['optimiz'],['optimizations']
Performance,"A happy side effect is that it makes it possible for me to run it locally -; ReadsSparkSink currently only seems to work when run on a cluster (I am; sure this can be fixed though). That said, I have not had the luxury of time to optimize this whole thing,; so someone will get all the fun of comparing the two approaches,; identifying the bottlenecks, and melding the two together in the best way; possible. My goal is merely to make this algorithm available quickly so we; can see if it can be as helpful here as it has been on the Dataflow side. On Mon, Oct 12, 2015 at 7:37 AM, Tom White notifications@github.com wrote:. > At a high-level, the approach here seems to be to create shards (of size 1; > million bps) and load the reads, variants and references for each shard in; > memory. Each shard then has a recalibration table created for it, then the; > tables are merged into one.; > ; > The existing version finds the variants for each read, and has two; > shuffles (this is for the reference broadcast approach, there's an extra; > one for the reference shuffle approach). The first is to join the variants; > and reads together, and a second to aggregate the variants for each read.; > The optimization in this PR has no shuffles, since it loads all variants; > into memory rather than doing distributed joins. Is that a valid; > assumption? If so, it would be possible to load the variants into memory in; > the driver and broadcast to all workers to remove the shuffles (in the; > existing implementation).; > ; > I noticed that AddContextDataToReadSpark#subdivideAndFillReads opens the; > BAM itself, rather than using ReadsSparkSink. This won't work well in the; > Hadoop case since you lose locality - i.e. the work won't be scheduled on; > the node where the BAM files are stored. It would be much better to find a; > way of using ReadsSparkSink.; > ; > ; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/pull/987#issuecomment-147415253.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/987#issuecomment-147476006:2494,optimiz,optimization,2494,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987#issuecomment-147476006,3,"['load', 'optimiz']","['load', 'loads', 'optimization']"
Performance,A lot of data we have lives on NFS (or underlying IFS - Isilon FS). Copying files in and out is a bottleneck and a pain. This ticket for an implementation of a parallel copy of a BAM/CRAM file to HDFS (sharded or unsharded),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1509:98,bottleneck,bottleneck,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1509,1,['bottleneck'],['bottleneck']
Performance,"A quick look at the code results in the following finding:. The error message : `""SA-BWT inconsistency: seq_len is not the same.""` is generated in function `bwt_restore_sa()` defined in `bwt.c`, and resulted in an `abort()` call.; `bwt_restore_sa()` itself was called in `bwa_idx_load_bwt()` defined in `bwa.c`, when trying to restore the suffix array from a file with extension "".sa"". This function call is issued when bwa mem (in its main) tries to load the reference information. This happens before input are read. Because of the `abort()` call, letting the `BwaMem` class handle the error is difficult, so I would propose two possible solutions:; 1. changing the behavior of `BwaIndex` class, so that it eagerly loads the reference, and throws an `Java.lang.IOException` when this error is encountered. Of course this must lead to code duplication, i.e. copying a large part of index loading code from bwa itself and walk around`abort()`ing.; 2. ask @lh3 to change the behavior so that it will not `abort()` any more, but return a null pointer. But the null pointer return error covers so many error cases that this might not be a good idea.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243181189:451,load,load,451,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243181189,3,['load'],"['load', 'loading', 'loads']"
Performance,"A recent palantir investigation showed that disabling BAQ has only a very miniscule effect on variant calls, while providing a 77% performance boost. We should disable it by default in GATK4 (unless @vdauwera objects).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2060:131,perform,performance,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2060,1,['perform'],['performance']
Performance,"A user gets an issue with the HDF5 library when running DenoiseReadCounts on an arm64 processor. We would like to create a fallback solution for this tool, since in this case the tool is not working with HDF5 files. This request was created from a contribution made by dbpiero on June 01, 2021 10:22 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0. b) Exact command used: gatk DenoiseReadCounts -I sample.counts.tsv --annotated-intervals annotated\_intervals.tsv --standardized-copy-ratios sample.standardizedCR.tsv --denoised-copy-ratios sample.denoisedCR.tsv. c) Entire error log: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. Dear Administrators,. I try to run DenoiseReadCounts on new apple silicon chip (M1) with arm64 architecture, but I got this error: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. I created a docker with ubuntu 20.04 to launch gatk and I have already installed libhdf5-103:arm64 library and hdf5-tools inside but launching DenoiseReadCounts i get the same error. Is there a way to solve this issue?. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/161375'>Zendesk ticket #161375</a>)<br>gz#161375</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7297:920,load,load,920,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7297,2,['load'],['load']
Performance,"A user has encountered the following error when running GenotypeGVCFs on an input: ; ```Using GATK jar /nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /lustre/haven/proj/UTHSC0013/Tristan_GATK/reference/genome.fa -V gendb:///lustre/haven/proj/UTHSC0013/Tristan_GATK//DB/chr7 -G StandardAnnotation --use-new-qual-calculator -O /lustre/haven/proj/UTHSC0013/Tristan_GATK//gvcf//merged//joint_called_gvcfs_chr7.vcf; 23:15:47.053 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 23:15:47.249 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 07, 2020 11:15:49 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 23:15:49.543 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.545 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 23:15:49.546 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:15:49.547 INFO GenotypeGVCFs - Executing as hchen3@acf-knl002 on Linux v3.10.0-514.26.1.el7.x86_64 amd64; 23:15:49.548 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 23:15:49.548 INFO GenotypeGVCFs - Start Date/Time: January 7, 2020 11:15:47 PM EST; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.551 INFO ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357:877,Load,Loading,877,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357,1,['Load'],['Loading']
Performance,"A user has reported an error in the following code, on inspection it seems the only way we could be seeing this ArrayIndexOutOfBounds exception is if the byte in the recovered base array is -2, which should absolutely not be the case. I am asking for more context in the hopes of figuring out what is going on but this seems to be related with Cache misses due to the traversal pattern for CombineGVCFs. The issue can be found here: https://gatkforums.broadinstitute.org/gatk/discussion/24705/gatk-combinegvcfs-java-lang-arrayindexoutofboundsexception-index-2-out-of-bounds-for-length-256#latest. ```; java.lang.ArrayIndexOutOfBoundsException: Index -2 out of bounds for length 256; at org.broadinstitute.hellbender.utils.BaseUtils.convertIUPACtoN(BaseUtils.java:120); at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.getSubsequenceAt(CachingIndexedFastaSequenceFile.java:326); at org.broadinstitute.hellbender.engine.ReferenceFileSource.queryAndPrefetch(ReferenceFileSource.java:78); at org.broadinstitute.hellbender.engine.ReferenceDataSource.queryAndPrefetch(ReferenceDataSource.java:64); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6338:344,Cache,Cache,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6338,1,['Cache'],['Cache']
Performance,"A user reported that when running GetPileupSummaries on gnomad vcf, the tool runs out of java heap memory. Xmx value was set to `-Xmx30G` and the machine has >200G RAM. **User Report**: I'm trying to run the cross sample contamination check on my samples, but GetPileupSummaries (4.1.1.0) keeps running out of memory, even when running a single sample on a VM that has >200GB of RAM available. <pre>; 14:35:16.874 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:35:17.116 INFO GetPileupSummaries - ------------------------------------------------------------; 14:35:17.117 INFO GetPileupSummaries - The Genome Analysis Toolkit (GATK) v4.1.1.0; 14:35:17.117 INFO GetPileupSummaries - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:35:17.118 INFO GetPileupSummaries - Executing as root@c64bec8aea6f on Linux v4.15.0-47-generic amd64; 14:35:17.118 INFO GetPileupSummaries - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 14:35:17.118 INFO GetPileupSummaries - Start Date/Time: April 24, 2019 2:35:16 PM UTC; 14:35:17.118 INFO GetPileupSummaries - ------------------------------------------------------------; 14:35:17.119 INFO GetPileupSummaries - ------------------------------------------------------------; 14:35:17.119 INFO GetPileupSummaries - HTSJDK Version: 2.19.0; 14:35:17.119 INFO GetPileupSummaries - Picard Version: 2.19.0; 14:35:17.120 INFO GetPileupSummaries - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:35:17.120 INFO GetPileupSummaries - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:17.120 INFO GetPileupSummaries - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:35:17.120 INFO GetPileupSummaries - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:17.120 INFO GetPileupSummaries - Deflater: IntelDeflater; 14:35:17.120 INFO GetPileupSummaries - Inflater: IntelInflater;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5918:441,Load,Loading,441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5918,1,['Load'],['Loading']
Performance,"A user reports getting the following unhelpful stack trace when running a local job. . ```; Gokalps-Mac-mini:1000GVCFs sky$ gatk SelectVariants -V 1000G_CEU_chr16.vcf.gz -O 1000G_CEU_AFfilt_chr16.vcf.gz -select ""AF > 0.0""; Using GATK jar /Users/sky/scripts/gatk-package-4.0.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /Users/sky/scripts/gatk-package-4.0.9.0-local.jar SelectVariants -V 1000G_CEU_chr16.vcf.gz -O 1000G_CEU_AFfilt_chr16.vcf.gz -select AF > 0.0; 14:35:45.842 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/sky/scripts/gatk-package-4.0.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 24, 2018 2:35:47 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: No route to host (connect failed); at java.net.PlainSocketImpl.socketConnect(Native Method); at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); at java.net.Socket.connect(Socket.java:589); at sun.net.NetworkClient.doConnect(NetworkClient.java:175); at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); at sun.net.www.http.HttpClient.New(HttpClient.java:339); at sun.net.www.http.HttpClient.New(HttpClient.java:357); at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5220:646,Load,Loading,646,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220,1,['Load'],['Loading']
Performance,"ACKTRACE_ON_USER_EXCEPTION=true --jar /Users/markw/IdeaProjects/gatk/build/libs/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar -- PrintReadsSpark -I gs://mw-pathseq-test/hs37d5cs.reads.sorted.bam -O hs37d5cs.reads.txt --apiKey XXXXXXXXXXXXXXXXXXXXX --verbosity DEBUG --sparkMaster yarn; Copying file:///Users/markw/IdeaProjects/gatk/build/libs/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar [Content-Type=application/java-archive]...; - [1 files][ 95.3 MiB/ 95.3 MiB] 9.0 MiB/s; Operation completed over 1 objects/95.3 MiB.; Job [5b3d4225-0547-4aa9-8a83-ab26460aa2d2] submitted.; Waiting for job output...; 21:42:45.768 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/tmp/5b3d4225-0547-4aa9-8a83-ab26460aa2d2/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 21:42:45.791 DEBUG IntelGKLUtils - Extracted Intel GKL to /tmp/root/libgkl_compression6493251482684327282.so. 21:42:45.792 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [February 6, 2017 9:42:45 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output hs37d5cs.reads.txt --input gs://mw-pathseq-test/hs37d5cs.reads.sorted.bam --apiKey XXXXXXXXXXXXXXXX --sparkMaster yarn --verbosity DEBUG --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [February 6, 2017 9:42:45 PM UTC] Executing as root@mw-test-m on Linux 3.16.0-4-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: Version:4.alpha.2-157-g7d7c5ec-SNAPSHOT; 21:42:45.795 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; 21:42:45.795 INFO PrintReadsSpark - Defaults.COMPRESSION_LEVEL : 1; 21:42:45.795 INFO PrintReadsSpark - Defaults.CREATE_INDEX : false; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277823929:2645,load,loaded,2645,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277823929,1,['load'],['loaded']
Performance,ADAM ReadsSparkSinkUnitTest.loadReadsADAM test fails on NA12878.chr17_69k_70k.dictFix.bam,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1267:28,load,loadReadsADAM,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1267,1,['load'],['loadReadsADAM']
Performance,"AD_FOR_SAMTOOLS : false; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:44:02.752 INFO GenomicsDBImport - Deflater: IntelDeflater; 15:44:02.752 INFO GenomicsDBImport - Inflater: IntelInflater; 15:44:02.752 INFO GenomicsDBImport - GCS max retries/reopens: 20; 15:44:02.752 INFO GenomicsDBImport - Requester pays: disabled; 15:44:02.752 INFO GenomicsDBImport - Initializing engine; 15:44:07.262 INFO FeatureManager - Using codec BEDCodec to read file file:///home/akansha/vivekruhela/gatk_bundle/hglift_genome1.bed; 15:44:07.274 INFO IntervalArgumentCollection - Processing 2759468497 bp from intervals; 15:44:07.276 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 15:44:07.307 INFO GenomicsDBImport - Done initializing engine; 15:44:07.591 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 15:44:07.592 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/akansha/vivekruhela/pon_db/vidmap.json; 15:44:07.592 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/akansha/vivekruhela/pon_db/callset.json; 15:44:07.592 INFO GenomicsDBImport - Complete VCF Header will be written to /home/akansha/vivekruhela/pon_db/vcfheader.vcf; 15:44:07.592 INFO GenomicsDBImport - Importing to workspace - /home/akansha/vivekruhela/pon_db; 15:44:07.592 WARN GenomicsDBImport - GenomicsDBImport cannot use multiple VCF reader threads for initialization when the number of intervals is greater than 1. Falling back to serial VCF reader initialization.; 15:44:07.592 INFO ProgressMeter - Starting traversal; 15:44:07.593 INFO ProgressMeter - Current L",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811:2610,perform,performance,2610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811,1,['perform'],['performance']
Performance,"AD_FOR_SAMTOOLS : false; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:42:22.413 INFO HaplotypeCaller - Deflater: IntelDeflater; 22:42:22.413 INFO HaplotypeCaller - Inflater: IntelInflater; 22:42:22.413 INFO HaplotypeCaller - GCS max retries/reopens: 20; 22:42:22.413 INFO HaplotypeCaller - Requester pays: disabled; 22:42:22.413 INFO HaplotypeCaller - Initializing engine; 22:42:22.705 INFO IntervalArgumentCollection - Processing 2001 bp from intervals; 22:42:22.710 INFO HaplotypeCaller - Done initializing engine; 22:42:22.712 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 22:42:22.719 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 22:42:22.720 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.dylib; 22:42:22.722 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 22:42:22.724 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 22:42:22.724 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 22:42:22.734 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 22:42:22.734 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 22:42:22.734 INFO NativeLibrar",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:2641,Load,Loading,2641,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['Load'],['Loading']
Performance,"ATK jar /share/apps/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -Xms60g -jar /share/apps/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar GenomicsDBImport --genomicsdb-workspace-path data/interim/combined_database_bpres/0004 --batch-size 50 --reader-threads 6 --sample-name-map data/processed/sample_map --intervals data/processed/scattered_intervals/0004-scattered.intervals --tmp-dir /scratch/sdturner/genomicsdbimport/0004; ```. #### Expected behavior; My understanding is that it may be more efficient to use a small buffer and write the final database in full. . #### Actual behavior; Again my (limited) understanding is that the tool is writing output multiple times and throwing out all but the last write. Here is an example of a log for a 2.6 Mb region and 295 samples: ; ; ```; 07:24:39.198 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/apps/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 28, 2020 7:24:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 07:24:39.616 INFO GenomicsDBImport - ------------------------------------------------------------; 07:24:39.617 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.4.1; 07:24:39.617 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 07:24:39.617 INFO GenomicsDBImport - Executing as sdturner@c6-74 on Linux v4.15.0-65-generic amd64; 07:24:39.617 INFO GenomicsDBImport - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_20-b26; 07:24:39.617 INFO GenomicsDBImport - Start Date/Time: February 28, 2020 7:24:39 AM PST; 07:24:39.617 INFO GenomicsDBImport - ---------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6487:1378,Load,Loading,1378,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6487,1,['Load'],['Loading']
Performance,"ATK version (4.1.4.1), however it still persists in the current GATK version. #### Steps to reproduce; Issue was reproduced by the support team using just chr2. #### Associated forum post; https://gatk.broadinstitute.org/hc/en-us/community/posts/360072506131-leftalignindels-java-lang-IllegalArgumentException-the-range-cannot-contain-negative-indices. Command: ; gatk LeftAlignIndels \; -R /projects/beck-lab/walawi/GATK/hg38.fa \; -I /projects/beck-lab/walawi/GATK/sorted_bam/H1-L_GT19_35266_GACCTGAA-TTGGTGAG_S2.withRG.sorted.bam \; -O /projects/beck-lab/walawi/GATK/sorted_bam/leftalignindels/H1-L_GT19_35266_GACCTGAA-TTGGTGAG_S2_leftaligned.withRG.sorted.bam. ```; gatk LeftAlignIndels -R /projects/beck-lab/walawi/GATK/hg38.fa -I /projects/beck-lab/walawi/GATK/sorted_bam/H1-L_GT19_35266_GACCTGAA-TTGGTGAG_S2.withRG.sorted.bam -O /projects/beck-lab/walawi/GATK/sorted_bam/leftalignindels/H1-L_GT19_35266_GACCTGAA-TTGGTGAG_S2_leftaligned.withRG.sorted.bam. 10:43:19.190 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/walawi/miniconda3/envs/gatk4_venv/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so. Aug 18, 2020 10:43:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine. INFO: Failed to detect whether we are running on Google Compute Engine. 10:43:19.667 INFO LeftAlignIndels - ------------------------------------------------------------. 10:43:19.668 INFO LeftAlignIndels - The Genome Analysis Toolkit (GATK) v4.1.7.0. 10:43:19.668 INFO LeftAlignIndels - For support and documentation go to https://software.broadinstitute.org/gatk/. 10:43:19.668 INFO LeftAlignIndels - Executing as walawi@sumner055 on Linux v3.10.0-1062.1.2.el7.x86_64 amd64. 10:43:19.668 INFO LeftAlignIndels - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01. 10:43:19.668 INFO LeftAlignIndels - Start Date/Time: August 18, 2020 10:43:19 AM EDT. 10:43:19.668 INFO LeftAlignIndels - -------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6765:1335,Load,Loading,1335,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6765,1,['Load'],['Loading']
Performance,"About 3% of our GATK 4.0.0.0 GenotypeGVCFs runs (with a GenomicsDB as input) are failing with a `__pthread_tpp_change_priority` error and exiting with status -6. The stderr of such a run ends like this:; ```; 2018-03-10T07:14:27.165578644Z GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),74.01474455399772,Cpu time(s),62.96424693700022; 2018-03-10T07:14:27.168329699Z java: tpp.c:84: __pthread_tpp_change_priority: Assertion `new_prio == -1 || (new_prio >= fifo_min_prio && new_prio <= fifo_max_prio)' failed.; ```. Stdout from the same run:; ```; 2018-03-09T13:13:41.095913747Z 13:13:41.095 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk.jar!/com/intel/gkl/native/libgkl_compression.so; 2018-03-09T13:13:41.329888610Z 13:13:41.327 INFO GenotypeGVCFs - ------------------------------------------------------------; 2018-03-09T13:13:41.329934964Z 13:13:41.327 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.0.0; 2018-03-09T13:13:41.329942970Z 13:13:41.327 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 2018-03-09T13:13:41.329952404Z 13:13:41.328 INFO GenotypeGVCFs - Executing as root@localhost on Linux v4.4.0-112-generic amd64; 2018-03-09T13:13:41.329960555Z 13:13:41.328 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_151-8u151-b12-1~deb9u1-b12; 2018-03-09T13:13:41.329988865Z 13:13:41.328 INFO GenotypeGVCFs - Start Date/Time: March 9, 2018 1:13:41 PM UTC; 2018-03-09T13:13:41.329995417Z 13:13:41.328 INFO GenotypeGVCFs - ------------------------------------------------------------; 2018-03-09T13:13:41.330000910Z 13:13:41.328 INFO GenotypeGVCFs - ------------------------------------------------------------; 2018-03-09T13:13:41.330011002Z 13:13:41.328 INFO GenotypeGVCFs - HTSJDK Version: 2.13.2; 2018-03-09T13:13:41.330022980Z 13:13:41.328 INFO GenotypeGVCFs - Picard Version: 2.17.2; 2018-03-09T13:13:41.330030226Z 13:13:41.329 INFO GenotypeGVCFs - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4518:640,Load,Loading,640,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4518,1,['Load'],['Loading']
Performance,Access.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:3542,concurren,concurrent,3542,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,2,['concurren'],['concurrent']
Performance,"According to #2858, the new [GATK CNV pipeline](https://github.com/broadinstitute/gatk/tree/master/scripts/cnv_wdl/somatic) is intended to replace AllelicCNV (because it now segments jointly on total copy ratio and allelic fraction). We've found the segmentation to be great for WGS data, but the new workflow does not create the same outputs as AllelicCNV - in particular, AllelicCNV generated a *-sim-final.acs.seg that could be used for [ABSOLUTE](https://software.broadinstitute.org/cancer/cga/absolute) and [DeTiN](https://github.com/getzlab/deTiN). We'd like to run these tools - Is there any way to get the equivalent of this file from the workflow's outputs? None of the outputs look like *-sim-final.acs.seg. . If not, I had planned to simply run AllelicCNV (or AllelicCapseg) using files from the new workflow. The only issue is that the input files are unclear to me - I've provided a table below with what I believe the matchups relative to the old GATK CNV workflow to be, but it would be great to get clarification!. Name of file | Old GATK CNV (task) | New GATK CNV (task); -- | -- | --; tumorHets | *.tumor.hets.tsv (GetHetCoverage) | *.hets.tsv (ModelSegments); segments | *.seg (PerformSegmentation) | *.modelFinal.seg (ModelSegments); tangentNormalized | *.tn.tsv (NormalizeSomaticReadCounts) | ????? (maybe .denoisedCR.tsv from DenoiseReadCounts?)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6685:1197,Perform,PerformSegmentation,1197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6685,1,['Perform'],['PerformSegmentation']
Performance,"According to this paper https://www.nature.com/articles/s41467-018-03590-5. it is: ""Each resulting qualified captured library with the SureSelect Human; All Exon kit (Aglient) was then loaded on *BGISEQ-5000 *sequencing; platforms, and we performed high-throughput sequencing for each captured; library. High-quality reads were aligned to the human reference genome; (GRCh37) using the Burrows-Wheeler Aligner (BWA v0.7.15) software. All; genomic variations, including single-nucleotide polymorphisms and InDels; were detected by *HaplotypeCaller of GATK *(v3.0.0).; "". On Wed, Dec 12, 2018 at 3:18 PM Louis Bergelson <notifications@github.com>; wrote:. > @yfarjoun <https://github.com/yfarjoun> Do you know if BGI's sequencing; > is compatible with our tools without any special treatment?; >; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446729153>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0mujY7YzxUJ-6IPU8B7jPiZWQuzMks5u4WR3gaJpZM4ZQNxZ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446769514:185,load,loaded,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446769514,3,"['load', 'perform', 'throughput']","['loaded', 'performed', 'throughput']"
Performance,"Actually, if you have the time, an even more valuable test would be to repeat your comparison with latest master vs. a rebased copy of this branch onto latest master. That would tell us whether the performance difference you saw is due to the downsampling, or due to the differences in the traversal code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330921307:198,perform,performance,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330921307,1,['perform'],['performance']
Performance,Add a mergeWithRemapping() method in ReferenceConfidenceVariantContextMerger to perform allele remapping prior to genotyping,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8318:80,perform,perform,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8318,1,['perform'],['perform']
Performance,Add an option to merge intervals for better GGVCFs performance on GDB exome input,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5741:51,perform,performance,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5741,1,['perform'],['performance']
Performance,"Add dependency on jbwa snapshot, script to release future jbwa snapshots, and method to load the library at runtime",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1847:88,load,load,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847,1,['load'],['load']
Performance,Add load lock file to prevent accidental re-loading of data to BQ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7138:4,load,load,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7138,2,['load'],"['load', 'loading']"
Performance,Add option to allow for concurrent BQ queries in PrepareCallset,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7505:24,concurren,concurrent,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7505,1,['concurren'],['concurrent']
Performance,Add warning to SelectVariants about poor performance if the samples are not sorted in the VCF header,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7732:41,perform,performance,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7732,1,['perform'],['performance']
Performance,"Addded abstract class MachineLearningUtils to provide an interface and; handle common tasks. These include loading data, splitting data into; training and test sets, cross-validation, and optimizing classifier; hyperparameters. Also added XGBoostUtils which provides a concrete implemention of; MachineLearningUtils (by wrapping xgboost4j) and serves as an example; of how to provide access to a 3rd-party machine learning library. Finally, added an example tool: ExampleTrainXGBoostClassifier. This; demonstrates a typical training use case of loading data, training a; classifier, assessing accuracy, and saving the classifier. It also; demonstrates a typical filtering use case of loading a saved classifer,; and using it to calculate probabilities or class labels. This is working towards issue 4922 by providing the tools necessary to; train classifiers in general, but does not provide tools to train a; BreakpointEvidence filter, so does not resolve it. Additionally, this; framework should eventually be extended to provide a bayesian; hyperparameter optimizer. One outstanding problem with these changes is that xgboost4j threading; does not appear to work on OSX, resulting in slower training. However,; it does work on linux.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5146:107,load,loading,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146,5,"['load', 'optimiz']","['loading', 'optimizer', 'optimizing']"
Performance,Added FilterIntervals to perform annotation-based and count-based filtering in the gCNV pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5307:25,perform,perform,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5307,1,['perform'],['perform']
Performance,Added Utils::split (and tests). Added performance test for string splitting.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3776:38,perform,performance,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3776,1,['perform'],['performance']
Performance,Added some javadoc notes about disk throughput to MarkDuplicatesSpark and SortSamSpark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5672:36,throughput,throughput,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5672,1,['throughput'],['throughput']
Performance,"Added the ability to specify IntervalMergingRule.NONE so so that no merging is performed. Also added the ability to request from IntervalArgumentCollection the unmerged user intervals. I have not solved the underlying issue that the GenomeLocSet requires non-overlapping intervals, though I acknowledge that replacing or refactoring that class is the long term solution to this problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5887:79,perform,performed,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5887,1,['perform'],['performed']
Performance,Added the following Adam optimization / learning parameters to the command-line:. - learning rate; - beta1; - beta2; - epsilon; - clipnorm,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8483:25,optimiz,optimization,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8483,1,['optimiz'],['optimization']
Performance,"Added:. - service account support for SitesOnly task, added GATK flag to not output a timestamp info VCF to help Call Caching; - switch to SSD, increased size and moved to pre-emptibles for Annotate task; - service account support for BQ Loading; - service account support for BQ Smoke Test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7345:238,Load,Loading,238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7345,1,['Load'],['Loading']
Performance,"Adding a new method `getVariantCacheLookAheadBases` to `VariantWalkerBase` which allows subclasses to set how far to look ahead when caching variants. This may help reduce memory use in GenotypeGVCFs. This also changes the side inputs to use FeatureDataSource.DEFAULT_QUERY_LOOKAHEAD_BASES which is `1000`, this is the value used by the other tools. I'm not sure if that's the right thing to do, but it makes variant walkers more consistent with other tools. Alternatively we could add a separate configuration method that lets tools change the side input value. We could also expose an optional parameter in the feature input that lets you set that on a per input basis if we need it. . This doesn't seem to have any negative effect on performance for genotypegvcfs, but it's hard to tell from short runs. It's also hard to tell if it's improving memory usage. It doesn't seem to make an appreciable difference at random places in the genome, but I'm hoping it will make a difference in very bad locations that have a lot of variation. Ideally our caches would be based on size rather than number of variants, but that's a more complicated change. fixes #3471",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3480:737,perform,performance,737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3480,2,"['cache', 'perform']","['caches', 'performance']"
Performance,Adding a note to developers that if they change the install_R_scripts script they must rebuild gatkbase and test with clean caches.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4289:124,cache,caches,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4289,1,['cache'],['caches']
Performance,"Adding an R dependency for a package that improves performance and memory usage for reading large TSV files. This is convenient for generating CNV plots from WGS data. Note: The current CNV plotting code has other issues that make it unsuitable for WGS data; these are addressed in the sl_wgs_acnv dev branch for the new pipeline by new versions of the plotting tools. However, I do not plan on making these fixes to the old versions of the plotting tools. The real purpose of this PR is just to get the additional R dependency merged into master so I can build a new docker for the dev branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3693:51,perform,performance,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3693,1,['perform'],['performance']
Performance,Adding an advanced option --sampleNameMap to GenomicsDBImport which can be specified instead of -V; * This allows providing a file mapping samples to their vcf uri which is used instead of loading each header and checking the sample names.; * The first file in the mapping is chosen to act as the header template. If the headers are not consistent there may be data corruption. A mapping file is a tab seperated file with no header in the format. Sample1	file1; Sample2 file2; Sample3 file3; ...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2675:189,load,loading,189,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2675,1,['load'],['loading']
Performance,"Adding the following helps debug where snappy is being loaded from. ```; --driver-java-options -verbose:class; ```. And gives. ```; [Loaded org.xerial.snappy.SnappyNativeLoader from __JVM_DefineClass__]; [Loaded org.xerial.snappy.SnappyNativeAPI from __JVM_DefineClass__]; [Loaded org.xerial.snappy.SnappyNative from __JVM_DefineClass__]; [Loaded org.xerial.snappy.SnappyErrorCode from __JVM_DefineClass__]; [Loaded org.xerial.snappy.OSInfo from file:/opt/cloudera/parcels/CDH-5.7.0-1.cdh5.7.0.p0.45/jars/snappy-java-1.0.4.1.jar]; ...; [Loaded org.xerial.snappy.SnappyNativeAPI from file:/opt/cloudera/parcels/CDH-5.7.0-1.cdh5.7.0.p0.45/jars/snappy-java-1.0.4.1.jar]; [Loaded org.xerial.snappy.SnappyNative from file:/opt/cloudera/parcels/CDH-5.7.0-1.cdh5.7.0.p0.45/jars/snappy-java-1.0.4.1.jar] ; ```. If I add a later version of snappy as an extra JAR on the command line using. ```; --conf spark.driver.extraClassPath=/home/tom/workspace/gatk/build/install/gatk/lib/snappy-java-1.1.2.jar; ```. then it works. And I only get one `Loaded org.xerial.snappy.SnappyNative` and the one from `__JVM_DefineClass__` doesn't appear:. ```; [Loaded org.xerial.snappy.SnappyNative from file:/home/tom/workspace/gatk/build/install/gatk/lib/snappy-java-1.1.2.jar]; ```. If I try to add snappy-java-1.1.2.jar as a dependency and include it in the spark jar then it doesn't work and it's loaded from snappy-java-1.0.4.1.jar, despite `spark.driver.userClassPathFirst` being set to `true`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1873#issuecomment-229319855:55,load,loaded,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1873#issuecomment-229319855,11,"['Load', 'load']","['Loaded', 'loaded']"
Performance,"Additional cleanup on the VAT--specifically focused on the failing shards and optimizing the workflow with them in mind. As a next step, this workflow will be split up into 3 sub-workflows to keep the failures from knocking over the remaining likely-successful shards",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7531:78,optimiz,optimizing,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7531,1,['optimiz'],['optimizing']
Performance,"Additionally. You may wish to compare your variant depths and allelic balances to those captured in gnomAD 4.1 ; [Variant page here](https://gnomad.broadinstitute.org/variant/15-93002203-G-GA?dataset=gnomad_r4); Looking at the balances of heterozygous calls your sites don't seem to display any inconsistencies compared to those available in gnomAD sample set. . Artificial Haplotypes are produced by HaplotypeCaller are available only for debugging purposes. Bamout also includes those informative reads used to produce these haplotypes to provide additional debugging support. Soft clipped bases, realignment, overlapping pairs, duplicates and reads that are available to HaplotypeCaller due to interval restrictions all affect the way you observe these bamouts. . HaplotypeCaller by design is not 100% deterministic in performing its calls. But this does not mean it is inconsistent in making calls when provided all the available reads and reference for its function. That is why we don't recommend restricting HaplotypeCaller when making variant calls with short intervals.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8959#issuecomment-2304607950:822,perform,performing,822,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8959#issuecomment-2304607950,1,['perform'],['performing']
Performance,"Addresses [219](https://github.com/broadinstitute/dsp-spec-ops/issues/219). Major changes. - calculate site level metrics in `feature_extract.sql`; - extract metrics, apply thresholds, and set filter field in ExtractFeature; - CreateSiteFilteringFiles to translate from input VCF with filter fields into format for BQ loading, especially `location` fields; - update WDL to call CreateSiteFilteringFiles and upload results to BQ. Minor changes; - added call_GQ to alt_allele creation; - reduced memory requirements in WDL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7197:318,load,loading,318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7197,1,['load'],['loading']
Performance,"Adds PathSeqPipelineSpark master tool, which required some minor changes:; -Refactored PSScoreUtils class to a PSScorer, which now includes the main Score tool code; -Moved code for paring down the pathogen header into a new function removeUnmappedHeaderSequences(). Spark-related optimizations:; -Removed cache() calls when possible, and replaced with persist(), spilling to disk with serialization, if necessary; -Removed try-with-resources in Filter and Bwa tools, which seemed to be causing the BWA/kmer references to be unloaded prematurely. Other changes:; -Changed ambiguous base filter from using a fraction of bases to number of bases; -Added function for closing all kmer filter instances; -Optimized PSBwaAligner SA tag construction; -Renamed repartitionPairedReads() to repartitionReadsByName(); -Resolved some conflicting tool argument names; -Created PathSeq tool program group; -Filled out tool summary strings",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3271:281,optimiz,optimizations,281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3271,3,"['Optimiz', 'cache', 'optimiz']","['Optimized', 'cache', 'optimizations']"
Performance,"Adds a new ReadWalker tool, `RealignSoftClippedReads`, that realigns soft-clipped fragments using BWA. This tool is motivated by a specific artifact produced by Illumina DRAGEN v3.7.8 in which reads containing small indels are erroneously soft-clipped, often within mobile element contexts (LINE, SINE, ALU, SVA, etc). This is particularly problematic for mobile element insertion callers such as [Scramble](https://github.com/GeneDx/scramble) that rely on soft-clips for identifying potential insertion sites but do not perform a local assembly. In some cases, these soft-clipped reads are aligned to the incorrect region (confirmed by BLAT query and comparison to BWA alignments). An example of a false positive site produced by Scramble is shown below. <img width=""1008"" alt=""Screenshot 2023-11-16 at 2 09 45 PM"" src=""https://github.com/broadinstitute/gatk/assets/5686877/9d2c1dfd-9673-49f0-9372-c4c9cf6ffd9f"">. This PR includes the new tool and unit/integration tests and some minor refactoring to expose non-Spark BWA read mapping. This tool should be considered experimental until thorough benchmarking and analysis can be performed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8588:521,perform,perform,521,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8588,2,['perform'],"['perform', 'performed']"
Performance,"Adds cloud-optimized (scattered) version of gcnv case wdl to the dockstore file. Should be equivalent to the regular case workflow, just faster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8217:11,optimiz,optimized,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8217,1,['optimiz'],['optimized']
Performance,"After #5416 one of the lingering potential sources of difference between HaplotypeCallerSpark and HaplotypeCaller are in the downsampling, which could cost Spark both correctness and performance at pathological sites. This likely requires #5437 or some equivalent change to be implemented so we can save ourselves from materializing and shuffling all the reads in their AssemblyRegions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5476:183,perform,performance,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5476,1,['perform'],['performance']
Performance,"After doing this, do another comparison run against GATK3 BQSR to see how much eliminating this code bought us in terms of performance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1056:123,perform,performance,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1056,1,['perform'],['performance']
Performance,"After more experimentation, one issue I was running into with the ApproxKernSeg method was failure on small and ""epidemic"" events. This is because 1) the segment cost function used in that paper is extensive (growing with the number of points in a segment), and 2) binary segmentation is a global, greedy algorithm. These both cause long events to be preferred over short events, and thus the first changepoints found (and retained after applying the penalty) may not include those for small, obvious events. For example, see performance on this simulated data, which includes events of size 10, 20, 30, and 40 within 100,000 points at S/N ratio 3:1 in addition to sine waves of various frequency at S/N ratio 1:2 (to roughly simulate GC waves). Changepoints arising from the sine waves will be found first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:526,perform,performance,526,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045,1,['perform'],['performance']
Performance,"After we have the non-Spark prototype in https://github.com/broadinstitute/gatk/issues/3283, write a Spark version that partitions the input data in a sensible and scalable way (this ticket itself might need to spawn many sub-tickets).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3285:164,scalab,scalable,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3285,1,['scalab'],['scalable']
Performance,Agreed @samuelklee this is likely due to changes in default parameters but still concerning to see differences like this. The defaults were changed in accordance with the gCNV Nature paper but are designed for exomes. @Stikus you may want to look at the [GATK-SV default settings](https://github.com/broadinstitute/gatk-sv/blob/dc92e9f4bc46a1ab19459b515c24c9747a73a967/wdl/GermlineCNVCohort.wdl#L495) which are tuned for genome calling.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8628#issuecomment-1856621583:411,tune,tuned,411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628#issuecomment-1856621583,1,['tune'],['tuned']
Performance,Ah check the is loaded field in feature extract,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7475:16,load,loaded,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7475,1,['load'],['loaded']
Performance,"Ah, I thought that might be the case, to be honest. . My vote is for ripping out the old. If for whatever reason you find an edge case where the new doesn't perform as well, then in my view that just means the new can be improved. . Ultimately it seems to me that @ldgauthier is the one with the most experience in this space and should make the call.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258170795:157,perform,perform,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258170795,1,['perform'],['perform']
Performance,"Aha, after clearing the travis cache for the PR build it passed! Merging",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5194#issuecomment-422499733:31,cache,cache,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5194#issuecomment-422499733,1,['cache'],['cache']
Performance,"Aha, good -- that's consistent with @skwalker's results showing a ~30%-40% performance regression vs. GATK3 when using a large interval list + the latest GATK4 HC. I think this is something we can resolve through profiling -- I'll update you in a few days with my progress. In the mean time, it would be valuable to me to know whether you see the same performance difference in Mutect2 when running *without* an interval list (latest master vs. this downsampling branch). Perhaps you could create a large-but-not-too-large bam snippet to test that out?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330910227:75,perform,performance,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330910227,2,['perform'],['performance']
Performance,"All that needs to be done here is prove that GenomicsDBImport can run on an 11k sample callset over a single interval without exploding or running out of memory. We don't need to hyper-optimize memory usage, or optimize the instance types for cost, etc. We should also probably pair it with GenotypeGVCFs in a single WDL script, to make sure that tool doesn't blow up either. Once done, we should share the settings we used with red team. Details on how to access the 11k sample set are in a Google doc that has been shared privately.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2633:185,optimiz,optimize,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2633,2,['optimiz'],['optimize']
Performance,"All that you get is the recapitulated command and, e.g.:; ```; 21:54:28.439 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/shlee/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:54:28.625 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:54:28.625 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.0.4.0; 21:54:28.625 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:54:28.625 INFO GermlineCNVCaller - Executing as shlee@brie on Linux v4.13.0-1017-gcp amd64; 21:54:28.625 INFO GermlineCNVCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-0ubuntu0.16.04.1-b11; 21:54:28.625 INFO GermlineCNVCaller - Start Date/Time: May 28, 2018 9:54:28 PM UTC; 21:54:28.625 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:54:28.625 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Version: 2.14.3; 21:54:28.626 INFO GermlineCNVCaller - Picard Version: 2.18.2; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:54:28.627 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:54:28.627 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:54:28.627 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:54:28.627 INFO GermlineCNVCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:54:28.627 WARN GermlineCNVCaller - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4826:103,Load,Loading,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4826,1,['Load'],['Loading']
Performance,"All walkers now have comprehensive sequence dictionary validation performed on their inputs (via the `GATKTool` base class, which is aware of all primary tool inputs and so is able to perform this check automatically -- see `GATKTool.validateSequenceDictionaries()`). At present, we need to do this validation manually in dataflow tools, but it would be nice if we could get it to happen automatically in a base class as it does on the walker side of things.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/669:66,perform,performed,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/669,2,['perform'],"['perform', 'performed']"
Performance,"Allow tools to opt-in for read caching (enabled in this PR for CNNScoreVariants and FilterAlignmentArtifacts) when issuing forward-only queries such as those used to query a ReadsContext for all reads backing each variant. The cache matches the (sometimes surprising - see https://github.com/broadinstitute/gatk/issues/4901) results that are returned when caching is not used, specifically that an unmapped but placed read that is mated with a mapped read is only returned if the *start position* overlaps the interval query interval, whereas the mapped read is returned if any part of the read overlaps the query interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4902:227,cache,cache,227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902,1,['cache'],['cache']
Performance,Almost looks like there is a buffer overrun somewhere. Most of our testing has been on `nfs` and have not encountered a tcache(thread local cache) issue. Is `gpfs` available as open source?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1935180685:140,cache,cache,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1935180685,1,['cache'],['cache']
Performance,"Alright, I'll try to get it published to the Broad artifactory by end of the week. Once it's there, this branch will need to be modified to depend on the version in artifactory, locate the library on the classpath, and extract and load it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-220158188:231,load,load,231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-220158188,1,['load'],['load']
Performance,"Also as part of the mock-up, we should actually package the mock config files inside of our jar, load them off the classpath, and test file-based override ability.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3126#issuecomment-309543353:97,load,load,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126#issuecomment-309543353,1,['load'],['load']
Performance,"Also extracted some argument collections and genotyping code (see https://github.com/broadinstitute/gatk/issues/3915), fixed up some documentation, and did some refactoring to the Segmenter classes. This is just a first implementation for evaluation and feedback. There is some redundant (but cheap) computation performed in the genotyping step and both the genotyping and segmentation steps are not optimized for memory use. However, since requirements are not onerous (probably around ~10GB memory and <10 minutes for ~10 typical WGS samples), it might not be worth fixing up at the expense of extra code. Likewise, this implementation requires all inputs be available. We could relax this to allow optional dimensions of input (i.e., copy ratios or allele counts) and/or case-only mode (as in ModelSegments), at the expense of extra control-flow code. One could also perform segmentation with an external tool and pass it to ModelSegments, as long as it is properly formatted. Closes #2924.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499:312,perform,performed,312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499,3,"['optimiz', 'perform']","['optimized', 'perform', 'performed']"
Performance,Also fill in some cost optimizations for some tasks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3940#issuecomment-351478645:23,optimiz,optimizations,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3940#issuecomment-351478645,1,['optimiz'],['optimizations']
Performance,"Also updated code and documentation to indicate how to properly run this; test. Sadly it has to be done manually because I don't know of a; reasonable way to disable default credentials. Nevertheless it's good that the test is there even for automated runs,; so we can check that loading the explicit credentials does not break; anything.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2879:280,load,loading,280,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2879,1,['load'],['loading']
Performance,"Also, FeatureDataSource.getGenomicsDBFeatureReader requires the reference, but it's never used, apparently:. https://github.com/broadinstitute/gatk/blob/bc0994c180312cdca7afbe45b410b2c6fc312043/src/main/java/org/broadinstitute/hellbender/engine/FeatureDataSource.java#L387. Should something related to GenomicsDBFeatureReader be getting the reference? FeatureData source has an error if it's not provided: ""You must provide a reference if you want to load from GenomicsDB"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-747448285:451,load,load,451,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-747448285,1,['load'],['load']
Performance,"Also, and again not part of this PR, does the concept of trying to run VariantEvalEngine over an interval, serializing its state to disk somehow (which would involve doing something with StratificationManager), and then writing a tool that loads/aggregates those per-interval objects seem reasonable as a way to support scatter/gather?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-754946232:240,load,loads,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-754946232,1,['load'],['loads']
Performance,"Also, if we can't figure this out, then I really think it's worth kicking it up to Cromwell to see if call caching can be made more scalable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-632309675:132,scalab,scalable,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-632309675,1,['scalab'],['scalable']
Performance,"Also, interestingly, if you look at the pipeline we run nightly in jenkins, there hasn't been any performance regression there. https://gatk-jenkins.broadinstitute.org/view/Performance/job/gatk-perf-test-spark-readpipeline/buildTimeTrend",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-364188535:98,perform,performance,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-364188535,2,"['Perform', 'perform']","['Performance', 'performance']"
Performance,"Also, it seems that if the input VCF and the dbSNP are not sorted in the same way, for example:; Input VCF; >chr22; >chrX; >chrY; >chrM. dbSNP; >chr22; >chrM; >chrX; >chrY. again, the comparison analysis will fail with the error; > java.lang.IllegalStateException: The elements of the input Iterators are not sorted according to the comparator htsjdk.variant.variantcontext.VariantContextComparator. where the workaround is again to sort both the Input VCF and the dbSNP according to a specific dict, in order to make sure they are matching (if, for example, you don't know how the input VCF and dbSNP were handled upstream), thus leading to always having to perform sorting before VariantEval, which can be 'expensive' if using the recent dbSNP databases (which are around 80GB in size). (Reposting this, originally posted from a wrong account)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6855#issuecomment-710908612:659,perform,perform,659,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6855#issuecomment-710908612,1,['perform'],['perform']
Performance,"Also, we should check that the maven dependencies are being cached properly per-branch (they don't seem to be at the moment)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1769#issuecomment-214503329:60,cache,cached,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1769#issuecomment-214503329,1,['cache'],['cached']
Performance,An advanced argument to skip multiple inputs. We still want to discourage this behavior for performance reasons so its not the default behavior. Fixes #5973,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5974:92,perform,performance,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5974,1,['perform'],['performance']
Performance,"An initial very basic java interface to allow loading vcfs into GenomicsDB from GATK.; This is expected as a first step towards greater write support for genomics db. Could possibly require json files, but ideally could create them itself. one possible example interface:. ```; GenomicsDBLoader.loadSingleSampleGVCFs(URI workspace, String arrayname, File vidJson, List<File> vcfs); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2087:46,load,loading,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2087,2,['load'],"['loadSingleSampleGVCFs', 'loading']"
Performance,"An optimization introduced in https://github.com/broadinstitute/gatk/pull/5466 was removed in https://github.com/broadinstitute/gatk/pull/6885. The latter exposed Smith-Waterman parameters, allowing them to be changed from their default values and thus to possibly violate conditions assumed by the former. We could restore the optimization if we added explicit checks of these conditions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7441:3,optimiz,optimization,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7441,2,['optimiz'],['optimization']
Performance,"An update: apparently asking the user to regenerate their index and dictionary files for their references has resolved the issue. We should soften the blow of null pointers in the future. Apparently the dictionary was present (as it wouldn't work otherwise) but was corrupt somehow, a proposal from @droazen would be to add to the sequnce dictionary check we already perform a check that the file is readable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6142#issuecomment-529558237:367,perform,perform,367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6142#issuecomment-529558237,1,['perform'],['perform']
Performance,"Another development, a new inference method claiming improved performance over ADVI:. https://arxiv.org/abs/1706.02375. We can think of such inference methods (along with the three with implementations available in Stan, namely MAP, ADVI, and NUTS) as interchangeable black boxes that take the optimization target specified by the modeling language and the corresponding automatically generated derivative as inputs. Whatever JNI layer we implement would ideally allow us to build such boxes in pure Java that call out to the JNI for each target/derivative evaluation, as the amount and complexity of code for such boxes should be relatively manageable (in comparison to that required for the autodiff/autotransformation/modeling infrastructure). However, it remains to be seen whether the overhead of such calls will be acceptable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2746#issuecomment-311091138:62,perform,performance,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2746#issuecomment-311091138,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"Another library that just popped up on my radar---and this one is actually in Java!. http://www.amidsttoolbox.com/; https://arxiv.org/abs/1704.01427; https://arxiv.org/abs/1604.07990. The approach is quite different from the other libraries we have been considering; here, the focus seems to be on streaming/parallel data and inference via message passing. I think our models can be expressed in their framework (although, at a glance, the modeling language is not as nice as Stan---it looks like you have to build DAGs explicitly), but I have to admit that I am not familiar with message passing and how performance compares to MCMC, ADVI, etc. @mbabadi @davidbenjamin any thoughts?. I still think it's worth playing around with this library. We could investigate how quick it would be to implement a streaming/minibatch version of VQSR, for example, which might be useful for @eitanbanks @ldgauthier.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2746#issuecomment-318674946:605,perform,performance,605,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2746#issuecomment-318674946,1,['perform'],['performance']
Performance,"Any objections to exposing SW parameters to the command line? This looks like something we will want to explore for malaria. I'm also not convinced that our current parameters have been justified and/or optimized in any documented way. A few questions:. 1) There are 3 sets of parameters used in various ways, a) haplotype-to-reference alignment, b) read-to-haplotype alignment, and c) dangling ends. Any chance we can evaluate the effect of consolidating at least c), if not all sets? @emeryj I was told that you might be the one to ask about c) in particular; @davidbenjamin speculated that these might effectively yield STR-specific parameters. In general, if there are any quick and readily available evaluations (which ideally include variant normalization), I'd appreciate pointers to them. 2) Any suggestions on what the resulting command line should look like? I don't want to add 12 parameters, in the worst case. I also think that using integer arrays might be clunky. Perhaps I can suggest the use of args files in the doc string---although I don't think that those are expanded in the `##GATKCommandLine`, right?. 3) Should I touch `SWOverhangStrategy` at all? See e.g. https://github.com/broadinstitute/gatk/issues/6576. It looks like we thread both this and the `SWParameters` through many methods and classes, so the code could stand quite a bit of refactoring, but for now I will stick to the minimal changes required to expose. @droazen @ldgauthier any thoughts?. In some simple experiments of changing the a) parameters (from the somewhat questionable `NEW_SW_PARAMETERS = new SWParameters(200, -150, -260, -11)` back to `STANDARD_NGS = new SWParameters(25, -50, -110, -6)`), I've seen that there are non-negligible differences in the calls (beyond representation) at the few percent level, as well as changes in annotations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863:203,optimiz,optimized,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863,1,['optimiz'],['optimized']
Performance,"Any update on this? I just ran into a form of this problem in the context of some pipeline unit tests. I have a task that runs the following:. ```; gatk GenomicsDBImport \; --sample-name-map ${sample_map} \; --genomicsdb-workspace-path ${cohort_name}_gdb \; --genomicsdb-shared-posixfs-optimizations \; -L ${interval_list}. gatk GenotypeGVCFs \; -R ${ref_fasta} \; -V gendb://${cohort_name}_gdb \; -O ${cohort_name}.joint.vcf \; -L ${interval_list} ; ```. Which runs fine, but if I re-run the test suite the system complains it can't delete the gdb workspace. I have to manually `sudo rm` which is gross. I can work around this by adding either `chmod 777 -R ${cohort_name}_gdb` or `rm -r ${cohort_name}_gdb` as a cleanup step, but that seems gross too. . My use case is just a toy example for training purposes, but I worry about what this could mean for a production environment. Am I missing something?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078126120:286,optimiz,optimizations,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078126120,1,['optimiz'],['optimizations']
Performance,"Apart from using htsjdk asyncIO, we should maybe look into running the prefetching (or reads and variants) asynchronously - maybe prefetching+filtering could in be done in a different thread and fed to the walker via a blocking queue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1606#issuecomment-202534392:228,queue,queue,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1606#issuecomment-202534392,1,['queue'],['queue']
Performance,"ApplyBQSR (aka PrintReads) is done. Performance stats look very good. user time, gatk4 is 37% better; GATK3 57555.36 seconds; GATK4 35718.76 seconds . and that's using less CPU: ""Percent of CPU this job got""; GATK3 121%; GATK4 106%. Maximum resident set size (kbytes) - gatk4 uses 82% less memory (?); GATK3 84675872 kbytes; GATK4 14657904 kbytes",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1413#issuecomment-188319033:36,Perform,Performance,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1413#issuecomment-188319033,1,['Perform'],['Performance']
Performance,"Are gradle dependencies cached anywhere for the Travis build? Pretty sure they aren't, but I'm out of ideas why Travis is failing for this repo.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296293145:24,cache,cached,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296293145,1,['cache'],['cached']
Performance,"Are there any updates on this feature? The use of soft-clipping is not only confusing, but can negatively affect the performance of other tools that use this sort of information. Ignoring soft-clipped reads altogether, if possible at all, is not a good solution. We are forced to use GATK3 because the output of the GATK4 version does not work well with others tools we need for the detection of certain variants in RNA-seq.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7356#issuecomment-1846884589:117,perform,performance,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7356#issuecomment-1846884589,1,['perform'],['performance']
Performance,Are we interested in writing some definitive guide on how to tune the `af-of-alleles-not-in-resource` parameter for different contexts?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4745#issuecomment-387218068:61,tune,tune,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4745#issuecomment-387218068,1,['tune'],['tune']
Performance,"Are you sure that there is enough space? Could you run something like:. ```; df -h /storage/home/data/gendb/; ```; And check that available space is in line with your expectations? Also, is there any user specific quota being enforced on your system?. Not completely sure, but another possibility is that you have too many (fairly small) intervals in your `chr13.bed` file. Unless you are specifically trying to exclude intervals not in the bed file, you should get a lot better performance/efficiency by specifying fewer intervals (think <100 intervals, and in your case since you are doing a single chromosome, maybe much less than that). . If you don't want to manually specify fewer intervals, you could try setting `--merge-input-intervals` to `true` which will return one interval for the contig spanning all the intervals you've specified. Having many smaller intervals results in many small files created which will definitely be bad for performance, and may also cause quota issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6950#issuecomment-726416409:479,perform,performance,479,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6950#issuecomment-726416409,2,['perform'],['performance']
Performance,"As @nalinigans suggested, the `--genomicsdb-shared-posixfs-optimizations` should help, though probably mostly for import. Similarly, I would highly recommend `--bypass-feature-reader` [link](https://gatk.broadinstitute.org/hc/en-us/articles/13832686645787-GenomicsDBImport#--bypass-feature-reader) for the import as well. As I mentioned before, reblocking will help import and query - mainly because it reduces the input GVCF size by 5x-8x. Shouldn't be necessary for the number of samples you indicate, but will become more important as number of samples scales up (and does help at any number of samples, I should add). That doesn't seem to the crux of your problem though...you note that running serially does better than trying to parallelize across many cores. I don't have a lot of insight into Lustre specifically, but do you have any metrics on how the IOPS looks for the Lustre FS in each case? Also, the bit about the the first set of variants taking a while - does that time look different when running serially versus in parallel?. One experiment to consider - maybe try to copy the workspace to the `$PBS_JOBFS` folder on the compute node before running `GenotypeGVCFs`. Not sure it is feasible in terms of amount of storage, etc but it would at least rule out possible Lustre issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879964821:59,optimiz,optimizations,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879964821,1,['optimiz'],['optimizations']
Performance,As I discovered when making the fix in PR #2021 bam files will fail validation if overhang clipping is used when running SplitNCigarRead because the mate reference start position might be changed. The tool can be refactored to perform a second walker pass over the reads in order to identify locations where this will be a problem by checking for sites where the primary read gets clipped by OverhangClippingManager.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2075:227,perform,perform,227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2075,1,['perform'],['perform']
Performance,"As John loads data into the Echo callset, he will certainly run into issues in the documentation, that should be addressed",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8649:8,load,loads,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8649,1,['load'],['loads']
Performance,"As a result, every walker that takes a BAM as input can now read it from; the cloud directly. Sample use: ./gatk-launch CountReads -I; ""gs://.../CEUTrio.HiSeq.WGS.b37.ch20.4m-12m.NA12878.bam""; (...); Tool returned:; 7275701. Note that this CL doesn't add a prefetcher or multithreading, so the; performance is inferior to that of the NIO proof of concept which was; using both. cc: @droazen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2224:295,perform,performance,295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2224,1,['perform'],['performance']
Performance,"As described in #1464, I ported a `LocusWalker` class as the GATK3 one using `LocusIteratorByState` (LIBS). The default implementation uses no downsampling (probably it should be change once #64 is addressed), includes reads with deletions and does not track the previous reads in the LIBS. One important think is that the `intervalsForTraversal` is not used at all, so it is up to the author discard regions out of this ones. I was thinking to check every position for overlap in any of the interval in the list, but I'm not sure if the `intervalsForTraversal` is sorted or not; and an exhaustive checking could reduce performance. I don't know if it could be possible to implement some query in LIBS, to return only the positions that overlaps some intervals, but that will be easier.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1526:620,perform,performance,620,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1526,1,['perform'],['performance']
Performance,"As discussed in GATK office hours, this issue was reported by a user with HaplotypeCaller Spark. The entire stack trace is included below. This request was created from a contribution made by stanedav on August 03, 2020 10:07 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360072104512-HaplotypeCallerSpark-error](https://gatk.broadinstitute.org/hc/en-us/community/posts/360072104512-HaplotypeCallerSpark-error). \--. Hello, I am testing HaplotypeCallerSpark algorithm on my local machine for speeding up the variant calling. I tried to apply algorithm on my BQSR bam but I am getting this error (full log below):. ERROR Executor: Exception in task 15.0 in stage 5.0 (TID 1324) ; ; java.util.ConcurrentModificationException ... (more in log). Version of GATK: 4.1.7.0. Command I used:. $gatk --java-options ""-Xmx48g -Xms32g"" HaplotypeCallerSpark \\ ; ; \-R hg19.fasta \\ ; ; \-I remdup\_recal.bam \\ ; ; \-O output.g.vcf \\ ; ; \-L wes.bed \\ ; ; \-ERC GVCF \\ ; ; \--dont-use-soft-clipped-bases. Full log:. [https://www.dropbox.com/s/iez0zixclsh86zp/hc.log?dl=0](https://www.dropbox.com/s/iez0zixclsh86zp/hc.log?dl=0)<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/6546'>Zendesk ticket #6546</a>)<br>gz#6546</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6738:714,Concurren,ConcurrentModificationException,714,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6738,1,['Concurren'],['ConcurrentModificationException']
Performance,"As discussed in https://github.com/broadinstitute/gatk/issues/1203, we are keeping SHUFFLE so that we have a baseline during performance testing. Closing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2231#issuecomment-288534664:125,perform,performance,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2231#issuecomment-288534664,1,['perform'],['performance']
Performance,"As far as I can tell, getting that error message means that BaseTest is being loaded at runtime, and running it's static initializer block which calls `SparkContextFactory.enableTestSparkContext();`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330658299:78,load,loaded,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330658299,1,['load'],['loaded']
Performance,"As for TableReader.... we could make it a bit more efficient by reusing DataLine instances. Currently it creates one per each input line, but same instance could be reused loading each new line data onto it before calling ```createRecord```. . We are delegating to a external library to parse the lines into String[] arrays (one element per cell) .... we could save on that by implementing it ourselves more efficiently but of course that would be take one of our some of his/her precious development time... In any case I don't know what the gain would be considering that these operations are done close to I/O that typically should be dominating the time-cost. . The DataLine reuse may save some memory churning and wouldn't take long to code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316489131:172,load,loading,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316489131,1,['load'],['loading']
Performance,"As mentioned in the discussion for https://github.com/broadinstitute/gatk/pull/987, we want to compare the manual sharding approach taken to optimizing BQSR in that branch against an alternative approach of broadcasting the reference and variants. The latter approach would be simpler and more flexible/idiomatic (allow spark to handle sharding and data localization rather than doing it manually), but might be slower. Let's find out what the performance is like for both approaches before making a decision.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/995:141,optimiz,optimizing,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/995,2,"['optimiz', 'perform']","['optimizing', 'performance']"
Performance,"As part of my work in the Pipeline Dev team, I created 2 GATK images to address issue discussed [here](https://github.com/broadinstitute/gatk/issues/8684) (ie. having too many docker layers, we hit ACR limits very quickly). The images are in terrapublic, a premium-tier ACR and is publicly accessible. I made two images, one is squashed to just 1 layer, the other is reduced to just 12 layers (from the original 45). With these changes and the fact that terrapublic is on [premium](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling) tier, the maximum docker pulls per minute becomes 833 (ie. 10k readOps / 12 layers) for the reduced-layers image and 10,000 for the squashed one. We have yet to test these in our pipelines but I anticipate the squashed version to be slower since it wont be able to take advantage of any parallel pulls or caching, hence the two versions to allow pipeline devs to decide which one is better for their use-case.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8808:574,throughput,throughput-and-throttling,574,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8808,1,['throughput'],['throughput-and-throttling']
Performance,"As reported in https://github.com/broadinstitute/gatk/issues/4133 piped commands that work in Picard, don't work when running with GATK. The immediate culprit is the line:; ```; 15:29:29.595 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.8.2.jar!/com/intel/gkl/native/libgkl_compression.dylib; ```; Which is emitted to stdout in GATK, a similar warning is instead output to STDERR in picard. . There are likely other problematic logging lines.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4135:218,Load,Loading,218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4135,1,['Load'],['Loading']
Performance,"As scientists we must strive to standardize to the most rational units of measurement, including our measurement of the passage of time. That is why we, the GATK authors, would like to announce that we are updating the GATK display time to display the current system time stamped according to the French Revolutionary Calendar. Here is an example of the new logging outputs:. ```; Dcadi, 10-Germinal-232, 7:74:79, Loutil:Couvoir INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/emeryj/hellbender/gatk/build/libs/gatk-package-4.5.0.0-20-g105b63e-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; Dcadi, 10-Germinal-232, 7:74:79, Loutil:Couvoir INFO CountReads - ------------------------------------------------------------; Dcadi, 10-Germinal-232, 7:74:79, Loutil:Couvoir INFO CountReads - The Genome Analysis Toolkit (GATK) v4.5.0.0-20-g105b63e-SNAPSHOT; Dcadi, 10-Germinal-232, 7:74:79, Loutil:Couvoir INFO CountReads - For support and documentation go to https://software.broadinstitute.org/gatk/; Dcadi, 10-Germinal-232, 7:74:79, Loutil:Couvoir INFO CountReads - Executing as emeryj@wm85b-6ec on Mac OS X v13.2.1 x86_64; Dcadi, 10-Germinal-232, 7:74:79, Loutil:Couvoir INFO CountReads - Java runtime: OpenJDK 64-Bit Server VM v17.0.6+10; Dcadi, 10-Germinal-232, 7:74:79, Loutil:Couvoir INFO CountReads - Start Date/Time: March 29, 2024 at 2:35:42 PM EDT; Dcadi, 10-Germinal-232, 7:74:79, Loutil:Couvoir INFO CountReads - ------------------------------------------------------------; Dcadi, 10-Germinal-232, 7:74:79, Loutil:Couvoir INFO CountReads - ------------------------------------------------------------; Dcadi, 10-Germinal-232, 7:74:79, Loutil:Couvoir INFO CountReads - HTSJDK Version: 4.1.0; Dcadi, 10-Germinal-232, 7:74:79, Loutil:Couvoir INFO CountReads - Picard Version: 3.1.1; Dcadi, 10-Germinal-232, 7:74:79, Loutil:Couvoir INFO CountReads - Built for Spark Version: 3.5.0; Dcadi, 10-Germinal-232, 7:74:79, Lou",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8756:458,Load,Loading,458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8756,1,['Load'],['Loading']
Performance,"As stated in the title. I tried the new gatk version 4.2.1.0 to update the GENCODE data for Funcotator. Log:; /home/robby/Tools/NGS/gatk-4.2.1.0/gatk IndexFeatureFile -I /home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; Using GATK jar /home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar IndexFeatureFile -I /home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; 14:34:51.448 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 02, 2021 2:34:51 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:34:51.566 INFO IndexFeatureFile - ------------------------------------------------------------; 14:34:51.566 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.1.0; 14:34:51.566 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:34:51.572 INFO IndexFeatureFile - Initializing engine; 14:34:51.572 INFO IndexFeatureFile - Done initializing engine; 14:34:51.674 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.676 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7385:711,Load,Loading,711,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7385,1,['Load'],['Loading']
Performance,"As we discussed on Slack, this will fix the NaNs, but I'm not convinced that we should allow the single-contig use case without at least a warning. The ploidy step will essentially perform no inference, since I think the per-contig bias and ploidy factors will cancel out with the way the likelihood is written---it will simply return the prior, and all samples will be guaranteed to have ploidy = 2. @asmirnov239 is going to do some more testing to make sure we understand this right and perhaps add a warning/documentation. The current likelihood is a bit confusing (I tried to address some of these issues in the unmerged ploidy-model update), but in any case, the problem is degenerate and it's hard to define appropriate behavior without additional priors and model structure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6613#issuecomment-631693589:181,perform,perform,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6613#issuecomment-631693589,1,['perform'],['perform']
Performance,AssemblyRegion traversal loads too many reads at once,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3516:25,load,loads,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3516,1,['load'],['loads']
Performance,AssertionError: The optimization step for ELBO update returned a NaN while running DetermineGermlineContigPloidy,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6573:20,optimiz,optimization,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6573,1,['optimiz'],['optimization']
Performance,Assigning to @akiezun to gather his past performance results into one place.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2012#issuecomment-233390678:41,perform,performance,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2012#issuecomment-233390678,1,['perform'],['performance']
Performance,"At a high-level, the approach here seems to be to create shards (of size 1 million bps) and load the reads, variants and references for each shard in memory. Each shard then has a recalibration table created for it, then the tables are merged into one. The existing version finds the variants for each read, and has two shuffles (this is for the reference broadcast approach, there's an extra one for the reference shuffle approach). The first is to join the variants and reads together, and a second to aggregate the variants for each read. The optimization in this PR has no shuffles, since it loads all variants into memory rather than doing distributed joins. Is that a valid assumption? If so, it would be possible to load the variants into memory in the driver and broadcast to all workers to remove the shuffles (in the existing implementation). I noticed that `AddContextDataToReadSpark#subdivideAndFillReads` opens the BAM itself, rather than using `ReadsSparkSink`. This won't work well in the Hadoop case since you lose locality - i.e. the work won't be scheduled on the node where the BAM files are stored. It would be much better to find a way of using `ReadsSparkSink`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/987#issuecomment-147415253:92,load,load,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987#issuecomment-147415253,4,"['load', 'optimiz']","['load', 'loads', 'optimization']"
Performance,Automate performance testing (including Spark),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1609:9,perform,performance,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1609,1,['perform'],['performance']
Performance,"Automatically choose appropriate import batch, preemptible, and max retry values for sample sets up to 20K. This makes 20K samples our effective threshold for where we feel comfortable with the current stability / performance of our import code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7925:214,perform,performance,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7925,1,['perform'],['performance']
Performance,Available threads: 64; 11:35:41.848 INFO IntelPairHmm - Requested threads: 4; 11:35:41.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:35:41.882 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 11:35:41.997 INFO ProgressMeter - Starting traversal; 11:35:41.997 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:35:42.019 DEBUG ReadsPathDataSource - Preparing readers for traversal; 11:35:42.470 DEBUG Mutect2 - Processing assembly region at chrM:1-300 isActive: false numReads: 0; 11:35:42.497 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.520 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBU,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:7333,cache,cache,7333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance,"Avoid loading all headers up-front in GenomicsDBImport, if possible",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2639:6,load,loading,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2639,1,['load'],['loading']
Performance,"Awaiting the Barclay snapshot with broadinstitute/barclay#33, which is ready, but I'll sanity check again in AM before I merge. NOTE: All of the GATK tests have passed locally with this branch. However, I had to make one temporary change because SelectVariants has a feature that clashes with the collection list file feature in https://github.com/broadinstitute/barclay/pull/28. SelectVariants currently has two arguments that are defined as `List<File>`, that are each intended to take a list of file names, each of which in turn contains a list of sample names. SelectVariants manually loads all of the samples from all of the files in the list, and creates a list of unique sample names. With the https://github.com/broadinstitute/barclay/pull/28, the CLP now loads the list directly, and hands SelectVariants a list of sample names rather than the list of file names, which breaks one test. I think both features are working as intended, but collide when a .list file is used. I temporarily renamed the test file to not have a .list extension (so it won't trigger the CLP file loading), but we'll have to decide how to properly reconcile these two features.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2388:589,load,loads,589,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2388,3,['load'],"['loading', 'loads']"
Performance,"BI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 18:01:51.712 INFO SortSam - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 18:01:51.712 INFO SortSam - Defaults.REFERENCE_FASTA : null; 18:01:51.712 INFO SortSam - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 18:01:51.713 INFO SortSam - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 18:01:51.713 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 18:01:51.713 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:01:51.713 INFO SortSam - Defaults.USE_CRAM_REF_DOWNLOAD : false; 18:01:51.713 INFO SortSam - Deflater IntelDeflater; 18:01:51.713 INFO SortSam - Initializing engine; 18:01:51.713 INFO SortSam - Done initializing engine; 18:02:01.512 INFO SortSam - Shutting down engine; [December 7, 2016 6:02:01 PM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=1911029760; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.br",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2299#issuecomment-265469924:1812,Load,LoadSnappy,1812,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2299#issuecomment-265469924,1,['Load'],['LoadSnappy']
Performance,BQSR optimization - reduce object churn by not creating as many Cigar objects and less bases/quals,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1477:5,optimiz,optimization,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1477,1,['optimiz'],['optimization']
Performance,BQSR time and memory optimizations - mostly object allocation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1476:21,optimiz,optimizations,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1476,1,['optimiz'],['optimizations']
Performance,"BTW, we could actually get a more accurate DP estimate by using the INFO field instead of the FORMAT DP (so we wouldn't have to touch the genotypes at all), but I'm still thinking through the implementation details. It would introduce batch effects, but it would be an improvement in results and rescue your performance optimization.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377568302:308,perform,performance,308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377568302,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.Composit,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:2353,concurren,concurrent,2353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['concurren'],['concurrent']
Performance,Base cohort extract scatter width on loaded sample count [VS-1513],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9028:37,load,loaded,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9028,1,['load'],['loaded']
Performance,"BaseRecalibrator, optimized",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/894:18,optimiz,optimized,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/894,1,['optimiz'],['optimized']
Performance,"Based on my HaplotypeCaller GVCF performance evaluation, we are spending a significant amount of time in `ReferenceConfidenceModel.calcNIndelInformativeReads()', including upwards of 40% of the overall runtime on a bam under some conditions. To this end we have already done some performance work optimizing its constituent methods (#5469, #5470). Even with those changes it appears that the method can take upwards of 25% of the total runtime, which appears to be a consequence of the nature of the algorithm. It appears that the core of the problem appears to be associated with the calls we make to `isReadInformativeAboutIndelsOfSize()` which has a complexity overall of approximately `O(pileupsPerRegion * readsPerPileup * basesPerRead * maxIndelSize)` which ends up being a large number. One approach to fixing this problem be to rethink the repetitive operations we do for every pileup and instead do it on a per-read basis, and furthermore we could exploit the nature of the existing algorithm to avoid checking mismatches at the front of the read when we know that down the line we will fail out because of mismatches at the end of the read. Furthermore there is the broader philisophical question of whether there are changes that could be made to the algorithm that might carry a bigger risk of changing the results, like applying some heuristic based on the complexity of the reference sequence at a given site to reduce the amount of work we have to do.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5488:33,perform,performance,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5488,3,"['optimiz', 'perform']","['optimizing', 'performance']"
Performance,"Based on testing performed on google cloud, this problem seems to have resolved itself after we updated to the newest googleCloudJava package in this #5135. Something fixed by switching off of our ancient fork must have been responsible for the file being misread by spark. I suggest we close this issue as it appears to be resolved unless @droazen you want to do a post-mortem to figure out what the relevant change must have been?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5133#issuecomment-419539317:17,perform,performed,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5133#issuecomment-419539317,1,['perform'],['performed']
Performance,Beat GATK3 performance (CPU and memory) of GenotypeGVCFs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1608:11,perform,performance,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1608,1,['perform'],['performance']
Performance,Beat GATK3 performance (CPU and memory) of HaplotypeCaller,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1607:11,perform,performance,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1607,1,['perform'],['performance']
Performance,Beat GATK3 performance (CPU and memory) of HaplotypeCaller (GVCF mode),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1800:11,perform,performance,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1800,1,['perform'],['performance']
Performance,Beat GATK3 performance (CPU and memory) of HaplotypeCaller (VCF mode),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1799:11,perform,performance,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1799,1,['perform'],['performance']
Performance,"Before Sam's awesome AGBT talk extolling the virtues of the new GATK-CNN filtering tool we want to be able to make a GATK release with the following:; [ ] A cool name!; [ ] Model training script (in Python, eventually in Java); [ ] Pretrained model for WGS; [ ] Pretrained model for WEx; [ ] Model inference and VCF annotation (in Java); [ ] Solution for applying filters based on CNN score cutoff; [ ] Alternate joint calling WDL? Or for re-filtering? (ideally with a $$$ estimate); [ ] Performance optimizations?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4225:488,Perform,Performance,488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4225,2,"['Perform', 'optimiz']","['Performance', 'optimizations']"
Performance,BlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:15537,cache,cache,15537,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['cache']
Performance,Breakpoints(NovelAdjacencyReferenceLocations.java:78); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:293); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:42); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$discoverNovelAdjacencyFromChimericAlignments$7(DiscoverVariantsFromContigAlignmentsSAMSpark.java:409); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:147); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3874:2383,concurren,concurrent,2383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3874,2,['concurren'],['concurrent']
Performance,"Btw - it is interesting (as in, unexpected, at least to me) that appending new data is slowing down as the workspace grows. Adding new data is sorta fire-and-forget in that it shouldn't care much about what already exists...offhand, I'm not sure why you're seeing a slowdown. You mentioned needing to use smaller batch sizes...were you otherwise seeing larger memory overheads than before? Or just general slower performance?. There is a new `--bypass-feature-reader` option in the latest release that should help with dramatically lowering memory usage, and potentially offering a slight speedup for imports. Might be worth a shot. edit: you mentioned consolidate, so I should also add that consolidate is expected to help with read performance (shouldn't affect import) after we've had a fair number of batches imported (left intentionally vague. As a guess, ~100 or so?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964432955:413,perform,performance,413,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964432955,2,['perform'],['performance']
Performance,"Build is failing because of a warning. It's odd that we didn't see this before, it seems unrelated to my change:. ```; :compileJava/home/travis/.gradle/caches/modules-2/files-2.1/com.google.cloud/gcloud-java-nio/0.2.8/57e30d28f80ab3a320e560e7b4aaac07baf98c4/; gcloud-java-nio-0.2.8-shaded.jar(com/google/cloud/storage/contrib/nio/CloudStorageFileSystemProvider.class): warning:; Cannot find annotation method 'value()' in type 'AutoService': class file for shaded.cloud-nio.com.google.auto.service.AutoService not found; error: warnings found and -Werror specified; 1 error; 1 warning; FAILED; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-257033597:152,cache,caches,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-257033597,1,['cache'],['caches']
Performance,But I agree that it would be good to quantify the performance difference.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/115#issuecomment-70113718:50,perform,performance,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/115#issuecomment-70113718,1,['perform'],['performance']
Performance,BwaMemIntegrationTest fails on gsa5 and crashes the test suite. . Bwa produces ```[bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!``` and then exits which kills the test suite in a gross way. @SHuang-Broad points out that this often indicates that the version of bwa that was used to generate the index has a difference from the version that is used to load the index. It's unclear what's happening here because that same test with the same files passes on travis and on mac.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2451:375,load,load,375,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2451,1,['load'],['load']
Performance,BwaSpark - Race condition in parsing sam records,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2050:11,Race condition,Race condition,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2050,1,['Race condition'],['Race condition']
Performance,BwaSpark parameter optimization,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8897:19,optimiz,optimization,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8897,1,['optimiz'],['optimization']
Performance,"By default we only support OSX and modernish linux on amd64. We don't have a plan to test or officially support aarch64. However, it's mostly java so it might mostly work? The things I can think of off the bat that won't work are the optimizations for intel hardware, i.e. the fast pairhmm and deflate/inflate optimizations will definitely not work. They **should** degrade to java implementations automatically though. . IBM has a fork that works on power8 which reimplements some of the optimizations for PairHMM, so it's definitely possible to implement the various native optimizations for other architectures although not a small project. . GenomicsDB won't work since we only bundle libs for amd64/osx. You could in theory compile it yourself and pass the library on the library path. Other things that will need special attention would any of the library wrappers, for bwa-mem, fermlite, and hdf5. Those will similarly need custom builds supplied to the gatk. . I suspect that you could get the reads pipeline working, but newer tools with weirder native dependencies will be tricky.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6118#issuecomment-524959548:234,optimiz,optimizations,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6118#issuecomment-524959548,4,['optimiz'],['optimizations']
Performance,"By using 5 executors, 24 cores on each when no one else seem to be using the local cluster and having the number of partitions approximately equal to the number of FASTQ files, [this](http://dataflow01.broadinstitute.org:18088/history/application_1464285223085_0420/jobs/) is the time it takes to run on the 44188 small FASTQ files, without capturing the stdout and stderr of SGA processes. Note that this time the RDD caching is done differently than the way it is done in code living in master:. ```; results.cache(); // cache because Spark doesn't have an efficient RDD.split(predicate) yet; results.count(); // ugly hack to make the actual computation happen, so later filtering step will be based on what has been actually computed. // save fasta file contents or failure message; final JavaPairRDD<Long, SGAAssemblyResult> success = results.filter(entry -> entry._2().assembledContigs!=null);; final JavaPairRDD<Long, SGAAssemblyResult> failure = results.filter(entry -> entry._2().assembledContigs==null);. if(!success.isEmpty()){; success.map(entry -> entry._1().toString() + ""\n"" + entry._2().assembledContigs.toString()); .saveAsTextFile(outputDir+""_0"");; }. if(!failure.isEmpty()){; failure.map(entry -> entry._1().toString() + ""\n"" + entry._2().collectiveRuntimeInfo.toString()); .saveAsTextFile(outputDir+""_1"");; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1877#issuecomment-225187611:511,cache,cache,511,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1877#issuecomment-225187611,2,['cache'],['cache']
Performance,"CEUTrio.HiSeq.WEx.b37.NA12892.chr10.bam (1.2Gb on local drive); Mac OS X 10.9.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_25-b17;. initial optimized run on 1.2Gb file after ripping out some of the indel code:; 4.pre-alpha-71-g50b094b. ```; real 4m58.795s; user 5m3.401s; ```. For reference, here are times for pre-optimization GATK3 and GATK4 (some data from #1033). GATK4 master branch, with indels. ```; real 9m7.691s; user 9m2.250s; ```. GATK4 master branch, the `-DIQ` option (ie skip indel quals). ```; real 5m19.914s; user 5m14.710s; ```. (which is already faster than GATK3.4.46 - numbers listed below). GATK3.4.46 with indels . ```; real 11m21.538s; user 17m24.320s; ```. GATK3.4.46 with the `-DIQ` option (ie skip indel quals). ```; real 6m3.662s; user 10m34.859s; ```. For reference, the best possible bottom line (for bqsr optimizations, not reading/writing itself) is established by PrintReads on the same data:. ```; real 2m48.873s; user 2m27.752s; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152595055:145,optimiz,optimized,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152595055,3,['optimiz'],"['optimization', 'optimizations', 'optimized']"
Performance,"CF. As a result of this and the clever compression in the `.pgen` file, these files are typically much smaller than equivalent VCFs. For more information on the PGEN file format, see the official spec [here](https://github.com/chrchang/plink-ng/blob/master/pgen_spec/pgen_spec.pdf). ## The code; The code for the PGEN extract can be divided into 3 parts:; 1. The PGEN-JNI, a C++/JNI library that handles writing HTSJDK VariantContext objects to PGEN files,; 2. ExtractCohortToPgen, a GATK tool based on ExtractCohortToVcf that processes VariantContexts and passes them to PGEN-JNI for writing, and; 3. GvsExtractCallsetPgenMerged, a WDL workflow based on GvsExtractCallset that uses ExtractCohortToPgen to write a series of PGEN files and then merges them by chromosome. ### Part 1: PGEN-JNI; The PGEN-JNI library was written by Chris Norman of the GATK Engine Team and lives [here](https://github.com/broadinstitute/pgen-jni). It is written primarily in C++ for performance purposes and also compatibility with the pgenlib library (part of the [plink repo](https://github.com/chrchang/plink-ng/tree/master)). It builds on top of pgenlib to provide a writer for creating PGEN files and writing to them from HTSJDK VariantContext objects. PGEN-JNI is compatible with Linux and macOS. A build of this library is currently hosted on the Broad's artifactory repo, and that is being used as a dependency for GATK. Ownership of the PGEN-JNI library will stay with the GATK Engine Team and we will provide support for it if y'all encounter any issues with it. ### Part 2: ExtractCohortToPgen; ExtractCohortToPgen is a GATK tool that inherits from ExtractCohort and is based very closely on ExtractCohortToVcf. It produces 3-4 files:. 1. A `.pgen` file, which contains a mapping of samples and sites to variants,; 2. A `.psam` file, which contains a list of sample names,; 3. A `.pvar.zst` file, which is a zstd compressed list of sites with alleles, similar to a sites-only VCF, and; 4. Optionally (if speci",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8708:2239,perform,performance,2239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708,1,['perform'],['performance']
Performance,"CNNScoreVariant relies on a computationally demanding operation - a deep neural network. Using an Intel-optimized version of TensorFlow gives a 10X improvement in performance (e.g. 50 hours to 5 hours for a typical input). However, these improvements mean that we now have a minimum hardware requirement - the availability of AVX.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291:104,optimiz,optimized,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,CNV performallelefractionsegmentation: give example command,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2811:4,perform,performallelefractionsegmentation,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2811,1,['perform'],['performallelefractionsegmentation']
Performance,CNV performallelefractionsegmentation: tag doc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2810:4,perform,performallelefractionsegmentation,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2810,1,['perform'],['performallelefractionsegmentation']
Performance,"CRAM + NIO looks to be ~3 cents per sample. This essentially includes disk optimizations, since the disk size is determined by the CRAM size; this is not too large, so this results in disk costs of ~0.3 cents per sample. Note that I ran on the CRAMs in gs://broad-sv-dev-data/TCGA_blood_normals.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5715#issuecomment-467608484:75,optimiz,optimizations,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715#issuecomment-467608484,1,['optimiz'],['optimizations']
Performance,"CRAM w/o NIO is also ~3 cents per sample (it was marginally more expensive than CRAM w/ NIO, but within the noise). CRAM w/o NIO w/ SSD is ~5 cents. So I'd say CRAM w/ or w/o NIO is fine. Strictly speaking, we can't directly compare the BAM and CRAM costs, since they were done on different sets of TCGA samples. But both are well under the goal of ~15 cents per sample, so I think it's safe to say that we can turn our attention to optimizing inference costs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5715#issuecomment-467612453:433,optimiz,optimizing,433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715#issuecomment-467612453,1,['optimiz'],['optimizing']
Performance,CS max retries/reopens: 20; 10:56:25.358 INFO GermlineCNVCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 10:56:25.360 WARN GermlineCNVCaller -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: GermlineCNVCaller is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 10:56:25.361 INFO GermlineCNVCaller - Initializing engine; 10:56:54.347 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:56:55.287 INFO GermlineCNVCaller - Retrieving intervals from first read-count file (hdf5/grexome0426.hdf5)...; 10:56:55.384 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 10:56:55.482 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 10:56:55.485 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 10:56:55.511 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0426.hdf5 (1 / 387); 10:56:55.812 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0342.hdf5 (2 / 387); 10:56:56.274 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0316.hdf5 (3 / 387); 10:56:56.635 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0338.hdf5 (4 / 387); 10:56:57.092 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0360.hdf5 (5 / 387); 10:56:57.728 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0384.hdf5 (6 / 387); 10:56:58.144 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0213.hdf5 (7 / 387); 10:56:58.681 INFO GermlineCNVCaller - Aggregating read-count,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053:3330,perform,performed,3330,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053,1,['perform'],['performed']
Performance,"C_IO_WRITE_FOR_TRIBBLE : false; 09:01:25.951 INFO HaplotypeCaller - Deflater: IntelDeflater; 09:01:25.951 INFO HaplotypeCaller - Inflater: IntelInflater; 09:01:25.951 INFO HaplotypeCaller - GCS max retries/reopens: 20; 09:01:25.951 INFO HaplotypeCaller - Requester pays: disabled; 09:01:25.952 INFO HaplotypeCaller - Initializing engine; 09:01:26.059 INFO HaplotypeCaller - Done initializing engine; 09:01:26.060 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 09:01:26.067 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output; 09:01:26.067 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 09:01:26.077 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:01:26.078 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 09:01:26.089 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 09:01:26.089 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 09:01:26.090 INFO IntelPairHmm - Available threads: 36; 09:01:26.090 INFO IntelPairHmm - Requested threads: 4; 09:01:26.090 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 09:01:26.121 INFO ProgressMeter - Starting traversal; 09:01:26.121 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:01:26.406 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position 1A:145 and possibly subse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8192:3468,Load,Loading,3468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192,1,['Load'],['Loading']
Performance,"C_IO_WRITE_FOR_TRIBBLE : false; 12:00:43.084 INFO HaplotypeCaller - Deflater: IntelDeflater; 12:00:43.084 INFO HaplotypeCaller - Inflater: IntelInflater; 12:00:43.084 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:00:43.084 INFO HaplotypeCaller - Requester pays: disabled; 12:00:43.084 INFO HaplotypeCaller - Initializing engine; 12:00:43.217 INFO HaplotypeCaller - Done initializing engine; 12:00:43.218 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 12:00:43.230 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output; 12:00:43.230 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 12:00:43.239 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:00:43.240 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils15289766804525936146.so: libgomp.so.1: cannot open shared object file: No such file or directory); 12:00:43.240 WARN IntelPairHmm - Intel GKL Utils not loaded; 12:00:43.240 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 12:00:43.240 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:00:43.241 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils1618756704004128857.so: libgomp.so.1: cannot open shared object file: No such file or directory); 12:00:43.241 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8194:2946,Load,Loading,2946,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194,1,['Load'],['Loading']
Performance,Cache the codec class used for a FeatureInput so we only have to discover it once.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2740:0,Cache,Cache,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2740,1,['Cache'],['Cache']
Performance,Cache the codec used for a FeatureInput so we only have to discover it once.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2737:0,Cache,Cache,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2737,1,['Cache'],['Cache']
Performance,"Caller - Initializing engine; 09:13:59.096 INFO IntervalArgumentCollection - Processing 818575866 bp from intervals; 09:13:59.161 INFO HaplotypeCaller - Done initializing engine; 09:13:59.164 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 09:13:59.598 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 09:14:00.256 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 09:14:00.284 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/home/chenwei/biosoft/gatk-4.0.10.1/gatk-package-4.0.10.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:14:00.312 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 09:14:00.312 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 09:14:00.595 INFO ProgressMeter - Starting traversal; 09:14:00.596 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:14:12.096 INFO ProgressMeter - D01:2563 0.2 20 104.3; 09:14:24.689 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 09:14:24.690 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 09:14:24.884 INFO ProgressMeter - D01:10031 0.4 60 148.2; 09:14:36.441 INFO ProgressMeter - D01:19554 0.6 130 217.6; 09:14:51.359 INFO ProgressMeter - D01:21053 0.8 140 165.5; 09:15:02.193 INFO ProgressMeter - D01:34263 1.0 220 214.3; 09:15:13.398 INFO ProgressMeter - D01:56554 1.2 360 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7454:9600,multi-thread,multi-threaded,9600,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454,1,['multi-thread'],['multi-threaded']
Performance,"CallerSpark - Shutting down engine; [May 18, 2017 5:12:04 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 131.63 minutes.; Runtime.totalMemory()=16201547776; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3019:7107,concurren,concurrent,7107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019,1,['concurren'],['concurrent']
Performance,"Can someone explain this pseudoinverse business to me? We standardize the counts to give a matrix C, calculate the SVD to get the left-singular matrix U and the pseudoinverse of C, but then perform *another* SVD on U to get the pseudoinverse of U. Why do we need these pseudoinverses? @LeeTL1220 @davidbenjamin?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316267546:190,perform,perform,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316267546,1,['perform'],['perform']
Performance,"Can someone modify the underlying read without calling a setter?. On Monday, July 25, 2016, droazen notifications@github.com wrote:. > @akiezun https://github.com/akiezun That's why I was suggesting; > invalidating all cached values on every call to any setter -- that way we; > don't have to think about the nuances of when it's necessary to; > recalculate, and greatly reduce the risks that normally come with caching; > while still getting most of the performance benefit in typical usage.; > ; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235136126,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/AB5rL8K8hmn3JygZbx39Covn8lc14S5sks5qZWIxgaJpZM4JR8AP; > . ## . Sent from Gmail Mobile",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235139023:219,cache,cached,219,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235139023,2,"['cache', 'perform']","['cached', 'performance']"
Performance,"Can you please test this change with the `HaplotypeCaller` in protected and make sure nothing changes? In particular, can you run `HaplotypeCallerIntegrationTest` and `HaplotypeCallerEngineUnitTest` and make sure they pass with this change? This PR makes me a little nervous given the centrality of the classes touched, even though the optimization itself is simple enough...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216595202:336,optimiz,optimization,336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216595202,1,['optimiz'],['optimization']
Performance,Can you provide a summary of the optimizations included here @pnvaidya ?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4379#issuecomment-364256682:33,optimiz,optimizations,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4379#issuecomment-364256682,1,['optimiz'],['optimizations']
Performance,"Can you try a few more cache sizes like 200000 and 500000? Also, when you do the PR, could you create `VariantWalker.FEATURE_CACHE_SIZE = 100000` (or whatever the final value is) and pass it into the `FeatureManager` in `initializeFeatures()`? `FeatureDataSource.DEFAULT_QUERY_LOOKAHEAD_BASES` should stay at 1000 (for ReadWalkers, which have a lot more overlap in their query access patterns)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155505129:23,cache,cache,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155505129,1,['cache'],['cache']
Performance,Catching when SAM with .bam extension is loaded in Spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3488:41,load,loaded,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3488,1,['load'],['loaded']
Performance,Cause workflows to fail immediately if HDF5 cannot be loaded (at start),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2871:54,load,loaded,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2871,1,['load'],['loaded']
Performance,Cf. performance results in https://github.com/broadinstitute/gatk/pull/1695,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1696#issuecomment-207488641:4,perform,performance,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1696#issuecomment-207488641,1,['perform'],['performance']
Performance,Changed so that PopulateFilterSetInfo now explicitly prints to STDERR any error it encounters. . Example run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/2f86bfa0-bf1b-451a-8217-a0ab4cba5834).; stderr has this error now:. > Error loading combined TSV into gvs-internal.gg_VS_1056.filter_set_info:; > BigQuery error in load operation: Provided Schema does not match Table gvs-; > internal:gg_VS_1056.filter_set_info. Cannot add fields (field: score2),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8530:277,load,loading,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8530,2,['load'],"['load', 'loading']"
Performance,"Changes to enable multi-threaded native AVX PairHMM using OpenMP. Also includes a performance improvement in the native C++ `Context` class. `VectorLoglessPairHMM.java` is hardcoded to set the maximum number of PairHMM threads (`maxNumberOfThreads`) to 100. This is the maximum number of threads **allowed** by GATK, not the number of threads **requested**. C code in the native library will query OpenMP for the number of threads available on the platform, and use min(OpenMP threads available, `maxNumberOfThreads`) threads. **Measured Speedup**; Command. ```; ./gatk-launch HaplotypeCaller -R src/test/resources/large/human_g1k_v37.20.21.fasta -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.g.vcf -ERC GVCF; ```. 1 thread; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 36.882098080000006; 2 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 18.160468659000003; 3 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 12.541517043; 4 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 9.727374342000001. **Potential issues**; - The target platform running GATK must have OpenMP installed; - The code has not been tested on Mac. **Todo**; - New Java code to allow the user to specify `maxNumberOfThreads` variable in `VectorLoglessPairHMM.java`.; - Move `maxNumberOfThreads` to the native `initialize` function, once we migrate to the new native library.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1813:18,multi-thread,multi-threaded,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813,2,"['multi-thread', 'perform']","['multi-threaded', 'performance']"
Performance,Chaning the holding collection to a Vector which is synchronized. There is no performance impact as this add and single collection iteractiion should happen just a finite number of times.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899839756:78,perform,performance,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899839756,1,['perform'],['performance']
Performance,Check for bad sequence dictionaries in ReferenceUtils.loadFastaDictionary,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2803:54,load,loadFastaDictionary,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2803,1,['load'],['loadFastaDictionary']
Performance,"Cherry pick of #7754 with very minor conflict resolution. Co-authored-by: Louis Bergelson <louisb@broadinstitute.org>, James Emery <emeryj@broadinstitute.org>. Also cherry picked #7727 to pick up an attempt at a git lfs optimization. Even though there's a bug in that that prevents it from running at all, git lfs not running at all works out to be better than git lfs that runs without limits.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7804:220,optimiz,optimization,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7804,1,['optimiz'],['optimization']
Performance,"Chiming in here -- the default output for GenotypeGVCFs should be a VCF that follows the spec. An important consideration here is that many users/pipelines usually only perform _site level_ filtering. Genotype level filtering is rarely performed. This causes problems in cases where most genotypes are of high quality at a site and a small number are missing but called as ""0/0"". Those 0/0 calls would then slip by. Coding missing data as 0/0 also breaks common site level filters such as vcftools' --max-missing, which relies on missing sites being coded per the spec.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1932170780:169,perform,perform,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1932170780,2,['perform'],"['perform', 'performed']"
Performance,"Chunk full list of .tsv files ready to load to bq into sets that are less than the 15tb limit set on each bq load. From the original; datatype_tsvs directory, each set is moved to its own directory, and when the load is complete, the data is moved into a done directory within each set. . Assuming pet tsvs and 1 set, at the start:; gs://bucket/pet_tsvs/pet_001_*. At end:; gs://bucket/pet_tsvs/set_1/done/pet_001_*. --; The output file, `bq_final_job_statuses.txt`, contains the following columns (and example data):; 1. bq load job ID : bqjob_r2715fbcab1fd0e44_00000178708f0abe_1; 2. set number:; 3. path to set data: gs://fc-13e1680e-eb3d-4102-975a-be0142ee9618/full_15tb_test_2/pet_tsvs/set_1/; 4. status of the bq load: SUCCESS/FAIL. What should be the best user experience in case of FAIL?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7167:39,load,load,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7167,5,['load'],['load']
Performance,"Closes #3561. @vruano could you review this? Note that I ended up implementing Dijkstra's algorithm instead of using a library, but it's only a few lines of code. This PR does not affect the outputs of HC or M2 at all. Also, @vruano, I recall your misgivings about the current haplotype enumeration (which this preserves):. >However, the current algorithm and the k-dijkstra still would show the same problems in terms of doing a suboptimal selection of haplotypes in terms of their coverage of plausible variation. I had implemented an alternative that fixed that issue . . . simulate haplotypes based on those same furcation likelihoods and wstop when we have not discovered anything new for a while... the problem of such an approach is to make it deterministic. Although this PR doesn't do that, it could easily be extended to do so just by running Dijkstra's algorithm until you have the amount of variation you want. That is, instead of terminating when the Dijkstra priority queue is empty or when we have discovered the maximum number of haplotypes, we could terminate based on some `Predicate<List<KBestHaplotype>>` on the list of haplotypes found so far. And it's deterministic since Dijkstra's algorithm is greedy. So basically, it's a nice refactoring for now but it also sets up some worthwhile extensions if we want.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5462:982,queue,queue,982,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5462,1,['queue'],['queue']
Performance,"Closes #4867. @takutosato Here it is. I'm not quite ready to make it the M2 default, but it looks really good. @meganshand I have tested it on every mixture in your workspace and results look very similar to the previous hand-tuned pruning results. I'm hoping it's good enough to become best practices for mitochondria and would appreciate if you gave it a shot. You have the right to review if you wish but there's no pressure to do so. @ldgauthier HaplotypeCaller might also benefit from this. In particular, I wonder about #3697. I'll test it out.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5473:226,tune,tuned,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5473,1,['tune'],['tuned']
Performance,Closes #6291. Closes #6254. @takutosato The idea here is to only fill an expensive cache of all the different PL indices if the variant has PLs (ie is from HC and not M2).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6446:83,cache,cache,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6446,1,['cache'],['cache']
Performance,"Closes #6586. @droazen . `AlleleLikelihoods` caches the evidence-to-index `Map`. The previous implementation tried to update this map on the fly whenever evidence was removed. The new approach is to simply invalidate the cache and allow the existing code to generate it to run later. I don't expect this to cause performance problems for a few reasons:. 1. It only applies when we're doing contamination downsampling.; 2. It may save time whenever evidence is removed and we don't need the evidence-to-index map later.; 3. Regenerating the cache is O(N), but so is updating on-the-fly even when only one read is removed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6593:45,cache,caches,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6593,4,"['cache', 'perform']","['cache', 'caches', 'performance']"
Performance,Closing -- we now have a `--strict` mode that matches the regular `HaplotypeCaller` at the expense of speed. Next step is to work on performance (https://github.com/broadinstitute/gatk/issues/5263),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5265#issuecomment-460398930:133,perform,performance,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5265#issuecomment-460398930,1,['perform'],['performance']
Performance,Closing because of bad travis cache for builds. Will submit identical PR with cloned branch.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5119#issuecomment-414426377:30,cache,cache,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5119#issuecomment-414426377,1,['cache'],['cache']
Performance,"Closing this issue, since the performance issues that the `HaplotypeCaller` had in the early betas are believed to be resolved in the 4.0 release. @chandrans please open a new ticket if the user can replicate the performance issue in the 4.0 release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-361989230:30,perform,performance,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-361989230,2,['perform'],['performance']
Performance,"Code is polished and is ready for review. Partial TODO list:; - [ ] (ISSUE TO BE MADE) generate denoising sample summary statistics output (distribution of bias factors, HMM likelihoods, distribution of unexplained variance); - [ ] (ISSUE TO BE MADE) generate plots (training history, GC curves, histogram of unexplained variance, histogram of mean bias); - [ ] (ISSUE TO BE MADE) prior optimization based on real data. In addition, I will also make issues for the TODO comments in the code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3838:387,optimiz,optimization,387,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3838,1,['optimiz'],['optimization']
Performance,"CollectMultipleMetrics perform Percent-encoding of paths, breaks with Cromwell packed CWL workflows",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5931:23,perform,perform,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5931,1,['perform'],['perform']
Performance,"Command:; `java -Djava.io.tmpdir=/work/TMP \ ; -Xmx40g -jar ~/bin/gatk-4.0.8.1/gatk-package-4.0.8.1-local.jar \ ; GenomicsDBImport \ ; -V /work/Analysis/III_3P_RG_DupMark.raw.snps.indels.g.vcf \ ; -V /work/Analysis/IV_11N_RG_DupMark.raw.snps.indels.g.vcf \; -V /work/Analysis/IV_8N_RG_DupMark.raw.snps.indels.g.vcf \; -V /work/Analysis/IV_10P_RG_DupMark.raw.snps.indels.g.vcf \; -V /work/Analysis/IV_20P_RG_DupMark.raw.snps.indels.g.vcf \; --genomicsdb-workspace-path /work/Analysis/wang_chr19_re \; --intervals chr19`. **Error Log**. 15:00:35.770 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/wang/bin/gatk-4.0.8.1/gatk-package-4.0.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:00:35.944 INFO GenomicsDBImport - ------------------------------------------------------------; 15:00:35.944 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.8.1; 15:00:35.945 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:00:35.945 INFO GenomicsDBImport - Executing as wang@Ubuntu1604 on Linux v3.16.0-43-generic amd64; 15:00:35.945 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-2~14.04-b11; 15:00:35.945 INFO GenomicsDBImport - Start Date/Time: October 2, 2018 3:00:35 PM JST; 15:00:35.945 INFO GenomicsDBImport - ------------------------------------------------------------; 15:00:35.945 INFO GenomicsDBImport - ------------------------------------------------------------; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Version: 2.16.0; 15:00:35.946 INFO GenomicsDBImport - Picard Version: 2.18.7; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:00:35.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5342:575,Load,Loading,575,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342,1,['Load'],['Loading']
Performance,"Commit 558160ea5bfde8be3b6e4bdd5283c529fb905fca, which upgrades gkl to 0.3.1 fails on PowerPC. The reason is in gkl-0.3.1, the following code block in IntelGKLUtils.java:. try {; // try to extract from classpath; String resourcePath = ""native/"" + System.mapLibraryName(libFileName);; URL inputUrl = IntelGKLUtils.class.getResource(resourcePath);; if (inputUrl == null) {; logger.warn(""Unable to find Intel GKL library: "" + resourcePath);; return false;; }. logger.info(String.format(""Trying to load Intel GKL library from:\n\t%s"", inputUrl.toString()));. File temp = File.createTempFile(FilenameUtils.getBaseName(resourcePath),; ""."" + FilenameUtils.getExtension(resourcePath), tempDir);; FileUtils.copyURLToFile(inputUrl, temp);; temp.deleteOnExit();; logger.debug(String.format(""Extracted Intel GKL to %s\n"", temp.getAbsolutePath()));. System.load(temp.getAbsolutePath());; logger.info(""Intel GKL library loaded from classpath."");; } catch (IOException ioe) {; // not supported; logger.warn(""Unable to load Intel GKL library."");; return false;; }. does not check machine architecture, nor catches any exception from `System.load()` function. On PowerPC, the dynamic library (.so file) still exists, but it's in illegal format. Hence the crash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302:494,load,load,494,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302,5,['load'],"['load', 'loaded']"
Performance,"Comprises the commits after 7992f64. The only commit with real substance is `Updated metadata and abstract collection classes.`. The rest of the commits simply update calling code, related tests, and test files. These updates were slightly less trivial for the plotting classes, so these are also split off into separate commits. Again, probably could be engineered better (there are two parallel class hierarchies for metadata and collection classes, which is kind of gross), but we can refactor later if needed. @asmirnov239 please review. Again, lower priority than gCNV VCF, but the sooner this is in master the easier it will be to get things into FireCloud. Let's try for early next week. I'll start doc updates concurrently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3914:718,concurren,concurrently,718,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3914,1,['concurren'],['concurrently']
Performance,ConcurrentModificationException causes HaplotypeCallerSparkIntegrationTest.testVCFModeIsConcordantWithGATK3_8Results failure.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513:0,Concurren,ConcurrentModificationException,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513,1,['Concurren'],['ConcurrentModificationException']
Performance,ConcurrentModificationException in ReadsPipelineSparkIntegrationTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680:0,Concurren,ConcurrentModificationException,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680,1,['Concurren'],['ConcurrentModificationException']
Performance,Consider restoring CigarUtils optimization for short-circuiting to M-only CIGARs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7441:30,optimiz,optimization,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7441,1,['optimiz'],['optimization']
Performance,"Consolidated with #2498. Now that #6885 is done, I'm going to kick off a Bayesian optimization (using the pipeline-optimizer from https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer I presented on long ago) over all 3 sets of SW parameters using unfiltered HaplotypeCaller -> vcfeval F1 on NA12878 chr22 (with F1 optimized on GQ threshold and enabling decomposition of variants) as the target. This is probably not what we want to ultimately do in practice---we may want to more heavily weight sensitivity after calling, hook up variant filtering, stratify on high/low confidence regions or variant characteristics, etc.---but I'm just curious to see what happens. I see two potentially useful outcomes: 1) we demonstrate that parameters don't have much of an effect and can be consolidated, or 2) we find more optimal sets of parameters. Potentially we could also show that 3) our parameters are already optimal (I'd say this would be by pure dumb luck), in which case we could at least demonstrate and document some justification for them. If the parameters don't have much of an impact on NA12878, I'm curious to see whether this holds for low coverage or messier data---and ultimately, in malaria. Just starting with NA12878 because of the availability of truth and the potential impact for the primary use case of calling in human data. Some preliminary results: I ran the aforementioned comparison on chr22 with 1) 4.1.8.1 master and 2) 4.1.8.1 with haplotype-to-reference SW parameters changed from `NEW_SW_PARAMETERS` to `STANDARD_NGS` on two replicates of NA12878 (O1D1 and O2D2 from the 2018 NovaSeq snapshot experiment). On each replicate, 2) demonstrated slightly lower performance, but it was well within the sample-to-sample variation between these two replicates. Here are the corresponding vcfeval summaries:. ```; ::::::::::::::; NA12878/O1D1/4.1.8.1/summary.txt; ::::::::::::::; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sen",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566:82,optimiz,optimization,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566,4,['optimiz'],"['optimization', 'optimized', 'optimizer']"
Performance,Contamination is run. The exception I get is:; org.apache.commons.math3.exception.TooManyEvaluationsException: illegal state: maximal count (20) exceeded: evaluations; 	at org.apache.commons.math3.optim.BaseOptimizer$MaxEvalCallback.trigger(BaseOptimizer.java:242); 	at org.apache.commons.math3.util.Incrementor.incrementCount(Incrementor.java:155); 	at org.apache.commons.math3.optim.BaseOptimizer.incrementEvaluationCount(BaseOptimizer.java:191); 	at org.apache.commons.math3.optim.univariate.UnivariateOptimizer.computeObjectiveValue(UnivariateOptimizer.java:148); 	at org.apache.commons.math3.optim.univariate.BrentOptimizer.doOptimize(BrentOptimizer.java:225); 	at org.apache.commons.math3.optim.univariate.BrentOptimizer.doOptimize(BrentOptimizer.java:43); 	at org.apache.commons.math3.optim.BaseOptimizer.optimize(BaseOptimizer.java:153); 	at org.apache.commons.math3.optim.univariate.UnivariateOptimizer.optimize(UnivariateOptimizer.java:70); 	at org.broadinstitute.hellbender.utils.OptimizationUtils.max(OptimizationUtils.java:40); 	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationModel.lambda$calculateContamination$13(ContaminationModel.java:214); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationModel.calculateContamination(ContaminationModel.java:215); 	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationModel.<init>(ContaminationModel.java:67); 	at org.broa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6282:1033,Optimiz,OptimizationUtils,1033,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6282,1,['Optimiz'],['OptimizationUtils']
Performance,"CopyGCSDirectoryIntoHDFSSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Deflater IntelDeflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Inflater IntelInflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Initializing engine; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.or",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363:1899,load,loaded,1899,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363,1,['load'],['loaded']
Performance,"Correct for newly non-optional inputs to `GvsJointVariantCalling`, plus fix an unrelated race condition between creating the dataset and running this workflow. Integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f8797021-f5d4-4003-92be-010001b7f6af).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8833:89,race condition,race condition,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8833,1,['race condition'],['race condition']
Performance,Could I have some feedback about the efficiency of the Fisher's Exact Test implemented here? I'm planning to use it in other context where the performance could be reduced and I think that this is a good opportunity to have some information about it. Thanks in advance!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-267036486:143,perform,performance,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-267036486,1,['perform'],['performance']
Performance,Could not load genomicsdb native library,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4124:10,load,load,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4124,1,['load'],['load']
Performance,"Could this be related to having sliced objects in the gsutils buckets but not using a code path that goes through a native CRC implementation? I ask because I noticed that when I try to download the file. ```; gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; ```. with gsutil, I get this error:. ```; CommandException: ; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see:. $ gsutil help crcmod. To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file.; ```. Could the GATK command path be computing all of the CRC hashes in Java code, slowing it down?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-223982882:541,throttle,throttle,541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-223982882,3,"['perform', 'throttle']","['performance', 'throttle']"
Performance,"Couldn't open hdf5 files.; ![Screenshot_2020-10-29_17-20-17](https://user-images.githubusercontent.com/29140765/97586406-459fe000-1a0b-11eb-86cf-d70a28c55637.png); Running without the optional --sequence-dictionary argument also causes an error.; `17:00:00.556 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/lmbs02/bio/biosoft/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 29, 2020 5:00:00 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:00:00.683 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 17:00:00.684 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.1.9.0; 17:00:00.684 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Executing as lmbs02@Lmbs01 on Linux v5.4.0-48-generic amd64; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-8u265-b01-0ubuntu2~18.04-b01; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Start Date/Time: October 29, 2020 5:00:00 PM MSK; 17:00:00.684 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 17:00:00.684 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 17:00:00.684 INFO PostprocessGermlineCNVCalls - HTSJDK Version: 2.23.0; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Picard Version: 2.23.3; 17:00:00.684 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:00:00.685 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:00:00.685 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:00:00.685 INFO PostprocessGermlineCNV",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718787427:288,Load,Loading,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718787427,1,['Load'],['Loading']
Performance,CountVariants in Spark. exposed Loading VariantContexts in parallel,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1496:32,Load,Loading,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1496,1,['Load'],['Loading']
Performance,Create Spark partitioner that can efficiently load records for overlapping genomic regions,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1988:46,load,load,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1988,1,['load'],['load']
Performance,CreateVariantIngestFiles handles partially or fully loaded samples [VS-262] [VS-258],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7831:52,load,loaded,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7831,1,['load'],['loaded']
Performance,CreateVariantIngestFiles robust to partially / fully loaded samples [VS-262],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7843:53,load,loaded,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7843,1,['load'],['loaded']
Performance,"Current status of this: The tool can physically run on 11k samples, but with a 1-5% failure rate, depending on the combination of arguments used. The failures are almost all due to https://github.com/broadinstitute/gatk/issues/2685 (see the stack trace in https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308541727 for a representative example error). . One possibility is that we are being throttled in a way that GATK itself can't recover from. GATK is retrying in the face of these SSL errors 20 times, with increasing wait times between each attempt, and still running out of retries. See @jean-philippe-martin 's latest hypothesis in https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308586876. @kcibul @Horneth take note.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-308755736:406,throttle,throttled,406,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-308755736,1,['throttle'],['throttled']
Performance,"Currently GATK4 walker mode won't recognize altivec-based libVectorLoglessPairHMM.so only AVX based library. On POWER it will Falling back to the MUCH slower LOGLESS_CACHING implementation. To avoid performance degradation on POWER for Haplotyecaller, please include support for Altivec based pairhmm library libVectorLoglessPairHMM.so; Two possible ways to do it:; 1. integrate support by using ""grep -i altivec /proc/cpuinfo"" to identify availability of Altivec support and then integrate the library; 2. We can setup Java path or other options that will look for any libVectorLoglessPairHMM.so available and test compatibility. We would do all necessary works to get this done, but would appreciate your direction on which way to peruse. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3180:199,perform,performance,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3180,1,['perform'],['performance']
Performance,"Currently `GATKRead.copy()` is unable to guarantee a deep copy, since we only have a deep copy method for Google `Read`s (`GenericData.clone()`, which it inherits), not `SAMRecord`s. We should write a deep copy method for `SAMRecord`, hook it up to the `GATKRead.copy()` implementation in `SAMRecordToGATKReadAdapter`, and change the method contract to guarantee that a deep copy will be performed. This is not a huge priority, since `GATKRead` already guarantees that defensive copies will be made of all mutable reference types returned from accessor methods (which means that shallow copies should be safe to use freely), but would be nice for consistency and peace of mind.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/623:388,perform,performed,388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/623,1,['perform'],['performed']
Performance,"Currently all VariantWalkers use `100_000` as the feature cache look-ahead settings. This works well for most, but is likely causing memory issues for GenotypeGVCFs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3471:58,cache,cache,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3471,1,['cache'],['cache']
Performance,"Currently it defaults to 1 core. It seems like it scales well to 4 cores and correctly throttles back if there are fewer cores, so we should make it use the available resources if possible.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2717:87,throttle,throttles,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2717,1,['throttle'],['throttles']
Performance,Currently it seems like the pull request build on travis is running the wrong commit's tests in docker. The docker build script takes a commit hash as part of it's inputs and then performs a checkout of that when building the docker. This is failing for the PR builds because the .travis.yml is currently getting the hash from calling rev-parse on the current branch which gives the commit number of master. It should be using $TRAVIS_COMMIT as the hash.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3216:180,perform,performs,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3216,1,['perform'],['performs']
Performance,"Currently such information can only be extracted indirectly via a conversion to SamRecord, which incurs unnecessary performance penalty:. ```; GATKRead read = ... // proper initialization; SAMFileHeader header = ...; SamPairUtil.PairOrientation readOrientation = SamPairUtil.getPairOrientation(read.convertToSAMRecord(header)); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1550:116,perform,performance,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1550,1,['perform'],['performance']
Performance,"Currently the best way to select a subset of reads overlapping an interval is to provide a list of -L reader interval arguments. Unfortunately, this requires that the bam be sorted and indexed, which can often be a pain point when trying to perform quick tests, as a sort is often slow and requires copying the bam a second time. To alleviate this, it would be nice to add a ReadFilter level interval argument, for example an `IntervalOverlapReadFilter` or some such which can take similarly formatted arguments to -L. This would allow one to use `PrintReads` to select an interval over unsorted/unindexed bams more easily.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4860:241,perform,perform,241,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4860,1,['perform'],['perform']
Performance,"Currently we emit `cpx.vcf` for complex SV's, and one record for each variant.; But this may hurt us in terms of performance evaluation when smaller, simple variants are incorporated into a complex one.; We should have a tool to extract these smaller variants and link them with `EVENTID`.; This shouldn't be difficult.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4323:113,perform,performance,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4323,1,['perform'],['performance']
Performance,Currently we get this warning every time we run a spark program. We should exclude the older version of slf4j so we don't get this warning. ```; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.10/b3eeae7d1765f988a1f45ea81517191315c69c9e/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-jdk14/1.7.7/25d160723ea37a6cb84e87cd70773ff02997e857/slf4j-jdk14-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1063:254,cache,caches,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1063,2,['cache'],['caches']
Performance,"Currently we have Spark performance tests on gatk-jenkins which fail intermittently due to normal variance in runtimes. As a result, we don't always closely look into failures. We need Spark correctness tests in jenkins that are separate from the performance tests and that fail only when there's an actual regression -- and when these fail, it should always trigger a timely investigation by an engineer into what's gone wrong.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2288:24,perform,performance,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288,2,['perform'],['performance']
Performance,"Currently, Feature arguments for each tool are discovered automatically through reflection via FeatureManager and initialized/added to a query pool. Perhaps for the sake of consistency we should perform the same kind of auto-discovery for all common kinds of input arguments (reads, reference, etc.)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/300:195,perform,perform,195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/300,1,['perform'],['perform']
Performance,"Currently, SelectVariants includes a call to `initalizeAlleleAnyploidIndicesCache` for every processed variant. `initalizeAlleleAnyploidIndicesCache` requires extracting genotype information, which forces the LazyGenotypeContext machinery in htsjdk to fully decode all genotypes, even if the subsetting operation being performed does not require genotype information. This can cause such subsetting operations to take unnecessarily long amounts of time when run on large mutlisample vcfs. As an example, it takes ~24 hours to extract all snps from a 1000Genomes vcf. This PR bumps the version of htsjdk to no longer need the call to `initalizeAlleleAnyploidIndicesCache`, thus saving the performance boost from lazy genotype parsing. On the example mentioned above, this results in a ~13x speedup, as the time to extract all snps from a 1000Genomes vcf drops to under 2 hours. requires samtools/htsjdk#1500",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6754:319,perform,performed,319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6754,2,['perform'],"['performance', 'performed']"
Performance,"Currently, `StandardCallerArgumentCollection`, which contains the `-contamination-file` argument, requires that the contamination file be loaded externally, and its contents then passed back in to the argument collection via `setSampleContamination()`. This is rather poor design, and an invitation for bugs. We should refactor so that `StandardCallerArgumentCollection` handles the loading of the contamination file internally, and remove the `setSampleContamination()` method.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4483:138,load,loaded,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4483,2,['load'],"['loaded', 'loading']"
Performance,D.scala:823); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:10381,concurren,concurrent,10381,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,"DBImport, I randomly select 50 samples from our history samples(using the same probe set) along with the current batch.; time ${gatk} --java-options ""-Xmx8g -Xms2g"" GenomicsDBImport \; --tmp-dir /paedyl01/disk1/yangyxt/test_tmp \; --genomicsdb-update-workspace-path ${vcf_dir}/genomicdbimport_chr${1} \; -R ${ref_gen}/ucsc.hg19.fasta \; --batch-size 0 \; --sample-name-map ${gvcf}/batch_cohort.sample_map \; --reader-threads 5; check_return_code. # For GenotypeGVCFs; time ${gatk} --java-options ""-Xmx8g -Xms2g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" GenotypeGVCFs \; -R ${ref_gen}/ucsc.hg19.fasta \; -V gendb://${vcf_dir}/genomicdbimport_chr${1} \; -G StandardAnnotation \; -G AS_StandardAnnotation \; -L chr${1} \; -O ${bgvcf}/all_${seq_type}_samples_plus_${sample_batch}.chr${1}.HC.vcf. # These are log records:; 02:07:51.286 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 02:07:51.321 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/yangyxt/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl; Nov 06, 2020 2:07:56 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 02:07:56.529 INFO GenotypeGVCFs - ------------------------------------------------------------; 02:07:56.529 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 02:07:56.530 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:08:01.543 INFO GenotypeGVCFs - Executing as yangyxt@paedyl01 on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 02:08:01.543 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 02:08:01.543 INFO GenotypeGVCFs - Start Date/Time: November 6, 2020 2:07:51 AM HKT; 02:08:01.543 INFO GenotypeGVCFs - ----------------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-722764059:1257,Load,Loading,1257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-722764059,1,['Load'],['Loading']
Performance,DBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReaders(GenomicsDBImport.java:419); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:344); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:740); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Remote host closed connection during handshake; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 28 more; Caused by: com.google.cloud.storage.StorageException: Remote host closed connection during handshake; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685:4381,concurren,concurrent,4381,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685,1,['concurren'],['concurrent']
Performance,DDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.scala:911); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:360); at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.writeVariants(HaplotypeCallerSpark.java:205); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.runTool(HaplotypeCallerSpark.java:115); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3019:10823,concurren,concurrent,10823,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019,2,['concurren'],['concurrent']
Performance,"DWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz`. #### Expected behavior; Should run to completion and create reblocked GVCF. #### Actual behavior; ```; Reblocking gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz to gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; 11:25:55.531 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 30, 2021 11:25:55 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:25:55.708 INFO ReblockGVCF - ------------------------------------------------------------; 11:25:55.709 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.0.0; 11:25:55.709 INFO ReblockGVCF - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:25:55.709 INFO ReblockGVCF - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.25.1.el7.x86_64 amd64; 11:25:55.709 INFO ReblockGVCF - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 11:25:55.709 INFO ReblockGVCF - Start Date/Time: June 30, 2021 11:25:55 AM EDT; 11:25:55.710 INFO ReblockGVCF - -----------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7334:1485,Load,Loading,1485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334,1,['Load'],['Loading']
Performance,Date/Time: 28 novembre 2019 15:47:36 CET; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - HTSJDK Version: 2.20.3; 15:47:37.246 INFO Mutect2 - Picard Version: 2.21.1; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:37.247 INFO Mutect2 - Deflater: IntelDeflater; 15:47:37.247 INFO Mutect2 - Inflater: IntelInflater; 15:47:37.247 INFO Mutect2 - GCS max retries/reopens: 20; 15:47:37.247 INFO Mutect2 - Requester pays: disabled; 15:47:37.247 INFO Mutect2 - Initializing engine; 15:47:41.204 INFO Mutect2 - Done initializing engine; 15:47:42.352 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 15:47:42.423 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 15:47:42.482 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 15:47:42.483 INFO IntelPairHmm - Available threads: 8; 15:47:42.483 INFO IntelPairHmm - Requested threads: 4; 15:47:42.483 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 15:47:42.936 INFO ProgressMeter - Starting traversal; 15:47:42.936 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 15:47:53.565 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:19555 0.2 90 508.0; 15:48:05.962 INFO ProgressMete,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:2052,Load,Loading,2052,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['Load'],['Loading']
Performance,"Date/Time: March 7, 2019 9:49:03 AM EST; 09:49:05.901 INFO Mutect2 - ------------------------------------------------------------; 09:49:05.901 INFO Mutect2 - ------------------------------------------------------------; 09:49:05.901 INFO Mutect2 - HTSJDK Version: 2.18.2; 09:49:05.901 INFO Mutect2 - Picard Version: 2.18.25; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:49:05.902 INFO Mutect2 - Deflater: IntelDeflater; 09:49:05.902 INFO Mutect2 - Inflater: IntelInflater; 09:49:05.902 INFO Mutect2 - GCS max retries/reopens: 20; 09:49:05.902 INFO Mutect2 - Requester pays: disabled; 09:49:05.902 INFO Mutect2 - Initializing engine; 09:49:06.887 INFO Mutect2 - Done initializing engine; 09:49:06.935 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/Tools/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:49:06.937 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 09:49:06.937 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 09:49:07.007 INFO ProgressMeter - Starting traversal; 09:49:07.007 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:49:17.023 INFO ProgressMeter - 1:139173 0.2 480 2875.7; 09:49:27.704 INFO ProgressMeter - 1:763661 0.3 2590 7508.3; 09:49:38.001 INFO ProgressMeter - 1:958723 0.5 3290 6369.0; 09:49:49.182 INFO ProgressMeter - 1:981050 0.7 3380 4808.5; 09:50:02.383 INFO ProgressMeter - 1:988991 0.9 3440 3727.3; 09:50:13.586 INFO ProgressMeter - 1:1227096 1.1 4290 3866.1; 09:50:23.594 INFO ProgressMeter - 1:1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844:1817,Load,Loading,1817,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844,1,['Load'],['Loading']
Performance,"Dealing with case when two alignment blocks contain each other in their ref span, indicating duplication.; Instead of outputting CIGARs for the duplicated units like we did for duplication records now in master pipeline, we output alt haplotype sequence from the evidence contigs we assembled, since we did an experiment work with inverted duplications. Some performance evaluation based on the logs. ```; master.vcf. 10:28:46.262 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - Discovered 6324 variants.; 10:28:46.277 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - INV: 237; 10:28:46.277 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - DEL: 3680; 10:28:46.277 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - DUP: 1141; 10:28:46.277 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - INS: 1266; 10:28:46.483 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - Shutting down engine; [October 4, 2017 10:28:46 AM EDT] org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark done. Elapsed time: 0.53 minutes.; Runtime.totalMemory()=3954180096. ==============. feature.vcf. 13:51:48.490 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - Discovered 6543 variants.; 13:51:48.502 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - INV: 229; 13:51:48.502 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - DEL: 3679; 13:51:48.502 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - DUP: 1365; 13:51:48.502 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - INS: 1270; 13:51:48.770 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - Shutting down engine; [October 5, 2017 1:51:48 PM EDT] org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark done. Elapsed time: 0.49 minutes.; Runtime.totalMemory()=4026531840; ```. No variants that were dropped are simple variants, and they are expected to be brought back with the correct interpretation once complex sv PR series are fully coded. __Two known i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3668:359,perform,performance,359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3668,1,['perform'],['performance']
Performance,"Dear @droazen I might now have an idea.; Since we use the container in a restricted are, we pull the container from docker hub, save it as tar.gz, transfer it to the server and load it to the docker local image library. On this system we had the latest version of docker installed that was available from the ubuntu repository (20.something). This might lead to the error.; Best,; Daniel",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1710415398:177,load,load,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1710415398,1,['load'],['load']
Performance,"Dear @sooheelee . I am wondering if I need to perform base recalibration on the normal bam files before I proceed with creating the panel of normals, which will then be used for somatic mutation calling with MuTect2. . The tutorial (https://software.broadinstitute.org/gatk/documentation/article?id=11136) and the documentation (https://software.broadinstitute.org/gatk/documentation/article?id=11127) suggest to me that baseRecalibration is not required, but I wanted to confirm before proceeding with the pipeline. Regards,; Sangjin Lee",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5836:46,perform,perform,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5836,1,['perform'],['perform']
Performance,"Dear GATK developers,. Thank you very much for the tool! I recently encountered an issue while running GATK gCNV, specifically with the step GermlineCNVCaller in cohort mode. I ran the workflow using WDL and the GATK v4.3.0.0 docker image. I am not sure how the error should be interpreted and thus would like to seek your advice on debugging. Since the entire cohort contains 75 samples, I omitted some verbose debug messages in the error log below. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/paedyl01/disk1/louisshe/work/NGS/wdl/test_workflow_cnv/germline/cromwell-executions/CNVGermlineCohortWorkflow/d53c0a12-6b07-4f74-acb; 21:05:38.275 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:05:38.297 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /paedyl01/disk1/louisshe/tmp/gatk/libgkl_compression8230524266824482022.so; 21:05:38.388 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:05:38.388 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0; 21:05:38.388 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:05:38.388 INFO GermlineCNVCaller - Executing as louisshe@paedyl02 on Linux v3.10.0-1160.11.1.el7.x86_64 amd64; 21:05:38.389 INFO GermlineCNVCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 21:05:38.389 INFO GermlineCNVCaller - Start Date/Time: August 13, 2024 9:05:38 PM GMT; 21:05:38.389 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:05:38.389 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:05:38.390 INFO GermlineCNVCaller - HTSJDK Version: 3.0.1; 21:05:38.390 INFO GermlineCNVCaller - Picard Version: 2.27.5; 21:05:38.390 INFO GermlineCNVCaller - Built for Spark Version: 2.4.5; 21:05:38.391 INFO GermlineCNVCaller -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:673,Load,Loading,673,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Load'],['Loading']
Performance,"Dear all, ; May I ask about the DP, AD, and AF information of Mutect2?. If we perform somatic mutation calling with the same pair of tumor samples, we can see that:; >chr2 227915762 . T C 240 PASS ; >STATUS=StrongSomatic;SAMPLE=yjhzy_T1;TYPE=SNV;DP=155;VD=69;AF=0.4452;SHIFT3=0;MSI=2.000;MSILEN=1;SSF=0;SOR=Inf;LSEQ=GAGCCTGGAGGGCCTGGGGG;RSEQ=CCAGGAGGCCCTGGCTGACC ; >GT:DP:VD:ALD:RD:AD:AF:BIAS:PMEAN:PSTD:QUAL:QSTD:SBF:ODDRATIO:MQ:SN:HIAF:ADJAF:NM; >0/1:155:69:34,35:45,41:86,69:0.4452:2,2:31.6:1:39.3:1:0.74799:1.12895:60:68:0.4416:0:1.4; >0/0:106:0:0,0:59,47:106,0:0:2,0:35.6:1:35.1:1:1:0:60:105:1:0:0.4. VarDict said the DP, AD, AF were 155, 69, and 0.4452. The control DP and AD were 106, 0, 0. >chr2 227915762 . T C . PASS ; >DP=262;MQ=60.00;MQ0=0;NT=ref;QSS=242;QSS_NT=3070;ReadPosRankSum=1.63;SGT=TT->CT;SNVSB=0.00;SOMATIC;SomaticEVS=19.98;TQSS=1;TQSS_NT=1 ; >DP:FDP:SDP:SUBDP:AU:CU:GU:TU; >106:0:0:0:0,0:0,0:0,0:106,106; >156:0:0:0:0,0:69,69:0,0:87,87. Strelka2 said the DP of tumor was 156. The control DP was 106, (The sequence was opposite with VarDict). However, Mutect2 said that; >chr2 227915762 . T C . PASS ; >CONTQ=93;DP=88;ECNT=1;GERMQ=118;MBQ=35,40;MFRL=205,190;MMQ=60,60;MPOS=38;NALOD=1.70;NLOD=14.70;POPAF=3.00;SAAF=0.364,0.333,0.378;SAPP=0.014,0.036,0.950;TLOD=50.64 ; >GT:AD:AF:DP:F1R2:F2R1 ; >0/1:23,14:0.385:37:11,7:12,7 ; >0/0:49,0:0.019:49:25,0:24,0; The DP, AD, AF were 37, 14, and 0.385. The control DP, AD and AF were 49, 0, 0.019. There are serveral things which are confusing:. 1. Why the DP reported in Mutect2 results are different in ##INFO and ##FORMAT fields? We can see: CONTQ=93;**DP=88**, vs. **DP=37**.; 2. Why the AF of Mutect2 is strange? **14 / ( 23 + 14) = 0.378 != 0.385**.; 3. Why in the control sample, we got AF = 0.019, but **0 / (0 + 49) = 0**?; 4. Why all the sites in the VCFs showed as above?. Any reply will be appreciated. Best regards!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6067:78,perform,perform,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6067,1,['perform'],['perform']
Performance,"Dear all,. I installed the latest version of gatk as follows;. download the latest version of gatk (https://github.com/broadinstitute/gatk/releases). followed this (https://gatk.broadinstitute.org/hc/en-us/articles/360035889851--How-to-Install-and-use-Conda-for-GATK4) (conda env create -n gatk4 -f gatkcondaenv.yml). installation was completed as below;. Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117482 sha256=5e0f0b2eb6027268eb5814acd8c8b57d265b7aeb371702c736dd4723aa1beee4; Stored in directory: /tmp/pip-ephem-wheel-cache-gyc4oo9g/wheels/86/46/5d/d5d2d327a9cdc718f906fa1d0cd6e18392bd4eea267f327437; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done. when I started to run this command;. gatk CNNScoreVariants -V 21002.HaplotypeCaller.output.g.vcf.gz -R hg19.fa -O annotated.vcf; it gives an error as below;. java.lang.RuntimeException: A required Python package (""gatktool"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7397:835,cache,cache-,835,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397,1,['cache'],['cache-']
Performance,Default for smithwaterman will use optimized AVX code,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4356:35,optimiz,optimized,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4356,1,['optimiz'],['optimized']
Performance,DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:3744,concurren,concurrent,3744,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,2,['concurren'],['concurrent']
Performance,"Deflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not inst",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:5156,load,loaded,5156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['load'],['loaded']
Performance,"Did we change the name of the files since the initial bug was filed? Because my earlier comment talks about downloading to my desktop in 2.5min, but retrying now it takes about an hour! The file is 34.56 GiB. I guess that's what I get for moving to a different office. Assuming this time is correct, I see that NIO (when multithreaded) matches gsutil performance. Output below. gsutil: . ```; $ time gsutil cp gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam .; Copying gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam...; Downloading ..../CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam: 8.64 GiB/8.64 GiB ; Downloading ..../CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam: 8.64 GiB/8.64 GiB ; Downloading ..../CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam: 8.64 GiB/8.64 GiB ; Downloading ..../CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam: 8.64 GiB/8.64 GiB . real 55m11.517s; user 11m27.584s; sys 13m44.240s; ```. NIO (via ParallelCountBytes). ```; Reading the whole file using 4 threads...; Read all 37107966478 bytes in 3354s. ; The MD5 is: 0x73B5B38156DF9E2FF25FC57DD858DA0F; ```. (3354s is 55min54s). Next step: try on a Google Compute Engine computer, to get datacenter speeds.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-227605962:351,perform,performance,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-227605962,1,['perform'],['performance']
Performance,Did you reproduce the issue after using `--genomicsdb-shared-posixfs-optimizations` with GenomicsDBImport?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761265484:69,optimiz,optimizations,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761265484,1,['optimiz'],['optimizations']
Performance,"Different traversal options might optimize some pipelines and give flexibility to the API user to have more efficient tools. Some of the different traversal per-pass that I am interested are:; * Allow different filters applied in each pass. Example usage: collect some metric with all reads in the first pass, and use it for the second pass only on the filtered reads.; * Allow different transformers in each pass. Example usage: in the first pass, untransformed reads are used to collect a metric; based on that metric, the second pass transform the reads to be output.; * Allow different intervals applied in each pass. Example usage: iterate over the user intervals to get some information about the reads, and then in the second pass use that information to iterate over a different set of intervals. I came out with this feature in the `TwoPassReadWalker` to implement a tool for pair-end data. The use case is to iterate in the first pass to the user intervals/filters, collecting where the mates are located (and/or read names), and in the second pass iterate over the reads and its mates independently of where they are located. If giving this flexibility to the `TwoPassReadWalker` is not desired, another option is to implement another walker-type `ReadAndMatesWalker` which implement the first pass in a private method and the second as a `ReadWalker.apply` for the reads and its mates. Which one is the preferred option for the GATK engine team, @droazen?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4849:34,optimiz,optimize,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4849,1,['optimiz'],['optimize']
Performance,"Discussed with @LeeTL1220 just now, and we decided that an approach based on the vanilla `LocusWalker` with calls to `splitContextBySampleName()` would result in a cleaner and more easily-parallelizable implementation, without the need for maintaining state across calls. So this branch is getting put on hold for now until we see whether the overhead of having to merge bams and then pull apart the pileups in the single-pass implementation is a significant performance issue. @LeeTL1220 Let me know the outcome of that evaluation, and we'll decide whether to resurrect or kill this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2374#issuecomment-275748013:459,perform,performance,459,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2374#issuecomment-275748013,1,['perform'],['performance']
Performance,"Dispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.ConcurrentModificationException; 	at java.util.Vector$Itr.checkForComodification(Vector.java:1184); 	at java.util.Vector$Itr.next(Vector.java:1137); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:92); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 120 more; ```. @jamesemery @tomwhite I've seen this once, so it may be a super rare one that w",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680:11020,concurren,concurrent,11020,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680,1,['concurren'],['concurrent']
Performance,Dispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.testng.TestNGException:An error occurred while instantiating class org.broadinstitute.hellbender.engine.spark.ReadsPreprocessingPipelineSparkTestData. Check to make sure it can be instantiated; 	at org.testng.internal.InstanceCreator.createInstanceUsingObjectFactory(InstanceCreator.java:134); 	at org.testng.internal.InstanceCreator.createInstance(InstanceCreator.java:79); 	at org.testng.internal.ClassImpl.getDefaultInstance(ClassImpl.java:110); 	at org.testng.internal.ClassImpl.getInstances(ClassImpl.java:195); 	at org.testng.TestClass.getInstances(TestClass.java:102); 	at org.testng.TestClas,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858:1938,concurren,concurrent,1938,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858,1,['concurren'],['concurrent']
Performance,Do further profiling/optimization of HaplotypeCallerEngine,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5264:21,optimiz,optimization,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5264,1,['optimiz'],['optimization']
Performance,"Do you manually download the files in src/main/resources/large from git-lfs, you need those in order to perform the build. I'm not sure a good way to get those without using git. You could of course pull them and then tar them though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584454437:104,perform,perform,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584454437,1,['perform'],['perform']
Performance,"Do you mean in production? In the ~2.92Gbp of the WGS calling regions, I see all ~2.44Gbp of the GIAB HCR, as well as ~485Mbp / ~778Mbp of the GIAB ""LCR"" (naively, just the complement of the HCR). Or did you mean in another context?. I'm kicking off runs with the CHM now. Hopefully, optimizations of sensitivity outside the CHM HCR will be more meaningful, since I see a decent amount of calls outside of it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-713785389:284,optimiz,optimizations,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-713785389,1,['optimiz'],['optimizations']
Performance,Doc generation fails on tools that use classes in cachemanager package,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2988:50,cache,cachemanager,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2988,1,['cache'],['cachemanager']
Performance,Document how multi-threading support works in GATK4,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345:13,multi-thread,multi-threading,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345,1,['multi-thread'],['multi-threading']
Performance,Doing so comes at a significant performance cost that we have to consider.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/312#issuecomment-82518368:32,perform,performance,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/312#issuecomment-82518368,1,['perform'],['performance']
Performance,"Don't mean to derail the conversation, but I just wanted to chime in about this:; > A good setting for `--nativePairHmmThreads` is probably 4-8, you won't see any improvement after that. This doesn't sound right. We have informally tested how the performance of PHMM changes with this parameter, and we've observed roughly linear performance improvement as you increase the value of `nativePairHmmThreads`, up to the number of physical cores in the system. These performance tests were done a few months ago, mind you, but I can't think of anything that would have altered this behavior since then, at least in the GKL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332916917:247,perform,performance,247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332916917,3,['perform'],['performance']
Performance,"Done, the problematic commit was https://github.com/broadinstitute/gatk/commit/a304725a60f5000ec6381040137043a557fc3dc1. Below are the results with versions suggested by git bisect.; Calling performance was evaluated with chr22 in the query vcf and full truth vcf, therefore the calculation of the recall is invalid (but irrelevant for the purpose):; ![FD_TN_4181_chr22_FD_TN_-4 1 8 1-28-g6d8cdfc_chr22_FD_TN-4 1 8 1-42-g851c840_chr22_FD_TN-4 1 8 1-49-g4e8b73e_chr22_FD_TN-4 1 8 1-50-ga304725_chr22_FD_TN-4 1 8 1-51-g66570bf_chr22_FD_TN-4 1 8 1-52-g1238565_chr22_FD_TN_4190_chr22](https://user-images.githubusercontent.com/15612230/236503899-e922abb0-5d9f-44d3-bf44-a801652744ea.png). As stated in the pull request https://github.com/broadinstitute/gatk/pull/6821, the change was evaluated with the DREAM3 benchmark and made sense at the time. To be fair, in the HCC1395 benchmark there is still a gain in recall (<2%, see the first figure in my original report above), which I assume to be mostly due to this commit, but the recall/precision tradeoff looks different now with the better benchmark callset. I have not figured out what exactly the code in the commit does, maybe we can fine-tune the changes @davidbenjamin ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1536484624:191,perform,performance,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1536484624,2,"['perform', 'tune']","['performance', 'tune']"
Performance,"Done. Full results are on the internal presentation slides. The summary table follows:. | speedup vs async on a 1-core machine | slice | whole | intervals |; |-------------|----------|---------|----------|; | vcf | 0.91| 1.40| 0.57|; | bam (exome)| 40.42| 1.06| 1.02|; | bam (wgs)| 111.18| 1.21| 0.99|. This table compares the execution time of a single machine running PrintReads or SelectVariants, getting its input either directly from the Google bucket (NIO), or by first copying it with gsutil and then running off the local disk (with the async option turned on, allowing eager decompression of the stream - a feature the NIO code does not have). Each experiment is run four times, and each number here represents the ratio of two such experiments. Numbers larger than 1 indicate that NIO was faster. Each row is a different input file: vcf, small bam (exome), large bam (whole genome). Each column is a selection of what to read from the file (via the `-L` argument): a megabase slice, the whole file, or a long list of intervals. The NIO code relies heavily on prefetching, so it doesn't perform well with the many disjoint accesses of the right column. When processing only a small part of the (already small) vcf file, NIO loses out to copy + local processing. Everywhere else the direct-to-bucket ""NIO"" code performs quite well, up to 111x faster than the ""copy then process"" approach. I also ran the full set with async disabled. It makes a difference but NIO still wins and loses at the same places by similar margins (in particular the 111x win remains).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2424#issuecomment-284056956:1096,perform,perform,1096,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2424#issuecomment-284056956,2,['perform'],"['perform', 'performs']"
Performance,"Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.di$able=true --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --conf spark.cores.max=720 --executor-cores 20 --executor-memory 50g --conf spark.driver.memory=50g /home/myname/gatk4/gatk$build/libs/gatk-package-4.alpha.2-1125-g27b5190-SNAPSHOT-spark.jar BwaAndMarkDuplicatesPipelineSpark -I hdfs://ln16/user/myname/NA12878/wes/NA12878-NGv3-LAB1360-A.unaligned.bam -O hdfs://ln16/user/myname/gatk4test/B$aAndMarkDuplicatesPipelineSpark_out.bam -R hdfs://ln16/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit --bwamemIndexImage /hadoop/myname/GRCh37.fa.img --disableSequenceDictionaryValidation --sparkMaster spark://$n16:7077; 16:55:20.195 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/myname/gatk4/gatk/build/libs/gatk-package-4.alpha.2-1125-g27b5190-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [June 29, 2017 4:55:20 PM CST] BwaAndMarkDuplicatesPipelineSpark --bwamemIndexImage /hadoop/myname/GRCh37.fa.img --output hdfs://ln16/user/myname/gatk4test/BwaAndMarkDuplicatesPipelineSpark_out.bam --reference hdfs$//ln16/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit --input hdfs://ln16/user/myname/NA12878/wes/NA12878-NGv3-LAB1360-A.unaligned.bam --disableSequenceDictionaryValidation true --sparkMaster spark://ln16:7077 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedOutput false --numReducers 0 --help fal$e --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters fals",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:2468,Load,Loading,2468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['Load'],['Loading']
Performance,"Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; >; > INFO: Failed to detect whether we are running on Google Compute Engine.; >; > 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.843 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; >; > 16:17:05.843 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; >; > 16:17:05.843 INFO HaplotypeCaller - Executing as robert@powerlinux on Linux v4.4.0-184-generic ppc64le; >; > 16:17:05.843 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:2175,load,load,2175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,"EF ALT QUAL FILTER INFO FORMAT CGAAGAGGTAGGTGCGAG-1; chr19 55910646 . AC A,<NON_REF> 352.6 . . GT:AD:DP:GQ:PGT:PID:PS 0|1:6,9,0:15:99:0|1:55910646_AC_A:55910646; chr19 55910648 . AAATCCCCC A,<NON_REF> 352.6 . . GT:AD:DP:GQ:PGT:PID:PS 0|1:6,9,0:15:99:0|1:55910646_AC_A:55910646; chr19 55910653 . CCCCAT *,C,<NON_REF> 227.84 . . GT:AD:DP:GQ:PGT:PID:PS 2|1:0,9,6,0:15:99:1|0:55910646_AC_A:55910646; chr19 55910675 . T C,<NON_REF> 30.64 . . GT:AD:DP:GQ 0/1:13,2,0:15:38; ```. ```; Using GATK jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar GenotypeGVCFs -R chr19.fa -V output.g.vcf -O output.vcf; Picked up JAVA_TOOL_OPTIONS: -Djava.net.useSystemProxies=true; 13:40:59.573 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:40:59.636 INFO GenotypeGVCFs - ------------------------------------------------------------; 13:40:59.653 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.4.0.0; 13:40:59.653 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:40:59.654 INFO GenotypeGVCFs - Executing as gleixner@odcf-worker02 on Linux v3.10.0-1160.76.1.el7.x86_64 amd64; 13:40:59.654 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v17+35-2724; 13:40:59.655 INFO GenotypeGVCFs - Start Date/Time: October 26, 2023 at 1:40:59 PM CEST; 13:40:59.655 INFO GenotypeGVCFs - ------------------------------------------------------------; 13:40:59.655 INFO GenotypeGVCFs - ------------------------------------------------------------; 13:40:59.658 INFO GenotypeGVCFs - HTSJDK Version: 3.0",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195:17114,Load,Loading,17114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195,1,['Load'],['Loading']
Performance,EN they do something different that we had to replicate to achieve concordance. Dragen still performs equivalent modifications for steps 1-3 as they apply to the reads but rather than performing step 4 and using those reads for genotype assignment it instead for genotyping reaches back for each read (that has survived filtering) to its original BAM alignment (before being unclipped/hardclipped) and uses those reads for FRD/BQD calling. When running GATK with the new argument `--use-original-alignments-for-genotyping-overlap` this is what happens as well (step 4 is skipped entirely in addition). The results were somewhat surprising (listed below): . ![RealignmentPlotIndels](https://user-images.githubusercontent.com/16102845/87588690-13fc4680-c6b2-11ea-98e9-4c69259c2869.png); ![RealignmentPlotSNPs](https://user-images.githubusercontent.com/16102845/87588692-1494dd00-c6b2-11ea-96dc-ba06f45357c2.png). This says that running GATK in DRAGEN mode without realigning reads performs slightly better for low complexity region SNPs than it does with realignment. This could perhaps be a side effect of the BQD algorithm as it cares about the specific bases that are applied for SNPs. I have theorized that perhaps the explanation for this behavior has to do with the fact that the reads at stage 4 have undergone 2 different rounds of clipping (at stages 1 and 2) and could in actuality be as short as 11 bases long by this stage. If this is indeed the problem then realigning the reads (without anchoring information from the rest of the read that might have resulted in more accurate placement) might well result in significant noise as to where reads actually get placed after realignment. A necessary step to completing this ticket would have to be evaluating what sites account for the lost sensitivity and understanding why the realignment is responsible and evaluating if there is not a better way to perform the realignment that can handle these short stubs if indeed they are the problem.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6706:3705,perform,perform,3705,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6706,1,['perform'],['perform']
Performance,"EVEL : 2; > 17 15:07:52.441 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 18 15:07:52.441 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 19 15:07:52.442 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 20 15:07:52.442 INFO Mutect2 - Deflater: IntelDeflater; > 21 15:07:52.442 INFO Mutect2 - Inflater: IntelInflater; > 22 15:07:52.442 INFO Mutect2 - GCS max retries/reopens: 20; > 23 15:07:52.443 INFO Mutect2 - Requester pays: disabled; > 24 15:07:52.443 INFO Mutect2 - Initializing engine; > 25 15:07:52.848 INFO FeatureManager - Using codec VCFCodec to read file file://ref_nobackup/af-only-gnomad.hg38.vcf.gz; > 26 15:07:53.126 INFO Mutect2 - Done initializing engine; > 27 15:07:53.196 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 28 15:07:53.201 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 29 15:07:53.223 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 30 15:07:53.223 INFO IntelPairHmm - Available threads: 2; > 31 15:07:53.224 INFO IntelPairHmm - Requested threads: 4; > 32 15:07:53.224 WARN IntelPairHmm - Using 2 available threads, but 4 were requested; > 33 15:07:53.224 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; > 34 15:07:53.231 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; > 35 15:07:53.314 INFO ProgressMeter - Starting traversal; > 36 15:07:53.314 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; > 37 15:07:54.410 INFO VectorLoglessPairHMM - Time spent in setup for",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632:3105,Load,Loading,3105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632,1,['Load'],['Loading']
Performance,"E_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:40:45.792 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:40:45.792 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:40:45.792 INFO HaplotypeCaller - Inflater: IntelInflater; 14:40:45.792 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:40:45.792 INFO HaplotypeCaller - Requester pays: disabled; 14:40:45.792 INFO HaplotypeCaller - Initializing engine; 14:40:47.694 INFO IntervalArgumentCollection - Processing 50818468 bp from intervals; 14:40:47.714 INFO HaplotypeCaller - Done initializing engine; 14:40:47.826 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:40:47.864 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:40:47.868 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:40:47.921 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:40:47.922 INFO IntelPairHmm - Available threads: 1; 14:40:47.922 INFO IntelPairHmm - Requested threads: 4; 14:40:47.922 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:40:47.922 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:40:48.005 INFO ProgressMeter - Starting traversal; 14:40:48.006 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:40:51.792 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes; 14:40:58.312 INFO ProgressMeter - chr22:10659064 0.2 35790 208384.3; 14:41:09.992 INFO ProgressMeter - chr22:10687910 0.4 35990 98217.0. ```. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7076:8386,Load,Loading,8386,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076,2,"['Load', 'multi-thread']","['Loading', 'multi-threaded']"
Performance,"E_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 03:15:02.171 INFO HaplotypeCaller - Deflater: IntelDeflater; 03:15:02.171 INFO HaplotypeCaller - Inflater: IntelInflater; 03:15:02.171 INFO HaplotypeCaller - GCS max retries/reopens: 20; 03:15:02.171 INFO HaplotypeCaller - Requester pays: disabled; 03:15:02.171 INFO HaplotypeCaller - Initializing engine; 03:15:02.438 INFO FeatureManager - Using codec VCFCodec to read file dbsnp.vcf.gz; 03:15:02.563 INFO FeatureManager - Using codec BEDCodec to read file targets.bed; 03:15:02.578 INFO IntervalArgumentCollection - Processing <redacted> bp from intervals; 03:15:02.588 INFO HaplotypeCaller - Done initializing engine; 03:15:02.590 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 03:15:02.593 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/conda/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 03:15:02.598 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/conda/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 03:15:02.599 INFO IntelSmithWaterman - Using CPU-supported AVX-512 instructions; 03:15:02.599 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 03:15:02.601 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 03:15:02.601 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 03:15:02.623 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:conda/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 03:15:02.667 INFO IntelPairHmm - Using CPU-su",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6889:5315,Load,Loading,5315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6889,1,['Load'],['Loading']
Performance,E_FOR_TRIBBLE : false; 11:47:51.057 INFO Mutect2 - Deflater: IntelDeflater; 11:47:51.057 INFO Mutect2 - Inflater: IntelInflater; 11:47:51.057 INFO Mutect2 - GCS max retries/reopens: 20; 11:47:51.057 INFO Mutect2 - Requester pays: disabled; 11:47:51.057 INFO Mutect2 - Initializing engine; 11:47:51.372 INFO FeatureManager - Using codec VCFCodec to read file file:///home/proj/stage/cancer/reference/GRCh37/variants/dbsnp_grch37_b138.vcf.gz; 11:47:51.457 INFO FeatureManager - Using codec BEDCodec to read file file:///home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed; 11:47:51.465 INFO IntervalArgumentCollection - Processing 74592 bp from intervals; 11:47:51.474 INFO Mutect2 - Done initializing engine; 11:47:51.487 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:47:51.489 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:47:51.534 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 11:47:51.534 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:47:51.534 INFO IntelPairHmm - Available threads: 16; 11:47:51.534 INFO IntelPairHmm - Requested threads: 4; 11:47:51.534 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:47:51.557 INFO ProgressMeter - Starting traversal; 11:47:51.557 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:47:52.683 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 11:47:52.683 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 11:47:52.683 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.24 sec; 11:47:52.684 INFO Mutect,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482:3853,Load,Loading,3853,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482,1,['Load'],['Loading']
Performance,"Each scatter of ModelSegments will run as before, aside from skipping the segmentation step in favor of using the joint segmentation. We will repeat the het-genotyping step, but this is cheap and it's probably better to repeat it to make sure filtering is applied consistently. It would also require more changes to the command line to specify where to output the hets for each sample during multisample segmentation and to skip genotyping in each scatter, if we were to go that route. There are many possible combinations of inputs that need to be tested, but the same is already true of the current ModelSegments. Furthermore, there are slight wrinkles when running in tumor-only mode (i.e., when `--normal-allelic-counts` are not available). Because each sample is genotyped indiviudally, each may yield a different set of hets (in contrast to genotyping in matched-normal mode, in which the normal determines the set of hets used in all samples). We will thus have to take the intersection of these hets before performing multisample segmentation. Unfortunately, we will not be able to re-perform this intersection in each scatter, since we will no longer have access to the hets from the other samples. However, we *will* ultimately intersect the hets from each sample with the joint segmentation before modeling, which may be a rough proxy for the intersection of hets from all samples. As always, tumor-only mode may yield suboptimal results in certain scenarios, e.g., high purity CNLOH. I think I'm OK with just documenting these wrinkles, rather than working too hard to iron them out. I think this structure sets us up nicely to accommodate germline tagging/filtering in the near future. We can still pass the Picard interval list containing the joint segmentation to the scatter for the normal, but can instead subsequently pass the *.modelBegin.seg result from the normal to the tumors. This modeled-segment file will have breakpoints identical to those from the joint segmentation (as op",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549:3026,perform,performing,3026,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549,1,['perform'],['performing']
Performance,"Empirical testing has shown that this tool performs best at scale with cloud buffering; disabled. With cloud buffering on and thousands of concurrent GenomicsDBImport tasks,; we do too many simultaneous GCS accesses (since the prefetcher spawns a new thread for each; reader upon a query) and start seeing intermittent failures, even with aggressive retries.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3110:43,perform,performs,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3110,2,"['concurren', 'perform']","['concurrent', 'performs']"
Performance,Enable SmithWaterman AVX optimized code by default,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4356:25,optimiz,optimized,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4356,1,['optimiz'],['optimized']
Performance,Enable multi-threaded AVX PairHMM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1813:7,multi-thread,multi-threaded,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813,1,['multi-thread'],['multi-threaded']
Performance,Ensure that multi-threaded AVX PairHMM falls back to single-threaded mode when OpenMP is not available or is the wrong version,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1819:12,multi-thread,multi-threaded,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1819,1,['multi-thread'],['multi-threaded']
Performance,"Error type 1: . gatk --java-options ""-Xmx32G -XX:ParallelGCThreads=8 -Djava.io.tmpdir=/tmp"" SplitNCigarReads --spark-runner LOCAL -I 10_mkdup/SAMD00025146_mkdup.bam -R Gallus_gallus.GRCg6a.dna.toplevel.fa -L chicken_chr.list -O 11_cigar/SAMD00025146_cigar.bam --create-output-bam-index true --max-reads-in-memory 5000. 00:01:27.347 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dguan/anaconda3/envs/Chicken_GTEx/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 21, 2021 12:01:27 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:01:27.611 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.612 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.1.9.0; 00:01:27.612 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:01:27.612 INFO SplitNCigarReads - Executing as dguan@bigmem8 on Linux v4.15.0-118-generic amd64; 00:01:27.613 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 00:01:27.613 INFO SplitNCigarReads - Start Date/Time: February 21, 2021 at 12:01:27 AM PST; 00:01:27.613 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.613 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.614 INFO SplitNCigarReads - HTSJDK Version: 2.23.0; 00:01:27.614 INFO SplitNCigarReads - Picard Version: 2.23.3; 00:01:27.615 INFO SplitNCigarReads - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 00:01:27.615 INFO SplitNCigarReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:01:27.615 INFO SplitNCigarReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 00:01:27.615 INFO SplitNCigarReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:01:27.615 INFO Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7091:359,Load,Loading,359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091,1,['Load'],['Loading']
Performance,Error: Cannot read from buffer; Error: cannot load book-keeping,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012:46,load,load,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012,1,['load'],['load']
Performance,"Essentially using preclustering of samples (e.g., by performing clustering on a subset of the coverage bins) to automate building of multiple PoNs when appropriate. Proof-of-principle was implemented for germline (#5633) in https://github.com/broadinstitute/gatk-evaluation/tree/sl_mega_wdl, but it hasn't been merged yet nor do we have plans to put it in production or heavy use anytime soon---sticking with manual preclustering when needed, for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5634#issuecomment-525798707:53,perform,performing,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5634#issuecomment-525798707,1,['perform'],['performing']
Performance,"EstimateLibraryComplexity` do not match, even though those from `GATK MarkDuplicatesSpark` are the same as `Picard MarkDuplicatesWithMateCigar`. #### Expected behavior; I'd expect that the metrics from `GATK MarkDuplicatesSpark` and `GATK EstimateLibraryComplexity` would be the same, since [here](https://gatk.broadinstitute.org/hc/en-us/articles/360050814112-MarkDuplicatesSpark) recommends to run `GATK MarkDuplicatesSpark` without metrics (it is faster) and run `GATK EstimateLibraryComplexity` afterwards. #### Actual behavior. EstimateLibraryComplexity ; ```; ## htsjdk.samtools.metrics.StringHeader; # EstimateLibraryComplexity INPUT=[temp/align/bwa_aln/c_lib1_L001.sorted.bam] OUTPUT=stats/align/estimate_library_complexity/c_lib1.metrics.txt MIN_IDENTICAL_BASES=5 MAX_DIFF_RATE=0.03 MIN_MEAN_QUALITY=20 MAX_GROUP_RATIO=500 MAX_READ_LENGTH=0 MIN_GROUP_COUNT=2 READ_NAME_REGEX=<optimized capture of last three ':' separated fields as numeric values> OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 MAX_OPTICAL_DUPLICATE_SET_SIZE=300000 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=2279706 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false; ## htsjdk.samtools.metrics.StringHeader; # Started on: Wed Mar 24 21:31:32 CET 2021. ## METRICS CLASS picard.sam.DuplicationMetrics; LIBRARY UNPAIRED_READS_EXAMINED READ_PAIRS_EXAMINED SECONDARY_OR_SUPPLEMENTARY_RDS UNMAPPED_READS UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES READ_PAIR_OPTICAL_DUPLICATES PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; Unknown 0 9951 0 0 0 0 0 0. ## HISTOGRAM java.lang.Integer; duplication_group_count Unknown; 1 9951; ```. MarkDuplicatesSpark; ```; ## htsjdk.samtools.metrics.StringHeader; # MarkDuplicatesSpark --output temp/align/markduplicates/c_lib1.bam --metrics-file stats/align/markduplicates/c_lib1.metrics.txt --input temp/align/bwa_aln/c_lib1_L001.sorted.bam --read-validation-stringency LENIEN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7161:1139,optimiz,optimized,1139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7161,1,['optimiz'],['optimized']
Performance,Evaluate ReferenceConfidenceModel.calcNIndelInformativeReads() optimizations,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5488:63,optimiz,optimizations,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5488,1,['optimiz'],['optimizations']
Performance,"Evaluation of THCA/STAD/LUAD TCGA WGS/WES CR concordance with SNP arrays was implemented on FC last summer and showed good performance. For WES, comparisons against GATK CNV and CODEX showed comparable to highly improved performance, respectively, with minimal parameter tuning. WGS comparisons were unavailable due to limitations of competing tools. This evaluation will be expanded to include CR/MAF concordance against PanCanAtlas ABSOLUTE results. Some curation of the samples could be performed; some batch effects were observed in some LC WGS LUAD samples. Comparisons to other tools will probably be removed for ease of maintenance. Will be adapted to fit into whatever framework arises from #4630; same goes for HCC1143 and CRSP validations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-459833697:123,perform,performance,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-459833697,3,['perform'],"['performance', 'performed']"
Performance,"Even if we had default methods, `Locatable` should be simple (like `Comparable`) and shouldn't be polluted with every possible operation you might want to perform on an interval.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/305#issuecomment-79198026:155,perform,perform,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/305#issuecomment-79198026,1,['perform'],['perform']
Performance,"Even without any `-L` intervals, running `CountReads` on the cram is 3x slower than running it on the bam, so the performance problems are not confined to the interval-parsing code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1787#issuecomment-215521825:114,perform,performance,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1787#issuecomment-215521825,1,['perform'],['performance']
Performance,"Example codes are include to demonstrate the two ways variant contexts can be read to Spark RDD from GenomicsDB. The code takes a loader JSON file, a query JSON file and a hosts file as input.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2181:130,load,loader,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2181,1,['load'],['loader']
Performance,"Excellent, thank you @davidbenjamin and @ddrichel!. 1. Since it has been three weeks, are there any news on whether the regenerated PoN works, and where from it is available?; 2. Also, since there are likely many people (some of which have spoken up in the thread) who have been using the affected releases throughout the years, possibly affecting patients' diagnoses, does the Broad have a communication plan to notify Mutect2 users of this serious performance degradation in the specific cases you outlined?; 3. Last, are there plans to incorporate fully functional benchmark regression tests such as @ddrichel's into the Mutect2 (and later Mutect3) CI pipeline, or at least into the build process for each version that is released to the public?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1225309315:450,perform,performance,450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1225309315,1,['perform'],['performance']
Performance,"Executive summary: . My main concern is that the amount of unsupported code is continuing to grow. Adding this PR would bring the total to about ~5k lines of WDL, Java, and test code. In comparison, the amount of corresponding supported CNV code clocks in around ~33k---this includes all of gCNV, as well! Development time has also been non-negligible and dates back to pre-4.0 release. Another concern is that the number of users of this unsupported code is also growing. In fact, it seems like we are actively pointing users to it. This seems unsustainable going forward. Finally, I don't think we have satisfactorily demonstrated which of the functions accomplished by this code (format conversion, post-hoc blacklisting, germline/""CNLOH"" tagging and imputation) are necessary or cannot be performed by existing code or more streamlined and principled methods. (Some of these functions, such as IGV conversion, are already performed by existing code.) Of those functions, I think format conversion is the only one we should retain from this code in an unsupported fashion. So if this PR introduces a useful GISTIC conversion, no harm in merging that. This all sounds like a decision for the new tech lead! @mwalker174 any thoughts? . More detailed responses follow:. > Users are already using this branch and giving me positive feedback (definitely more positive than adjusting num_changepoints_penalty_factor). I suggest merging mostly for practical reasons. It buys us more time to put in a principled solution. And this workflow is clearly marked as an unsupported prototype anyway (as are the GATK CLIs). I want to emphasize that this whole workflow is not a long-term solution. In other words, I would like to get this in and then focus on a supported solution. While it's great that users are giving positive feedback, I refer you to CellBender team's manifesto at https://github.com/broadinstitute/CellBender/commit/28f02f8dbd716aff922bb8da1e56da29347b245b. Can these users help us definitiv",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199:793,perform,performed,793,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199,2,['perform'],['performed']
Performance,Executor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:602); ... 8 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:94); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSyst,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:2064,concurren,concurrent,2064,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance,Explore whether the native PairHMM could be custom-tuned or modified to benefit Mutect2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2562:51,tune,tuned,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2562,1,['tune'],['tuned']
Performance,"Extends BwaMemImageSingleton into a cache, BwaMemImageCache, that can",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3359:36,cache,cache,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3359,1,['cache'],['cache']
Performance,Extract Performance Improvements,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7686:8,Perform,Performance,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7686,1,['Perform'],['Performance']
Performance,"ExtractCohort supports -XL exclusion and follows intervals, other optimizations",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7181:66,optimiz,optimizations,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7181,1,['optimiz'],['optimizations']
Performance,"ExtractFeatures supports -XL exclusion and follows intervals, other optimizations",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7184:68,optimiz,optimizations,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7184,1,['optimiz'],['optimizations']
Performance,"F-248_Backup/before.recal.FMF-248.table --after-report-file /home/detagen/Desktop/pipeline/playground//BACKUP/FMF-248_Backup/after.recal.FMF-248.table --plots-report-file /home/detagen/Desktop/pipeline/playground//NECESSARY/FMF-248/AnalyzeCovariates.FMF-248.pdf; Using GATK jar /home/detagen/Desktop/programlar/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/detagen/Desktop/programlar/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar AnalyzeCovariates --before-report-file /home/detagen/Desktop/pipeline/playground//BACKUP/FMF-248_Backup/before.recal.FMF-248.table --after-report-file /home/detagen/Desktop/pipeline/playground//BACKUP/FMF-248_Backup/after.recal.FMF-248.table --plots-report-file /home/detagen/Desktop/pipeline/playground//NECESSARY/FMF-248/AnalyzeCovariates.FMF-248.pdf; 12:57:16.643 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/detagen/Desktop/programlar/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Dec 17, 2020 12:57:16 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:57:16.774 INFO AnalyzeCovariates - ------------------------------------------------------------; 12:57:16.775 INFO AnalyzeCovariates - The Genome Analysis Toolkit (GATK) v4.1.8.1; 12:57:16.775 INFO AnalyzeCovariates - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:57:16.775 INFO AnalyzeCovariates - Executing as detagen@detagen on Linux v5.4.0-58-generic amd64; 12:57:16.775 INFO AnalyzeCovariates - Java runtime: OpenJDK 64-Bit Server VM v11.0.9.1+1-Ubuntu-0ubuntu1.18.04; 12:57:16.775 INFO AnalyzeCovariates - Start Date/Time: December 17, 2020 at 12:57:16 PM TRT; 12:57:16.775 INFO AnalyzeCovariates - -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7006:1144,Load,Loading,1144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7006,1,['Load'],['Loading']
Performance,FO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:57:50.897 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:57:50.897 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:57:50.897 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:57:50.897 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:57:50.897 INFO GenomicsDBImport - Inflater: IntelInflater; 11:57:50.897 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:57:50.898 INFO GenomicsDBImport - Requester pays: disabled; 11:57:50.898 INFO GenomicsDBImport - Initializing engine; 11:57:52.588 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/isilon/experiment/resources/final_mm10_exon.bed; 11:57:53.791 INFO IntervalArgumentCollection - Processing 199895151 bp from intervals; 11:57:53.825 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. It is recommended that intervals be aggregated together.; 11:57:53.837 INFO GenomicsDBImport - Done initializing engine; 11:57:54.037 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/isilon/experiment/18075-01/aligned/variant_calling/genomics_db/vidmap.json; 11:57:54.037 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/isilon/experiment/18075-01/aligned/variant_calling/genomics_db/callset.json; 11:57:54.037 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/isilon/experiment/18075-01/aligned/variant_calling/genomics_db/vcfheader.vcf; 11:57:54.037 INFO GenomicsDBImport - Importing to array - /mnt/isilon/experiment/18075-01/aligned/variant_calling/genomics_db/genomicsdb_array; 11:57:54.038 INFO ProgressMeter - Starting traversal; 11:57:54.038 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 12:05:27.926 INFO GenomicsDBImport - Starting batch inpu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453590820:3182,perform,performance,3182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453590820,1,['perform'],['performance']
Performance,"FO HaplotypeCaller - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:50:16.819 INFO HaplotypeCaller - Initializing engine; 14:50:18.950 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 14:50:18.965 INFO HaplotypeCaller - Done initializing engine; 14:50:19.021 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:50:19.280 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.481 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.776 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:50:19.795 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:50:19.847 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:50:19.848 INFO IntelPairHmm - Available threads: 48; 14:50:19.848 INFO IntelPairHmm - Requested threads: 4; 14:50:19.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:50:19.926 INFO ProgressMeter - Starting traversal; 14:50:19.926 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:50:30.309 INFO ProgressMeter - chr17:740224 0.2 3010 17395.5; 14:50:41.016 INFO ProgressMeter - chr17:1675683 0.4 7020 19973.4; 14:50:51.041 INFO ProgressMeter - chr17:2415218 0.5 10100 19477.4; 14:51:01.041 INFO ProgressMeter - chr17:3591332 0.7 14920 21773.6; 14:51:11.059 INFO ProgressMeter - chr17:4574538 0.9 19100 22412.6; 14:51:21.089 INFO ProgressMeter - chr17:5",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678:6822,Load,Loading,6822,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678,1,['Load'],['Loading']
Performance,"FYI @sooheelee I pointed you at the docs recently, but realized they're slightly out of date. CreatePanelOfNormals currently expects proportional coverage, upon which it initially performs a series of filtering steps. The docs state that these steps are performed on integer counts, which is incorrect. The fact that filtering yields different post-filter coverage for the two types of input ultimately results in slightly different denoising. Not a big deal, but we should decide what the actual method should be doing and clarify in the code/docs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-314529956:180,perform,performs,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-314529956,2,['perform'],"['performed', 'performs']"
Performance,"Fair points. I agree that legacy tools/versions that are part of a canonical or relatively widely used pipeline should have good documentation. However, there are many of the CNV tools that are basically prototypes---they have never been part of a pipeline, have no tutorial materials, and the chances that any external users have actually used them are probably extremely low. The sooner they are deprecated, the less the overall burden on both comms and methods---I don't think comms should need to feel protective of code or tools that developers are willing to scrap wholesale!. I'd like to cordon off or hide such tools so the program group doesn't get too cluttered---if we can do this in a way that doesn't require @cmnbroad to add more categories, that would be great. For example, we will have 5 tools that one might reasonably try to use for segmentation (PerformSegmentation, ModelSegments, PerformAlleleFractionSegmentation, PerformCopyRatioSegmentation, and PerformJointSegmentation). The first two are part of the legacy and new pipelines, respectively, but the last 3 were experimental prototypes. I think it's definitely confusing to have these 3 presented in the program group, and treating them the same as the other tools in terms of documentation is just extra work for everyone. In any case, I definitely think an additional program group to separate the legacy and new tools is warranted, since many of the updated tools in the new pipeline have very similar names to the legacy tools. If this is OK with everyone, I'll just add a ""LegacyCopyNumber"" program group, which I don't think should require extra work on anyone else's part.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346187604:866,Perform,PerformSegmentation,866,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346187604,4,['Perform'],"['PerformAlleleFractionSegmentation', 'PerformCopyRatioSegmentation', 'PerformJointSegmentation', 'PerformSegmentation']"
Performance,"FeatureDataSource was not correctly keeping track of the interval covered; by the contents of its cache, and as a result was producing many more; cache misses than it should have.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/942:98,cache,cache,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/942,2,['cache'],['cache']
Performance,FeatureDataSource.java:326); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.initializeDrivingVariants(VariantLocusWalker.java:76); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:67); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:709); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.onStartup(VariantLocusWalker.java:63); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.io.IOException: GenomicsDB JNI Error: VariantQueryProcessorException : Could not open array 3$1$121351753 at workspace: /data1/EquCab/GenomicsDB/ECA3_GenomicsDB_260/3; TileDB error message : [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading MBR failed; 	at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.generateHeadersForQuery(GenomicsDBFeatureReader.java:200); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.<init>(GenomicsDBFeatureReader.java:85); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:407); 	... 12 more,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:8446,load,load,8446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,2,['load'],['load']
Performance,FeatureDataSource: output cache hit rate to logger.debug() upon close,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1109:26,cache,cache,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1109,1,['cache'],['cache']
Performance,"FeatureManager dynamically discovers all `FeatureInput` arguments in a tool by accepting a (pre-populated) CommandLineProgram object, which it then passes to static methods in Barclay. Those methods perform the same `@Argument`/`@ArgumentCollection` discovery already implemented by the parser, but using separate, out-of-date code paths that currently don't discover `@PositionalArguments` or plugin descriptor arguments. Rather than fixing the redundant code paths and static methods in Barclay, they can be eliminated and replaced with an instance method on the parser. Since FeatureManager already requires that the parser have been run on the tool, the parser already has the state necessary to just collect the results. However, this means that FeatureManager would require a CommandLineParser object instead of the tool itself. (Alternatively, we could extract the results from the parser and pass them in directly to FeatureManager instead of the parser). @droazen do you have any opinion on this before I refactor this part of the parser ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4480:199,perform,perform,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4480,1,['perform'],['perform']
Performance,"FileChannelImpl.write(FileChannelImpl.java:280); at java.base/java.nio.channels.Channels.writeFullyImpl(Channels.java:74); at java.base/java.nio.channels.Channels.writeFully(Channels.java:97); at java.base/java.nio.channels.Channels.access$000(Channels.java:62); at java.base/java.nio.channels.Channels$1.write(Channels.java:172); at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81); at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:220). Error type2:. gatk --java-options ""-Xmx32G -XX:ParallelGCThreads=8 -Djava.io.tmpdir=/group/zhougrp2/dguan/tmp"" SplitNCigarReads --spark-runner LOCAL -I 10_mkdup/SAMN05828173_mkdup.bam -R /group/zhougrp2/dguan/00_ref/Gallus_gallus.GRCg6a.dna.toplevel.fa -L /group/zhougrp2/dguan/00_ref/chicken_chr.list -O 11_cigar/SAMN05828173_cigar.bam --create-output-bam-index true --max-reads-in-memory 5000. 00:01:27.003 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dguan/anaconda3/envs/Chicken_GTEx/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 21, 2021 12:01:27 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:01:27.275 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.276 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.1.9.0; 00:01:27.276 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:01:27.276 INFO SplitNCigarReads - Executing as dguan@bigmem6 on Linux v4.15.0-122-generic amd64; 00:01:27.276 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 00:01:27.277 INFO SplitNCigarReads - Start Date/Time: February 21, 2021 at 12:01:26 AM PST; 00:01:27.277 INFO SplitNCigarReads - ---------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7091:62279,Load,Loading,62279,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091,1,['Load'],['Loading']
Performance,Filed #2331 to add prefetching. As expected this has a big positive impact on performance.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-270988561:78,perform,performance,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-270988561,1,['perform'],['performance']
Performance,"Finally ready to close for two further reasons:. * After David R taught me how to use the profiler I was surprised to see that assembly in M2 is twice as expensive as pairHMM, so post-assembly optimizations aren't so valuable.; * Any issue of spurious or wasteful haplotypes could be addressed more elegantly using a string graph for assembly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2945#issuecomment-381082611:193,optimiz,optimizations,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945#issuecomment-381082611,1,['optimiz'],['optimizations']
Performance,Find the source of the 5% HaplotypeCaller performance regression in the past year,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8327:42,perform,performance,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8327,1,['perform'],['performance']
Performance,"Fine but this is clearly premature optimization. How about a class called Intervals or intervalutils for this sort of random; utility ?. On Thursday, February 18, 2016, droazen notifications@github.com wrote:. > @akiezun https://github.com/akiezun What will actually happen is that; > someone will need that functionality months from now, forget that it; > already exists (embedded in some random tool), and re-implement it. It; > should be moved back now before this is allowed to happen.; > ; > ; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185886728. ## . Sent from Gmail Mobile",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185888065:35,optimiz,optimization,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185888065,1,['optimiz'],['optimization']
Performance,"Finished, [2D CNN inference](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/CNNScoreVariants.java) and training merged, many new issues spawned.:; - [X] A cool(?) name CNNScoreVariants; - [x] Model training script (in Python, eventually in Java); - [x] Pretrained model for WGS; - [x] Pretrained model for WEx (still being validated and was only trained on NA12878); - [x] Model inference and VCF annotation (in Java); - [x] Solution for applying filters based on CNN score cutoff (tranches.py script); - [x] Currently just re-filtering. Still no joint calling solution...; - [x] Hyperparameters optimized for small 2d model similar performance but .25 params.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-377623706:663,optimiz,optimized,663,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-377623706,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"Finishing up a tieout using old MalariaGEN Pf7 CNV genotyping runs, and I think things look good from this perspective, at least. This tieout uses a subset of the Pf7 samples containing 300 cohort and 1683 case samples (which were indeed treated as a cohort-case cluster in the original Pf7 CNV genotyping analysis). ~4k genomic bins are covered. We compare this branch against 4.5.0.0, as well as this branch against itself (checking for reproducibility). Costs for this branch ($10.92) and 4.5.0.0 ($10.96) were quite comparable. Note that a small portion of these costs derives from Pf7-specific genotyping steps, which I did not bother to remove from the workflow. Runtime for the ploidy modeling and postprocessing steps were comparable. Interestingly, **runtime for the gCNV was ~20-25% longer with this branch than with 4.5.0.0, but memory usage fell by a factor of ~3 (~6GB to ~2GB)!** I am not sure if we could recoup the runtime with some more tweaking of the environment (perhaps double checking that optimized BLAS/MKL/etc. packages are properly used, changing environment variables/flags, etc.), but I think the decrease in memory usage is quite nice. Concordance was checked for the following quantities (4.5.0.0 is on the x-axis and this branch is on the y-axis in all plots below):. 1) Variational posterior means (`mu_*`) and standard deviations (`std_*`) for all analogous variables in the ploidy and gCNV models. There were some slight changes to the gCNV model in this branch (e.g., the functional form of the ARD prior was changed), which means some variables are no longer directly comparable. Furthermore, some variables (such as the bias factors W) are degenerate and cannot be immediately compared. Otherwise, there is good concordance between the remaining variables, e.g.:. ![image](https://github.com/broadinstitute/gatk/assets/11076296/614cf501-ca31-4199-badb-3194b7f78154); ![image](https://github.com/broadinstitute/gatk/assets/11076296/f615084d-d0bf-44e9-bcf5-98abd26ce",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268:1012,optimiz,optimized,1012,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268,1,['optimiz'],['optimized']
Performance,"First comment, I would like to see what happens when there are 2 concurrent failing commits being run by travis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6247#issuecomment-550468443:65,concurren,concurrent,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6247#issuecomment-550468443,1,['concurren'],['concurrent']
Performance,"First, question: what must we do to convince ourselves to pull the plug on the old QUAL? In every test so far, the new model performs equally well on biallelics and better on multiallelics. We will eliminate about 6000 lines of code, many future bugs, and GATK workshop slides that nobody wants to present. Assuming that we are soon convinced, the ticket is pretty simple: delete every implementation of `AFCalculator` and replace anything that calls for an abstract `AFCalculator` with the concrete class `AlleleFrequencyCalculator`. Then clean up hacky parts of `AlleleFrequencyCalculator` that were put in to implement `AFCalculator`. Finally, delete all the ancillary classes like `StateTracker` that comprised the `AFCalculator`'s military-industrial complex.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2255:125,perform,performs,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255,1,['perform'],['performs']
Performance,Fix AnalyzeCovariates R resource loading.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2645:33,load,loading,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2645,1,['load'],['loading']
Performance,"Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7917:4,Race Condition,Race Condition,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7917,1,['Race Condition'],['Race Condition']
Performance,Fix RefAPISource test and improve performance,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/825:34,perform,performance,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/825,1,['perform'],['performance']
Performance,Fix VariantAnnotator performance bug,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6672:21,perform,performance,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6672,1,['perform'],['performance']
Performance,Fix a performance regression in SelectVariants,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6729:6,perform,performance,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6729,1,['perform'],['performance']
Performance,Fix bug when disabling optimizations #7123,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7125:23,optimiz,optimizations,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7125,1,['optimiz'],['optimizations']
Performance,Fix maven release and perform manual release of 4.0.9.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5212:22,perform,perform,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5212,1,['perform'],['perform']
Performance,Fix performance bug in the FeatureDataSource caching code,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/942:4,perform,performance,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/942,1,['perform'],['performance']
Performance,Fix performance issues in Hadoop-BAM with large interval lists,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2313:4,perform,performance,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2313,1,['perform'],['performance']
Performance,Fix performance issues in interval subsetting in Spark and large interval lists,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1532:4,perform,performance,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1532,1,['perform'],['performance']
Performance,Fix performance issues with VariantFiltration and cluster options,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1151:4,perform,performance,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1151,1,['perform'],['performance']
Performance,Fix performance regression in BaseRecalibratorSparkOptimized,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1006:4,perform,performance,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1006,1,['perform'],['performance']
Performance,Fix regression in ReadSparkSource: null out SAMFileHeaders when loading reads,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1110:64,load,loading,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1110,1,['load'],['loading']
Performance,Fix up FQ and race condition issues with volatile tasks work [VS-478],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7888:14,race condition,race condition,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7888,1,['race condition'],['race condition']
Performance,Fixed an optimization in KBestHaplotypeFinder that caused some bestPaths to be lost while still preserving some of the optimization.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5952:9,optimiz,optimization,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5952,2,['optimiz'],['optimization']
Performance,Fixed bugs and simplified AlleleLikelihoods evidence-to-index cache,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6593:62,cache,cache,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6593,1,['cache'],['cache']
Performance,Fixed concurrent modification exception for local runs of HapltoypeCallerSpark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6741:6,concurren,concurrent,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6741,1,['concurren'],['concurrent']
Performance,Fixes #4581. Had to change a configuration method to allow Funcotator tests to work. Currently funcotator will not use the config cache - it will always; create a new configuration on startup. This should be fine for now.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4960:130,cache,cache,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960,1,['cache'],['cache']
Performance,Fixes for spanning deletion performance,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6816:28,perform,performance,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6816,1,['perform'],['performance']
Performance,"Fixes https://github.com/broadinstitute/gatk/issues/2865. To match GATK without sacrificing performance for existing GenotypeGVCF modes, this adds a new traversal type `VariantWalkerGroupedByLocus` which can process variants one-at-a-time, or optionally grouped by Locus. GenotypeGVCFs itself then has to further filter the variants to emulate the ""prioritize start location"" behavior in GATK3. If we keep this traversal, I'll add some unit tests, and we should find a better name. In order to match GATK3 output for sites where the only alt allele is a spanning deletion, I had to use `OutputMode.EMIT_ALL_SITES` when emitting nonvariant sites only (GATK3 uses `OutputMode.EMIT_ALL_CONFIDENT_SITES` in that case). Specifically, its necessary to to prevent GenotypingEngine from entering [this](https://github.com/broadinstitute/gatk/blob/33af6de5fb990f4642245b7a80b76d251371ef09/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java#L285) recently added code block, and short circuiting out. @ldgauthier Any opinion on whether this is too permissive ? Another option would be to special-case this, perhaps by adding another OutputMode state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5219:92,perform,performance,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5219,1,['perform'],['performance']
Performance,"Fixes https://github.com/broadinstitute/gatk/issues/4768. The ReservoirDownsampler currently declares reads to be finalized immediately after they're submitted, but in order to guaranty that every read has equal probability of being discarded, it should consume the entire stream of input items before declaring any item to be finalized. When used with a ReadsDownsamplingIterator, no reads are ever downsampled because the iterator populates it's internal cache by eagerly consuming finalized reads as soon as they become available. Also, PositionalDownsampler doesn't reset it's internal ReservoirDownsampler's state correctly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5594:457,cache,cache,457,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5594,1,['cache'],['cache']
Performance,"Fixes https://github.com/broadinstitute/gatk/issues/5065. This test verifies that we receive output written to Python's stderr, but it fails occasionally due to an inherent race condition in the assert (occasionally the assert executes before the data is received and the test fails). I added a Python statement that explicitly flushes stderr first, and ran the test 1000 times and it still failed once for the same reason. Since I don't see any way to have a reliable test condition (that doesn't involve polling inside a loop and a long timeout), I'm just removing the test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5097:173,race condition,race condition,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5097,1,['race condition'],['race condition']
Performance,Fixes issue #816 and improves performance. Marking those tests as mandatory since they pass now.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/821:30,perform,performance,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/821,2,['perform'],['performance']
Performance,Fixing LoadData error discovered in delcho run,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8556:7,Load,LoadData,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8556,1,['Load'],['LoadData']
Performance,FlowFeatureMapper optimization,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8957:18,optimiz,optimization,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8957,1,['optimiz'],['optimization']
Performance,"For @davidadamsphd to review, please. If @jean-philippe-martin could also review the modifications to the dataflow BQSR, it would be much appreciated (keeping in mind that we still need to rebase on top of the optimized BQSR branch).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142118507:210,optimiz,optimized,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142118507,1,['optimiz'],['optimized']
Performance,For @lbergelson. The general plan for HaplotypeCaller work in Q2 is:. -I will work on validation/testing of the walker version; -Louis will take the Spark version; -Adam will work on further HC performance optimizations (potentially for both walker and spark),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1639#issuecomment-202447009:194,perform,performance,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1639#issuecomment-202447009,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"For Mark Duplicates, the timings after the latest Spark improvements (like https://github.com/broadinstitute/gatk/pull/1190) are; - Walker: 18s; - Spark: 36s (sharded BAM, all cores), 55s (sharded BAM, 1 core), 93s (single BAM, all cores), 105s (single BAM, 1 core). I'm going to close this now since the runner itself is OK (and as I said before is not going to match hand tuned implementations typically). We can make further improvements to the different algorithms, e.g. in https://github.com/broadinstitute/gatk/issues/1100.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1047#issuecomment-159577490:374,tune,tuned,374,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1047#issuecomment-159577490,1,['tune'],['tuned']
Performance,For TAG team after release. This will mostly involve checking that cost-optimizations don't break. Should add corresponding WDL tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3940:72,optimiz,optimizations,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3940,1,['optimiz'],['optimizations']
Performance,"For evaluations, we don't look at calls outside the high confidence regions. For production we deliver calls over the whole genome (except for Y in WGS, which is another story). Even for CHM, I wouldn't optimize outside the high confidence regions. For example, seg dupes are excluded from the high confidence regions. There likely will be calls there, but there's no guarantee that even a real variant should map to that copy of the segment. Of course if either @davidbenjamin or @fleharty has a differing opinion I'm open to discussion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714493769:203,optimiz,optimize,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714493769,1,['optimiz'],['optimize']
Performance,For improving performance by rapidly joining local sites we could do something like:. PCollection<Event> -> key event by which 10kb window of the genome it falls into ; keyed events -> Combine.PerKey into interval trees; combined keyed events -> drop keys; Combine.globally the interval trees into a single interval tree,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/469#issuecomment-97176875:14,perform,performance,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/469#issuecomment-97176875,1,['perform'],['performance']
Performance,"For multi-allelic variants, funcotator should cache all the annotations that are not allele-dependent. This will give a minor speed improvement because it will not have to mechanically create the new annotations from scratch for each allele.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4906:46,cache,cache,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4906,1,['cache'],['cache']
Performance,"For production we use a different execution engine (Zamboni, not Queue). The number of scatters is a static part of the workflow and Zamboni has the responsibility of splitting the interval list per scatter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270436261:65,Queue,Queue,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270436261,1,['Queue'],['Queue']
Performance,"For record keeping, as the comments and replies may be buried in the many commits. ------------; ### On the problem of too many splits of RDD and performance concerns. Initial comment by @cwhelan :; > I'm starting to really not like this approach of splitting up the RDD into lots of smaller RDDs for later processing. It seems inefficient to me: it launches tons of different Spark stages each of which has a bunch of overhead. Perhaps not in this PR, but I think it would be better to classify the contigs on the fly and dispatch them to the right processing methods in a single pass over the RDD. Reply by @SHuang-Broad. > I tried to fix it in this PR, but that seems to be a big task,; and probably is impossible to achieve in a single pass,; because currently each class of contig ends up producing a different type of object; (3 general classes: simple -> SimpleNovelAdjacency, complex -> ComplexVariantCanonicalRepresentation, and unknown -> SAM records of the contigs); and a groupBy() operation is necessary in the middle using these objects as keys; due to the fact that different contigs may produce the same variant; So what I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:146,perform,performance,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030,1,['perform'],['performance']
Performance,"For reference, plots of memory usage for a 50x10000 shard:. ![gcnv-10k-mem](https://user-images.githubusercontent.com/11076296/53906618-6be46100-4019-11e9-8d70-5971fd5d2e35.png). It appears that the 0.9.0 leak only occurs during denoising + sampling. Not sure if differences in runtime are indicative of OpenBLAS vs. MKL performance or within the noise.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5764#issuecomment-470236552:321,perform,performance,321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5764#issuecomment-470236552,1,['perform'],['performance']
Performance,"For some performance comparisons with a small dataset:. Smith-Waterman runtime without AVX: `256.88`s; Smith-Waterman runtime with AVX: `34.09`s (`7.5`x faster ). It's not surprising, but it is a huge difference.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8485#issuecomment-1684147845:9,perform,performance,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8485#issuecomment-1684147845,1,['perform'],['performance']
Performance,"For some reason, this user is getting an error in 4.0.5.1 when this was supposed to have been fixed in GATK3. ----; User Report; ----; I'm trying to get a set of robust variants to use to recalibrate quality scores. I called variants using gatk4, and then tried to perform VariantFiltration:. gatk-4.0.5.1/gatk VariantFiltration -R data/genome.fasta -V variants/6753_12-15-2015_first_pass_filtered.vcf -filter 'QD > 2 && FS > 60 && SOR < 3 && MQ > 40 && MQRankSum > -3 && ReadPosRankSum > -4' -output variants/6753_12-15-2015_second_pass_filtered.vcf -filter-name ""default"" . However, it complains with a java.lang.NumberFormatException:. Using GATK jar gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar VariantFiltration -R data/genome.fasta -V variants/6753_12-15-2015_first_pass_filtered.vcf -filter QD > 2 && FS > 60 && SOR < 3 && MQ > 40 && MQRankSum > -3 && ReadPosRankSum > -4 -output variants/6753_12-15-2015_second_pass_filtered.vcf -filter-name default; 15:42:33.964 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; 15:42:34.114 INFO VariantFiltration - ------------------------------------------------------------; 15:42:34.115 INFO VariantFiltration - The Genome Analysis Toolkit (GATK) v4.0.5.1; 15:42:34.115 INFO VariantFiltration - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:42:34.115 INFO VariantFiltration - Executing as sherlock@DN52ehae.SUNet on Mac OS X v10.13.5 x86_64; 15:42:34.116 INFO VariantFiltration - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_91-b14; 15:42:34.116 INFO VariantFiltration - Start Date/Time: June 15, 2018 3:42:33 PM PDT; 15:42:34.116 INFO VariantFiltration - ----------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4921:265,perform,perform,265,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4921,1,['perform'],['perform']
Performance,"For the SV project, we would like to be able to change BWA's alignment parameters to tune them for the problem of aligning assembled contigs to the reference. In particular, we'd like to be able to set these parameters:. `-B9 -O16 -E1`. This will require adding some way to pass parameters to BWA instance creation to jBWA.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1942:85,tune,tune,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1942,1,['tune'],['tune']
Performance,"For the following variants: . `chr1 819955 . TTCACGAATT T . PASS .`; `chr1 819979 . CATGAGCAT C . PASS .`. _VEP_ seems to produce **incorrect** data: ; ```; ##fileformat=VCFv4.1; ##VEP=""v94"" time=""2018-10-30 19:13:50"" cache=""/nfs/public/release/ensweb-data/latest/tools/grch37/e94/vep/cache/homo_sapiens/94_GRCh37"" db=""homo_sapiens_core_94_37@hh-mysql-ens-grch37-web"" 1000genomes=""phase3"" COSMIC=""81"" ClinVar=""201706"" ESP=""20141103"" HGMD-PUBLIC=""20164"" assembly=""GRCh37.p13"" dbSNP=""150"" gencode=""GENCODE 19"" genebuild=""2011-04"" gnomAD=""170228"" polyphen=""2.2.2"" regbuild=""1.0"" sift=""sift5.2.2""; ##INFO=<ID=CSQ,Number=.,Type=String,Description=""Consequence annotations from Ensembl VEP. Format: Allele|Consequence|IMPACT|SYMBOL|Gene|Feature_type|Feature|BIOTYPE|EXON|INTRON|HGVSc|HGVSp|cDNA_position|CDS_position|Protein_position|Amino_acids|Codons|Existing_variation|DISTANCE|STRAND|FLAGS|SYMBOL_SOURCE|HGNC_ID|TSL|APPRIS|ENSP|SIFT|PolyPhen|AF|AFR_AF|AMR_AF|EAS_AF|EUR_AF|SAS_AF|AA_AF|EA_AF|gnomAD_AF|gnomAD_AFR_AF|gnomAD_AMR_AF|gnomAD_ASJ_AF|gnomAD_EAS_AF|gnomAD_FIN_AF|gnomAD_NFE_AF|gnomAD_OTH_AF|gnomAD_SAS_AF|CLIN_SIG|SOMATIC|PHENO|PUBMED|MOTIF_NAME|MOTIF_POS|HIGH_INF_POS|MOTIF_SCORE_CHANGE"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO; chr1	819955	.	TTCACGAATT	T	.	PASS	CSQ=-|splice_acceptor_variant&coding_sequence_variant&intron_variant|HIGH|AL645608.2|ENSG00000269308|Transcript|ENST00000594233|protein_coding|3/3|2/2|||?-38|?-38|?-13|||||1||Clone_based_ensembl_gene||||ENSP00000470877|||||||||||||||||||||||||||; chr1	819979	.	CATGAGCAT	C	.	PASS	CSQ=-|coding_sequence_variant&3_prime_UTR_variant|MODIFIER|AL645608.2|ENSG00000269308|Transcript|ENST00000594233|protein_coding|3/3||||54-?|54-?|18-?|||||1||Clone_based_ensembl_gene||||ENSP00000470877|||||||||||||||||||||||||||; ```. There is no cDNA string / amino acid change and the positions have `?` characters in them (not 100% sure the question marks are wrong - there is no real spec for these fields). _Oncotator_ seems to produce **incorre",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4307#issuecomment-434440224:218,cache,cache,218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4307#issuecomment-434440224,2,['cache'],['cache']
Performance,For the new GATK framework the multi-thread support is through Spark (see https://github.com/broadinstitute/gatk/issues/2345 for more details).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4448#issuecomment-367984760:31,multi-thread,multi-thread,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4448#issuecomment-367984760,1,['multi-thread'],['multi-thread']
Performance,"For this ticket, we need to modify `ReferenceUtils.loadFastaDictionary()` to throw a `UserException` if the header returned from its `SAMTextHeaderCodec` contains no sequence dictionary",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2609#issuecomment-305017587:51,load,loadFastaDictionary,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2609#issuecomment-305017587,1,['load'],['loadFastaDictionary']
Performance,"For those who might be interested, I finally found a good set up to run gcnvkernel outside a conda environment. Modules loaded: GATK/4.2.4.0 and python/3.8.2. In my python virtualenv:. pip install --no-index --upgrade pip; pip install --no-index --ignore-installed numpy==1.21.0; pip install --no-index scipy==1.2.0; pip install pymc3==3.1; pip install Theano==1.0.4; pip install --no-index tqdm==4.19.5; pip install --no-index PyVCF==0.6.8. git clone https://github.com/broadinstitute/gatk.git; cd gatk/src/main/python/org/broadinstitute/hellbender; python setup_gcnvkernel.py install; python setup.py install",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387#issuecomment-1613468762:120,load,loaded,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387#issuecomment-1613468762,1,['load'],['loaded']
Performance,For various reasons that backport would be too awful to contemplate even if we wanted to backport. The question remains for GATK4 whether we want to preserve the old qual in case there's some edge case where it performs better. I consider this highly unlikely but others might disagree.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258167008:211,perform,performs,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258167008,1,['perform'],['performs']
Performance,"For what it's worth, if we go forward with NIO then we'll need the splitting index and then we should be able to offer this functionality for little additional effort (since we'd be writing a partitioner anyways to work with the splitting index). ; The naive (least-work) approach would read each partition independently so we wouldn't save on reads (but we'd still save from not having to go through a shuffle).; An optimized approach would be to read & parse the overlapping reads only once (since parsing is currently a non-trivial expense), but that'd be more work.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-250231487:417,optimiz,optimized,417,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-250231487,1,['optimiz'],['optimized']
Performance,"From an analysis by @jean-philippe-martin:. **The Problem**. Doing a preliminary performance analysis of Hellbender, I found that ReadBAM did not scale with the number of workers. ![image](https://cloud.githubusercontent.com/assets/798637/8913030/757d9552-3464-11e5-8244-46931fed8383.png). Logs indicated it was running on a single worker, regardless of how many were specified for the job. **The Cause**. The underlying cause is a combination of ReadBAM's design and Dataflow's own perhaps over-eager optimization. ReadBAM is implemented as a series of transforms. It could also have been implemented as a Dataflow BoundedSource, but the latter is much more complicated. The transforms are as follows:; Start with a collection of filenames and a collection of contigs.; Transform 1 -- input: filenames, side input: contigs. Generates a list of regions to read (""BAMShard""); Transform 2 -- input: `PCollection<BAMShard>`, output: `PCollection<Read>`. Each worker reads from the BAM file, using the index to find where to read from. Dataflow sees that transform 2 takes as input transform 1's output, and so these two can be run in sequence on the same machines, skipping a serialization/deserialization step. This optimization is called ""fusing"" and it's generally a very good thing. However in this case, the input PCollection has a single element (the file we want to read), so only one worker is involved. Because of the fusion, that same worker then ends up doing all of the reading work, ruining our day. **The Solutions**. There are multiple ways to solve this problem. ; 1. change transform 1 to have the contig collection as a primary input in the hope that we always have more than one contig. ; This solution's very brittle (our benchmark, for example, reads a single chromosome so the contig list has effectively only one element). I did not pursue it.; 2. Insert a groupby step between the two transforms.; pro: this gets all the workers involved again; con: the groupby itself takes some ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/756:81,perform,performance,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/756,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"From ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/ the file `gencode/hg38/gencode.config` makes reference to `gencode.v27.pc_transcripts.fasta` . ```; # Required field for GENCODE files.; # Path to the FASTA file from which to load the sequences for GENCODE transcripts:; gencode_fasta_path = gencode.v27.pc_transcripts.fasta; ```. which is not present in the directory. . ```; $ ls; gencode.config; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf.idx; gencode.v27.transcripts.dict; gencode.v27.transcripts.fasta; gencode.v27.transcripts.fasta.fai; ```. Downloading the pc_transcript.fasta from gencode does not fix the errors thrown when trying to use the reference. The error message is a fairly uninformative null pointer exception. . ```; 00:27:35.745 INFO Funcotator - Done initializing engine; 00:27:35.758 INFO Funcotator - Shutting down engine; [March 12, 2018 12:27:35 AM UTC] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2258108416; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:330); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:897); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4521:249,load,load,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521,1,['load'],['load']
Performance,From the stack trace I had assumed the error was likely related to loading the dbsnp vcf. From: gs://broad-references/hg38/v0/. I have attached a 1Mbase subset of my gvcf and a script that reliably reproduces the error for me. [script.txt](https://github.com/broadinstitute/gatk/files/2217511/script.txt); [NA12878.hg38.g.vcf.gz](https://github.com/broadinstitute/gatk/files/2217512/NA12878.hg38.g.vcf.gz),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406906206:67,load,loading,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406906206,1,['load'],['loading']
Performance,"From what I have heard from Dmitriy, this loss in Mutect2 precision between Mutect2 v.4.1.8.1. and v.4.1.9.0 seems to be perfectly reproducible using default parameters on the HCC1395 somatic benchmark gold-standard from the [Sequencing Quality Control Phase II Consortium](https://pubmed.ncbi.nlm.nih.gov/?term=Somatic+Mutation+Working+Group+of+Sequencing+Quality+Control+Phase+II+Consortium%5BCorporate+Author%5D). The drop in performance seems to still persist in the current Mutect2 version. If true, then this could have dramatic affects on Mutect2 clinical variant calling quality since v.4.1.9.0 until now. It would appear that isolating the root cause of this drop in performance has high importance for maintaining the clinical validity of Mutect2. It seems quite a number of Mutect2 code details have [changed between v.4.1.8.1. and v.4.1.9.0](https://github.com/broadinstitute/gatk/compare/4.1.8.1...4.1.9.0), as for instance [this commit concerning quality filtering](https://github.com/broadinstitute/gatk/commit/a304725a60f5000ec6381040137043a557fc3dc1), or [this one concerning soft-clipped bases](https://github.com/broadinstitute/gatk/commit/4982c2fa60e89f699a81150116d058aeac2f7573). Perhaps the circumstance of the drop in precision affecting WES but not WGS data may point to the culprit?. Also, may I ask whether the GATK team is doing real-world regression test on gold-standard variant callsets between version releases, and have these tests passed between v.4.1.8.1. and v.4.1.9.0?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1169735509:429,perform,performance,429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1169735509,2,['perform'],['performance']
Performance,Fun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1560:2716,concurren,concurrent,2716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560,2,['concurren'],['concurrent']
Performance,Funcotator - Need Performance Upgrades,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4586:18,Perform,Performance,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4586,1,['Perform'],['Performance']
Performance,Funcotator fails to perform annotation when supplied with minimal VCF of canonical cancer variants,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5777:20,perform,perform,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5777,1,['perform'],['perform']
Performance,"Funcotator needs a way to read in a locatable XSV but also have it match alleles before adding an annotation (in the same way that VCF data sources require matching alleles). This would allow transformation of large VCF data sources (such as gnomAD) into more easily compressed and read XSV files. . After implementation this needs to be benchmarked for gnomAD 2.1 Alelle Frequency VCFs and if the XSVs are faster, the VCFs should be transformed into XSV files and uploaded to the Funcotator data source bucket. The current speed of gnomAD annotation on the canonical hg19 file is ~2500 variants / minute after the first couple of minutes. Full stats (as pulled from the `ProgressMeter` output):; MIN: 1674.8; MAX: 2996.8; MEAN: 2459.4; MEDIAN: 2344.3; STDEV: 217.057; NUM DATA POINTS: 573. For the same file, Oncotator performs at ~12000 variants/minute with all the same data sources except gnomAD. . Even with gnomAD there is no reason Funcotator should be slower than Oncotator.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5605:820,perform,performs,820,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5605,1,['perform'],['performs']
Performance,"Funcotator needs to be able to handle variants in the upstream and downstream flanks of a gene. Right now because we walk over variants that match genes in Gencode, there will be no gene matches for upstream and downstream variants. . To do this we will need to update our features to match +/- the maximum of the padding for each en(5' or 3' padding). This will also affect IGR processing. We will need to update the caching scheme in `FeatureCache` to cache around a locus rather than just in front of it (in a configurable manner). 5' should default to 5000 and 3' should default to zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4771:454,cache,cache,454,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4771,1,['cache'],['cache']
Performance,Funcotator should cache annotations for multi-allelic variants that are not allele-dependent,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4906:18,cache,cache,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4906,1,['cache'],['cache']
Performance,GATK - 4.0.0.0 [BaseRecalibratorSpark low performance],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4300:42,perform,performance,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4300,1,['perform'],['performance']
Performance,"GATK4 has a number of options that presumably alter performance in different ways under different conditions, including and probably not limited to: Intel Deflater/Inflater, snappy, and HTSJDK's various USE_ASYNC_XXXXX_READ params. I can appreciate there is probably not a one-size fits all answer, but would it be possible to provide some type of general guidance on what's available, and when one or the other might be worth evaluating? Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3648:52,perform,performance,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3648,1,['perform'],['performance']
Performance,"GCNV: AssertionError: Loaded mean for ""log_mean_bias_t"" has an unexpected shape; loaded: (11903,), expected: (11901,)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8740:22,Load,Loaded,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740,2,"['Load', 'load']","['Loaded', 'loaded']"
Performance,"GCS max retries/reopens: 20; >; > 16:17:05.844 INFO HaplotypeCaller - Requester pays: disabled; >; > 16:17:05.845 INFO HaplotypeCaller - Initializing engine; >; > 16:17:05.928 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater; >; > 16:17:05.932 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater; >; > 16:17:06.503 INFO FeatureManager - Using codec VCFCodec to read file file:///home/robert/test/snps.vcf; >; > 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals; >; > 16:17:06.551 INFO HaplotypeCaller - Done initializing engine; >; > 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:5178,load,load,5178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,GJlbmRlci91dGlscy9HQVRLUHJvdGVjdGVkTWF0aFV0aWxzLmphdmE=) | `69.531% <> (-13.802%)` | `51 <0> (-8)` | |; | [...stitute/hellbender/utils/icg/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pY2cvRHVwbGljYWJsZU51bWJlci5qYXZh) | `80% <> ()` | `5 <0> (?)` | |; | [...bender/tools/exome/NormalizeSomaticReadCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9Ob3JtYWxpemVTb21hdGljUmVhZENvdW50cy5qYXZh) | `79.167% <> ()` | `6 <0> ()` | :arrow_down: |; | [...institute/hellbender/tools/exome/CallSegments.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9DYWxsU2VnbWVudHMuamF2YQ==) | `83.333% <> ()` | `3 <0> ()` | :arrow_down: |; | [...broadinstitute/hellbender/utils/icg/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pY2cvQ2FjaGVOb2RlLmphdmE=) | `80.645% <> ()` | `9 <0> (?)` | |; | [...ntationbiasvariantfilter/OrientationBiasUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9vcmllbnRhdGlvbmJpYXN2YXJpYW50ZmlsdGVyL09yaWVudGF0aW9uQmlhc1V0aWxzLmphdmE=) | `84.956% <> ()` | `56 <0> ()` | :arrow_down: |; | [...rg/broadinstitute/hellbender/utils/io/IOUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pby9JT1V0aWxzLmphdmE=) | `59.686% <> ()` | `49 <0> ()` | :arrow_down: |; | [...tute/hellbender/tools/exome/TargetTableReader.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3903#issuecomment-348528911:2713,Cache,CacheNode,2713,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3903#issuecomment-348528911,1,['Cache'],['CacheNode']
Performance,GKL v0.5.2 is known to hang / fail to load on CentOS - should upgrade to 0.5.3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3389:38,load,load,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3389,1,['load'],['load']
Performance,"GatherBAMFiles and some other tools use BamFileIoUtils.gatherWithBlockCopying to do block transfer of BAM file blocks, but degenerate to decoding individual records for CRAM. A similar optimization should be able to be done for CRAM containers and then integrated with those tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1163:185,optimiz,optimization,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1163,1,['optimiz'],['optimization']
Performance,GatherVcfsCloud.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); 	at org.broadinstitute.hellbender.Main.main(Main.java:239); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: 403 Forbidden; 443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to broad-jg-dev-11k-call-set/JointGenotyping/0cb36821-b8bf-4e6d-a352-07b101f6b7d1/call-ApplyRecalibration/shard-1734/GMKF_Seidman_CHD_WGS_904.filtered.1734.vcf.gz.; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 10 more; Caused by: com.google.cloud.storage.StorageException: 403 Forbidden; 443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to broad-jg-dev-11k-call-set/JointGenotyping/0cb36821-b8bf-4e6d-a352-07b101f6b7d1/call-ApplyRecalibration/shard-1734/GMKF_Seidman_CHD_WGS_904.filtered.1734.vcf.gz.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:526); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3735:2888,concurren,concurrent,2888,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3735,1,['concurren'],['concurrent']
Performance,"Gatk performance work, do we have output files included in our data bucket? We need to add those so people can compare.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1913:5,perform,performance,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1913,1,['perform'],['performance']
Performance,"GenomicsDB loads now, and I can run the tests. The MIN_DP issue is happening because the BCF codec creates the VariantContext with an Integer value for the attribute since it has the type descriptor at hand, whereas the VCF codec just creates it as a String (it doesn't consult the header, which I might call a bug). The comparator doesn't compare these correctly, and ironically, its the string that gets converted to a double. There also seem to be other problematic type differences between the VCs created from BCF vs. VCF. I need to do think a bit about how to properly fix this; I'm not sure just making the comparator more tolerant of these differences is a good idea, since it could wind up masking real problems.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294962429:11,load,loads,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294962429,1,['load'],['loads']
Performance,"GenomicsDB sometimes returns a 64 bit type as a result of computations. This type is not supported by BCF2Codec while decoding resulting in NPEs - see #6548 and #6667. The initial reservation against using VCFCodec as the default was performance related, benchmarks show the BCF2Codec to be about 10-15% faster than VCF2Codec, but VCFCodec handles all the types correctly. This PR makes VCFCodec the default and the argument `--genomicsdb-use-vcf-codec` has been replaced by `--genomicsdb-use-bcf-codec`. Also, included in this PR are some argument documentation fixes and one bug fix where a com.google.cloud.storage.StorageException was being thrown if a -V argument pointing to a genomicsdb GCS workspace(e.g. gendb.gs://mybucket/myworkspace) did not end in a slash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6675:234,perform,performance,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6675,1,['perform'],['performance']
Performance,GenomicsDBImport fails on Mac due to library loading issues,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4062:45,load,loading,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4062,1,['load'],['loading']
Performance,"GenomicsDBImport lets users writes variants to GenomicsDB. The inputs are a loader JSON configuration file, callsets JSON file containing sample names and corresponding stream names and a stream JSON file containing files names of the streams. Note: This code uses GenomicsDB v0.4.0. Please check whether Maven central has the updated version first. @kgururaj , please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389:76,load,loader,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389,1,['load'],['loader']
Performance,"GenotypeGVCFs - Deflater: IntelDeflater; 12:31:14.785 INFO GenotypeGVCFs - Inflater: IntelInflater; 12:31:14.785 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 12:31:14.785 INFO GenotypeGVCFs - Requester pays: disabled; 12:31:14.785 INFO GenotypeGVCFs - Initializing engine; 12:31:15.766 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.3-6069e4a; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 12:31:19.634 INFO IntervalArgumentCollection - Processing 3714165 bp from intervals; 12:31:19.665 INFO GenotypeGVCFs - Done initializing engine; 12:31:19.700 INFO FeatureManager - Using codec BEDCodec to read file file:///home/groups/pgpdata/ColonyData/207/@files/sequenceOutputs/mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed; 12:34:40.705 INFO ProgressMeter - Starting traversal; 12:34:40.705 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute. and then at: 15 Feb 2022 12:37:38,923:; [TileDB::StorageBuffer] Error: (gzip_read_buffer) Cannot read to buffer; Mem allocation error errno=12(Cannot allocate memory); [TileDB::StorageBuffer] Error: (read_buffer) Cannot decompress and/or read bytes path=/home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb/1$1$223616942/__9b9a9e96-139c-4105-81ec-ab1455d1f01d140490873108224_1597099702436/__book_keeping.tdb.gz errno=12(Cannot allocate memory); [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading domain size failed.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674:4790,cache,cachedData,4790,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674,2,"['cache', 'load']","['cachedData', 'load']"
Performance,GenotypeGVCFs Poor Performace on Whole Genome Sequencing,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8637:19,Perform,Performace,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8637,1,['Perform'],['Performace']
Performance,Get Hadoop reference loading code into htsjdk,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/831:21,load,loading,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/831,1,['load'],['loading']
Performance,"Getting closer; the GenomicsDB tests all pass locally for me now, but we seem to have a few other issues. All of the GenomicsDB tests are failing on travis when trying to load GenomicsDB, and the failure looks somewhat similar to the stack we got when we had the UnsatisfiedLinkException (manifests as java.lang.NoClassDefFoundError: Could not initialize class com.intel.genomicsdb.GenomicsDBImporter). Not sure if its the same thing or not; as I said I can run this branch fine. Maybe the dylib works, but not the so ?. There is a seemingly unrelated unit test failure in AlleleFrequencyCalculatorUnitTest, which I'll look at. Also, we probably shouldn't have the getGCPTestInputPath() calls in static initializers in GenomicsDBImportIntegrationTest, since if it throws it can fail badly (I don't think thats affecting travis though).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296190917:171,load,load,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296190917,1,['load'],['load']
Performance,"Glad to see we're not the only one's noticing this. We have been using 4.0.11.x for quite a while with success. During a recent revalidation effort, we have been working with 4.2.x versions and missing quite a few calls we consider ""truth"", while making these calls with other variant callers. We have since verified v4.1.8.0 is performing acceptably, so our data supports the issue occurring during the 4.1.9.0 update.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171550473:329,perform,performing,329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171550473,1,['perform'],['performing']
Performance,"Goal was to get WGS coverage collection at 100bp at ~15 cents per sample. Since this is I/O bound (takes ~2 hours to stream or localize a BAM, or about the same to decompress a CRAM), cost reduction can be most easily achieved by reducing the memory requirements and moving down to a cheaper VM. . Memory requirements at 100bp are dominated by manipulations of the list of ~30M intervals. There were a few easy fixes to reduce requirements that did not require changing the collection method (which can be easily modified for future investigations, see #4551):. -removed WellformedReadFilter. See #5233. EDIT: We decided after PR review to retain this filter by default and disable it at the WDL level when Best Practices is released. Leaving the issue open.; -initialized HashMultiSet capacity; -removed unnecessary call to OverlapDetector.getAll; -avoided a redundant defensive copy in SimpleCountCollection; -used per-contig OverlapDetectors, rather than a global one. This brought the cost down to ~9 cents per sample using n1-standard-2's with 7.5GB of memory when collecting on BAMs with NIO. Note that I didn't optimize disk size, which accounts for ~50% of the total cost and is unused when running with NIO, so we are closer to ~5 cents per sample. It is possible that using CRAMs with or without NIO and with or without SSDs might be cheaper. Note that OverlapDetectors may be overkill for our case, since bins are guaranteed to be sorted and non-overlapping and queries are also sorted. We could probably roll something that is O(1) in memory. However, since we are I/O bound, as long as we are satisfied with the current cost, I am willing to sacrifice memory for implementation and maintenance costs, as well as the option to change strategies easily. In any case, @lbergelson found some easy wins in OverlapDetector that may further bring the memory usage down, and will issue a fix in htsjdk soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5715:1118,optimiz,optimize,1118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715,1,['optimiz'],['optimize']
Performance,"Going ahead and making this change so TAG team can start trying out the ModelSegments pipeline. It's possible that we could use NIO for other files (reference, interval lists), but this will not have as much impact as using it for BAMs (and in some cases, decreases performance, see comments in #4806). Closes #4806.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5015:266,perform,performance,266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015,1,['perform'],['performance']
Performance,"Good news. User says this is no longer an issue:. `We found the reason why HaplotypeCaller 4.0.0.0 performed worse than 4.beta.2. We are scattering tools using intervals from a custom BED file. Before, each instance of HaplotypeCaller received a BAM file from ApplyBQSR that was produced using a specific interval, but not the interval itself. This worked for 4.beta.2, but was causing poor performance for 4.0.0.0. We now pass both the BAM and the interval to HaplotypeCaller 4.0.0.0 and it performs just as well as 4.beta.2 with the same amount of memory.`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4169#issuecomment-359944183:99,perform,performed,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4169#issuecomment-359944183,3,['perform'],"['performance', 'performed', 'performs']"
Performance,"Good point @akiezun on the array allocation in KMP. Here's a clean room brute force `lastIndexOf`. Initial testing show identical output from HaplotypeCaller and better performance than KMP. I'll push this later today and include the Smith-Waterman profiling code. ``` java; private int lastIndexOf(final byte[] reference, final byte[] query) {; int N = reference.length;; int M = query.length;. for (int i = N - M; i >= 0; i--) {; int j = 0;; while (j < M && reference[i+j] == query[j]) {; j++;; }; if (j == M) {; return i;; }; }. return -1;; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1677#issuecomment-204418438:169,perform,performance,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1677#issuecomment-204418438,1,['perform'],['performance']
Performance,"Great! And yes, LL will be optimized separately for SNPs and INDELs. How about this for a first workflow to target?. 1) Run ExtractVariantAnnotations on a training set of chromosomes. You can keep training/truth labels as in Best Practices, for now.; 2) Run TrainVariantAnnotationsModel on that. We'll use the truth scores generated here for any sensitivity conversions---i.e., we'll be calibrating scores only to the truth sites that are contained in the training set of chromosomes.; 3) Use the trained model to run a single shard of ScoreVariantAnnotations on a validation set of chromosomes.; 4) Run some variation of the above script on the resulting outputs to determine SNP and INDEL score thresholds for optimizing the corresponding LL scores. We can also add some code to the script to use the truth scores from step 2 to convert these score thresholds into truth-sensitivity thresholds.; 5) Provide these truth-sensitivity thresholds to ScoreVariantAnnotations and use them to hard filter. Evaluate on a test set of chromosomes. If all looks good, we can later move steps 3-4 into the train tool and automate the passing of sensitivities in 5 via outputs in the model directory. This will let us keep the basic interface of ScoreVariantAnnotations the same, but we'll have to add a few basic parameters to TrainVariantAnnotationsModel to control the train/validation split. So I think all this branch is missing is step 5---we'll simply need to add command-line parameters for the SNP/INDEL sensitivity thresholds and then do the hard filtering in the VCF writing method highlighted above. Do you think you can handle implementing that in this branch, and then the rest at the WDL level? I can help with the python script for the LL stuff (or anything else), if needed. Not sure if you got a chance to check out what your collaborators are doing in the methods you're looking to compare against, but it would be good to understand if this basic scheme for train/validation/test splitting can",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084:27,optimiz,optimized,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084,2,['optimiz'],"['optimized', 'optimizing']"
Performance,"H-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/87/5fa0df303dc4f06212547353be621c/BAMS/cluster.aaaaaaacx.bam.list -O TMP/jeter2.vcf.gz Using GATK jar /LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running: ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx5g -Dj; ava.io.tmpdir=TMP -jar /LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar HaplotypeCaller -R /LAB-DATA/BiRD/resources/species/human/cng.fr/hs38me/hs38me_all_chr.fasta --minimum-mapping-quality 10 --sample-ploidy 2 --do-not-run-physical-phasing --alleles TMP/jeter.vcf.gz -L TMP/jeter.vcf.gz -I /SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/87/5fa0df303dc4f06212547353be621c/BAMS/cluster.aaaaaaacx.bam.list -O TMP/jeter2.vcf.gz; 18:15:19.107 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.j; ar!/com/intel/gkl/native/libgkl_compression.so ; 18:15:21.727 INFO HaplotypeCaller - ------------------------------------------------------------ ; 18:15:21.728 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0 ; 18:15:21.728 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:15:21.728 INFO HaplotypeCaller - Executing as lindenbaum-p@bigmem002 on Linux v3.10.0-1160.66.1.el7.x86_64 amd64; 18:15:21.728 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11 ; 18:15:21.728 INFO HaplotypeCaller - Start Date/Time: November 24, 2022 6:15:19 PM CET ; 18:15:21.728 INFO HaplotypeCaller - ------------------------------------------------------------; 18:15:21.728 INFO HaplotypeCaller - ------------------------------------------------------------ ; 18:15:21.728 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8106:1635,Load,Loading,1635,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8106,1,['Load'],['Loading']
Performance,"HI @sooheelee I did try using BWA to index the 2-bit reference, but it doesn't work as well. @lbergelson Do you think the reference loading issue can be fixed soon? like this month?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-251738251:132,load,loading,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-251738251,1,['load'],['loading']
Performance,"HORT -L ../Genom.filtered.interval_list -I 0028-21.hdf5 -I 0045-21.hdf5 -I 0098-18.hdf5 -I 0156-21.hdf5 -I 0429-20.hdf5 -I 0779-18.hdf5 -I 1030-20.hdf5 -I 1098-13.hdf5 -I 1450-20.hdf5 -I 1495-17.hdf5 -I 1575-20.hdf5 -I 1586-18.hdf5 -I 1658-14.hdf5 -I 1974-20.hdf5 -I 2008-20.hdf5 -I 0030-21.hdf5 -I 0058-21.hdf5 -I 0129-20.hdf5 -I 0249-04.hdf5 -I 0614-20.hdf5 -I 0834-19.hdf5 -I 1080-20.hdf5 -I 1331-18.hdf5 -I 1460-18.hdf5 -I 1498-18.hdf5 -I 1576-20.hdf5 -I 1592-20.hdf5 -I 1716-15.hdf5 -I 1985-20.hdf5 -I 2167-20.hdf5 -I 0038-21.hdf5 -I 0094-21.hdf5 -I 0139-18.hdf5 -I 0345-20.hdf5 -I 0641-18.hdf5 -I 0949-20.hdf5 -I 1081-20.hdf5 -I 1416-20.hdf5 -I 1491-20.hdf5 -I 1553-18.hdf5 -I 1577-20.hdf5 -I 1600-20.hdf5 -I 1720-20.hdf5 -I 1995-20.hdf5 --contig-ploidy-calls ../ploidy-calls/ --annotated-intervals ../Genom.annotated.tsv --interval-merging-rule OVERLAPPING_ONLY --output /media/Data/AnnotationDBs/CNV/Genom --output-prefix CNV --tmp-dir /media/Data/tmp/; 17:28:28.660 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/BioinfSoftware/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 27, 2021 5:28:28 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:28:28.779 INFO GermlineCNVCaller - ------------------------------------------------------------; 17:28:28.779 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0; 17:28:28.779 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:28:28.779 INFO GermlineCNVCaller - Executing as root@k-hg-srv3 on Linux v5.3.18-24.37-default amd64; 17:28:28.780 INFO GermlineCNVCaller - Java runtime: OpenJDK 64-Bit Server VM v11.0.8+10-suse-3.45.1-x8664; 17:28:28.780 INFO GermlineCNVCaller - Start Date/Time: April 27, 2021 at 5:28:28 PM CEST; 17:28:28.780 INFO GermlineCNVCaller - --------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7234:2648,Load,Loading,2648,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234,1,['Load'],['Loading']
Performance,HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:11:57.326 INFO FilterAlignmentArtifacts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:11:57.326 INFO FilterAlignmentArtifacts - Deflater: IntelDeflater; 19:11:57.326 INFO FilterAlignmentArtifacts - Inflater: JdkInflater; 19:11:57.326 INFO FilterAlignmentArtifacts - GCS max retries/reopens: 20; 19:11:57.326 INFO FilterAlignmentArtifacts - Requester pays: disabled; 19:11:57.326 WARN FilterAlignmentArtifacts - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:11:57.326 INFO FilterAlignmentArtifacts - Initializing engine; 19:11:57.666 INFO FeatureManager - Using codec VCFCodec to read file file:///output/sample.FilterMutectCalls.vcf.gz; 19:11:57.757 INFO FilterAlignmentArtifacts - Done initializing engine; 19:11:57.827 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 19:11:57.861 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 19:11:57.862 INFO IntelPairHmm - Available threads: 4; 19:11:57.862 INFO IntelPairHmm - Requested threads: 4; 19:11:57.862 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 19:11:57.862 INFO ProgressMeter - Starting traversal; 19:11:57.862 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; *** glibc detected *** /for/bar/bin/java: double free or corruption (out): 0x00007f450af58700 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x3d01675dee)[0x7f45058afdee]; /lib64/libc.so.6(+0x3d01678c80)[0x7f45058b2c80]; /tmp/libgkl_smithwaterman410767516409374085.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f4499f4cfa8]; /tmp/libgkl_smithwaterman410767516409374085.so(Java_com_intel_gkl_smithwaterman,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:3880,Load,Loading,3880,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['Load'],['Loading']
Performance,Hadoop-BAM: add support for efficiently loading a subset of a BAM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1399:40,load,loading,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1399,2,['load'],['loading']
Performance,"Haplotype caller crashing with NULL pointer exception.; gatk version 4.1.4.0; Steps followed:; 1. bwa alignment; 2. Sorting; 3. Mark duplicate; 4. BaseRecalibrator; 5. ApplyBQSR; 6. HaplotypeCaller. Here is the haplotype caller log. Using GATK jar /home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar HaplotypeCaller -R /home/administrator/IGIB/samples/Homo_sapiens_assembly38.fasta -I data_gatk/aligned.sort.dup.rg.recal.bam -O data_gatk/aligned.sort.dup.rg.recal.bam.vcf; 02:07:46.602 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 28, 2019 2:07:51 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 02:07:51.772 INFO HaplotypeCaller - ------------------------------------------------------------; 02:07:51.773 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.4.0; 02:07:51.773 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:07:51.773 INFO HaplotypeCaller - Executing as mvasimud@uarch-compression on Linux v4.4.0-142-generic amd64; 02:07:51.773 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v9-internal+0-2016-04-14-195246.buildd.src; 02:07:51.773 INFO HaplotypeCaller - Start Date/Time: 28 November 2019 at 2:07:46 AM IST; 02:07:51.773 INFO HaplotypeCaller - ------------------------------------------------------------; 02:07:51.773 INFO HaplotypeCaller - ------------------------------------------------------------; 02:07:51.774 INFO HaplotypeCaller - HTSJDK Version: 2.20.3; 02:07:51.774 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6292:775,Load,Loading,775,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6292,1,['Load'],['Loading']
Performance,HaplotypeCaller does not resume from where it stopped. If you need to perform the same task again restart the whole task using the very same commandline.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7454#issuecomment-918232900:70,perform,perform,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454#issuecomment-918232900,1,['perform'],['perform']
Performance,HaplotypeCaller should create optimized indexes for GVCFs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4294:30,optimiz,optimized,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4294,1,['optimiz'],['optimized']
Performance,HaplotypeCaller: major performance improvements and fixes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4031:23,perform,performance,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4031,1,['perform'],['performance']
Performance,"Hello - I was running the exact command with GATK-Mutect2 4.0.0.0 and then switched to 4.0.1.2; an error message was returned:. ```**BETA FEATURE - WORK IN PROGRESS**; USAGE: Mutect2 [arguments]; Call somatic SNVs and indels via local assembly of haplotypes; Version:4.0.1.2; ...; ***********************************************************************; A USER ERROR has occurred: dbsnp is not a recognized option; ***********************************************************************; ```. The exact command works with 4.0.0.0:; ```12:56:44.435 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/group/bioinformatics/software/GATK/4.0.0.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:56:44.750 INFO Mutect2 - ------------------------------------------------------------; 12:56:44.750 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.0.0.0; 12:56:44.750 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:56:44.751 INFO Mutect2 - Executing as rbao@cri16in002 on Linux v2.6.32-573.12.1.el6.x86_64 amd64; 12:56:44.751 INFO Mutect2 - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_92-b14; 12:56:44.751 INFO Mutect2 - Start Date/Time: February 11, 2018 12:56:44 PM CST; 12:56:44.751 INFO Mutect2 - ------------------------------------------------------------; 12:56:44.751 INFO Mutect2 - ------------------------------------------------------------; 12:56:44.751 INFO Mutect2 - HTSJDK Version: 2.13.2; 12:56:44.751 INFO Mutect2 - Picard Version: 2.17.2; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:56:44.752 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:56:44.752 INFO Mutect2 - Deflater: IntelDeflater; 12:56:44.752 INFO Mutect2 - Inflater: IntelInflater; ```. I was ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4390:575,Load,Loading,575,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4390,1,['Load'],['Loading']
Performance,"Hello @bharathramh thank you for your question. We just recently merged the bulk of the code for DRAGEN-GATK (#6634) which means that we are slated to release the DRAGEN-GATK for the next release of GATK. When that happens we will also be releasing an official wdl workflow to use all the new features together. . As for STR variants, one of the code improvements in the new genotyping engine is to better model STR errors by performing a per-sample pre-processing step where a table of empirically generated STR priors is generated by the tools `ComposeSTRTableFile` and `CallibrateDragstrModel`. These work by modifying the priors for genotying to be more in line with the sample PCR/Sequencing errors and can't really be thought of as a feature to ""find STRs"" (except `ComposeSTRTableFile` which is used to find STRs in the reference). These will be featured in the published wdls with appropriate commands. . I would recommend in the future that you direct questions like this one to our forums since this ticket tracker is used to track bugs/ongoing development on the GATK and our Comms Team is better equipped to answer questions there: https://gatk.broadinstitute.org/hc/en-us/community/topics",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6912#issuecomment-716747266:426,perform,performing,426,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6912#issuecomment-716747266,1,['perform'],['performing']
Performance,"Hello @droazen and @nalinigans , we tried out the parameter you suggested and did improve the performance but not as great as compared to writing the data locally and then moving to FSx for luster. Using the parameter, the job completed in ~2 hrs 52 mins, whereas when we run to write on EBS volumes it completes within 1 hr.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039658049:94,perform,performance,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039658049,1,['perform'],['performance']
Performance,"Hello @nalinigans,. As part of gatk-sv pipeline we are using GATK : v4.1.8.1 which doesn't have bypass-feature-reader option. Also, we didnt capture strace for the run with just ""--genomicsdb-shared-posixfs-optimizations"" so wont be able to share the FUTEX process counts. So after using v4.2.4.1 we get below results. 	- Using ""--genomicsdb-shared-posixfs-optimizations"" & ""--bypass-feature-reader"" the process took 118 mins.; ""FUTEX_WAIT_PRIVATE, 0, NULL"" : 1266. 	- Using ""--genomicsdb-shared-posixfs-optimizations"" & ""--bypass-feature-reader"" and ; TILEDB_UPLOAD_BUFFER_SIZE=5242880 as env variable the process took 113 mins.; 	""FUTEX_WAIT_PRIVATE, 0, NULL"" : 3. 	- Even using 10 MB as buffer size resulted in same execution time of 113 mins.; 	- Using a buffer size bigger i.e. 50 MBs caused the process to run slower so we aborted it. Please let us know if we can improve it further.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1040947845:208,optimiz,optimizations,208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1040947845,3,['optimiz'],['optimizations']
Performance,"Hello GATK team!. ## Bug Report. ### Affected tool(s) or class(es). mutect2. ### Affected version(s); - Latest public release version: 4.2.6.1. ### Description . getting fail for all scatter task with the argument ""--emit-ref-confidence GVCF"", no fails without it. error:; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx3000m -jar /root/gatk.jar GetSampleName -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-ce3y1s.hg38.bam -O tumor_name.txt -encode --gcs-project-for-requester-pays broad-firecloud-ccle; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.b3fd1830; 14:13:40.205 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:13:40.275 INFO Mutect2 - ------------------------------------------------------------; 14:13:40.276 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.2.6.1; 14:13:40.277 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:13:40.277 INFO Mutect2 - Executing as root@0b46ce3a6ba5 on Linux v5.10.107+ amd64; 14:13:40.277 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:13:40.278 INFO Mutect2 - Start Date/Time: May 13, 2022 2:13:40 PM GMT; 14:13:40.278 INFO Mutect2 - ------------------------------------------------------------; 14:13:40.278 INFO Mutect2 - ------------------------------------------------------------; 14:13:40.279 INFO Mutect2 - HTSJDK Version: 2.24.1; 14:13:40.280 INFO Mutect2 - Picard Version: 2.27.1; 14:13:40.284 INFO Mutect2 - Built for Spark Version: 2.4.5; 14:13:40.284 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:13:40.284 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:13:40.285 INFO Mutect2 - H",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849:813,Load,Loading,813,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849,1,['Load'],['Loading']
Performance,"Hello GATK team, I'm running the following command but getting the following error. Do you know how to solve it? Thank you very much!. ```; java -Xmx80g -Djava.io.tmpdir=/lustre/home/xyliu/02_tmp -jar /lustre/home/ksun/software/GATK-4.2.3.0/gatk-package-4.2.3.0-local.jar PathSeqBuildKmers --reference pathseq_host.fa --output pathseq_host.hss --bloom-false-positive-probability 0.001 --kmer-mask 16 --kmer-size 31 &; ```. ```; $ 10:49:50.605 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/home/ksun/software/GATK-4.2.3.0/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2023 10:49:50 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:50.812 INFO PathSeqBuildKmers - ------------------------------------------------------------; 10:49:50.813 INFO PathSeqBuildKmers - The Genome Analysis Toolkit (GATK) v4.2.3.0; 10:49:50.813 INFO PathSeqBuildKmers - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:50.813 INFO PathSeqBuildKmers - Executing as xyliu@fat16 on Linux v3.10.0-862.el7.x86_64 amd64; 10:49:50.813 INFO PathSeqBuildKmers - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_161-b14; 10:49:50.813 INFO PathSeqBuildKmers - Start Date/Time: February 15, 2023 10:49:50 AM CST; 10:49:50.813 INFO PathSeqBuildKmers - ------------------------------------------------------------; 10:49:50.814 INFO PathSeqBuildKmers - ------------------------------------------------------------; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Version: 2.24.1; 10:49:50.814 INFO PathSeqBuildKmers - Picard Version: 2.25.4; 10:49:50.814 INFO PathSeqBuildKmers - Built for Spark Version: 2.4.5; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:49:50.814 INFO PathSeqBuildKmer",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8204:470,Load,Loading,470,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8204,1,['Load'],['Loading']
Performance,"Hello there,. ## Documentation request. It would be awesome if each major/minor release of GATK contained benchmarking results run against a truth set. For calling short germline variants, you could evaluate precision and recall using one of the NIST samples (i.e. HG002) and for calling short somatic variants you could use the SEQC2 tumor-normal pair. . I would like to see how new releases of GATK and how your recommendations/best practices impact precision and recall against a known truth set. This transparency would help anyone using your tools decide if it's worth upgrading to a new release of GATK, or if their implementation of your recommended set of best practices is performing as expected. . If this information could be added to the best practices pages, a relevant tutorial, or a new section of the documentation (containing hap.py/som.py benchmarking information and run times for a set of tested tools and/or workflows), that would be great!. Please let me know what you think, and if you have any questions. Best regards,; @skchronicles",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/9019:682,perform,performing,682,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/9019,1,['perform'],['performing']
Performance,"Hello!. When I try running gatk 4.0.7.0 on spark 2.2 Microsoft Azure hdinsight cluster (using data from here https://gatkforums.broadinstitute.org/gatk/discussion/6484/how-to-generate-an-unmapped-bam-from-fastq-or-aligned-bam#optionA) I get ""java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender"" error like the one below. The thing that gatk-package-4.0.7.0-spark.jar has that class as can be verified by ; $jar tvf gatk-package-4.0.7.0-spark.jar; but nonetheless it seems like it does not load it correctly somehow. $java -version; openjdk version ""1.8.0_171""; OpenJDK Runtime Environment (build 1.8.0_171-8u171-b11-0ubuntu0.16.04.1-b11); OpenJDK 64-Bit Server VM (build 25.171-b11, mixed mode). What would be the possible fix for it?. $./gatk PrintReadsSpark -I ../6484_snippet.bam -O ../output.bam -- --spark-runner SPARK --spark-master spark://10.0.0.21:7077; Using GATK jar /root/gatk-4.0.7.0/gatk-package-4.0.7.0-spark.jar; Running:; /usr/hdp/current/spark2-client/bin/spark-submit --master spark://10.0.0.21:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /root/gatk-4.0.7.0/gatk-package-4.0.7.0-spark.jar PrintReadsSpark -I ../6484_snippet.bam -O ../output.bam --spark-master spark://10.0.0.21:7077; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark2/jars/sl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:525,load,load,525,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['load'],['load']
Performance,"Hello, . I made two changes in output files of cnv_somatic_pair_workflow.wdl to fix bugs:; 1. Change ""File"" into ""String"" for all ""entity_id"" outputs. The ""File"" type will make error when Output Copying was used in cromwell, as there is no such file in output;; 2. Change ""File select_first([CNVOncotatorWorkflow.oncotated_called_file, ""null""])"" into ""File? oncotated_called_file_tumor"" for oncotate and funcotate outputs. If oncotate or funcotate was not performed, the original will output ""null"" and make error when Output Copying was used in cromwell, as there is ""null"" file in output;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6735:456,perform,performed,456,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6735,1,['perform'],['performed']
Performance,"Hello, ; I have been using he GenotypeGVCFs function to call variants on roughly 300 whole genome sequenced individuals. I have not run into any issue when calling variants for these same individuals using the majority of chromosomes, however when I use the same script for chromosomes 1, 2 and 3 of the species I get the error ""Couldn't create GenomicsDBFeatureReader"" as in issue #6616 although I believe our issues may differ because I also have the errors ""Cannot read from buffer"" and ""cannot load book-keeping; Reading-tiles offset"". . Below is the computer output: . Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx16g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/1 -O ECA3_GenomicsDB_260.1.g.vcf.gz; 13:56:51.939 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Dec 21, 2020 1:56:52 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:56:52.185 INFO GenotypeGVCFs - ------------------------------------------------------------; 13:56:52.186 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 13:56:52.186 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:56:52.186 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 13:56:52.186 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_275-b01; 13:56:52.186 INFO Gen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012:498,load,load,498,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012,1,['load'],['load']
Performance,"Hello, ; I was trying to create a candidates SNP list for **GATK4 BaseRecalibrator** to recalibrate the alignment for SNP calling. And I met a problem with the indexing of the my vcf file of candidates SNPs. ; The indexing step and recalibrating step ran without any error, but only very small amount of SNPs (~2900) were detected from a genome **~15Gbp** size, which is definitely not correct as compared with other methods when **~million SNPs** were detected. ; I tracked down the problem is at the indexing step for the candidates vcf file (**925751 SNPs, through HaplotypeCaller**). The problem looks like only the **last chromosome** was indexed.; This is my log file in which the Google engine related part was omitted as I did not use it: ; ```; $ cat ${LOGDIR}/index_candidates.log. (09:28:38.902 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/storage/ppl/yifang/download-software/anaconda3/envs/exome/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method). ...... May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused. ...... 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5917:833,Load,Loading,833,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917,1,['Load'],['Loading']
Performance,"Hello, I am getting this error when running **gatk MarkDuplicatesSpark -I file.bam -O spark.bam** :. A USER ERROR has occurred: Failed to load reads from file.bam; Caused by:Unsupported class file major version 55. Any solutions?. Thank you.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6350:138,load,load,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6350,1,['load'],['load']
Performance,"Hello, When I implement ""HaplotypeCaller"" commands, the reference genome is about 15G , every chromosome is more then 600M, I get some errors, could you give me some advice?; the commands; ```; # the step is after marked duplication ; samtools index -c markdup.bam.gz; gatk --java-options ""-Xmx100G -Djava.io.tmpdir=./"" HaplotypeCaller -R Triticum_aestivum.IWGSC.dna.toplevel.fa -I rmarkdup.bam.gz -O SRR9851087.gvcf.gz -ERC GVCF -OVI >3gvcf.log 2>&1; ```; the bug:; ```; Using GATK jar /home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx100G -Djava.io.tmpdir=./ -jar /home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar HaplotypeCaller -R Triticum_aestivum.IWGSC.dna.; toplevel.fa -I rmarkdup.bam.gz -O SRR9851087.gvcf.gz -ERC GVCF -OVI; 09:01:25.845 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:01:25.950 INFO HaplotypeCaller - ------------------------------------------------------------; 09:01:25.950 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0; 09:01:25.950 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:01:25.950 INFO HaplotypeCaller - Executing as ywt@ywt-Precision-5820-Tower on Linux v5.15.0-41-generic amd64; 09:01:25.950 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.5+8-Ubuntu-2ubuntu122.04; 09:01:25.950 INFO HaplotypeCaller - Start Date/Time: February 8, 2023 at 9:01:25 AM CST; 09:01:25.950 INFO HaplotypeCaller - ------------------------------------------------------------; 09:01:25.950 INFO HaplotypeCaller - ------------------------------------------------------------; 09:01:25.951 INFO HaplotypeCaller - HTSJDK V",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8192:996,Load,Loading,996,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192,1,['Load'],['Loading']
Performance,"Hello, more information on the parameters and runtime can be found here: #7492 . the stacktrace is now:; ```; ...; 22:14:59.985 INFO ProgressMeter - chrUn_JTFH01001653v1_decoy:301 116.6 2161460 18530.7; 22:15:11.142 INFO ProgressMeter - chrUn_JTFH01001673v1_decoy:301 116.8 2161540 18501.9; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-02waxZ.hg38.bam -tumor TUHR14TKB --germline-resource gs://depmapomicsdata/gnomad.genomes.r3.0.sites.vcf.bgz -pon gs://depmapomicsdata/1000g_pon.hg38.vcf.gz -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/cec2a1a6-ffc3-4f1b-ba94-27ae918c56e9/Mutect2/b389d86b-8b0b-4d77-8224-a5a3e3a0b4e5/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0004-scattered.interval_list -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --gcs-project-for-requester-pays broad-firecloud-ccle; ln: failed to access '/cromwell_root/*normal-pileups.table': No such file or directory; ln: failed to access '/cromwell_root/*tumor-pileups.table': No such file or directory; 2021/10/05 22:15:24 Starting delocalization.; ...; ```. I run mutect2 in tumor only mode. ; Interestingly, this error always only happen at the last shard only (every other shard runs to completion). Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7494:1004,cache,cacheCopy,1004,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494,1,['cache'],['cacheCopy']
Performance,"Hello,. Could you help me with this? I ran this code:; ```; prg=/home/user1/Programs/gatk-4.5.0.0; log_dir=/home/user1/Programs/logs; java -Xmx64g -XX:ParallelGCThreads=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true \; -jar ${prg}/gatk-package-4.5.0.0-local.jar IndexFeatureFile -I ${dir}/snp_allsamples.vcf.gz \; --output snp_allsamples.vcf.tbi \; 2>${log_dir}/snp_allsamples_gvcf_index.err. ```; and I received the following error message. ```; 09:36:35.254 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/user1/Programs/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:36:35.386 INFO IndexFeatureFile - ------------------------------------------------------------; 09:36:35.389 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.5.0.0; 09:36:35.389 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:36:35.389 INFO IndexFeatureFile - Executing as user1@xxx.xx on Linux v5.4.0-150-generic amd64; 09:36:35.389 INFO IndexFeatureFile - Java runtime: OpenJDK 64-Bit Server VM v17.0.3-internal+0-adhoc..src; 09:36:35.389 INFO IndexFeatureFile - Start Date/Time: March 21, 2024 at 9:36:35 a.m. CST; 09:36:35.390 INFO IndexFeatureFile - ------------------------------------------------------------; 09:36:35.390 INFO IndexFeatureFile - ------------------------------------------------------------; 09:36:35.390 INFO IndexFeatureFile - HTSJDK Version: 4.1.0; 09:36:35.391 INFO IndexFeatureFile - Picard Version: 3.1.1; 09:36:35.391 INFO IndexFeatureFile - Built for Spark Version: 3.5.0; 09:36:35.391 INFO IndexFeatureFile - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:36:35.391 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:36:35.392 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:36:35.392 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:36:35.392 INFO IndexFeature",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8747:483,Load,Loading,483,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8747,1,['Load'],['Loading']
Performance,"Hello,. I am running GATK GenotypeGVCFs v4.2.5.0 using a command similar to the one below. I downloaded the gatk ZIP from github, unzipped, and we run the local JAR (renamed to GenomeAnalysisTK4.jar). I am consistently getting a strange NoClassDefFoundError error (below). I noticed com/google/common/base is relocated in the shadowJar step, and that is the class it's complaining about here. Have you seen an error like this before?. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations; ```. and the exception:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hellbender/relocated/com/google/common/base/Function; 	at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hellbender.relocated.com.google.common.base.Function; 	at java.net.URLClassLoader$1.run(URLClassLoader.java:370); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:362); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:361); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 1 more; Caused by: java.util.zip.ZipException: invalid LOC h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7675:685,cache,cachedData,685,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675,1,['cache'],['cachedData']
Performance,"Hello,. I did encounter the same behaviour that I reported one year ago with the official Docker GATK 4.1.0.0 container converted into a singularity image. ## Version of softwares:. Singularity : 2.5.1, GATK : 4.1.0.0. ### Command. Singularity : ; singularity build gatk-4.0.4.0.img docker://broadinstitute/gatk:4.0.4.0. ### Actual behavior; ```; 14:39:21.762 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:39:24.079 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 14:39:24.079 INFO DetermineGermlineContigPloidy - The Genome Analysis Toolkit (GATK) v4.1.0.0; 14:39:24.080 INFO DetermineGermlineContigPloidy - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:39:24.081 INFO DetermineGermlineContigPloidy - Executing as tintest@dahu38 on Linux v4.9.0-8-amd64 amd64; 14:39:24.081 INFO DetermineGermlineContigPloidy - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 14:39:24.081 INFO DetermineGermlineContigPloidy - Start Date/Time: May 26, 2019 2:39:21 PM UTC; 14:39:24.081 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 14:39:24.081 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 14:39:24.082 INFO DetermineGermlineContigPloidy - HTSJDK Version: 2.18.2; 14:39:24.082 INFO DetermineGermlineContigPloidy - Picard Version: 2.18.25; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:39:24.083 INFO DetermineGermline",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:387,Load,Loading,387,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['Load'],['Loading']
Performance,"Hello,. I think we're pretty close on porting this. However, i wanted to ask again if I could get access to the test data used in GATK3 VariantEvalWalkerUnitTest and VariantEvalIntegrationTest. We dont need to port those files to GATK4 unless some of them already exist; however, having these files would help confirm the ported tool is behaving exactly as GATK3. Second, I am hoping to confirm a behavior for FeaureContext: many GATK3 stratifiers follow roughly this pattern (VariantEval / knownCNVsFile being an example). The pattern is: 1) some VCF/BED file provided as an argument, 2) in initialize(), the walker reads this file in memory into a List<GenomeLoc> or IntervalTree<GenomeLoc>. This is so each locus can quickly find overlapping intervals. In GATK4, there is no longer a point in loading the whole file into memory, right? If I define an argument as FeatureInput<Feature>, it will automatically be initialized. FeatureContext.getValues() seems to give me overlapping intervals (including partial overlaps). Therefore there is no reason to try to replicate that GATK3 pattern of reading intervals into memory. Can someone confirm this is correct?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-343563884:796,load,loading,796,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-343563884,1,['load'],['loading']
Performance,"Hello,. I'm running GATK 4.2.5.0 with a command line this. Note --max-alternate-alleles and --genomicsdb-max-alternate-alleles. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--genomicsdb-max-alternate-alleles 9 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations; ```. I'm getting this NPE after it runs for about 40 minutes, meaning many sites were processed fine:. ```; 18 Feb 2022 11:32:59,897 DEBUG: 	java.lang.NullPointerException; 18 Feb 2022 11:32:59,907 DEBUG: 		at org.broadinstitute.hellbender.utils.GenotypeUtils.computeDiploidGenotypeCounts(GenotypeUtils.java:85); 18 Feb 2022 11:32:59,919 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.ExcessHet.calculateEH(ExcessHet.java:96); 18 Feb 2022 11:32:59,943 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.ExcessHet.annotate(ExcessHet.java:84); 18 Feb 2022 11:32:59,955 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.addInfoAnnotations(VariantAnnotatorEngine.java:355); 18 Feb 2022 11:32:59,971 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:334); 18 Feb 2022 11:32:59,985 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:306); 18 Feb 2022 11:32:59,995 DEBUG: 		at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7687:378,cache,cachedData,378,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7687,2,"['cache', 'optimiz']","['cachedData', 'optimizations']"
Performance,"Hello,. I'm testing gatk4.3 against 3 WES samples. For less time comsuption, I scattered the bed file into 45 small interval list. But an error prompted out in the GermlineCNVCaller step with the err below:. ```; 23:43:52.386 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gpfs/hpc/home/lijc/xiangxud/software/miniconda3/envs/gatk4/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:43:52.401 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/libgkl_compression16416313594950485777.so; 23:43:52.469 INFO GermlineCNVCaller - ------------------------------------------------------------; 23:43:52.469 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0; 23:43:52.469 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:43:52.470 INFO GermlineCNVCaller - Executing as xiangxud@cu22 on Linux v3.10.0-1160.el7.x86_64 amd64; 23:43:52.470 INFO GermlineCNVCaller - Java runtime: OpenJDK 64-Bit Server VM v11.0.8-internal+0-adhoc..src; 23:43:52.470 INFO GermlineCNVCaller - Start Date/Time: August 3, 2024 at 11:43:52 PM CST; 23:43:52.470 INFO GermlineCNVCaller - ------------------------------------------------------------; 23:43:52.470 INFO GermlineCNVCaller - ------------------------------------------------------------; 23:43:52.470 INFO GermlineCNVCaller - HTSJDK Version: 3.0.1; 23:43:52.470 INFO GermlineCNVCaller - Picard Version: 2.27.5; 23:43:52.470 INFO GermlineCNVCaller - Built for Spark Version: 2.4.5; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.BUFFER_SIZE : 131072; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.CREATE_INDEX : false; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.CREATE_MD5 : false; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 23:43:52.471 INFO GermlineCNVCaller - HTSJ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:253,Load,Loading,253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Load'],['Loading']
Performance,"Hello,. I'm trying to use GATK4 (4.1.8.1) on an Ubuntu (16.04) machine. The machine is a ""PowerLinux"" machine and I'm guessing that the most relevant info for the following problem is that it is a ppc64le system. When I use HaplotypeCaller, I see the following messages on the screen:. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100. 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so. 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so. 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine. INFO: Failed to detect whether we are running on Google Compute Engine. 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------. 16:17:05.843 INFO HaplotypeCaller - The Gen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794:649,Load,Loading,649,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794,2,"['Load', 'load']","['Loading', 'load']"
Performance,"Hello,. It seems the parameter `--sequence-dictionary` does not change the dictionary looked by **HaplotypeCaller**. ```; averdier@bioinfo:~/test/dna-seq-pipeline$ ./dna-seq-pipeline.pl -1 CACTTCGA-ACACGACC_S156_L003_R1_001.fastq.gz -2 CACTTCGA-ACACGACC_S156_L003_R2_001.fastq.gz -r Triticum_aestivum_Claire_EIv1.1.fa.gz -s ClaireTest --nb_threads 30; --mem_limit 100; Mapping; Mark Duplicates; Variants Calling; 09:54:54.531 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/gatk/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Sep 11, 2020 9:54:54 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 09:54:54.730 INFO HaplotypeCaller - ------------------------------------------------------------; 09:54:54.731 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.7.0; 09:54:54.731 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:54:54.731 INFO HaplotypeCaller - Executing as averdier@bioinfo on Linux v4.4.0-178-generic amd64; 09:54:54.731 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-8u265-b01-0ubuntu2~18.04-b01; 09:54:54.731 INFO HaplotypeCaller - Start Date/Time: September 11, 2020 9:54:54 AM CEST; 09:54:54.731 INFO HaplotypeCaller - ------------------------------------------------------------; 09:54:54.731 INFO HaplotypeCaller - ------------------------------------------------------------; 09:54:54.732 INFO HaplotypeCaller - HTSJDK Version: 2.21.2; 09:54:54.732 INFO HaplotypeCaller - Picard Version: 2.21.9; 09:54:54.732 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:54:54.732 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:54:54.732 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:54:54.732 INFO HaplotypeCaller - HTSJDK Defaults.USE_A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6808:453,Load,Loading,453,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6808,1,['Load'],['Loading']
Performance,"Hello,. We are working with ~1700 exome samples. The vcf file generated after joint genotyping is around 80 GB (uncompressed). When we are extracting samples from this master file, each extraction (regardless of singleton or trio exome) takes around 40-60 minutes. . We are using gatk 4.1.8.0 for production, and we tried the latest gatk (4.2.5.0) but we saw roughly 10 min decrease in extraction time. We gzipped/indexed our vcf, we still see similar performance. The variants / minute speed is ~50k. Any idea if our extraction rates are slow, or are we performing as expected?. For what its worth, our data is stored on a HDDs in RAID 5.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7671:452,perform,performance,452,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7671,2,['perform'],"['performance', 'performing']"
Performance,"Hello,. We're trying to run GenotypeGVCFs on a large genomicsDB workspace. The command is below, with the output. We run these jobs scatter/gather, with each job getting a define, small interval set. Despite being given a huge amount of RAM (testing >250GB), virtually all of the jobs die without any messages right after the ""Starting traversal' message. A few gave error messages like the one below. . In this example, you'll see it's running with Xmx178g. We added 60G to the cluster memory request to leave buffer for the C layer. We're on v4.2.5.0. Does this error look familiar, and/or do you have any troubleshooting suggestions? Thanks in advance. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations. 12:31:14.647 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2022 12:31:14 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:31:14.783 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.783 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 12:31:14.783 INFO GenotypeGVCFs - For support and documentation go to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674:906,cache,cachedData,906,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674,1,['cache'],['cachedData']
Performance,"Hello,. When using GATK GenomicsDbImport, I understand that both the java layer and C layer use memory and therefore we need to allow a buffer for the genomics DB code. We're getting odd behavior from GenotypeGVCFs memory. We run a command similar to:. ```; java -Djava.io.tmpdir=<path> -Xmx384g -Xms384g -Xss2m \; -jar GenomeAnalysisTK4.jar GenotypeGVCFs \; -R <FASTA> \; --variant gendb:///.....gdb -O <path> \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 --max-alternate-alleles 12 \; --force-output-intervals <BED_FILE> \; -L 3:129881515-132852846 \; --only-output-calls-starting-in-intervals \; --genomicsdb-shared-posixfs-optimizations. ```. We find the job is using ~466GB, not the 384 I'd expect it to be constrained by. Does using a genomics DB as input result in some other non-java code being used? I'm trying to understand why Xmx/Xmx isnt constraining it. . Thanks for any help.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542:646,optimiz,optimizations,646,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542,1,['optimiz'],['optimizations']
Performance,"Hello,; When I use GenomicsDBImport and GenotypeGVCFs , I get the following error: Couldn't create GenomicsDBFeatureReader, I have no problem with running CombineGVCFs with CombineGVCFs. I reference #6616 , but I think we have different errors, And I checked my environment variable, the parameter is displayed as ' declare -x TILEDB_DISABLE_FILE_LOCKING=""1"" '. Hope you can give me some help, thanks in advance. Exact GATK commands used : gatk GenotypeGVCFs -R path/hg38ncbi.fa -V gendb://mydatabase -O rawvariants.vcf. > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home//miniconda3/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar GenotypeGVCFs -R /home//workdir/data_single_cell/sperm/hg38ncbi.fa -V gendb://mydatabase -O rawvariants.vcf; > 21:14:29.330 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/miniconda3/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; > May 25, 2020 9:14:29 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; > INFO: Failed to detect whether we are running on Google Compute Engine.; > 21:14:29.494 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:14:29.495 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.7.0; > 21:14:29.495 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; > 21:14:29.495 INFO GenotypeGVCFs - Executing as lbjiang@mu01 on Linux v3.10.0-327.el7.x86_64 amd64; > 21:14:29.495 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; > 21:14:29.495 INFO GenotypeGVCFs - Start Date/Time: May 25, 2020 9:14:29 PM CST; > 21:14:29.495 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:14:29.495 INFO GenotypeGVCFs - ----------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6627:910,Load,Loading,910,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6627,1,['Load'],['Loading']
Performance,"Hello.; I believe there is an error in the description of the algorithm in the pairHMM alignment stage. It states that it does **local** alignment, however the code implements a **global** alignment.; Therefore I fixed the comments and added the correct reference to Durbin's book. (The figure referenced is not the Finite State Machine implemented). This was discussed on the GATK forum before I made the pull request.; See discussion here : [Discussion Thread](https://gatkforums.broadinstitute.org/gatk/discussion/23197/question-about-the-alignment-performed-in-the-haplotypecaller-pairhmm#latest). I believe this is important, because the difference between the two FSMs is as important as the difference between running the NeedlemanWunsch and Smith-Waterman algorithm (global vs local). Regards.; Rick",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5528:552,perform,performed-in-the-haplotypecaller-pairhmm,552,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5528,1,['perform'],['performed-in-the-haplotypecaller-pairhmm']
Performance,"Here are some initial thoughts, after working on my local machine. It appears that writing a single BAM file is slowed significantly on the sort when the tasks have to spill to disk. So:; - Lots of memory is good. Decreases the chance of spilling.; - The sort itself should use multiple partitions. This should decrease the amount of data going to each reducer/sort task.; - When we end up writing to a single partition, we actually want to induce an additional shuffle. If we perform `.coalesce(1)` without the shuffle, then the `sortByKey` gets pushed into a single reducer task, ensuring lots of spilling and making things slower. It might make sense to modify the API to look more like this:. `ReadsSparkSink.writeReads(..., boolean sortReads, int outputPartitions, int sortPartitions)`. This would enable a flag that decides whether to to a total sort of the data, and if so, let you set the number of sort tasks (perhaps we leave this one off). Finally, the `outputPartitions` parameter lets us set how many partitions to perform on reshuffling. If either are set to 0, they can just use default values. If the output is set to 1 partition, it would write a single file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1015#issuecomment-152355638:477,perform,perform,477,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1015#issuecomment-152355638,2,['perform'],['perform']
Performance,"Here is a recap of what we discussed today during the CNV meeting:. For the first round of evaluations we decided to run Germline CNV pipeline on TCGA exomes using a range of key hyperparameters (namely psi-t-scale and p-alt) and establish the base level performance metrics using output of GenomeSTRiP on matched WGS samples as ground truth. . @mbabadi could you come up with a good range of hyperparameter values that you think should be cross-validated?. In particular we need to:. - Dockerize tools we will be evaluating against (XHMM, CODEX2, CLAMMS, GenomeSTRiP); - Write a WDL that runs Germline CNV that scatters across range of key hyperparameters and outputs array of VCFs; - Write VCF processors for output of CLAMMS and CODEX2 ; - Write WDLs for running XHMM, CODEX2, CLAMMS and GenomeSTRiP that output VCFs; - Write WDL that takes results of the above and uses @mbabadi 's gCNV evaluation python modules(located here /dsde/working/mehrtash/gCNV_theano_eval) to output performance metrics; - Decide on an automatic evaluation framework. For the next round of evaluations we need to:. - Decide on appropriate metrics for evaluating performance on trios and write scripts that implement them; - Expand the range of hyperparameters in search space (possibly include different bin sizes, GC vs no GC correction, and fragment mid point coverage collection vs largest fragment overlap coverage collection); - Use gnomAD subset of matched WES/WGS pairs for validation",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-362075071:255,perform,performance,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-362075071,3,['perform'],['performance']
Performance,"Here is a script I ran to run the import on 3500 unblockedgvcfs. The script imports one chromosomeper workspace. As the chromosomesget larger --more and more memory is needed. chr4 through 22 ran fine. The chr3 (see log below) ends without an error BUT with thecallset.json NOT being written out. I could split the chr1-3 at the centromere to try it again. Any other suggestions? Would increasing -Xmx150g to 240g help? . For chromosome 1, which is still running, top indicates is using about 240g (after importing the 65 batches).; ```; PID USER   PR NI  VIRT  RES  SHR S %CPU %MEM TIME+ COMMAND; 21698 farrell  20  0   443.7g 240.3g  1416 S 86.7 95.5  7398:14 java; ```. ```; #!/bin/bash -l; #$ -l mem_total=251; #$ -P casa; #$ -pe omp 32; #$ -l h_rt=240:00:00; module load miniconda/4.9.2; module load gatk/[4.2.6.1](http://4.2.6.1/); conda activate /share/pkg.7/gatk/[4.2.6.1/install/gatk-4.2.6.1](http://4.2.6.1/install/gatk-4.2.6.1). CHR=$1; DB=""genomicsDB.rb.chr$CHR""; rm -rf $DB; # mkdir -p $DB; # mkdir tmp; echo ""Processing chr$CHR""; echo ""NSLOTS: $NSLOTS""; # head sample_map.chr$CHR.reblock.list; head sample_map.chr$CHR; wc  sample_map.chr$CHR; gatk --java-options ""-Xmx150g -Xms16g"" \;    GenomicsDBImport \;    --sample-name-map sample_map.chr$CHR \;    --genomicsdb-workspace-path $DB \;    --genomicsdb-shared-posixfs-optimizations True\;    --tmp-dir tmp \;    --L chr$CHR\;    --batch-size 50 \;    --bypass-feature-reader\;    --reader-threads 5\;    --merge-input-intervals \;    --overwrite-existing-genomicsdb-workspace\;    --consolidate. ```; End of log on chr3. ```; 07:19:44.855 INFO GenomicsDBImport - Done importing batch 38/65; 08:05:11.651 INFO GenomicsDBImport - Done importing batch 39/65; 08:49:12.112 INFO GenomicsDBImport - Done importing batch 40/65; 09:32:39.526 INFO GenomicsDBImport - Done importing batch 41/65; 10:23:36.849 INFO GenomicsDBImport - Done importing batch 42/65; 1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232:800,load,load,800,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232,2,['load'],['load']
Performance,Here is another one; ```; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Read timed out; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at htsjdk.samtools.seekablestream.SeekableBufferedStream.read(SeekableBufferedStream.java:100); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:539); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:493); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:451); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:441); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:194); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:322); 	at htsjdk.tribble.readers.TabixReader.readIndex(TabixReader.java:215); 	at htsjdk.tribble.readers.TabixReader.readIndex(TabixReader.java:269); 	at htsjdk.tribble.readers.TabixReader.<init>(TabixReader.java:161); 	at htsjdk.tribble.readers.TabixReader.<init>(TabixReader.java:125); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:84); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getF,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:64,concurren,concurrent,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"Here is the command line. I'm using funcotator_dataSources.v1.2.20180329.tar.gz.; /omics/chatchawit/gatk/gatk Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38. Using GATK jar /omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38; 22:56:54.836 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:56:55.064 INFO Funcotator - ------------------------------------------------------------; 22:56:55.065 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.0.0; 22:56:55.065 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:56:55.065 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 22:56:55.066 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 22:56:55.066 INFO Funcotator - Start Date/Time: April 27, 2018 10:56:54 PM ICT; 22:56:55.066 INFO Funcotator - ------------------------------------------------------------; 22:56:55.066 INFO Funcotator - ------------------------------------------------------------; 22:56:55.067 INFO Funcotator - HTSJDK Version: 2.13.2; 22:56:55.067 INFO Funcotator - Picard Version: 2.17.2; 22:56:55.067 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 22:56:55.067 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:859,Load,Loading,859,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157,1,['Load'],['Loading']
Performance,"Here is the command line. ```; ./gatk Funcotator --variant /home/deepak/software_library/gatk-4.1.7.0/SAMPL3_VARIANTFIL.vcf --reference /media/deepak/EXTRA/Genomedir/hg38/hg38.fasta --ref-version hg38 --data-sources-path /media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES --output variants.funcotated.vcf --output-file-format VCF; Using GATK jar /home/deepak/software_library/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/deepak/software_library/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar Funcotator --variant /home/deepak/software_library/gatk-4.1.7.0/SAMPL3_VARIANTFIL.vcf --reference /media/deepak/EXTRA/Genomedir/hg38/hg38.fasta --ref-version hg38 --data-sources-path /media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES --output variants.funcotated.vcf --output-file-format VCF; 16:01:36.165 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/deepak/software_library/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 12, 2020 4:01:36 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:01:36.870 INFO Funcotator - ------------------------------------------------------------; 16:01:36.871 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.7.0; 16:01:36.871 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:01:36.871 INFO Funcotator - Executing as deepak@ngs on Linux v5.3.0-26-generic amd64; 16:01:36.871 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_241-b07; 16:01:36.871 INFO Funcotator - Start Date/Time: 12 May, 2020 4:01:35 PM IST; 16:01:36.871 INFO Funcotator - ------------------------------------------------------------; 16:01:36.871 INFO Funcotator -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036:989,Load,Loading,989,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036,1,['Load'],['Loading']
Performance,"Here is the equivalent output for the master branch:. ```; 2022-08-16T22:45:53.6066834Z Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.help.DocumentationGenerationIntegrationTest > documentationSmokeTest2 [31mFAILED[39m[0K; 2022-08-16T22:45:53.6067689Z java.lang.AssertionError: Loading source files for package javadoc...[0K; 2022-08-16T22:45:53.6068266Z programName: warning - No source files for package javadoc; 2022-08-16T22:45:53.6068997Z Loading source files for package org.broadinstitute.hellbender.tools.walkers.variantutils...; 2022-08-16T22:45:53.6079174Z Constructing Javadoc information...; 2022-08-16T22:45:53.6079564Z [search path for source files: src/main/java]; 2022-08-16T22:45:53.6082788Z [search path for class files: /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/sunrsasign.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jfr.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/classes,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/zipfs.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/dnsns.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunjce_provider.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/icedtea-sound.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/nashorn.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/java-atk-wrapper.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunec.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunpkcs11.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/cldrdata.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/jaccess.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/localedata.jar,/gatk/gatk-package-unspecified-SNAPSHOT-local.jar,/jars/gatk-package-4.2.6.1-50-g40182c7-SNAPSHOT-testDependencies.jar,/jars/gatk-package-4.2.6.1-50-g40182c7-SNAPSH",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:300,Load,Loading,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,2,['Load'],['Loading']
Performance,"Here the error log ; Using GATK jar /share/scientific_bin/gatk/4.1.4.1/gatk-package-4.1.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/scientific_bin/gatk/4.1.4.1/gatk-package-4.1.4.1-local.jar IndexFeatureFile -I output/called/final/allsites.filtered.vcf; 00:57:08.257 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/scientific_bin/gatk/4.1.4.1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Sep 11, 2023 12:57:08 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:57:08.789 INFO IndexFeatureFile - ------------------------------------------------------------; 00:57:08.789 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.1.4.1; 00:57:08.789 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:57:08.790 INFO IndexFeatureFile - Executing as [ychrysostomakis@compute-0-3.local](mailto:ychrysostomakis@compute-0-3.local) on Linux v3.10.0-1160.53.1.el7.x86_64 amd64; 00:57:08.790 INFO IndexFeatureFile - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_231-b11; 00:57:08.790 INFO IndexFeatureFile - Start Date/Time: 11. September 2023 00:57:07 MESZ; 00:57:08.790 INFO IndexFeatureFile - ------------------------------------------------------------; 00:57:08.790 INFO IndexFeatureFile - ------------------------------------------------------------; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Version: 2.21.0; 00:57:08.790 INFO IndexFeatureFile - Picard Version: 2.21.2; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8372#issuecomment-1733069316:447,Load,Loading,447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8372#issuecomment-1733069316,1,['Load'],['Loading']
Performance,"Here's a suggested set of things to look at as part of this ticket:. -See if we can avoid fetching all headers on startup by passing in the needed info via alternate args (https://github.com/broadinstitute/gatk/issues/2639). -Do profiling to find an appropriate value for the --batchSize argument,; once it's merged (https://github.com/broadinstitute/gatk/issues/2641). -Shrink NIO buffers (--cloudPrefetchBuffer and --cloudIndexPrefetchBuffer) down to the smallest values that still produce acceptable performance (https://github.com/broadinstitute/gatk/issues/2640). Thibault of red team aka @Horneth has agreed to take this on.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-298338616:503,perform,performance,503,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-298338616,1,['perform'],['performance']
Performance,"Here's some code you can add to the test to check that jbwa actually works. ```; @Test; public void testBwaMemAlignSingleRead() throws Exception {; final String libraryPath = NativeUtils.runningOnLinux() ? ""/lib/libbwajni.so"" : ""/lib/libbwajni_mac.jnilib"";; Assert.assertTrue(NativeUtils.loadLibraryFromClasspath(libraryPath), ""jbwa library was not loaded. "" +; ""This could be due to a configuration error, or your system might not support it."");. BwaIndex index= new BwaIndex(new File(b37_reference_20_21));; final BwaMem bwaMem = new BwaMem(index);; //real read taken from src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam; final String name=""20FUKAAXX100202:3:46:9213:168594"";; final byte[] seqs= ""GTTTTGTTTACTACAGCTTTGTAGTAAATTTTGAACTCTAAAGTGTTAGTTCTCTAACTTTGTTTGTTTTTCAAGAGTGTTTTGACTCTTCTTACTGCATC"".getBytes(); ;; final byte[] quals= ""DGFDGFDHFFFFGFEFHEGFFFGGHEHFHGHHGEGGGGGFGFHGHGHEHGGGFGAEFGDACAHHDHGCGFGGGFGDHGHFFHDDCGGDGEE"".getBytes();; final ShortRead read = new ShortRead(name, seqs, quals);; final AlnRgn[] align = bwaMem.align(read);; Assert.assertEquals(align.length, 1);; Assert.assertEquals(align[0].getChrom(), ""20"");; Assert.assertEquals(align[0].getCigar(), ""101M"");; Assert.assertEquals(align[0].getMQual(), 60);; Assert.assertEquals(align[0].getPos(), 9999997-1); #note difference from the bam file (9999997 in bam, 9999996 here); Assert.assertEquals(align[0].getNm(), 0);; bwaMem.dispose();; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1847#issuecomment-220755636:288,load,loadLibraryFromClasspath,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847#issuecomment-220755636,2,['load'],"['loadLibraryFromClasspath', 'loaded']"
Performance,Here's the performance benchmarking; sen_del sen_dup ppv_del ppv_dup sen_del_hist sen_dup_hist ppv_del_hist ppv_dup_hist; 1 0.2000000 0.4121864 0.9387755 0.8521127 345 279 98 142; 2 0.4598540 0.6086957 0.9647059 0.8602941 137 184 85 136; 3 0.6352941 0.7181208 0.9565217 0.8650794 85 149 69 126; 4 0.7796610 0.7744361 0.9636364 0.8606557 59 133 55 122; 5 0.8913043 0.8715596 0.9583333 0.8495575 46 109 48 113; 6 0.9024390 0.9019608 0.9750000 0.8545455 41 102 40 110; 7 0.9428571 0.9263158 0.9722222 0.8666667 35 95 36 105; 8 0.9310345 0.9534884 0.9666667 0.8736842 29 86 30 95; 9 0.9285714 0.9518072 1.0000000 0.9213483 28 83 26 89; 10 0.9565217 0.9487179 1.0000000 0.9259259 23 78 23 81. ### NEW gCNV docker; sen_del sen_dup ppv_del ppv_dup sen_del_hist sen_dup_hist ppv_del_hist ppv_dup_hist; 1 0.2034884 0.4107143 0.9387755 0.8581560 344 280 98 141; 2 0.4705882 0.6086957 0.9647059 0.8676471 136 184 85 136; 3 0.6547619 0.7248322 0.9565217 0.8730159 84 149 69 126; 4 0.7931034 0.7819549 0.9636364 0.8677686 58 133 55 121; 5 0.9111111 0.8807339 0.9583333 0.8558559 45 109 48 111; 6 0.9250000 0.9019608 0.9750000 0.8584906 40 102 40 106; 7 0.9705882 0.9263158 0.9722222 0.8712871 34 95 36 101; 8 0.9642857 0.9534884 0.9655172 0.8791209 28 86 29 91; 9 0.9629630 0.9518072 1.0000000 0.9186047 27 83 25 86; 10 1.0000000 0.9487179 1.0000000 0.9113924 22 78 22 79,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2187034329:11,perform,performance,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2187034329,1,['perform'],['performance']
Performance,"Hey @mwalker174 ,. Do you have any suggestions about how to perform step 1? I naively tried to use picard's `MergeBamAlignment` using the PathSeq output BAM as the aligned bam and the PathSeq input BAM as the unmapped BAM but I get the following error message. `IllegalArgumentException: Do not use this function to merge dictionaries with different sequences in them. Sequences must be in the same order as well. Found [NZ_DS990135.1, NZ_AJSY01000035.1, ... `. I tried sorting both BAM files by queryname and removing the alignment for the input BAM using `RevertSam` but neither of these worked. I suspect that it's because of the PathSeq output BAM given the references to the microbial sequences. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6655#issuecomment-644426370:60,perform,perform,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6655#issuecomment-644426370,1,['perform'],['perform']
Performance,"Hey @mwalker174,. Setting `--host-min-identity 20` lowered the `READS_AFTER_PREALIGNED_HOST_FILTER` to 131467, which is closer to what I expected (albeit still 12% of the reads) so thanks for the suggestion. I'm still unsure why the preferred approach is to use this fairly arbitrary metric that doesn't incorporate both mates instead of leveraging the known alignments from STAR, which does. Given what I've learned today, I think the better approach (for my use case) would be to filter the unique mappers, the multi-mappers and the chimeric reads (which in my data set represent 97.5% of the reads) and then apply the QUALITY_AND_COMPLEXITY_FILTER and the DUPLICATE_READS_FILTER. Would you agree?. To put myself in your shoes, I would guess that PathSeq is designed for general purpose use cases (which is probably `--is-host-aligned false`) and that performing such a filtering would require specific handling for each supported aligner, which would be a lot of work. Moreover, my use case with short paired-end reads is also probably not common. So I understand why you use the existing approach but are there any reasons that I'm missing as to why you'd suggest to stick with the basic approach for my use case instead of the one that I proposed above?. Best, Welles",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6687#issuecomment-652524772:854,perform,performing,854,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6687#issuecomment-652524772,1,['perform'],['performing']
Performance,"Hey there~ I have run the following command:. gatk HaplotypeCaller \; -R bbv18h27rm.fa \; -ERC GVCF \; --alleles db_raw_call_bbe_6largest.vcf \; -I bbe_off_xL3_68.concordant_withRG.bam \; -O test_call_off_xL3_68_allele_46samples.vcf.gz. and had some error:; ```; Using GATK jar /home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar HaplotypeCaller -R /db_students1/genetic_map/snp_calling/bbv18h27rm.fa -ERC GVCF --alleles /db_students1/gatk_out/db_raw_call_bbe_6largest.vcf -I /db_students1/genetic_map/reseqData_mapping_bam/bam/bbe_off_xL3_68.concordant_withRG.bam -O /db_students1/gatk_out/test_call_off_xL3_68_allele_46samples.vcf.gz; 17:04:13.440 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 12, 2019 5:04:15 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:04:15.137 INFO HaplotypeCaller - ------------------------------------------------------------; 17:04:15.138 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.3.0-25-g8d88f6e-SNAPSHOT; 17:04:15.138 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:04:15.138 INFO HaplotypeCaller - Executing as cc@hr18b on Linux v3.10.0-957.el7.x86_64 amd64; 17:04:15.138 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_212-b04; 17:04:15.138 INFO HaplotypeCaller - Start Date/Time: November 12, 2019 5:04:13 PM CST; 17:04:15.139 INFO HaplotypeCaller - --------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6260:983,Load,Loading,983,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6260,1,['Load'],['Loading']
Performance,"Hi @Hemantcnaik ,. This is a much better question for the GATK forum. We try to limit github issues to bugs and feature requests, while questions about tool usage should go to the forum where they're triaged by another team. We haven't done much RNA development in a long time, though hopefully that's about to change. In the meantime, a lot of other places in the GATK code we use 20 as a minimum base quality. If you're really concerned about optimizing this value, you could probably come up with an in silico experiment similar to what we do for BQSR. If you assume that every site in your sample that's in dbSNP is a real variant and everything that's not is a false positive (not true, but to a good approximation), then you can look at the base quality distributions for true positives and false positives and try to decide what's a good tradeoff between sensitivity and precision.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6753#issuecomment-678345871:445,optimiz,optimizing,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6753#issuecomment-678345871,1,['optimiz'],['optimizing']
Performance,"Hi @cccnrc, glad you found this discussion interesting and apologies for the very late reply. Unfortunately, there hasn't been much movement on this front, as I think we decided that the current model sufficed. I think we are moving in a direction so that we can obviate the need for a separate step to fit global (e.g., depth) and per-contig (e.g., contig ploidy) parameters for each sample. Recall that this is only necessary because each gCNV genomic shard needs these quantities to perform inference, but cannot infer them from the data they are responsible for fitting (which typically covers less than a contig). We are looking to reimplement gCNV in a more modern inference framework that could allow us to do away with such sharding entirely. We could thus fit global/per-contig quantities of interest jointly with the rest of the gCNV model. The timeframe for this work may be relatively long (~year), but I think it'll be worth it. That said, I think a key takeaway from this work is that genotype priors can be more powerful for breaking degeneracies than contig-ploidy priors, so we will probably try to incorporate that insight in future work. To answer your questions:; 1) There are no additional results much further beyond what is shown above.; 2) You can see snippets/comparisons of the genotype and contig-ploidy prior file above. If you're just looking for information about the contig-ploidy prior table used in the current pipeline, see an example at https://gatk.broadinstitute.org/hc/en-us/articles/360051304711-DetermineGermlineContigPloidy and feel free to modify it to be more/less strict as desired.; 3) Unfortunately I believe the samples discussed above are not publicly available. If you are having difficulties with the current model, it would probably be useful to hear about them!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-894432129:486,perform,perform,486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-894432129,1,['perform'],['perform']
Performance,"Hi @cmnbroad, we can do better than crash - you'll notice that in my pull request, I've added a check to `CNNScoreVariants.java` to test whether AVX is present (the method was checking if command-line arguments are valid, so I stretched a point and used it to test the environment). . ~. You could definitely offer your own build of TensorFlow, but do you _want_ to? Apart from anything else, it'll be 10X slower than the default. My understanding is that inference using CNNScoreVariants was taking ~ 50 hours on a typical real-world file before optimization. Wouldn't it be more cost-effective just to use instances that at least have AVX?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429323785:547,optimiz,optimization,547,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429323785,1,['optimiz'],['optimization']
Performance,"Hi @davidbenjamin ,. this ticket is already open for a while, but I haven't found a sensible multi-sample wdl yet, so I've spent some time putting a multi-sample calling workflow together and tested it extensively: https://github.com/phylyc/gatk4-somatic-snvs-indels. It's somewhat optimized for resource needs, at least much more so than the published gatk mutect2 wdl.; One option that I added is to run the realignment filter only on a subset of called variants, which is especially helpful for tumor-only calling to reduce costs if we are hard filtering variants afterwards based on some thresholds anyways (who doesn't?). One todo is still to choose the best normal sample for the contamination model based on which has the highest sequencing depth, as you had mentioned in some old gatk forum post. Happy to have some contributions there :); I also brushed up the PoN wdl, which is also tested. How important is cram input support? I took that part out, but it's easy to plug it back in, I suppose.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6022#issuecomment-1113847113:282,optimiz,optimized,282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6022#issuecomment-1113847113,1,['optimiz'],['optimized']
Performance,"Hi @davidbenjamin and @ldgauthier,. I'm running into this same problem with version 4.4.0.0 in tumor only mode (actually creating a panel of normal). As far as I understand ""--emit-ref-confidence GVCF"" is required for compatibility with GenomicsDBImport? Is there some workaround?. > 1 Using GATK jar /scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar; > 2 Running:; > 3 java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar Mutect2 -R /ref_nobackup/Homo_sapiens_assembly38.fasta -I /BQSR/BSSE_QGF_229563_bqsr.bam --emit-ref-confidence GVCF --germline-resource /ref_nobackup/af-only-gnomad.hg38.vcf.gz -max-mnp-distance 0 -O /output/BSSE_QGF_229563_pon.g.vcf.gz --tmp-dir /scratch; > 4 15:07:52.399 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; > 5 15:07:52.435 INFO Mutect2 - ------------------------------------------------------------; > 6 15:07:52.437 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.4.0.0; > 7 15:07:52.438 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; > 8 15:07:52.438 INFO Mutect2 - Executing as X on Linux v3.10.0-1160.66.1.el7.x86_64 amd64; > 9 15:07:52.438 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v17.0.6+10; > 10 15:07:52.438 INFO Mutect2 - Start Date/Time: June 19, 2023 at 3:07:52 PM CEST; > 11 15:07:52.438 INFO Mutect2 - ------------------------------------------------------------; > 12 15:07:52.438 INFO Mutect2 - ------------------------------------------------------------; > 13 15:07:52.439 INFO Mutect2 - HTSJDK Version: 3.0.5; > 14 15:07:52.440 INFO Mutect2 - Picard Version: 3.0.0; > 15 15:07:52.440 INFO Mu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632:958,Load,Loading,958,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632,1,['Load'],['Loading']
Performance,"Hi @droazen ,; I am sorry for the late response. I used the docker container from dockerhub broadinstitute/gatk:4.4.0.0 .; The which python command produces the correct path. I re-loaded the container and tried it again without modification. Again the pipeline crashed with BaseRecalibrator and the above mentioned error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1691458235:180,load,loaded,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1691458235,1,['load'],['loaded']
Performance,"Hi @droazen! Again, I'm skeptical that it would be a serialization change as we have only made one change to serializer registration for htsjdk classes since 0.20.0:. ```; $ git diff adam-parent_2.10-0.20.0 adam-core/src/main/scala/org/bdgenomics/adam/serialization/ADAMKryoRegistrator.scala | grep htsjdk; kryo.register(classOf[scala.Array[htsjdk.variant.vcf.VCFHeader]]); ```. Additionally, the [Kryo version used in ADAM has been unchanged since 0.20.0](https://github.com/bigdatagenomics/adam/commit/295fb5cf7bf1f089e82a1624e2056053e817b5f3). Since we do not shuffle `SAMRecord`s, we do not register a serializer for `SAMRecord`, which would presumably be the main record that HaplotypeCaller is serializing. The GATK's implementation of BQSR doesn't perform any shuffles other than when reducing the recalibration tables, correct? If so, any performance regression in GATK's BQSR would be exceedingly unlikely to be caused by serializer registrations in ADAM.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367112827:755,perform,perform,755,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367112827,2,['perform'],"['perform', 'performance']"
Performance,"Hi @fpbarthel,. You have a few options, but I think they would all require a bit of manual processing on your part. For example, you could use CollectReadCounts with your initial list of bins, but then you'd have to do the averaging step manually. Hopefully, it should not be too trouble to put together your own script to do this; however, since it is somewhat of a custom operation, it's unlikely we'll support it as a feature. In any case, note that the rest of the downstream tools in both our germline and somatic pipelines are only set up to work with integer counts. I don't know your reasons for wanting to average counts over multiple bins, but since 1) averages contain less information than the full resolution bins, 2) the BAM I/O to perform count collection is expensive (so we don't want to discard information during the CollectReadCounts step that we might need later), and 3) we want to build generative models of the counts, our philosophy is to always retain the integer counts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5432#issuecomment-439892644:746,perform,perform,746,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5432#issuecomment-439892644,1,['perform'],['perform']
Performance,"Hi @jamesemery Thanks for your email.; Actually, I was successful with calculation of ""DepthOfCoverage"" with following procedure:; 1. I found the exact reference fasta file which is used for mapping procedures in my pipeline + .dict and .fa.fai; 2. I made a bed file out of the reference fasta file, using the following command:; faidx -i bed genome.fa > out.bed; 3. Finally I did this for performing the ""DepthOfCoverage"" tool:; gatk DepthOfCoverage -R reference.fa -O result -I s269825.haplotagged.bam -L out.bed. So everything works fine but there is another problem: the size of output file is very huge (GBs)! Should it be like that?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7453#issuecomment-971402891:390,perform,performing,390,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7453#issuecomment-971402891,1,['perform'],['performing']
Performance,"Hi @lbergelson ; Thanks for the response!; I tried building the current gatk from the master branch, and now the error message says something a little different. java.lang.ClassCastException: class htsjdk.samtools.BAMRecord cannot be cast to class java.lang.Comparable (htsjdk.samtools.BAMRecord is in unnamed module of loader 'app'; java.lang.Comparable is in module java.base of loader 'bootstrap'); 	at java.base/java.util.Arrays$NaturalOrder.compare(Arrays.java:105); 	at java.base/java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.base/java.util.TimSort.sort(TimSort.java:234); 	at java.base/java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746); 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); 	at java.base/java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:408); 	at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:736); 	at java.base/java.util.Arrays.parallelSort(Arrays.java:1183); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.base/java.lang.Thread.run(Thread.java:829); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 		at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); 		at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SA",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452463087:320,load,loader,320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452463087,6,"['concurren', 'load']","['concurrent', 'loader']"
Performance,"Hi @lbergelson I added **--disableAllReadFilters**, the log still says ""null"". Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/genome.fa --disableAllReadFilters --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 12:08:44.856 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 12:08:44.905 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 17, 2016 12:08:44 PM EDT] org.broadinstitute.hellbender.tools.spark.bwa.BwaSpark --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam --threads 16 --reference /home/kh3/Resources/genome_b37/genome.fa --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --disableAllReadFilters true --fixedChunkSize 100000 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [September 17, 2016 12:08:44 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.2-45-ga30af5a-SNAPSHOT; 12:08:44.930 INFO BwaSpark - Defaults.BUFFER_SIZE : 131072; 12:08:44.930 INFO BwaSpark - Defaults.COMPRESSION_LEVEL : 1; 12:08:44.930 INFO BwaSpark - Defaults.CREATE_INDEX : false; 12:08:44.930 INFO BwaSpark - Defaults.CREATE_MD5 : false; 12:08:44.930 INFO BwaSpark - Defaults.CUSTOM_READER_FACTORY : ; 12:08:44.930 INFO BwaSpark - Defaults.EBI_REFERENCE_SERVICE_URL_MAS",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247785408:525,load,load,525,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247785408,2,['load'],"['load', 'loaded']"
Performance,"Hi @lbergelson Thanks for the quick response! I just used the fasta as reference, and it still doesn't work. The log only says ""null"". Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/genome.fa --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 16:55:32.261 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 16:55:32.310 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 4:55:32 PM EDT] org.broadinstitute.hellbender.tools.spark.bwa.BwaSpark --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam --threads 16 --reference /home/kh3/Resources/genome_b37/genome.fa --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSize 100000 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 4:55:32 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.2-45-ga30af5a-SNAPSHOT; 16:55:32.335 INFO BwaSpark - Defaults.BUFFER_SIZE : 131072; 16:55:32.335 INFO BwaSpark - Defaults.COMPRESSION_LEVEL : 1; 16:55:32.335 INFO BwaSpark - Defaults.CREATE_INDEX : false; 16:55:32.335 INFO BwaSpark - Defaults.CREATE_MD5 : false; 16:55:32.335 INFO BwaSpark - Defaults.CUSTOM_READER_FACTORY : ; 16:55:32.335 INFO BwaSpark - Default",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247709200:557,load,load,557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247709200,2,['load'],"['load', 'loaded']"
Performance,"Hi @lbergelson and @AJDCiarla ,. Thank you for your working!. I am the one who reported this bug and I had given it a try as @lbergelson suggested. I performed tests in two different scenarios:. 1. Using full path without any non-ascii characters as tmp path and it succeeded:; ```; /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx8G -Djava.io.tmpdir=/data/xieduo/gatktest"" BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/uksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; Using GATK jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/gatktest -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/uksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:35:32.710 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:32.890 INFO BaseRecalibrator - ------------------------------------------------------------; 13",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:150,perform,performed,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['perform'],['performed']
Performance,"Hi @lbergelson,. We experienced the related issue in GATK 4.1.8 (it persisted since 4.1.5 or early version as far as we know) when running `FilterAlignmentArtifacts` in one of our cluster but not the other. We narrowed down the issue, using the CPU differences (the working one does not support AVX2), to `libgkl_smithwaterman.so`. Paths are shortened for clarity in the following commands. ```; bash faa.sh ; Using GATK jar /app/gatk-package-4.1.8.0-local.jar; Running:; /bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /app/gatk-package-4.1.8.0-local.jar FilterAlignmentArtifacts -V /output/sample.FilterMutectCalls.vcf.gz -R /db/hs37d5.fa --bwa-mem-index-image /db/hg38.fa.img -I /output/sample.Mutect2.bam -O sample.somatic_filter.test.vcf.gz --use-jdk-inflater true; 19:11:56.929 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 19:11:56.943 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 19:11:56.944 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 19:11:57.168 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 19, 2020 7:11:57 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:11:57.324 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 19:11:57.324 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:11:57.325 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:11:57.325 INFO Fil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:933,Load,Loading,933,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['Load'],['Loading']
Performance,"Hi @ruslan-abasov,. I believe your GermlineCNVCaller results should have inherited the correct dictionary from the count files. The issue is you created some GermlineCNVCaller shards (e.g., shard 4) with inappropriately ordered intervals (since these were instead ordered w.r.t. to the idiosyncratic dictionary you attached). However, I think if you just reshard and rerun GermlineCNVCaller for any such shards, you may be able to reuse most of your results. For example, you could take your shard 4 interval list, which contains intervals from chr18, chr19, and chr1, and reshard these intervals into two shards: 4a containing chr18-19 intervals, and 4b containing chr1 intervals. After rerunning 4a and 4b through GermlineCNVCaller, you should be able to use PostprocessGermlineCNVCalls to stitch together shards 4b, 1, 2, 3, and 4a, since these will be ordered w.r.t. the correct dictionary from the count files (i.e, they will contain intervals in the order chr1, chr10-19). Of course, you will want not want to perform this exact procedure; you'll want to generalize it to whatever will yield the correct order for all 10 of your shards across all contigs. Again, this may be error prone and I can't guarantee that it will be successful, since I haven't tried it myself. I would personally just rerun the pipeline. You might be able to cut down on runtime by using smaller shards (I believe we typically shard the entire genome into far more than 10 shards, which we usually run in parallel) and making sure you set parameters appropriately for WGS. @mwalker174 has the most experience running on WGS and should be able to provide you the latest recommendations, or you might be able to find them by searching GitHub or the GATK Forums. Your point is well taken about failing earlier, and I think I've outlined the best strategy above. It is impossible to catch all possible errors early, but for some we can certainly fail before the GermlineCNVCaller step.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720091802:1016,perform,perform,1016,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720091802,1,['perform'],['perform']
Performance,"Hi @shengqh, tools such as you describe are still under development. We are also still working to tune hyperparameters in the main gCNV pipeline, so thanks for trying it out! Please let us know if you encounter any issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5373#issuecomment-434275953:98,tune,tune,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5373#issuecomment-434275953,1,['tune'],['tune']
Performance,"Hi @tedsharpe, . A quick follow-up question on this  how does the function currently handle the Q-scores from overlapping portions of paired-end reads? @BenjaminWehnert1008 noticed that read pre-merging and dual Q-score integration can help improve our performance for 1nt variants. Best wishes,; Max",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8995#issuecomment-2416095456:254,perform,performance,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8995#issuecomment-2416095456,1,['perform'],['performance']
Performance,"Hi @tomwhite! The serializer registrations in ADAM shouldn't effect any GATK serialization, unless you're serializing classes from ADAM (such as `org.bdgenomics.format.avro.AlignmentRecord`); additionally, we haven't seen any performance issues with serialization in ADAM 0.23.0 outside of the GATK. We actually made a number of patches to eliminate logging in 0.23.0 (relative to 0.22.0), so I'd doubt that is the culprit. The one exception to this is logging when writing Parquet out to disk, which greatly increased sometime between ADAM 0.21.0 and 0.23.0, due to a change in Parquet versions upstream in Spark. However, this would only impact you if you were writing Parquet, and additionally this issue was resolved with the release of Spark 2.2.0, so you should see the logging go away with #4314. If I had to hypothesize anything, I'd suggest that the 2bit file change is the one thing that could be biting you. That said, we've been running with this 2bit file code quite frequently on AWS and Azure for at least the last 6 months using GATK HC on WES and WGS data and haven't seen any performance issues. I show that the changes in the 2bit file code between ADAM 0.20.0 and 0.23.0 are API compatible, so if you check out ADAM 0.23.0 and then `git checkout adam-parent-spark2_2.11-0.20.0 adam-core/src/main/scala/org/bdgenomics/adam/util/TwoBitFile.scala` and build the ADAM JAR (and then package that jar into GATK), you should be able to test that hypothesis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366308897:226,perform,performance,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366308897,2,['perform'],['performance']
Performance,"Hi @zaneChou1,. Thanks for filing the issue---I'm actually working on this at the moment. The gCNV python code requires some updating, since the APIs for inference changed after the pymc3 version we use (which is the primary reason we stuck with it). Because the changes required go beyond those in the PR you opened, I'll go ahead and close it, but thanks for making the effort to help us keep things updated! We certainly appreciate it. I would be surprised if updating numpy led to drastic performance improvements (1.17.5 was only released in 1/2020), but it's possible there have been improvements in the pymc3 python code. However, we are planning to move much of the python inference code over to pyro, for which the numpy upgrade is also necessary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6978#issuecomment-733001543:493,perform,performance,493,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6978#issuecomment-733001543,1,['perform'],['performance']
Performance,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:159,load,loading,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857,2,"['load', 'perform']","['loading', 'performed']"
Performance,"Hi Adam,. Thank you for this amazing work, and would like to suggest something small without disturbing the test-flow. So unless I'm not mistaken, the class that is consuming the memory is [NestedIntegerArray.java](https://github.com/broadgsa/gatk/blob/master/public/gatk-utils/src/main/java/org/broadinstitute/gatk/utils/collections/NestedIntegerArray.java). Why not try an experiment where you implement something similar to [Google's SSTable](https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/) that got release as [leveldb](https://github.com/google/leveldb). For instance the key-index lookup for you can be hash or a hash+offset. Basically these structures borrow from the concept of a [Log-Structured Merge-Tree (LSM-Tree) - link is a PDF paper](http://paperhub.s3.amazonaws.com/18e91eb4db2114a06ea614f0384f2784.pdf). You can push most of these to disk instead of memory, and they can be compressed as well. The indices can remain in memory for fast-lookup with the data staying on disk. So for instance, you can generate an key-index digest of specific values via `hash_function(numReadGroups.id, qualDimension.index, eventDimension.type)`, whose result you then use to quickly lookup the value for that key - which would reside on disk. That can be further improved with a priority queue cache for the most accessed values. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1460#issuecomment-180626838:1318,queue,queue,1318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1460#issuecomment-180626838,2,"['cache', 'queue']","['cache', 'queue']"
Performance,"Hi Adam,. That is the verbosity level that is performed by `bwa`. So since [BWASpark](https://github.com/broadinstitute/gatk/blob/c9807a1a94a60b69e8184c1fcf3083516bb0a78b/src/main/java/org/broadinstitute/hellbender/tools/spark/bwa/BwaSpark.java#L47) launches `BwaSparkEngine` as follows:. ```; final BwaSparkEngine engine = new BwaSparkEngine(bwaArgs.numThreads, bwaArgs.fixedChunkSize, referenceFileName);; ```. and launches the alignment as follows:. ```; final JavaRDD<GATKRead> reads = engine.alignWithBWA(ctx, unalignedReads, readsHeader);; ```. That in turn calls the `align()` method within [BwaSparkEngine.java](https://github.com/broadinstitute/gatk/blob/9fb4d756afbbad58a7e709e2e9fd308983ad255b/src/main/java/org/broadinstitute/hellbender/tools/spark/bwa/BwaSparkEngine.java#L72):. ```; final JavaRDD<String> samLines = align(shortReadPairs);; ```. which then instantiates a new [BwaMem](https://github.com/broadinstitute/gatk/blob/9fb4d756afbbad58a7e709e2e9fd308983ad255b/src/main/java/org/broadinstitute/hellbender/tools/spark/bwa/BwaSparkEngine.java#L101) object:. ```; final BwaMem mem = new BwaMem(index);; ```. Since the BWA implementation at [lindenb/jbwa](https://github.com/lindenb/jbwa) is basically a direct call to Heng's BWA as a library, the BWA option for verbosity is set by the `-v` argument as noted [here](http://bio-bwa.sourceforge.net/bwa.shtml#3):. ```; -v INT Control the verbose level of the output. This option has not been fully supported ; throughout BWA. Ideally, a value 0 for disabling all the output to stderr; 1 for ; outputting errors only; 2 for warnings and errors; 3 for all normal messages; 4 or ; higher for debugging. When this option takes value 4, the output is not SAM. [3] ; ```. This is used in Heng's [bwamem.c](https://github.com/lh3/bwa/blob/5961611c358e480110793bbf241523a3cfac049b/bwamem.c#L1224-L1226) file - and several other places - which generates the printout you see, as follows:. ```; if (bwa_verbose >= 3); fprintf(stderr, ""[M::%s] P",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2054#issuecomment-235675398:46,perform,performed,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2054#issuecomment-235675398,1,['perform'],['performed']
Performance,"Hi Adam,. This is based on your feedback from Friday afternoon. ; Lots of issues are still to be addressed (multilevel collection, more test coverage, performance, etc).; If you are too busy to review, let me know and I'll bug @cwhelan or @tedsharpe or someone else. Branch travis log available [here](https://travis-ci.org/broadinstitute/gatk/builds/110702136). Thank you.; Steve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1514:151,perform,performance,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1514,1,['perform'],['performance']
Performance,"Hi All,. ### Affected tool(s) or class(es); gatk SplitNCigarReads. ### Affected version(s); Version 4.2.5.0. I'm having a issue with running SplitNCigarReads on my local cluster. When I try to run the following command:. #!/bin/bash -l. #$ -l h_rt=48:00:0; #$ -l mem=12G; #$ -l tmpfs=20G; #$ -N bam_recalibration_rna; #$ -wd /home/regmvcr/Scratch/workspace/JSBF; #$ -pe smp 2; #$ -t 1-128. shopt -s expand_aliases. cd $TMPDIR. #Load dependencies on Myriad. module load java/temurin-8/8u322-b06; module load gatk/4.2.5.0. #Parse parameter files for inputs. number=$SGE_TASK_ID. paramfile=/home/regmvcr/Scratch/jobscripts/JSBF/file_names.txt. SAMPLE=""`sed -n ${number}p $paramfile | awk '{print $1}'`"". #run SplitNCigarReads. gatk SplitNCigarReads \; -R /home/regmvcr/Scratch/reference/sarek/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta \; -I ""/home/regmvcr/Scratch/workspace/JSBF/star_salmon/""$SAMPLE"".markdup.sorted.bam"" \; -O ""/home/regmvcr/Scratch/workspace/JSBF/SplitNCigarReads/""$SAMPLE""_split.bam"". I get the following output:. GATK: Some GATK tools require conda and associated libraries.; To use them run:; module load python/miniconda3/4.10.3; source $UCL_CONDA_PATH/etc/profile.d/conda.sh; conda activate $GATK_CONDA; Using GATK jar /shared/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /shar; ed/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar SplitNCigarReads -R /home/regmvcr/Scratch/reference/sarek/resources_broad_hg38_v0_Homo_sapiens_assemb; ly38.fasta -I /home/regmvcr/Scratch/workspace/JSBF/star_salmon/I3O-MC-JSBF-100-1003.markdup.sorted.bam -O /home/regmvcr/Scratch/workspace/JSBF/SplitNCigarReads/I3O-MC-JSBF-10; 0-1003_split.bam; 19:40:24.551 INFO NativeLibraryLoader - Loading libgkl_compression.so fr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8522:428,Load,Load,428,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522,3,"['Load', 'load']","['Load', 'load']"
Performance,"Hi David,. 1. Here, we must distinguish between the WES minimal example (default parameters, command lines in my original bug report) and our ""production pipeline"". In the minimal example, we do not use PON. In the production pipeline, we use the exome PON file from: gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz as recommended here: https://gatk.broadinstitute.org/hc/en-us/articles/360035890631-Panel-of-Normals-PON-; In the production pipeline, we also use gnomAD 3.1.2, filtered for AF>0 and FILTER=PASS as ""germline resource"". Some more data from the production pipeline, just in case it could help. ; Since the PON is exome-only, we were not sure whether including the PON would improve the performance of our WGS workflow in the ""production pipeline"" with gatk version 4.1.7.0, so we ran experiments:; - calling without germline resource and without PON; - calling with germline resource only, without PON; - calling with both germline resource and PON. The results:. ![WGS_FD_tumor-normal_reference_workflow_v04_WGS_FD_tumor-normal_reference_workflow_v04_gnomAD_only_WGS_FD_tumor-normal_reference_workflow_v04_no_gnomAD_no_PON](https://user-images.githubusercontent.com/15612230/182359001-a173e711-748b-49ec-b03f-71e5a8293c51.png). We also conducted experiments with a subset of the WES FD sample (FD_1, 1/3 of the full ~100x FD dataset):. - calling without germline resource and without PON; - calling with PON only, without germline resource; - calling with germline resource only, without PON; - calling with both germline resource and PON. The results (please note that the labeling conventions are now different compared to WGS experiments, apologies for the inconvenience):. ![FD_1_T_tumor-normal_WES_muTect2_FD_1_tumor-normal_muTect2_PON_FD_1_tumor-normal_muTect2_gnomAD_FD_1_tumor-normal_muTect2_PON_gnomAD](https://user-images.githubusercontent.com/15612230/182358963-97f04d12-94c6-4f77-acef-c3ebcd78a98f.png). 2. The reference is GRCh38.primary_assembly.genome.fa; The",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1202344705:712,perform,performance,712,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1202344705,1,['perform'],['performance']
Performance,"Hi GATK team,. I had error message as following with GATK4.1.0.0 on our local cluster: ; `; Using GATK jar /dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java1.8 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx5; g -jar /dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar SelectVariants -R /dsgmnt/llfs2/masterdata/geno/hg38/resources_broad_hg38_v0_Homo_sapiens_assembl; y38.fasta -L chr1 -V /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR//ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.gz -O /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR//ExcessHet_joi; nt525_c1.SNP.VQSR.g.vcf.gz; 09:15:49.372 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/nati; ve/libgkl_compression.so; 09:15:51.131 INFO SelectVariants - ------------------------------------------------------------; 09:15:51.132 INFO SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.0.0; 09:15:51.132 INFO SelectVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:15:51.132 INFO SelectVariants - Executing as xhong@blade5-4-11.dsg.wustl.edu on Linux v2.6.32-573.12.1.el6.x86_64 amd64; 09:15:51.133 INFO SelectVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_31-b13; 09:15:51.133 INFO SelectVariants - Start Date/Time: June 27, 2019 9:15:49 AM CDT; 09:15:51.133 INFO SelectVariants - ------------------------------------------------------------; 09:15:51.133 INFO SelectVariants - ------------------------------------------------------------; 09:15:51.134 INFO SelectVariants - HTSJDK Version: 2.18.2; 09:15:51.134 INFO SelectVariants - Picard Version: 2.18.25; 09:15:51.134 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:15:51.135 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6021:772,Load,Loading,772,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6021,1,['Load'],['Loading']
Performance,"Hi GATK, I have done the steps, i.e., 1)haplotypecaller 2)combineGVCF 3) GenotypeGVCF. Now I am interested in performing the allele matching between two samples, like whether the alleles are the same or different at a particular chromosomal location. Does GATK offer to perform this test? if yes, then which function in GATK will be preferable?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8681:110,perform,performing,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8681,2,['perform'],"['perform', 'performing']"
Performance,"Hi again,; I tried installing java8 and switching to this version prior to running gatk. It runs and looks to be running the right Java, but spits out roughly the same error:. Thoughts?. /cold/drichard/gatk/./gatk --java-options ""-Xmx25g"" SplitNCigarReads \; -R /cold/drichard/VARIANTS/Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam \; --tmp-dir /thing -O thing.bam; Using GATK jar /cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx25g -jar /cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar SplitNCigarReads -R /cold/drichard/VARIANTS/Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam --tmp-dir /thing -O thing.bam; 15:34:59.974 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:35:00.220 INFO SplitNCigarReads - ------------------------------------------------------------; 15:35:00.226 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.3.0.0-44-g227bbca-SNAPSHOT; 15:35:00.226 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:35:00.226 INFO SplitNCigarReads - Executing as drichard@illuvatar on Linux v5.19.0-32-generic amd64; 15:35:00.226 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_362-8u362-ga-0ubuntu1~22.04-b09; 15:35:00.226 INFO SplitNCigarReads - Start Date/Time: March 2, 2023 3:34:59 PM EST; 15:35:00.226 INFO SplitNCigarReads - ------------------------------------------------------------; 15:35:00.226 INFO SplitNCigarReads - ------------------------------------------------------------; 15:35:00.227 INFO SplitNCigarReads - HTSJDK Version: 3.0.1; 15:35:00.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:940,Load,Loading,940,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['Load'],['Loading']
Performance,"Hi all,. I am using `gatk VariantFiltration` to filter my GBS data. I need to set a threshold for the missing rate, such as 0.7. I know I can use `vcftools -max-missing 0.7`, and then I can further use other parameters to filter: `--maf; --max-maf; --min-alleles; --max-alleles`. I try to perform the same code via gatk code, and I went through the official doc on the gatk website, which does not list all information, or it does, and I have a hard time understanding the arguments. I found some potential parameters, but I don't really know which one is the correct parameter.; --max-filtered-genotypes: Maximum number of samples filtered at the genotype level;; --max-fraction-filtered-genotypes: Maximum fraction of samples filtered at the genotype level;; --max-nocall-fraction: Maximum fraction of samples with no-call genotypes;. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8635:289,perform,perform,289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8635,1,['perform'],['perform']
Performance,"Hi everyone,. I'm facing a similar issue with GATK v4.1.0.0 (HTSJDK v2.18.2 and Picard v2.18.25). I'm using GATK Docker image broadinstitute/gatk:4.1.0.0. Following what I read here, I checked the bam file and everything seems fine:; `gatk ValidateSamFile --INPUT sorted.bam --MODE SUMMARY`; ```; Using GATK jar /gatk/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.0.0-local.jar ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY; 16:08:17.382 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Mar 07 16:08:17 UTC 2019] ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. La",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5807:704,Load,Loading,704,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807,1,['Load'],['Loading']
Performance,"Hi everyone. I try to run gatk 4.2.5.0 VariantAnnotator using gnomAD data. However I get this error message java.lang.IllegalStateException: Allele in genotype C not in the variant context [C*, CT] can you maybe advise whats going on? . java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx30G -jar /run/media/riadh/One Touch1/Analysis/gatk-4.2.4.1/gatk-package-4.2.5.0-local.jar VariantAnnotator -V PE69_chr3.vcf -R /run/media/riadh/One Touch/Reference_data_b38/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta --resource:gnomad /run/media/riadh/One Touch/Reference_data_b38/gnomad.genomes.v3.1.2.sites.chr3.vcf.bgz -E gnomad.nhomalt -E gnomad.ALT -E gnomad.AF -O PE69_ch3_vep_cadd_gnomad.vcf --resource-allele-concordance; 10:58:19.715 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/run/media/riadh/One%20Touch1/Analysis/gatk-4.2.4.1/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 17, 2022 10:58:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:58:19.796 INFO VariantAnnotator - ------------------------------------------------------------; 10:58:19.796 INFO VariantAnnotator - The Genome Analysis Toolkit (GATK) v4.2.5.0; 10:58:19.796 INFO VariantAnnotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:58:19.797 INFO VariantAnnotator - Executing as riadh@ikm-unix-1012.uio.no on Linux v5.16.12-200.fc35.x86_64 amd64; 10:58:19.797 INFO VariantAnnotator - Java runtime: OpenJDK 64-Bit Server VM v11.0.14.1+1; 10:58:19.797 INFO VariantAnnotator - Start Date/Time: March 17, 2022 at 10:58:19 AM CET; 10:58:19.797 INFO VariantAnnotator - ------------------------------------------------------------; 10:58:19.797 INFO VariantAnnotator - -------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053:881,Load,Loading,881,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053,1,['Load'],['Loading']
Performance,"Hi! I have the same issue as @chandrans.; When I run Mutect2 this is the error:; `(gatk) root@d387db9e4351:/Desktop# gatk Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; Using GATK jar /gatk/gatk-package-4.1.1.0-local.ja; Running:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.1.0-local.jar Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; 08:27:06.032 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.s; Apr 23, 2019 8:27:10 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngin; INFO: Failed to detect whether we are running on Google Compute Engine. 08:27:10.882 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.883 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.1.; 08:27:10.883 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/. 08:27:10.884 INFO Mutect2 - Executing as root@d387db9e4351 on Linux v4.9.125-linuxkit amd64. 08:27:10.884 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12. 08:27:10.885 INFO Mutect2 - Start Date/Time: April 23, 2019 8:27:05 AM UT; 08:27:10.885 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.886 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.887 INFO Mutect2 - HTSJDK Version: 2.19.; 08:27:10.887 INFO Mutect2 - Picard Version: 2.19.; 08:27:10.887 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:610,Load,Loading,610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['Load'],['Loading']
Performance,"Hi, . I am experiencing exactly the same issue. I also run gatk on a cluster using singularity. When using --include-non-variant-sites the same error message is reported for all but the first chromosome. When exluding non variant sites it works fine. . ```; 05:04:16.405 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 05:04:16.556 INFO GenotypeGVCFs - ------------------------------------------------------------; 05:04:16.559 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.5.0.0; 05:04:16.559 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 05:04:16.563 INFO GenotypeGVCFs - Initializing engine; 05:04:16.929 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.5.1-84e800e; 16:04:16.979 INFO NativeGenomicsDB - pid=680685 tid=680686 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 16:04:16.979 INFO NativeGenomicsDB - pid=680685 tid=680686 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 16:04:16.979 INFO NativeGenomicsDB - pid=680685 tid=680686 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 05:04:17.059 INFO GenotypeGVCFs - Done initializing engine; 05:04:17.104 INFO ProgressMeter - Starting traversal; 05:04:17.105 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 05:04:17.124 INFO GenotypeGVCFs - Shutting down engine; [June 26, 2024 at 5:04:17 AM GMT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=1126170624; java.lang.IllegalStateException: There are no sources based on those query parameters; at org.genomicsdb.reader.GenomicsDBFeature",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2191214079:298,Load,Loading,298,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2191214079,1,['Load'],['Loading']
Performance,"Hi, . I have the same issue reported here https://github.com/broadinstitute/gatk/issues/6766 that relates to CombineGVCFs. It was supposed to be fixed with the new version 4.1.9.0, however. I still get the same error when I tried it with the current version. . Here is the error report . > Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/orange/reed/nhouse/Raw_seqs/SEQ9_samples/tmp; 11:30:50.248 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:30:50.478 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 26, 2020 11:30:50 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:30:50.791 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.791 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.9.0; 11:30:50.792 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:30:50.792 INFO CombineGVCFs - Executing as nwijewardena@c3a-s8.ufhpc on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 11:30:50.792 INFO CombineGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_31-b13; 11:30:50.792 INFO CombineGVCFs - Start Date/Time: October 26, 2020 11:30:50 AM EDT; 11:30:50.793 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.793 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.794 INFO CombineGVCFs - HTSJDK Version: 2.23.0; 11:30:50.794 INFO CombineGVCFs - Picard Version: 2.23.3; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRI",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6913:563,Load,Loading,563,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6913,1,['Load'],['Loading']
Performance,"Hi, @jamesemery .I downloaded the `GTF` file for `basic gene annotation` in the CHR regions from gencode. The version I obtained was `Release 43 (GRCh38.p13)`. After making some modifications as follow, I managed to run SVAnnotate without encountering any errors. ; ```; sed 's/; tag ""Ensembl_canonical""//g' gencode.v43.basic.annotation.gtf|sed 's/; tag ""overlaps_pseudogene""//g'|sed 's/; tag ""readthrough_gene""//g'|sed 's/; tag ""artifactual_duplication""//g' > gencode.v43.basic.modified_annotation.gtf; ```; However, the tool still fails to provide meaningful annotation information. The output file remains unchanged compared to the original file. Based on the SVAnnotate output as follow, I suspect that the issue might be caused by 'Current Locus unmapped.' ; ```; 16:11:10.044 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/Division/1user/2_Exome/Tools/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 16:11:10.072 INFO SVAnnotate - ------------------------------------------------------------; 16:11:10.074 INFO SVAnnotate - The Genome Analysis Toolkit (GATK) v4.4.0.0; 16:11:10.074 INFO SVAnnotate - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:11:10.074 INFO SVAnnotate - Executing as user@localhost.localdomain on Linux v3.10.0-1160.90.1.el7.x86_64 amd64; 16:11:10.074 INFO SVAnnotate - Java runtime: Java HotSpot(TM) 64-Bit Server VM v17.0.7+8-LTS-224; 16:11:10.075 INFO SVAnnotate - Start Date/Time: 202375 CST 4:11:10; 16:11:10.075 INFO SVAnnotate - ------------------------------------------------------------; 16:11:10.075 INFO SVAnnotate - ------------------------------------------------------------; 16:11:10.075 INFO SVAnnotate - HTSJDK Version: 3.0.5; 16:11:10.075 INFO SVAnnotate - Picard Version: 3.0.0; 16:11:10.076 INFO SVAnnotate - Built for Spark Version: 3.3.1; 16:11:10.076 INFO SVAnnotate - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:11:10.076 INFO SVAnnotate - ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8394#issuecomment-1621377138:809,Load,Loading,809,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8394#issuecomment-1621377138,1,['Load'],['Loading']
Performance,"Hi, GATK team! I'm working on GATK WGS somatic CNV calling pipeline. . When I tried gatk --java-options ""-Xmx2800g"" ModelSegments --denoised-copy-ratios ${tumor}.denoisedCR.tsv --allelic-counts ${tumor}.allelicCounts.tsv --normal-allelic-counts ${normal}.allelicCounts.tsv --output-prefix ${tumor} -O ${outdir}, I got this type of error:. 10:00:18.408 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/home/acct-medliuyb/medliuyb-user1/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 10, 2022 10:00:18 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:00:18.544 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.544 INFO ModelSegments - The Genome Analysis Toolkit (GATK) v4.2.0.0; 10:00:18.544 INFO ModelSegments - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:00:18.544 INFO ModelSegments - Executing as medliuyb-user1@huge2.pi.sjtu.edu.cn on Linux v3.10.0-1062.el7.x86_64 amd64; 10:00:18.545 INFO ModelSegments - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 10:00:18.545 INFO ModelSegments - Start Date/Time: January 10, 2022 at 10:00:18 AM CST; 10:00:18.545 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.545 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.545 INFO ModelSegments - HTSJDK Version: 2.24.0; 10:00:18.545 INFO ModelSegments - Picard Version: 2.25.0; 10:00:18.545 INFO ModelSegments - Built for Spark Version: 2.4.5; 10:00:18.545 INFO ModelSegments - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7633:379,Load,Loading,379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7633,1,['Load'],['Loading']
Performance,"Hi, I am encountering a similar error attempting to run `GenotypeGVCFs` in `gatk v4.1.2.0`. It runs very briefly and writes a handful of variants from a single scaffold to the output file but then exits with `java.lang.ArrayIndexOutOfBoundsException` (see below). I have also tried adding the `-L` flag and an interval list, which performs similarly but outputs variants from a different scaffold. Any idea why this is happening or what I can do to overcome this problem? I have run `GenomicsDBImport` and `GenotypeGVCFs` successfully in the past (same version, same computer) on a different dataset, so I'm not sure what about this data is causing the problem. Any guidance is much appreciated!. Thanks,; Jessie. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /nfs/data1/jsalt/3RAD/colinus_virginianus_13May2017_V3Fw6_newchrom.fasta -V gendb://odont_cyr_8_snp_db -O odont_cyr_8_snp_db.vcf; 14:59:47.866 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 03, 2020 2:59:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:59:59.674 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:59:59.675 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 14:59:59.675 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:00:09.686 INFO GenotypeGVCFs - Executing as jsalt@mustard on Linux v3.10.0-957.1.3.el7.x86_64 amd64; 15:00:09.686 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 15:00:09.687 INFO GenotypeGVCFs - Start Date/Time: F",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640:331,perform,performs,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640,1,['perform'],['performs']
Performance,"Hi, I am working with WES data with 130 samples. I've been following GATK4 best practices and also using the GRCh38 reference from the GATK bundle. I've been able to pre-process all the samples and to use Haplotypecaller for the 130 samples, then I proceed to merge all into a single gVCF file to then perform a join-call of SNPs and INdels. However, I got the following error message when using ""GenotypeGVCF"" ; Thank you; Cristian. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); GATK v4.0.5.2; ### Description ; 12:37:00.202 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 12:37:00.306 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home-1/cvalenc1@jhu.edu/apps/GATK4/gatk-4.0.5.2/gatk-package-4.0.5.2-local.jar!/com/intel/gkl/native/libg; kl_compression.so; 12:37:00.524 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.524 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.5.2; 12:37:00.524 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:37:00.529 INFO GenotypeGVCFs - Executing as cvalenc1@jhu.edu@compute0207 on Linux v2.6.32-696.28.1.el6.x86_64 amd64; 12:37:00.530 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 12:37:00.530 INFO GenotypeGVCFs - Start Date/Time: July 12, 2018 12:37:00 PM EDT; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Version: 2.16.0; 12:37:00.530 INFO GenotypeGVCFs - Picard Version: 2.18.7; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defau",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5009:302,perform,perform,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5009,2,"['Load', 'perform']","['Loading', 'perform']"
Performance,"Hi, I encountered a similar error attempting to run GenotypeGVCFs in gatk v4.1.4.1. ; It ran very briefly and writed few variants to the output file but then exited with java.lang.ArrayIndexOutOfBoundsException. . I solved the problem (I created a merged vcf) by performing bgzip (and tabix) of each gvcf file before running GenomicsDBImport and GenotypeGVCFs.; I don't know if it could be a solution also for your issues. command:; for P in my_gvcf.list; do; bgzip -c $P.g.vcf > $P\.g.vcf.gz. tabix -p vcf $P.g.vcf.gz; done. gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport --genomicsdb-workspace-path my_database --sample-name-map sample_map --batch-size 50 --consolidate true --tmp-dir=tmp/ --reader-threads 5 -L file.bed. gatk --java-options ""-Xmx12g -Xms12g"" GenotypeGVCFs -R hg19.fa -V gendb://my_database -O global.vcf --new-qual --tmp-dir=tmp/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-583260728:263,perform,performing,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-583260728,1,['perform'],['performing']
Performance,"Hi, I encountered the following error while running GATK.; It is hard for me to say what exactly is wrong, and extensive searching has not been helpul. Thanks in advance!. ```; gatk ValidateVariants -V ../../data/geno/phased/chr1-22.phased.rename.reheader.vcf.gz -R ../../../../index/hg19.fa.gz; Using GATK jar ~/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/zepengmu/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar ValidateVariants -V ../../data/geno/phased/chr1-22.phased.rename.reheader.vcf.gz -R ../../../../index/hg19.fa.gz; 19:53:34.379 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/zepengmu/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 25, 2020 7:53:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:53:34.606 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.606 INFO ValidateVariants - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:53:34.606 INFO ValidateVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:53:34.607 INFO ValidateVariants - Executing as zepengmu@midway2-login1.rcc.local on Linux v3.10.0-1127.8.2.el7.x86_64 amd64; 19:53:34.607 INFO ValidateVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 19:53:34.607 INFO ValidateVariants - Start Date/Time: October 25, 2020 7:53:34 PM CDT; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - HTSJDK Version: 2.22.0; 19:53:34.607 INFO ValidateVarian",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:758,Load,Loading,758,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['Load'],['Loading']
Performance,"Hi, everyone! I'm trying to call mutation with Mutect2 with RNA-seq, and my scripts are given below. I simply use one sample as a test, with a prior knowledge that the mutation in ASXL1(c.1934dupG) can be detected with a pretty high VAF, and I can also see this mutation by using bam file in IGV, but I really wonder why my scripts can not call this mutation before filtering? Thank you so much!!!; ![image](https://github.com/user-attachments/assets/58bfd1be-748e-453c-be0b-d49569e14dd5); gatk Mutect2 \; -R ${ref}.fa \; -I ${sam}/${sam}.BQSR.bam \; -O ${sam}/gatk/${sam}_withpon.vcf \; --create-output-bam-index FALSE \; --af-of-alleles-not-in-resource 0.0000025 \; --create-output-variant-index false \; --germline-resource /home/cuiyiran/data/mtDNA_mutation/reference/somatic-hg38_af-only-gnomad.hg38.vcf \; --panel-of-normals /home/cuiyiran/data/mtDNA_mutation/reference/somatic-hg38_1000g_pon.hg38.vcf. gatk FilterMutectCalls \; -R ${ref}.fa \; -V ${sam}/gatk/${sam}_withpon.vcf \; --create-output-variant-index false \; -O ${sam}/gatk/${sam}_withpon_fv.vcf. bcftools norm -m -both ${sam}/gatk/${sam}_withpon_fv.vcf | bcftools norm -m +both -f ${ref}.fa ${sam}/gatk/${sam}_withpon_fv.vcf -Ov -o ${sam}/gatk/${sam}_withpon_norm.vcf; ####annotation; perl ~/miniconda3/envs/vep/bin/vcf2maf.pl \; --input-vcf ${sam}/gatk/${sam}_withpon_norm.vcf \; --output-maf ${sam}/gatk/${sam}_withpon_vep.maf \; --vep-path ~/miniconda3/envs/vep/bin/ \; --vep-data $vepcache \; --ncbi-build GRCh38 \; --cache-version=112 \; --ref-fasta ${ref}.fa \; --tumor-id ${sam}",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/9021:1491,cache,cache-version,1491,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/9021,1,['cache'],['cache-version']
Performance,"Hi, when I run gatk Funcotator, A USER ERROR has occurred; ```; Using GATK jar /export/.conda/envs/wes/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /export/bioinfo-team/home/liuhw/.conda/envs/wes/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar Funcotator --data-sources-path /export2/liuhw/software/gatk_Funcotator/funcotator_dataSources.v1.8.hg38.20230908s/ -V /export2/liuhw/wes_test//Mutect2_filter/K001137N_somatic_filtered.vcf.gz -R /Shared_Software/ref_genome/GATK_GRCh38/Homo_sapiens_assembly38.fasta --ref-version hg38 -O /export2/liuhw/wes_test//annotate/K001137N.funcotator.maf --output-file-format MAF; 02:55:31.904 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/export/bioinfo-team/home/liuhw/.conda/envs/wes/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 02:55:32.062 INFO Funcotator - ------------------------------------------------------------; 02:55:32.062 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.5.1-0.0.3; 02:55:32.062 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:55:32.062 INFO Funcotator - Executing as liuhw@RichardLi-Lab01 on Linux v5.4.0-74-generic amd64; 02:55:32.062 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_412-b08; 02:55:32.063 INFO Funcotator - Start Date/Time: July 12, 2024 2:55:31 AM EDT; 02:55:32.063 INFO Funcotator - ------------------------------------------------------------; 02:55:32.063 INFO Funcotator - ------------------------------------------------------------; 02:55:32.063 INFO Funcotator - HTSJDK Version: 2.15.1; 02:55:32.063 INFO Funcotator - Picard Version: 2.18.2; 02:55:32.063 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 02:55:32.063 INFO Funcotator - HTSJDK Defaults.USE_AS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8913:843,Load,Loading,843,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8913,1,['Load'],['Loading']
Performance,"Hi, you have probably deleted out the previous comment with the `No space left on device` error, but was wondering if you could check if you have enough space, import to a new workspace and turn on `--genomicsdb-shared-posixfs-optimizations` with GenomicsDBImport?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-759831424:227,optimiz,optimizations,227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-759831424,1,['optimiz'],['optimizations']
Performance,"Hi,. I am trying to call germline CNVs for a set of samples. After running DetermineGermlineContigPloidy and GermlineCNVCaller, I am using PostprocessGermlineCNVCalls to generate the VCF files with CNV calls. The ""interval"" VCF files are generated successfully. But I got the following error message when segmenting contigs:. org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python /tmp/shulik7/segment_gcnv_calls.2338024416841754264.py --ploidy_calls_path /scratch/users/shulik7/test_GATK_CNV/Postprocess/../DetermineGermlineContigPloidy/model/test_run-calls/ --model_shards /scratch/shulik7/test_GATK_CNV/Postprocess/../GermlineCNVCaller/cnvs/test_run-model --calls_shards /scratch/shulik7/test_GATK_CNV/Postprocess/../GermlineCNVCaller/cnvs/test_run-calls --output_path /tmp/shulik7/gcnv-segmented-calls28280883609685538 --sample_index 0; Stdout: 11:32:16.728 INFO segment_gcnv_calls - Loading ploidy calls...; 11:32:16.729 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:32:16.730 INFO segment_gcnv_calls - Instantiating the Viterbi segmentation engine...; 11:32:18.585 INFO gcnvkernel.postprocess.viterbi_segmentation - Assembling interval list and copy-number class posterior from model shards...; 11:32:25.158 INFO gcnvkernel.structs.metadata - Generating intervals metadata...; 11:32:27.543 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano forward-backward function...; 11:32:34.406 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano Viterbi function...; 11:32:40.598 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano variational HHMM...; 11:32:42.862 INFO gcnvkernel.postprocess.viterbi_segmentation - Processing sample index: 0, sample name: test_sample_0...; 11:32:43.631 INFO gcnvkernel.postprocess.viterbi_segmentation - Segmenting contig (1/24) (contig name: 1)... Stderr: Traceback (most recent call last):; File ""/t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4724:951,Load,Loading,951,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4724,1,['Load'],['Loading']
Performance,"Hi,. I don't know if it's a bug, but whenever I try to do a GenomicsDBImport I get an error, no matter which file I use. The command I run is (using GATK 4.0):. gatk GenomicsDBImport -V AD0616.10.g.vcf.gz --genomicsdb-workspace-path gdbworkspace-gatk -L 10. And the error I get is:. Using GATK jar /apps/GATK/4.0/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /apps/GATK/4.0/gatk-package-4.0.0.0-local.jar GenomicsDBImport -V /scratch/production/cluengo/genomicsdb/AD0616.10.g.vcf.gz --genomicsdb-workspace-path gdbworkspace-gatk -L 10; 17:00:53.658 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/apps/GATK/4.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:00:53.770 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.771 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.0.0; 17:00:53.771 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:00:53.771 INFO GenomicsDBImport - Executing as cluengo@login1 on Linux v2.6.32-696.13.2.el6.Bull.128.x86_64 amd64; 17:00:53.771 INFO GenomicsDBImport - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_102-b14; 17:00:53.771 INFO GenomicsDBImport - Start Date/Time: March 8, 2018 5:00:53 PM CET; 17:00:53.771 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.771 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Version: 2.13.2; 17:00:53.772 INFO GenomicsDBImport - Picard Version: 2.17.2; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:00:53.772 INFO GenomicsDBImport - HTS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4514:737,Load,Loading,737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514,1,['Load'],['Loading']
Performance,"Hi,. I'm trying to validate the performance of BwaSpark (I'm running it locally). The input ubam file size is 5.1 GB. It takes 65 minutes for GATK's BwaSpark to complete which is exactly same as bwa-mem. Below is the command that I used to run BwaSpark. Is there any way to make BwaSpark run faster while running it locally or will the performance increase only while running on spark cluster? Please let me know if I had to modify or add any parameter. . Also, please let me know where can I find the complete list of --conf parameters for BwaSpark? (I couldn't find these options in gatk BwaSpark --help). `time gatk BwaSpark --bwa-mem-index-image GRCh37.fasta.img --spark-master local[*] --bam-partition-size 4000000 --conf 'spark.executor.num=5' --conf 'spark.executor.cores=16' --conf 'spark.executor.memory=15G' --conf 'spark.driver.memory=30G' --conf 'spark.dynamicAllocation.enabled=true' -I unmapped_input.bam -O output.bam -R GRCh37.fasta 2> Log_file.log`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8897:32,perform,performance,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8897,2,['perform'],['performance']
Performance,"Hi,; I am trying to generate vcf using GATK pipeline from bam file, but everytime, I am getting the following exception:; 01:13:15.801 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/ngs/programs/gatk-4.0.0.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 01:13:16.075 INFO HaplotypeCaller - ------------------------------------------------------------; 01:13:16.075 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.0.0; 01:13:16.075 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:13:16.076 INFO HaplotypeCaller - Executing as shashank@grande on Linux v3.13.0-79-generic amd64; 01:13:16.076 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_72-internal-b15; 01:13:16.076 INFO HaplotypeCaller - Start Date/Time: January 18, 2020 1:13:15 AM IST; 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------; 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------; 01:13:16.077 INFO HaplotypeCaller - HTSJDK Version: 2.13.2; 01:13:16.077 INFO HaplotypeCaller - Picard Version: 2.17.2; 01:13:16.077 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 01:13:16.078 INFO HaplotypeCaller - Deflater: IntelDeflater; 01:13:16.078 INFO HaplotypeCaller - Inflater: IntelInflater; 01:13:16.078 INFO HaplotypeCaller - GCS max retries/reopens: 20; 01:13:16.078 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 01:13:16.078 INFO HaplotypeCaller - Initializing engine; 01:13:17.087 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6384:162,Load,Loading,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6384,1,['Load'],['Loading']
Performance,"Hi,; I am trying to generate vcf using GATK pipeline from bam file, but everytime, I am getting the following exception:; 01:13:15.801 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/ngs/programs/gatk-4.0.0.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 01:13:16.075 INFO HaplotypeCaller - ------------------------------------------------------------; 01:13:16.075 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.0.0; 01:13:16.075 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:13:16.076 INFO HaplotypeCaller - Executing as shashank@grande on Linux v3.13.0-79-generic amd64; 01:13:16.076 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_72-internal-b15; 01:13:16.076 INFO HaplotypeCaller - Start Date/Time: January 18, 2020 1:13:15 AM IST; 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------; 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------; 01:13:16.077 INFO HaplotypeCaller - HTSJDK Version: 2.13.2; 01:13:16.077 INFO HaplotypeCaller - Picard Version: 2.17.2; 01:13:16.077 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 01:13:16.078 INFO HaplotypeCaller - Deflater: IntelDeflater; 01:13:16.078 INFO HaplotypeCaller - Inflater: IntelInflater; 01:13:16.078 INFO HaplotypeCaller - GCS max retries/reopens: 20; 01:13:16.078 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 01:13:16.078 INFO HaplotypeCaller - Initializing engine; 01:13:17.087 INF",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-575601220:162,Load,Loading,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-575601220,1,['Load'],['Loading']
Performance,"Hi,; This is a part of my script and I get the mistakes after I run it:; for sample in $samples ; do ; sample_gvcfs=${sample_gvcfs}"" --variant /data/users/zhanglei/species/Medicago/result/${sample}.HC.g.vcf.gz ""; done; time gatk CombineGVCFs \; -R /data/users/zhanglei/species/Medicago/Medicago.fa \; ${sample_gvcfs} \; -O $outdir/population/${outname}.HC.g.vcf.gz && echo ""** ${outname}.HC.g.vcf.gz done **"" &&. 18:08:13.704 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/software/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:08:13.872 INFO CombineGVCFs - ------------------------------------------------------------; 18:08:13.873 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.0.3.0; 18:08:13.873 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:08:13.873 INFO CombineGVCFs - Executing as zhanglei@GenEngine on Linux v3.10.0-327.el7.x86_64 amd64; 18:08:13.873 INFO CombineGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_201-b09; 18:08:13.874 INFO CombineGVCFs - Start Date/Time: May 18, 2019 6:08:13 PM CST; 18:08:13.874 INFO CombineGVCFs - ------------------------------------------------------------; 18:08:13.874 INFO CombineGVCFs - ------------------------------------------------------------; 18:08:13.874 INFO CombineGVCFs - HTSJDK Version: 2.14.3; 18:08:13.874 INFO CombineGVCFs - Picard Version: 2.17.2; 18:08:13.874 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:08:13.875 INFO CombineGVCFs - Deflater: IntelDeflater; 18:08:13.875 INFO CombineGVCFs - Inflater: IntelInflater; 18:08:13.875 INFO CombineGVCFs - GCS max retries/reopens: 20; 18:08:13.875 I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5947:453,Load,Loading,453,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947,1,['Load'],['Loading']
Performance,"Hi. I have pulled your docker and I am certain bam files are in the path but gate can not locate them. ```; (gatk) root@34684eaa046e:/gatk/data/Continuum/WES/vcf# java -d64 -XX:+UseSerialGC -Xmx3G -jar /gatk/gatk.jar CollectSequencingArtifactMetrics -I NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup.bam -O NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup --FILE_EXTENSION .txt -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz; 12:49:41.698 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Aug 10 12:49:41 UTC 2023] CollectSequencingArtifactMetrics --FILE_EXTENSION .txt --INPUT NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup.bam --OUTPUT NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup --REFERENCE_SEQUENCE GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz --MINIMUM_QUALITY_SCORE 20 --MINIMUM_MAPPING_QUALITY 30 --MINIMUM_INSERT_SIZE 60 --MAXIMUM_INSERT_SIZE 600 --INCLUDE_UNPAIRED false --INCLUDE_DUPLICATES false --INCLUDE_NON_PF_READS false --TANDEM_READS false --USE_OQ true --CONTEXT_SIZE 1 --ASSUME_SORTED true --STOP_AFTER 0 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Aug 10, 2023 12:49:43 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Thu Aug 10 12:49:43 UTC 2023] Executing as root@34684eaa046e on Linux 4.15.0-208-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.3.0; [Thu Aug 10 12:49:43 UTC 2023] picard.analysis.artifacts.CollectSequencingArtifactMetrics done.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8462:479,Load,Loading,479,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8462,1,['Load'],['Loading']
Performance,"Hi. Same Error on 4.0.3.0; ```; java -jar /usr/hpc-bio/gatk/gatk-package-4.0.3.0-local.jar Mutect2 --verbosity WARNING -R /usr/bio-ref/GRCh38.p0.dnaref/dnaref.fa --germline-resource /usr/bio-ref/GRCh38.p0.dnaref/common.vcf --max-reads-per-alignment-start 100 -L X -I /biowrk/BaseSpace/bam.bwa/HiSeqX-PCR-free-v2.5-NA12878/md.bam -tumor HiSeqX-PCR-free-v2.5-NA12878 -O mutect2.tumor-only.vcf; 23:50:01.301 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; [March 28, 2018 11:50:04 PM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=2354577408; java.lang.IndexOutOfBoundsException: Index: 0, Size: 0; at java.util.ArrayList.rangeCheck(ArrayList.java:657); at java.util.ArrayList.get(ArrayList.java:433); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.isActive(Mutect2Engine.java:316); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:159); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:135); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-376937140:963,load,loadNextAssemblyRegion,963,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-376937140,1,['load'],['loadNextAssemblyRegion']
Performance,"Hi; I tried to run the recalibration using the gatk-generated vcf file as a known vcf file. I got an error which has been previously described ""The covariates table is missing ReadGroup V300019285_L2_ in RecalTable0"" but without the solution. ; I am wondering if the solution has been found. Anyone has the experience to fix this issue.; Thank ; **This is the batch file** ; java -Xmx16g -jar /scratch/ddo/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar ApplyBQSR \; -R /scratch/ddo/refgenomenew/New_IDs.fasta \; -I /scratch/ddo/markedsam/C18-436P.sort.rmdup.bam \; --bqsr-recal-file /scratch/ddo/reclibration/gatkmf01_C18-436P.recal_data.table \; -O /scratch/ddo/reclibration/C18-436P.bqsr.maf01.bam . **This is the log file**; -----------------------------------------------------------------------------------------------------; Picked up JAVA_TOOL_OPTIONS: -Xmx2g; 04:59:42.641 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/ddo/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 08, 2021 4:59:43 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 04:59:43.044 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.045 INFO ApplyBQSR - The Genome Analysis Toolkit (GATK) v4.1.9.0; 04:59:43.045 INFO ApplyBQSR - For support and documentation go to https://software.broadinstitute.org/gatk/; 04:59:43.045 INFO ApplyBQSR - Executing as on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 04:59:43.045 INFO ApplyBQSR - Java runtime: OpenJDK 64-Bit Server VM v13.0.2+8; 04:59:43.045 INFO ApplyBQSR - Start Date/Time: November 8, 2021 at 4:59:42 a.m. PST; 04:59:43.045 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.045 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.046 INFO ApplyBQSR - HTSJDK Version: 2.23.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7549:905,Load,Loading,905,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7549,1,['Load'],['Loading']
Performance,"Hm. Even though I totally expected to be able to reproduce this, I can't. I tried quite a few different combinations. I think this one captures the steps that were reported, but it works:. ```; (base) /tmp/test a /Users/cnorman/projects/gatk/gatk MergeVcfs -I data/calling/my.vcf.gz -I data/calling/b.vcf.gz -O out.vcf.gz; Using GATK jar /Users/cnorman/projects/gatk/build/libs/gatk-package-4.1.7.0-41-g79586b8-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /Users/cnorman/projects/gatk/build/libs/gatk-package-4.1.7.0-41-g79586b8-SNAPSHOT-local.jar MergeVcfs -I data/calling/my.vcf.gz -I data/calling/b.vcf.gz -O out.vcf.gz; 16:00:13.443 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/cnorman/projects/gatk/build/libs/gatk-package-4.1.7.0-41-g79586b8-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; [Mon Jun 22 16:00:13 EDT 2020] MergeVcfs --INPUT data/calling/my.vcf.gz --INPUT data/calling/b.vcf.gz --OUTPUT out.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Mon Jun 22 16:00:13 EDT 2020] Executing as cnorman@WMCEA-78B on Mac OS X 10.13.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.7.0-41-g79586b8-SNAPSHOT; [Mon Jun 22 16:00:13 EDT 2020] picard.vcf.MergeVcfs done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=373293056; Tool returned:; 0; ```; The only way I can reproduce it is to delete one of the files so it *really* doesn't exist at the specified location:; ```; (base) /tmp/test a /Users/cnorman/projects/gatk/gatk Merge",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647743321:814,Load,Loading,814,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647743321,1,['Load'],['Loading']
Performance,"Hm. Just yesterday we updated from TF 1.4 to 1.9. Although this makes it more compelling to switch the default to Intel-optimized, we may still have an issue for the reasons outlined in the previous PR (academic users, not all GCS zones guaranty AVX hardware, and its still unclear to me if Travis, which uses both GCS and EC2, makes such a guaranty). It [sounds like](https://github.com/tensorflow/tensorflow/issues/18689) the failure mode is to crash. For running inference at least (training may be a different story), we may need something better. Another option is that it sounds like its possible to build our own distribution without AVX dependencies to use as a fallback.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429316465:120,optimiz,optimized,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429316465,1,['optimiz'],['optimized']
Performance,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:79,load,loaded,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205,5,"['concurren', 'load']","['concurrency', 'load', 'loaded', 'loading']"
Performance,"Hmm, guess I didnt realize how important it was to subset to high-confidence regions. Most of the false positives above are coming from the low-confidence regions, so lower precision may actually indicate higher sensitivity to any real variants that may be there and missing in the truth set. When optimizing parameters in the past, have we always just focused on the high confidence regions and called it a day?. Also, do we typically run HC/M2 identically in low/high confidence regions? Im noticing that its taking much longer to get through the low confidence regions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-713168743:299,optimiz,optimizing,299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-713168743,1,['optimiz'],['optimizing']
Performance,"Hmm, it looks like we could perform more checks upstream in GermlineCNVCaller as well. I think it might be also possible to introduce inconsistencies there by using read counts and sharded interval lists with different dictionaries or contig orders (which might have happened here). The canonical dictionary is taken from the first read count file, but the intervals pass through the `-L` machinery and I think any dictionary information is not checked (probably because it will not be present if Picard interval lists were not used as input); intervals are also just (re)sorted by the read-count dictionary to use in subsetting counts. EDIT: Seems like checking intervals for the dictionary may be something we should properly add at the engine level. See e.g. the following `TODO`:. ````; /**; * Returns the ""best available"" sequence dictionary or {@code null} if there is no single best dictionary.; *; * The algorithm for selecting the best dictionary is as follows:; * 1) If a master sequence dictionary was specified, use that dictionary; * 2) if there is a reference, then the best dictionary is the reference sequence dictionary; * 3) Otherwise, if there are reads, then the best dictionary is the sequence dictionary constructed from the reads.; * 4) Otherwise, if there are features and the feature data source has only one dictionary, then that one is the best dictionary.; * 5) Otherwise, the result is {@code null}.; *; * TODO: check interval file(s) as well for a sequence dictionary; * ...; * @return best available sequence dictionary given our inputs or {@code null} if no one dictionary is the best one.; */; public SAMSequenceDictionary getBestAvailableSequenceDictionary() {; ````. We could also just require a sequence dictionary for the relevant tools and/or somehow try to reconcile with dictionaries from other inputs, but I think it's better to check and fail.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718792218:28,perform,perform,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718792218,1,['perform'],['perform']
Performance,"Hmm, looks like we lose events 1 and 3 with CollectReadCounts at 250bp using analogous ModelSegments parameters. However, I experimented with tweaking the segmentation to work on the copy ratios (rather than the log2 copy ratios), which seems to recover them. Although one of the goals of having evaluations backed by SV truth sets is to tune such parameters/methods, I'm beginning to think that SV integration might benefit from using the CNV tools in a more customized pipeline---especially if maximizing sensitivity at resolutions of ~100bp jointly with breakpoint evidence is the goal. For example, you might imagine a tool that directly uses CNV backend code to collect coverage over regions specified by `-L`, builds a PoN, denoises, and segments on the fly. Or we can put together a custom WDL optimized for sensitivity. Let's discuss in person?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372875222:338,tune,tune,338,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372875222,2,"['optimiz', 'tune']","['optimized', 'tune']"
Performance,"Hmm, running `./gradlew --info ...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compres",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:772,load,load,772,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088,1,['load'],['load']
Performance,"Hmm, thanks for suggesting the addition of a regression test @fleharty. This caused me to realize that I actually missed another gap in the previous filtering logic that might have yielded NaNs (resulting from division by zero interval medians) in this particular edge case, which actually takes effect before the rounding error I originally fixed. However, because of how HDF5 writes NaN values as 0, this apparently doesn't lead to any catastrophic failures. We should definitely check that behavior is reasonable in this case (i.e., when interval medians are zero); I've filed #6878. In the end, I added a regression test that only passes with the changes to address the rounding error. This was a bit of a pain because we use simulated data in the tests that cover this code, and the filters are applied in sequential order only on those elements that passed the previous filter. Note that there are many other possible filtering combinations that would be impractical to test. I think that all of this filtering logic was ported over from GATK CNV (I only rewrote the code to perform the filtering in-place to improve memory usage), and I'm not sure that all edge-case behavior was well defined by the original logic (which probably implicitly assumed typical, well formed data, i.e., using more than one sample, without too many uncovered intervals). Fortunately, these edge-case usages (i.e., using a single sample to build the PoN, mistakenly including too many uncovered intervals, and/or disabling various filters) are probably not too common.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6624#issuecomment-705843882:1081,perform,perform,1081,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6624#issuecomment-705843882,1,['perform'],['perform']
Performance,"Hmmm....as an alternate proposal, what if we implemented a custom serializer for `SimpleInterval` that does the contig name -> index and index -> contig name conversion transparently at serialization/deserialization time? Would that address the performance issue with shuffles and allow us all to share the same interval class?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5154#issuecomment-418509374:245,perform,performance,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5154#issuecomment-418509374,1,['perform'],['performance']
Performance,"Hmn, we could be seeing quota issues. We just dramatically increased the size of our build matrix and there's a ton of work going on today. It's hard to tell because there's no way to view the queue...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2733#issuecomment-305020745:193,queue,queue,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2733#issuecomment-305020745,1,['queue'],['queue']
Performance,How about I close the PR for now and open a ticket to investigate caching and its performance implications? I'll keep the branch so you can always compare etc.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235939855:82,perform,performance,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235939855,1,['perform'],['performance']
Performance,How do we profile and identify bottlenecks?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/284:31,bottleneck,bottlenecks,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/284,1,['bottleneck'],['bottlenecks']
Performance,"How much does count collection cost at the desired bin size? How does this compare to bincov? Perhaps we could eliminate one of these steps if redundant. Note that the read counts are read once and stored in memory, so unless this takes a significant amount of time, then indexing is probably not the highest priority here (although I agree it would be nice to have in general). One related issue, as you mention, is file localization---since each shard only operates on a portion of the counts in each sample, it is a bit wasteful to localize the whole file. But how much does file localization cost? I can't imagine that it is the lowest hanging fruit. One of the more important issues, which you also mention, is optimizing parameters for inference. This includes not only the minimum number of epochs for training, but also things like the learning rate, annealing schedule, iterations per epoch, conditions for epoch convergence, etc. I'll be talking about how to tune these inference parameters---as well as other things in the pipeline---at the next BSV meeting. Let's brainstorm more things to try and prioritize them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5288#issuecomment-427562932:716,optimiz,optimizing,716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5288#issuecomment-427562932,2,"['optimiz', 'tune']","['optimizing', 'tune']"
Performance,"How to reproduce:. Modify the code in ```AlignmentIntervalUnitTest.testConstructionFromSAMRecord``` to perform a validation of the read returned by ```applyAlignment```:. ```; final SAMRecord samRecord = BwaMemAlignmentUtils.applyAlignment(""whatever"", SVDiscoveryTestDataProvider.makeDummySequence(expectedContigLength, (byte)'A'), null, null, bwaMemAlignment, refNames, hg19Header, false, false);; if (samRecord.isValid() != null) {; throw new IllegalStateException(samRecord.isValid().stream().map(s -> s.getMessage()).collect(Collectors.joining("", "")));; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3459#issuecomment-323218384:103,perform,perform,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3459#issuecomment-323218384,1,['perform'],['perform']
Performance,Huh... That's not good. I wonder if it's because we removed the jcenter repository resolver from the build. Maybe local caches are hiding the problem for us. . Could you try checking out the branch `lb_add_back_jcenter` and see if that fixes the problem?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7636#issuecomment-1012346184:120,cache,caches,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636#issuecomment-1012346184,1,['cache'],['caches']
Performance,"I added -m to `gsutil cp` in a previous PR but missed the `gsutil mv` step post-`bq load` - so here that is. tested it from the command line, works well. also confirmed that it will throw an error if one (or more) files has an error:. ```; $ cat test_files_bucket.txt | gsutil -m mv -I gs://dsp-fieldeng-dev/test_mv/. If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. Copying gs://dsp-fieldeng-dev/test_cp/test1.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test2.txt [Content-Type=text/plain]...; CommandException: No URLs matched: gs://dsp-fieldeng-dev/test_cp/test4.txt; Copying gs://dsp-fieldeng-dev/test_cp/test3.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test5.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test6.txt [Content-Type=text/plain]...; Removing gs://dsp-fieldeng-dev/test_cp/test1.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test2.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test3.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test5.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test6.txt...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Operation completed over 5 objects/37.0 B.; CommandException: 1 file/object could not be transferred.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7129:84,load,load,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7129,1,['load'],['load']
Performance,I added a few fairly minor comments. Curious to see the performance implications.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/928#issuecomment-143605225:56,perform,performance,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/928#issuecomment-143605225,1,['perform'],['performance']
Performance,"I added a small test demonstrating how to broadcast the reference sequence here: https://github.com/broadinstitute/hellbender/compare/master...tomwhite:hadoop-references#diff-957e843e027a45b3ac07d9a6c0515534R42. This will need to be run on a cluster to see how it performs, but I'd like to understand better how it will fit into the work in the dr_read_preprocessing_pipeline_skeleton branch. What is the integration point ther do you think @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/567#issuecomment-113185670:264,perform,performs,264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-113185670,1,['perform'],['performs']
Performance,"I agree with @tedsharpe that `ctx.defaultParallelism()` is the recommended way to get the number of cores. If there's a race condition, then that is a bug that should be reported (stack trace if possible), so it can be fixed in Spark. Getting memory is harder. The `executorMemory()` method is probably the best bet (although it is not a public API any more), and for the local case could you use the JVM memory?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1947#issuecomment-233297988:120,race condition,race condition,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1947#issuecomment-233297988,1,['race condition'],['race condition']
Performance,"I also forgot to mention that this change needs an update for Hadoop-BAM due to a change in the way filesystems classes are loaded by Spark submit. @cmnbroad, would you be able to take a look at https://github.com/HadoopGenomics/Hadoop-BAM/pull/120, so I can merge it and do a new release?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2220#issuecomment-257583089:124,load,loaded,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2220#issuecomment-257583089,1,['load'],['loaded']
Performance,"I also just got this ConcurrentModificationException in HaplotypeCallerSparkIntegrationTest locally when running tests on one of my branches (this time in `testNonStrictVCFModeIsConsistentWithPastResults`, but the rest of the stack looks the same). I also vaguely recall seeing once before on travis on a branch where all I did was try to update miniconda to a newer version. My local branch doesn't have any dependency changes, so I suspect this is an existing issue that is just showing up intermittently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-602804665:21,Concurren,ConcurrentModificationException,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-602804665,1,['Concurren'],['ConcurrentModificationException']
Performance,"I am also getting this error on chromosome 11:; ```; 11:49:44.992 INFO gcnvkernel.postprocess.viterbi_segmentation - Segmenting contig (5/12) (contig name: 9)...; 11:49:44.996 INFO gcnvkernel.postprocess.viterbi_segmentation - Segmenting contig (6/12) (contig name: 11)... Stderr: Traceback (most recent call last):; File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/compile/function_module.py"", line 903, in __call__; self.fn() if output_subset is None else\; File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 963, in rval; r = p(n, [x[0] for x in i], o); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 952, in p; self, node); File ""scan_perform.pyx"", line 215, in theano.scan_module.scan_perform.perform; NotImplementedError: We didn't implemented yet the case where scan do 0 iteration. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/segment_gcnv_calls.6491270870870970325.py"", line 79, in <module>; viterbi_engine.write_copy_number_segments(); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/gcnvkernel/postprocess/viterbi_segmentation.py"", line 234, in write_copy_number_segments; for segment in self._viterbi_segments_generator():; File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/gcnvkernel/postprocess/viterbi_segmentation.py"", line 160, in _viterbi_segments_generator; log_prior_c, log_trans_contig_tcc, copy_number_log_emission_contig_tc); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/gcnvkernel/models/theano_hmm.py"", line 88, in perform_forward_backward; prev_log_posterior_tc, admixing_rate, temperature))); File ""/ngc/projects/gm/data/res",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5852#issuecomment-613371282:926,perform,perform,926,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5852#issuecomment-613371282,1,['perform'],['perform']
Performance,"I am also seeing this warning 3x with 4.0.11.0 on a cluster but outside of docker (centos 6). . ```; 18:05:08.861 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.11.0/install/bin/gatk-package-4.0.11.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 24, 2018 6:05:09 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.NoRouteToHostException: No route to host (Host unreachable); at java.net.PlainSocketImpl.socketConnect(Native Method); at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); at java.net.Socket.connect(Socket.java:589); at sun.net.NetworkClient.doConnect(NetworkClient.java:175); at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); at sun.net.www.http.HttpClient.New(HttpClient.java:339); at sun.net.www.http.HttpClient.New(HttpClient.java:357); at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156); at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050); at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:104); at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredential",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441873417:141,Load,Loading,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441873417,1,['Load'],['Loading']
Performance,"I am encountering a similar error: ; ```; Using GATK jar /nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /lustre/haven/proj/UTHSC0013/Tristan_GATK/reference/genome.fa -V gendb:///lustre/haven/proj/UTHSC0013/Tristan_GATK//DB/chr7 -G StandardAnnotation --use-new-qual-calculator -O /lustre/haven/proj/UTHSC0013/Tristan_GATK//gvcf//merged//joint_called_gvcfs_chr7.vcf; 23:15:47.053 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 23:15:47.249 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 07, 2020 11:15:49 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 23:15:49.543 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.545 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 23:15:49.546 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:15:49.547 INFO GenotypeGVCFs - Executing as hchen3@acf-knl002 on Linux v3.10.0-514.26.1.el7.x86_64 amd64; 23:15:49.548 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 23:15:49.548 INFO GenotypeGVCFs - Start Date/Time: January 7, 2020 11:15:47 PM EST; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.551 INFO GenotypeGVCFs - HTSJDK Version: 2.19.0; 23:15:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6340#issuecomment-571886057:831,Load,Loading,831,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6340#issuecomment-571886057,1,['Load'],['Loading']
Performance,I am getting the following exception when I set `--minimum-mapping-quality` to 60 (but not 50). . ```console; $ gatk --version; ...; The Genome Analysis Toolkit (GATK) v4.2.0.0; HTSJDK Version: 2.24.0; Picard Version: 2.25.0; ```. ```console; $ gatk HaplotypeCaller \; -I in.bam \; -L chr7:145945238-145945238 \; -stand-call-conf 0 \; --disable-optimizations \; --force-active -O out.vcf \; --reference /path/to/ucsc.hg19.fasta \; --minimum-mapping-quality 60;; ...; java.lang.IllegalStateException: There is no variation present.; 	at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyRegionTrimmer$Result.getVariantRegion(AssemblyRegionTrimmer.java:108); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:595); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:273); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. <details>; <summary>test.sam</summary>. ```; @HD	VN:1.6	SO:coordinate; @SQ	SN:chr1	LN:249250621; @SQ	SN:chr2	LN:243199373; @SQ	SN:chr3	LN:198022430; @SQ	SN:chr4	LN:191154276; @SQ	SN:chr5	LN:18,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7123:345,optimiz,optimizations,345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7123,1,['optimiz'],['optimizations']
Performance,I am not sure if any performance gained by having Spark versions of these tools is worth the support cost. Can reopen if we feel otherwise.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4198#issuecomment-900613847:21,perform,performance,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4198#issuecomment-900613847,1,['perform'],['performance']
Performance,"I am running GATK GenotypeGVCFs, v4.2.6.1. I am trying to call Genotypes on a GenomicsDB workspace with about 500 WGS samples. Note, this is the macaque MMul10 genome, so it has 2,939 contigs (including unplaced). We've run commands like this quite a lot before, though we periodically do have issues like this. We can consolidate on this workspace prior to running this (using a standalone tool @nalinigans provided on #7674). As you can see we ran java with relatively low RAM, but left ~150G for the C++ layer. I'm surprised this isnt good enough. . I'm going to try to interactively inspect this, but the error is from slurm killing my job, not a java memory error, which I believe means the -Xmx 92G isnt getting exceeded. I could be mistaken though. You'll also see: 1) I'm using --force-output-intervals, 2) I'm giving it -XL to excluded repetitive regions (and therefore also skipping some of the more gnarly and memory-intensive sites), and 3) I'm giving it a fairly small -L interval list (this is split into 750 jobs/genome). . ```; java8 \; -Djava.io.tmpdir=<folder> \; -Xmx92g -Xms92g -Xss2m \; -jar GenomeAnalysisTK4.jar \; GenotypeGVCFs \; -R 128_Mmul_10.fasta \; --variant gendb:///<path>/WGS_v2_db03_500.gdb \; -O WGS_v2_db03_500.temp.vcf.gz \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 \; -XL NCBI_Mmul_10.softmask.bed \; --max-alternate-alleles 6 \; --genomicsdb-max-alternate-alleles 9 \; --force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; -L 14:37234750-41196525 \; --only-output-calls-starting-in-intervals \; --genomicsdb-shared-posixfs-optimizations; ```. Each job gets about 250K to 800K variants into the data, and then they pretty consistently start to exceed memory and get killed. . Does anyone have suggestions on debugging or troubleshooting steps? Thanks in advance for any help.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968:1602,optimiz,optimizations,1602,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968,1,['optimiz'],['optimizations']
Performance,"I am running GATK4.3.0.0 in Conda environment on my Linux server. It seems that there are some trouble related to libgkl_utils.so, which could not be loaded and prevented PairHmm to be used. Curiously to me, the seemingly similar libgkl_compression.so could be normally loaded, though in the same tmp dir with the libgkl_utils.so. Can anyone help? Bug report is as follow:. ## Bug Report. ```; 12:00:40.974 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/GPUFS/sysu_mhwang_1/.conda/envs/gatk/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:00:43.082 INFO HaplotypeCaller - ------------------------------------------------------------; 12:00:43.082 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0; 12:00:43.083 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:00:43.083 INFO HaplotypeCaller - Executing as sysu_mhwang_1@zwei-ubuntu1804-9195440-86794746d6-89lcr on Linux v3.10.0-957.el7.x86_64 amd64; 12:00:43.083 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v16.0.1+9-24; 12:00:43.083 INFO HaplotypeCaller - Start Date/Time: February 9, 2023 at 12:00:40 PM GMT; 12:00:43.083 INFO HaplotypeCaller - ------------------------------------------------------------; 12:00:43.083 INFO HaplotypeCaller - ------------------------------------------------------------; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Version: 3.0.1; 12:00:43.084 INFO HaplotypeCaller - Picard Version: 2.27.5; 12:00:43.084 INFO HaplotypeCaller - Built for Spark Version: 2.4.5; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:00:43.084 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:00:43.084 INFO HaplotypeC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8194:150,load,loaded,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194,3,"['Load', 'load']","['Loading', 'loaded']"
Performance,"I am running sortsam and getting error Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2299:116,Load,LoadSnappy,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2299,2,['Load'],['LoadSnappy']
Performance,"I am trying to do SNPs call using GATK but I am getting below error. Any idea how can I solve this error? . (base) glier_ubuntu@glierubuntu-Precision-7920-Tower:/media/glier_ubuntu/4TB/Javad_Final/7gatk/ScriptforSNP$ call_snps_gatk_firstpass.sh 6 '/media/glier_ubuntu/4TB/Javad_Final/5-trinity/Fastajavad_Trinity/Trinity.fasta' '/media/glier_ubuntu/4TB/Javad_Final/6bwa/2/filtered_merged.bam' ; Using GATK jar /home/glier_ubuntu/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -jar /home/glier_ubuntu/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar CreateSequenceDictionary -R /media/glier_ubuntu/4TB/Javad_Final/5-trinity/Fastajavad_Trinity/Trinity.fasta -O /media/glier_ubuntu/4TB/Javad_Final/5-trinity/Fastajavad_Trinity/Trinity.dict; 15:46:56.167 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/glier_ubuntu/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Wed May 13 15:46:56 EDT 2020] CreateSequenceDictionary --OUTPUT /media/glier_ubuntu/4TB/Javad_Final/5-trinity/Fastajavad_Trinity/Trinity.dict --REFERENCE /media/glier_ubuntu/4TB/Javad_Final/5-trinity/Fastajavad_Trinity/Trinity.fasta --TRUNCATE_NAMES_AT_WHITESPACE true --NUM_SEQUENCES 2147483647 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; May 13, 2020 3:46:57 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Wed May 13 15:46:57 EDT 2020] Executing as glier_ubuntu@glierubuntu-Precision-7920-Tower on Linux 4.15.0-99-generic am",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6604:948,Load,Loading,948,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6604,1,['Load'],['Loading']
Performance,"I am using ASEReadCounter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) :; a) GATK version used: 4.1.8.1; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \; -jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \; ASEReadCounter \; -L scattered.interval_list \; -R Homo_sapiens_assembly19.fasta \; -V 1000G_phase1.snps.high_confidence.b37.vcf.gz \; -I downsample_10k.bam \; -O output.txt --verbosity INFO . c) Entire error log:; 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/broad/software/free/Linux/redhat_7_x86_64/pkgs/gatk_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 14, 2021 7:13:26 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1; 19:13:26.218 INFO ASEReadCounter - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:13:26.219 INFO ASEReadCounter - Executing as cbao@uger-c009.broadinstitute.org on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 19:13:26.219 INFO ASEReadCounter - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_181-b13; 19:13:26.219 INFO ASEReadCounter - Start Date/Time: June 14, 2021 7:13:25 PM ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7314:699,Load,Loading,699,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7314,1,['Load'],['Loading']
Performance,"I am using the GATK GenotypeGVCFs command to generate vcf through gvcf, but two samples in the same batch of individuals have the following error:; ```; Using GATK jar /share/org/YZWL/yzwl_hanxt/software/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/org/YZWL/yzwl_hanxt/software/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar GenotypeGVCFs -R /share/org/YZWL/yzwl_hanxt/leizhou/ref/ARS1.2_chr30_2.fasta -V H-4.g.vcf.gz -O /share/org/YZWL/yzwl_hanxt/leizhou/variant/temp/H-4.vcf.gz; 09:55:48.764 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/org/YZWL/yzwl_hanxt/software/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:55:48.863 INFO GenotypeGVCFs - ------------------------------------------------------------; 09:55:48.865 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.6.0.0; 09:55:48.865 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:55:48.865 INFO GenotypeGVCFs - Executing as yzwl_hanxt@c01n0583 on Linux v4.18.0-513.5.1.el8_9.x86_64 amd64; 09:55:48.865 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v17.0.12+8-LTS-286; 09:55:48.866 INFO GenotypeGVCFs - Start Date/Time: September 3, 2024 at 9:55:48 AM CST; 09:55:48.866 INFO GenotypeGVCFs - ------------------------------------------------------------; 09:55:48.866 INFO GenotypeGVCFs - ------------------------------------------------------------; 09:55:48.866 INFO GenotypeGVCFs - HTSJDK Version: 4.1.1; 09:55:48.866 INFO GenotypeGVCFs - Picard Version: 3.2.0; 09:55:48.866 INFO GenotypeGVCFs - Built for Spark Version: 3.5.0; 09:55:48.866 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:55:48.867 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8969:708,Load,Loading,708,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8969,1,['Load'],['Loading']
Performance,I ask that because for htsjdk defaults they must be system properties and they're final and set statically on load so mucking about resetting system properties after the JVM started already is going to be a bit of a fiddly ordering nightmare. . [Stack overflow](http://stackoverflow.com/questions/6736235/set-java-system-properties-with-a-configuration-file) doesn't seem to think that it's possible to initialize them from a file.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267126704:110,load,load,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267126704,1,['load'],['load']
Performance,I believe we discovered that `IntervalSkipList` had worse performance than htsjdk's `OverlapDetector`. It's probably best to remove IntervalSkipList in this case and prefer the htsjdk version.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3608:58,perform,performance,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3608,1,['perform'],['performance']
Performance,"I brew installed openssl, and now there is a different unresolved dependency:. java.lang.UnsatisfiedLinkError: /private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb6507285380909029818.dylib: dlopen(/private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb6507285380909029818.dylib, 1): Library not loaded: **/usr/local/opt/libcsv/lib/libcsv.3.dylib**; Referenced from: /private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb6507285380909029818.dylib; Reason: image not found",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294232254:340,load,loaded,340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294232254,1,['load'],['loaded']
Performance,"I called variants using gatk4, and then tried to perform VariantFiltration:. gatk-4.0.5.1/gatk VariantFiltration -R data/genome.fasta -V variants/6753_12-15-2015_first_pass_filtered.vcf -filter 'QD > 2 && FS > 60 && SOR < 3 && MQ > 40 && MQRankSum > -3 && ReadPosRankSum > -4' -output variants/6753_12-15-2015_second_pass_filtered.vcf -filter-name ""default"" . However, it complains with a java.lang.NumberFormatException:. Using GATK jar gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar VariantFiltration -R data/genome.fasta -V variants/6753_12-15-2015_first_pass_filtered.vcf -filter QD > 2 && FS > 60 && SOR < 3 && MQ > 40 && MQRankSum > -3 && ReadPosRankSum > -4 -output variants/6753_12-15-2015_second_pass_filtered.vcf -filter-name default; 15:42:33.964 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; 15:42:34.114 INFO VariantFiltration - ------------------------------------------------------------; 15:42:34.115 INFO VariantFiltration - The Genome Analysis Toolkit (GATK) v4.0.5.1; 15:42:34.115 INFO VariantFiltration - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:42:34.115 INFO VariantFiltration - Executing as sherlock@DN52ehae.SUNet on Mac OS X v10.13.5 x86_64; 15:42:34.116 INFO VariantFiltration - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_91-b14; 15:42:34.116 INFO VariantFiltration - Start Date/Time: June 15, 2018 3:42:33 PM PDT; 15:42:34.116 INFO VariantFiltration - ------------------------------------------------------------; 15:42:34.116 INFO VariantFiltration - ------------------------------------------------------------; 15:42:34.117 INFO VariantFiltration - HTSJDK Version: 2.15.1; 15:42:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4921:1218,Load,Loading,1218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4921,1,['Load'],['Loading']
Performance,"I can actually replicate this with an interval list with two intervals: ; ```; chr1	11719	18516	+	.; chr2	38664	202755	+	.; ```; throws; ```; org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:chr1:11719-18516 queried with: chr2:38664-202755; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:816); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:136); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:563); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. We typically run with `--reader-threads 5` in the pipeline, but if I change it to 1 I can get it to run. That's not a longterm solution, but hopefully it's a good hint and it's a good enough workaround for me for the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5300#issuecomment-438818966:627,concurren,concurrent,627,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300#issuecomment-438818966,3,['concurren'],['concurrent']
Performance,"I can confirm that in my own extensive runs of the tools I've seen it sometimes get hung when out of memory instead of exiting (resulting in bad data for that run). My own benchmarking was with PrintReads on a large file, using various cache buffer sizes. I remember seeing an increase in performance up to about 50MB buffer size and then it flattened out. I would expect a more CPU-intensive (or a more heavily loaded machine) would reduce the impact of the buffer size as I/O ceases to be the bottleneck. I also ran experiments on a 1-cpu machine with VCF and loading a single interval, which I think matches what you are asking about. In that experiment 10MB was enough, adding to the cache did not bring any improvement and of course too large a cache leads to running out of memory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298972380:236,cache,cache,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298972380,7,"['bottleneck', 'cache', 'load', 'perform']","['bottleneck', 'cache', 'loaded', 'loading', 'performance']"
Performance,"I can confirm that, GATK 4.1.3.0 with CollectRawWgsMetrics; ```; Exception in thread ""main"" java.lang.IllegalArgumentException: The requested position is not covered by this StartEdgingRecordAndOffset object.; ```. **Cmdline:**; ```; picard -Xms4000m \; CollectRawWgsMetrics \; INPUT=example.bam; VALIDATION_STRINGENCY=SILENT \; REFERENCE_SEQUENCE=Homo_sapiens_assembly38.fasta \; INCLUDE_BQ_HISTOGRAM=true \; INTERVALS=wgs_coverage_regions.hg38.interval_list \; OUTPUT=example.raw_wgs_metrics \; USE_FAST_ALGORITHM=true \; READ_LENGTH=250; ```. **Output:**; ```; 18:48:46.330 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/picard-2.20.7-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so; [Fri Sep 20 18:48:46 GMT 2019] CollectRawWgsMetrics INPUT=example.bam; [Fri Sep 20 18:48:46 GMT 2019] Executing as root@98e13b9f4ef1 on Linux 4.19.44+ amd64; OpenJDK 64-Bit Server VM 1.8.0_152-release-1056-b12; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.20.7-SNAPSHOT; [Fri Sep 20 18:49:00 GMT 2019] picard.analysis.CollectRawWgsMetrics done. Elapsed time: 0.24 minutes.; Runtime.totalMemory()=4054515712; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; Exception in thread ""main"" java.lang.IllegalArgumentException: The requested position is not covered by this StartEdgingRecordAndOffset object.; at htsjdk.samtools.util.AbstractRecordAndOffset.validateOffset(AbstractRecordAndOffset.java:146); at htsjdk.samtools.util.EdgingRecordAndOffset$StartEdgingRecordAndOffset.getBaseQuality(EdgingRecordAndOffset.java:112); at picard.analysis.FastWgsMetricsCollector.excludeByQuality(FastWgsMetricsCollector.java:189); at picard.analysis.FastWgsMetricsCollector.processRecord(FastWgsMetricsCollector.java:144); at picard.analysis.FastWgsMetricsCollector.addInfo(FastWgsMetricsCollector.java:105); at picard.analysis.WgsMetricsProcessorImpl.processFile(WgsMetricsProcessorImpl.java:93); at picard.an",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6163#issuecomment-533770047:604,Load,Loading,604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6163#issuecomment-533770047,1,['Load'],['Loading']
Performance,"I can reproduce this on a cluster running CDH 5.7.0. It's the same as this report:. https://community.cloudera.com/t5/Advanced-Analytics-Apache-Spark/Override-libraries-for-spark/m-p/32146. Despite the conversation being marked as ""solved"" I'm not sure what the resolution was. This error seems to occur if snappy is loaded multiple times:. http://mail-archives.apache.org/mod_mbox/incubator-kafka-users/201208.mbox/%3CCA+sHyy98F8JoQf3wDKKNCy8jcoPiddTBt4_PXr8J1zeN2Jq6AA@mail.gmail.com%3E. So its possible that the snappy jar (or jars) is on the classpath multiple times. It should be provided by CDH, and it's not in the GATK spark JAR - so I'm not sure where it's being duplicated.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1873#issuecomment-229098679:317,load,loaded,317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1873#issuecomment-229098679,1,['load'],['loaded']
Performance,"I can't reproduce this yet. I tried downloading the jar, unzipping it, and running the example command you gave, but I can't reproduce what you're seeing. I modified it for my local files:; ```; java -jar gatk-package-4.2.5.0-local.jar \; GenotypeGVCFs \; -R /Users/louisb/Workspace/gatk/src/test/resources/large/Homo_sapiens_assembly19.fasta.gz \; --variant gendb:///Users/louisb/Workspace/gatk/output \; -O out.vcf \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 \; --max-alternate-alleles 6 \; --force-output-intervals 20 \; -L 20 \; --only-output-calls-starting-in-intervals \; --genomicsdb-shared-posixfs-optimizations; ```; It runs to completion on my machine. ; My md5sum matches yours so that's not the problem. It's not clear to me what's going on here. Are the previous releases working on your cluster still?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042010522:627,optimiz,optimizations,627,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042010522,1,['optimiz'],['optimizations']
Performance,"I can, this only happens on 10 of our 2000 samples (only in WES) none of our 600 WGS seems to have the same issue. It is always on some small contig (you can see here range is 544, but all cases are small ranges like this one). Everything is the default mutect2 pipeline and params (e.g. [gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta](https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0?prefix=Homo_sapiens_assembly38.fasta&authuser=jkalfon%40broadinstitute.org)) : except the interval file: [gs://ccleparams/region_file_wgs.list](https://console.cloud.google.com/storage/browser/ccleparams?prefix=region_file_wgs.list&authuser=jkalfon%40broadinstitute.org); GATK 4.2.6.1. . Here is the VCF file to annotate `gs://ccleparams/test/CDS-2jucw0.hg38-filtered.vcf.gz`. Here is the stacktrace:. ```; ....; 10:53:39.044 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/2145; 10:53:39.249 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/1069225; 10:53:39.520 INFO Funcotator - Shutting down engine; [July 12, 2022 10:53:39 AM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 115.46 minutes.; Runtime.totalMemory()=2050490368; java.lang.StringIndexOutOfBoundsException: String index out of range: 544; at java.lang.String.substring(String.java:1963); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.initializeForInsertion(ProteinChangeInfo.java:293); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.<init>(ProteinChangeInfo.java:101); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.create(ProteinChangeInfo.java:399); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:2054); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createCodingRegionFunc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1182102653:945,cache,cache,945,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1182102653,1,['cache'],['cache']
Performance,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8237:1135,perform,performance,1135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237,1,['perform'],['performance']
Performance,"I checked in the spark JAR, and `META-INF/services/java.nio.file.spi.FileSystemProvider` contains . ```; com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider; hdfs.jsr203.HadoopFileSystemProvider; ```. So it looks like the service loader file is being created correctly. I tried to reproduce on GCS by running (essentially the same as @vdauwera's command.). ```; time ./gatk-launch ApplyBQSRSpark \; -I gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R gs://gatk-legacy-bundles/b37/human_g1k_v37.2bit \; -O gs://gatk-demo-tom/TEST/gatk4-spark/recalibrated.bam \; -bqsr gs://gatk-demo/TEST/gatk4-spark/recalibration.table \; -apiKey $GOOGLE_APPLICATION_CREDENTIALS \; -- \; --sparkRunner GCS \; --cluster cluster-tom \; --num-executors 40 \; --executor-cores 4 \; --executor-memory 10g; ```. But I got another error earlier on. Any ideas what this could be? (I can see the input bam with `gsutil cp`). ```; org.broadinstitute.hellbender.exceptions.UserException: A USER ERROR has occurred: Failed to read bam header from gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.bam; Caused by:Error reading null at position 0; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:182); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:376); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:357); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:347); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.insta",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-264909676:246,load,loader,246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-264909676,1,['load'],['loader']
Performance,I cleaned up the mutect2 wdl and added multi-sample support. I also optimized resource usage and exposed the memory parameters: https://github.com/phylyc/gatk4-somatic-snvs-indels,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7532#issuecomment-1125321665:68,optimiz,optimized,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532#issuecomment-1125321665,1,['optimiz'],['optimized']
Performance,"I copied 60 BAM file to an interactive Linux server with 768 GB physical RAM and eighty cores and used version 4.5.0.0. ```; %Cpu(s): 1.3 us, 0.0 sy, 0.1 ni, 98.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st; GiB Mem : 754.5 total, 52.1 free, 107.3 used, 600.3 buff/cache ; GiB Swap: 931.3 total, 924.9 free, 6.4 used. 647.3 avail Mem . PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND ; 171365 dario 20 0 35.0g 31.3g 23040 S 100.0 4.1 32:18.12 java ; ```. I removed `-Xmx` and using `top` to see the process is consistently at about 32 GB. So, `-Xmx` is irrelevant to the problem. ```; 12:15:04.531 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute; 12:57:32.208 INFO GetPileupSummaries - Shutting down engine; [January 13, 2024 at 12:57:32 PM AEDT] org.broadinstitute.hellbender.tools.walkers.contamination.GetPileupSummaries done. Elapsed time: 50.36 minutes.; Runtime.totalMemory()=20753416192; java.lang.OutOfMemoryError: Java heap space: failed reallocation of scalar replaced objects; ```. What does ""reallocation of scalar replaced objects"" mean? I don't think it could possibly have run out of memory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8654#issuecomment-1890249060:257,cache,cache,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8654#issuecomment-1890249060,1,['cache'],['cache']
Performance,"I created a panel of normals from 90 WGS TCGA samples with 250bp (~11.5M) bins, which took **~57 minutes** total and produced an **11GB PoN** (this file includes all of the input read counts---which take up 20GB as a combined TSV file and a whopping 63GB as individual TSV files---as well as the eigenvectors, filtering results, etc.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:961,optimiz,optimize,961,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503,1,['optimiz'],['optimize']
Performance,I definitely say kill the pretty streaming code if it's harming performance in these critical sections -- no shame in a straightforward loop!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2241#issuecomment-257582318:64,perform,performance,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2241#issuecomment-257582318,1,['perform'],['performance']
Performance,"I did a couple of quick tests, with GATK and htsjdk, including tight loops that just reset the header 10M times, and I see virtually no performance difference between strict and not. i.e, iterating through a bam with 500k records and just calling set vs strict.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1541#issuecomment-191898375:136,perform,performance,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1541#issuecomment-191898375,1,['perform'],['performance']
Performance,"I did a quick scalability ministudy - this seems to scale well up to 12 cores and then diminishes due to Amdahl's law I think that's fine. There is no diminished runtime due to OMP overhead when on 1 core. Note that our cluster on which I wan these was not empty (a few of the 48 cores were in use) and do this is just a ballpark estimate of scalability, in particular 24 was worse than 12 probably due to interference. Based on this I think OMP is a good idea and it's going to work on 1 CPU too. limited to 1 OMP thread, using 10GB of RAM. ```; real 2m15.621s; user 3m17.269s; Total compute time in PairHMM computeLogLikelihoods() : 50.964700625000006; ```. ---. limited to 1 OMP thread, using 32GB of RAM . ```; real 1m46.597s; user 3m17.363s; Total compute time in PairHMM computeLogLikelihoods() : 45.797104454; ```. limited to 2 OMP threads, using 32GB of RAM. ```; real 1m26.310s; user 3m24.636s; Total compute time in PairHMM computeLogLikelihoods() : 23.790980359000002; ```. limited to 4 OMP threads, using 32GB of RAM. ```; real 1m15.298s; user 3m29.834s; Total compute time in PairHMM computeLogLikelihoods() : 11.332445694; ```. limited to 6 OMP threads, using 32GB of RAM. ```; real 1m14.015s; user 3m20.876s; Total compute time in PairHMM computeLogLikelihoods() : 7.862075811; ```. limited to 12 OMP threads, using 32GB of RAM. ```; real 1m6.370s ; user 3m42.340s; Total compute time in PairHMM computeLogLikelihoods() : 4.585800097; ```. limited to 24 OMP threads, using 32GB of RAM (clearly, OMP hits the limit here). ```; real 1m8.779s; user 4m15.489s; Total compute time in PairHMM computeLogLikelihoods() : 3.047581173; ```. limited to 48 OMP threads, using 32GB of RAM (worse than 12 threads). ```; real 1m11.535s; user 6m26.100s; Total compute time in PairHMM computeLogLikelihoods() : 4.112299148; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1800#issuecomment-218810496:14,scalab,scalability,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1800#issuecomment-218810496,2,['scalab'],['scalability']
Performance,"I did, but then I rethought it and decided that I think it's too fragile to do it that way: You have to guarantee that all classes that might want to be registered are loaded and initialized before you instantiate the SparkConf. The problem is that it varies among JVM implementations exactly when that (class loading and initialization) happens. Some JVMs do the whole mess, chasing all references down from main recursively at the beginning, others are as lazy as possible and initialize only when actually traversing a reference for the first time. We could use that technique to ""inject"" a set of registrations into the GATKRegistrator from each main class. But since it's unreliable to do it in arbitrary classes, it seemed more straightforward to just let the normal object oriented method of overrides handle the problem, since it will need to be done only by direct subclasses, anyway. But I'm certainly open to other solutions, so long as we provide the functionality.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1767#issuecomment-214473964:168,load,loaded,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1767#issuecomment-214473964,2,['load'],"['loaded', 'loading']"
Performance,"I disagree, I think any common operation that you would want to perform on a `Locatable` should be lifted up to it. This was bad in java n < 8, because every implementing class would have to provide it's own implementation. Now that we have default methods I think the preference should be to put utility methods directly in the interface.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/305#issuecomment-79205767:64,perform,perform,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/305#issuecomment-79205767,1,['perform'],['perform']
Performance,"I discovered that it actually is possible to load unmapped reads with the existing google code code. I had thought it didn't include the ability to do so, but after further digging, there's a way to do it that we just haven't exposed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/560#issuecomment-114228839:45,load,load,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/560#issuecomment-114228839,1,['load'],['load']
Performance,I don't doubt that there could be issues caused by reads with previously filled caches. Ultimately this shouldn't have too significant an impact except in very pathological circumstances with highly repetitive regions or reads that hang beyond a certain length into the next region and happen to have had good looking indel sites without the cirgar actually containing any indels for that read. This should eliminate any of these circumstances entirely so we can be sure the cache is clear before every call. . Fixes #5908,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5911:80,cache,caches,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5911,2,['cache'],"['cache', 'caches']"
Performance,"I don't know if it is an issue or I am doing something wrong, but I report my experience in case might be useful for GATK developers:. I took some Performance Analysis for the tool BQSRPipelineSpark (when GATK 4.0 was still in Beta, for understanding there was still `gatk-launch` as command to execute the tool), processing a Whole Exome Sequencing Genome of about 14 GB (obviously after applying FastqToSam and BwaAndMarkDuplicatesPipelineSpark) and it took about 70 minutes. I tried to use the same tool in the same VM, with the same input data and now takes 626,95 minutes (a considerable difference of execution time). Is it normal or am I doing something wrong?. To be sure of what I am saying, I re-executed the old version tool with `gatk-launch` and it takes 65 minutes for example",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4479:147,Perform,Performance,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4479,1,['Perform'],['Performance']
Performance,"I don't think that hiding/disable arguments would work in every case: sometimes, an argument shouldn't be exposed but still available to set programmatically, or maybe just reduce visibility making it `@Hidden` and/or `@Advance`. What is the problem of making an interface for the top-level argument to the GATK? Changing the interface or the `CommadnLineProgram` has the same effect, but the API user can still behave the same as before. It is much more extensible and downstream-friendly. What's about making the `CLPConfigurationArgumentCollection` an interface always returning defaults to be able to change it in a proper way? The cycle of development of a new argument will be: 1) add a new method to the interface with a default returning what will be expected from the previous behaviour, 2) add and return by the argument in the GATK implementation, 3) use the getter in the CLP for perform the operation. This only adds the first point, and operating in 3 classes instead of 3. For API user it is really easy to maintain the previous behavior when upgrading the dependency by just using their own implementation of the class, or include the top-level new arguments by using the GATK implementation. It is much more flexible and extensible (I always think about GATK also as a library). In addition, I think that this approach is also important for evolving GATK. For example, if a new top-level argument is tagged as experimental (still not supported but requested in Barclay), removing it would allow to keep the interface (no version bump) the same and final users can still operate with the experimental argument. The same applies to the `GATKTool` base class (https://github.com/broadinstitute/gatk/issues/4341), and for downstream projects the aim should be to be able to extend safely the `CommandLineProgram` directly to implement their own toolkit using the powerful GATK framework.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003:892,perform,perform,892,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003,1,['perform'],['perform']
Performance,"I don't understand why you say ""Using a singleton should not be needed, since mocks are sufficient"". The existing code uses both a singleton and mocks. There are two issues at play here. (1) We want to have a way to make sure everyone gets the mock, without having to pass it as argument deep into the call hierarchy, and; (2) We want to cache the results of initializing the RefAPISource because it takes a long time. Let's start with point 1. When `BaseRecalibratorDataflow` calls `RefAPISource.getInstance()` or equivalent, how is `BaseRecalibratorDataflowIntegrationTest` going to ensure that `BaseRecalibratorDataflow` gets the mock instead of the internet-fetching version? ; (and by ""or equivalent"" I mean `new ReferenceDataflowSource(...)`). ; We still need a way to stash the mock somewhere for the user code to retrieve. Right now we do this by setting the ""singleton"" value. ReferenceDataflowSource is a wrapper instead, so we can add a method to stash the mock on a static member - but that isn't there right now so I'm surprised you seem to think this PR completely satisfies the uses cases that the singleton does.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/804#issuecomment-130831140:338,cache,cache,338,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/804#issuecomment-130831140,1,['cache'],['cache']
Performance,"I feel like this is going to be problematic in a different way than what @magicDGS is mentioning. We expect many versions of gatk to be compatible with the same python environment. Also for performance reasons we want to start avoid rebuilding the conda environment on every push and bake it into the base docker instead. This change means we definitely have to build it every time. . It feels like we need something more sophisticated. Instead of stamping the conda environment with the gatk version that matches it, maybe we should be stamping the gatk jar and the conda environment with some version based on the conda.env? Maybe we can do something like taking the md5 of the conda.yml and pushing that into both the jar manifest and the conda environment in some way? I'm guessing this scheme has an issue with the actual python code in the gatk since I think that's installed with conda as well? I'd really like to be able to preinstall the various dependencies though and then only update the code that's part of the gatk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5081#issuecomment-411214721:190,perform,performance,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5081#issuecomment-411214721,1,['perform'],['performance']
Performance,I found a sample that fails when CalculateContamination is run. The exception I get is:; org.apache.commons.math3.exception.TooManyEvaluationsException: illegal state: maximal count (20) exceeded: evaluations; 	at org.apache.commons.math3.optim.BaseOptimizer$MaxEvalCallback.trigger(BaseOptimizer.java:242); 	at org.apache.commons.math3.util.Incrementor.incrementCount(Incrementor.java:155); 	at org.apache.commons.math3.optim.BaseOptimizer.incrementEvaluationCount(BaseOptimizer.java:191); 	at org.apache.commons.math3.optim.univariate.UnivariateOptimizer.computeObjectiveValue(UnivariateOptimizer.java:148); 	at org.apache.commons.math3.optim.univariate.BrentOptimizer.doOptimize(BrentOptimizer.java:225); 	at org.apache.commons.math3.optim.univariate.BrentOptimizer.doOptimize(BrentOptimizer.java:43); 	at org.apache.commons.math3.optim.BaseOptimizer.optimize(BaseOptimizer.java:153); 	at org.apache.commons.math3.optim.univariate.UnivariateOptimizer.optimize(UnivariateOptimizer.java:70); 	at org.broadinstitute.hellbender.utils.OptimizationUtils.max(OptimizationUtils.java:40); 	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationModel.lambda$calculateContamination$13(ContaminationModel.java:214); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationModel.calculateContamination(ContaminationModel.java:215); 	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationModel.<init>(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6282:854,optimiz,optimize,854,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6282,2,['optimiz'],['optimize']
Performance,"I found that if an indel + snp combination straddles a shard boundary GenotypeGVCFs will produce different calls. More info:. https://gatkforums.broadinstitute.org/gatk/discussion/comment/58230. This is in GATK 3.7. I do not know how this issue is related, would it have been merged in 3.7? https://github.com/broadinstitute/gatk/issues/2735. What is the expected behaviour of sharding in this circumstance? I am unable to perform the test using gatk4+ as it would require immense amounts of recomputation, just to confirm a bug report. Hopefully the devs know if this is expected behaviour or has been fixed already.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5905:423,perform,perform,423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5905,1,['perform'],['perform']
Performance,"I found this online., this might be the reason why use include spark is different from system spark in this case. In spark, when i try to cast the variable to the same 'class' (with exact the same class name, in this case, you could try to cast any ""Variant"" to ""Variant"" in BroadcastJoinReadsWithVariants ), it always throw out the class exception error. It is because that ""The equality of two classes in Java depends on the fully qualified name and the class loader that loaded it.""; In system spark, it might split the JVM, which means the broadcast variable may cause problem to identify if it is the same class?. This is what i test in spark:; List<Variant> vs = variants.take(10);; MinimalVariant v = (MinimalVariant) vs.get(0);. this will throw ClassCastException error... http://stackoverflow.com/questions/826319/classcastexception-when-casting-to-the-same-class",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1386#issuecomment-166160181:462,load,loader,462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1386#issuecomment-166160181,2,['load'],"['loaded', 'loader']"
Performance,"I got the following error when running on a cluster (Centos 6.6). The file _/lib64/libz.so.1_ is symlinked to version 1.2.3, which is too low (<1.2.3.3). When I rebuilt jbwa locally it worked fine. ```; java.lang.UnsatisfiedLinkError: /data/11/yarn/nm/usercache/tom/appcache/application_1460561740089_0118/container_1460561740089_0118_01_000002/tmp/libbwajni.5713518835392178075.so: /lib64/libz.so.1: version `ZLIB_1.2.3.3' not found (required by /data/11/yarn/nm/usercache/tom/appcache/application_1460561740089_0118/container_1460561740089_0118_01_000002/tmp/libbwajni.5713518835392178075.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1937); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1822); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at org.broadinstitute.hellbender.utils.NativeUtils.loadLibraryFromClasspath(NativeUtils.java:63) ; at org.broadinstitute.hellbender.utils.bwa.BWANativeLibrary.load(BWANativeLibrary.java:14); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1916:635,load,load,635,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1916,5,['load'],"['load', 'loadLibrary', 'loadLibraryFromClasspath']"
Performance,"I guess I could, but what is GATKTool contract... is it supposed to only perform reference driven analysis? I thought that is what a Walker is.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-577247790:73,perform,perform,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-577247790,1,['perform'],['perform']
Performance,I have also stumbled over this. I am adding a detailed error log.; I think that the incompatibility of accelerated PairHMM with a tmp directory mounted noexec should be mentioned in ; the installation requirements. I found it well-documented in [the troubleshooting section](https://gatk.broadinstitute.org/hc/en-us/articles/18965297287067-How-to-setup-and-use-temporary-folder-for-GATK-local-execution). But everyone with this setup will experience falling back to the slow implementation for no other reason. . ```; INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/; miniconda2/envs/polyploidPhasing/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils9418239050694741169.so: /tmp/libgkl_utils9; 418239050694741169.so: failed to map segment from shared object: Operation not permitted); WARN IntelPairHmm - Intel GKL Utils not loaded; PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8453#issuecomment-1905717389:545,Load,Loading,545,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8453#issuecomment-1905717389,4,"['Load', 'load', 'multi-thread']","['Loading', 'load', 'loaded', 'multi-threaded']"
Performance,"I have found an error when using GATK4 Mutect2 ,error as follow,thanks and waitting your reply; Using GATK jar /home/vip/biosoft/gatk/gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx10g -jar /home/vip/biosoft/gatk/gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar Mutect2 -R /data/bigbiosoft/GATK/resources/bundle/hg19/ucsc.hg19.fasta -I 02_bamdst/zhaoxuelan.sorted.algn.bam --tumor zhaoxuelan --germline-resource /home/vip/data/Mutect2/af-only-gnomad.raw.sites.hg19.vcf.gz --max-reads-per-alignment-start 0 -L /home/vip/lxdata/bed/lung9_gene_format.bed -O 03_somatic/zhaoxuelan.sorted.algn.bam.raw.vcf --af-of-alleles-not-in-resource 0.00003125 --min-base-quality-score 20 --read-filter MateOnSameContigOrNoMappedMateReadFilter --create-output-variant-md5; 16:46:49.608 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/vip/biosoft/gatk/gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 16:46:49.698 INFO Mutect2 - ------------------------------------------------------------; 16:46:49.698 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.0.5.1; 16:46:49.698 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:46:49.698 INFO Mutect2 - Executing as vip@zjm-System-Product-Name on Linux v4.18.0-25-generic amd64; 16:46:49.698 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10; 16:46:49.699 INFO Mutect2 - Start Date/Time: November 6, 2019 4:46:49 PM CST; 16:46:49.699 INFO Mutect2 - ------------------------------------------------------------; 16:46:49.699 INFO Mutect2 - ------------------------------------------------------------; 16:46:49.699 INFO Mutect2 - HTSJDK Version: 2.15.1; 16:46:49.699 INFO Mutect2 - Picard Version: 2.18.2; 16:46:49.699 INFO Mutect2 - HTSJDK D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6248:963,Load,Loading,963,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6248,1,['Load'],['Loading']
Performance,"I have found that 'hadoop distcp' scales linearly with the size of file. Its runtime seems not to change as I scale the number of machines from 10 to 100. Furthermore, I have found that taken in concert with performance findings from #1675 that for large file sizes it is actually more efficient to first load a bam file into HDFS first using 'hadoop distcp' in order to run the spark BQSR. . Breakdown:; - 150GB bam file takes 25:30 minutes to download into HDFS; - BQSRSpark takes 47:25 minutes to run on a 150GB bam file in HDFS; - Total runtime = 72:55 minutes ; - BQSRSpark run from GCS bucket = 77:15 minutes . A small improvement in runtime but it is worth keeping in mind for #2015 going forward when evaluating which approach is best to take on spark.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2014#issuecomment-234267283:208,perform,performance,208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2014#issuecomment-234267283,2,"['load', 'perform']","['load', 'performance']"
Performance,"I have more examples of this now (90 and counting, ~1% of jobs) which seems to match with the above numbers:. `htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset, for input source: gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/1e300bb3-6990-4342-8959-118826efb3dd/PairedEndSingleSampleWorkflow/3b32519a-f910-49a6-a5fc-b7ec9700d281/call-GatherVCFs/S153-2.g.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:102); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:86); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.loadHeaderFromVCFUri(GenomicsDBImport.java:252); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.initializeHeaderAndSampleMappings(GenomicsDBImport.java:223); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onStartup(GenomicsDBImport.java:202); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:114); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:210,concurren,concurrent,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance,"I have not had time to do any profiling, but I have looked at a lot of commits. I think it's likely that my recent changes cause some haplotypes with leading indels to be kept when previously they may have been dropped. It's hard to believe that this could cause a 10-20% slowdown via a commensurate increase in the number of haplotypes assembled. However, haplotypes with leading indels would have a disproportionate pair HMM cost since they would spoil caching of the read-haplotype pair HMM matrix at the very beginning of the matrix. That is, in addition to being particularly expensive haplotypes because they would diverge from the previous haplotype at the first position and therefore not benefit from caching at all, they would also completely destroy whatever caching the previous haplotype would have gotten. We ought to think about haplotypes that start or end with indels. It seems to me that they are bad news and very likely artifacts of assembly windows and/or reads that end in the middle of an STR. I would worry about discarding them outright, because what if all the real variation is attached to haplotypes like this. Therefore, I think the best thing to do is to choose assembly windows more carefully and increase or decrease padding to avoid ending in an STR. Avoiding assembly windows that end in STRs is a wise thing to do regardless, so how about I make a branch for that and we can see if the performance regression goes away?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6567#issuecomment-623054595:1421,perform,performance,1421,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6567#issuecomment-623054595,1,['perform'],['performance']
Performance,"I have tested several program functions by GATK4, some of them work pretty well. GATK4 does a great job to organize all steps by different tools like BWA, samtools, picard. Besides, it seems that there is also some optimization inside. Like ""cleanSam"" step, GATK4 cuts the time half compared to the one using Picard. (From 8 mins to 4 mins on same data) . However, the problem about GATK4 is that some programs fail due to java related problem (Maybe some reasons else). So far, the functions I failed are ""FastqToSam"" and ""ReadsPipelineSpark "". . Note that, I am using spark locally not scale-out cluster. So the command I am running ReadsPipelineSpark is as below(with oracle java8). Then it gives me java error. . ./bin/gatk/gatklaunch \; ReadsPipelineSpark \; -O hdfs:<PATH>CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr \; -I hdfs:<PATH>/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam \; -R hdfs:<PATH>human_g1k_v37.2bit \; --knownSites hdfs:<PATH>dbsnp_138.b37.excluding_sites_after_129.vcf \; --shardedOutput true \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES. All the failure seems to do with the java.lang.IllegalArgumentException with different error causes:; 1. readpipeline: java.lang.IllegalArgumentException: Null object is not allowed here; 2. fastqtosam: java.lang.IllegalArgumentException: Self-suppression not permitted",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1876:215,optimiz,optimization,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1876,1,['optimiz'],['optimization']
Performance,"I haven't been able to reproduce @vdauwera error, but there are issues with the https checkout at the moment. ; There's one annoying issue witwhere it will prompt for a password before every individual file download. This will be fixed in https://github.com/github/git-lfs/issues/755. It can be worked around by using `git config credential.helper cache` but the easiest thing to do at the moment is just using the ssh checkout. . I need to investigate what happens with ssh checkout if you don't have a key set up.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/952#issuecomment-150376513:348,cache,cache,348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/952#issuecomment-150376513,1,['cache'],['cache']
Performance,"I haven't understood how multi-allele model exactly works in the old GATK, so can't comment on why it does not perform well. In general, I am supportive of making the new model the default going forward. However:. > when we remove the other models. I would suggest retaining the old model if possible. As I said on the method meeting, the old model takes the full power of population information (by full, I mean under the Wright-Fisher and HWE assumptions, you can't derive a more powerful model in theory). My understanding is that David's current model isn't. This is fine as long as the information from sequence data overwhelms the population information, which is usually true for highCov data. However, when data is thin, the population information will play a more important role. Without thorough evaluations in multiple scenarios, it is not clear when the loss of population information in the new model starts to matter. It would be good to keep the old model as a reference point, at least for biallelic SNPs, until we have more comparison.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242810127:111,perform,perform,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242810127,1,['perform'],['perform']
Performance,"I just got the same error:. ```; Using GATK jar /software/anaconda2/share/gatk4-4.0.12.0-0/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /software/anaconda2/share/gatk4-4.0.12.0-0/gatk-package-4.0.12.0-local.jar FilterMutectCalls -V tumor-vs-normal.mutect.temp1.vcf -O tumor-vs-normal.mutect.temp2.vcf; 22:58:25.052 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/software/anaconda2/share/gatk4-4.0.12.0-0/gatk-package-4.0.12.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:58:26.911 INFO FilterMutectCalls - ------------------------------------------------------------; 22:58:26.912 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.0.12.0; 22:58:26.912 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:58:26.912 INFO FilterMutectCalls - Executing as www-data@SpongeBob on Linux v4.15.0-39-generic amd64; 22:58:26.912 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 22:58:26.912 INFO FilterMutectCalls - Start Date/Time: January 6, 2019 10:58:24 PM SGT; 22:58:26.913 INFO FilterMutectCalls - ------------------------------------------------------------; 22:58:26.913 INFO FilterMutectCalls - ------------------------------------------------------------; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Version: 2.18.1; 22:58:26.913 INFO FilterMutectCalls - Picard Version: 2.18.16; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:58:26.914 INFO FilterMutectCalls - De",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5553#issuecomment-451749085:513,Load,Loading,513,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5553#issuecomment-451749085,1,['Load'],['Loading']
Performance,"I know we've had to tune this parameter to get things working, and different sizes are better for different tools. If we increase it by much we start thrashing memory and spilling to disk, but the partition size range depends on the tool. MD works more slowly if you increase this by several times, others you can increase it an order of magnitude and it's ok.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1166#issuecomment-158528360:20,tune,tune,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1166#issuecomment-158528360,1,['tune'],['tune']
Performance,I like it and want to put it in but we can't unless the code compiles and runs on the Mac. I'd be fine with having Mac use just 1 thread if need be but the code must compile and run. With some hacking @lbergelson was able to compile and run it but it's not something we want users to do. The official XCode on the Mac does not include OpenMP and having our users go through a complex install of an alternative clang is a no-starter for us. Also non-starter is removal of native PairHMM from the Mac. . So I think we have 2 options:; a) change the code+build to disable OMP on the Mac (ie have Mac be single threaded). The benefit is that we can start running with improved performance right away on linux. That includes production (when we're ready with the HaplotypeCaller); b) wait until the new intel gatk native library is done and then move the problem of compiling etc to that repo. That would remove the burden of building this from us and shift to the other repo's owners who can then build their library however they want and we'd only pick up compiled versions. @gspowley @lbergelson @droazen votes?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1813#issuecomment-218849928:673,perform,performance,673,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813#issuecomment-218849928,1,['perform'],['performance']
Performance,I loaded the docker repo GATK v4.1.4.0 and had the same (or similar) error result. ```; 2019-10-30T13:35:51.791637449Z java.lang.IllegalArgumentException: log10 p: Values must be non-infinite and non-NAN; 2019-10-30T13:35:51.792001654Z 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); 2019-10-30T13:35:51.792175325Z 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); 2019-10-30T13:35:51.792358868Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.clusterProbabilities(SomaticClusteringModel.java:203); 2019-10-30T13:35:51.792559803Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:96); 2019-10-30T13:35:51.792736667Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.TumorEvidenceFilter.calculateErrorProbability(TumorEvidenceFilter.java:27); 2019-10-30T13:35:51.792905235Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 2019-10-30T13:35:51.793072365Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 2019-10-30T13:35:51.793261944Z 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 2019-10-30T13:35:51.793456807Z 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 2019-10-30T13:35:51.793619935Z 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 2019-10-30T13:35:51.793810301Z 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 2019-10-30T13:35:51.794006885Z 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 2019-10-30T13:35:51.794191116Z 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 2019-10-30T13:35:51.794367593Z 	at java.util.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-547909227:2,load,loaded,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-547909227,1,['load'],['loaded']
Performance,"I may have a lead. The error occurs here (no insight so far, just looking up the line from the stack trace):; ```; final Object2IntMap<EVIDENCE> evidenceIndexes = evidenceIndexBySampleIndex(sampleIndex);; final int[] indexesToRemove = evidences.stream().mapToInt(e -> {; final int index = evidenceIndexes.getInt(e);; if (index == MISSING_INDEX) {; throw new IllegalArgumentException(""evidence provided is not in sample"");; }; ```; We get an error when `evidenceIndexBySampleIndex(sampleIndex)` yields a `Map` that for some reason doesn't contain a read that it should. So let's investigate `evidenceIndexBySampleIndex()`. This method returns the `evidenceIndexBySampleIndex.get(sampleIndex)` field if it is not `null` (ie uninitialized); otherwise it fills it and then returns it. The code for filling it seems fine, and it explicitly loops over every sample read, so it's hard to see that the error could come from there. It seems rather that the problem is in returning the cached value whenever it is not `null`. The cached value of `evidenceIndexBySampleIndex.get(sampleIndex)` becomes invalid whenever reads are added or removed. However, you can check all the accesses of `evidenceIndexBySampleIndex` (there are only six) and verify that the class never accounts for this. So, suppose that an `AlleleLikelihoods` object invokes `evidenceIndexBySampleIndex(sampleIndex)` more than once and adds or removes reads between these. The second call returns the cached map from the first call, which is bogus. Even if it doesn't explain this issue, it is a bug. Now let's think about which public methods `evidenceIndexBySampleIndex(sampleIndex)` is called in and where this occurs in HaplotypeCaller:. * `addEvidence` (in HC this happens only in the likelihoods for annotations, downstream of our issue, so this is not the culprit).; * `filterPoorlyModeledEvidence` (this happens after Pair-HMM to the haplotype likelihoods, so not the culprit either); * `contaminationDownsampling`; * `retainEvidence`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625021336:976,cache,cached,976,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625021336,1,['cache'],['cached']
Performance,"I might be missing something, but this looks like it iterates over assembly regions, rather than loading them all at once. `Utils.stream` converts an iterator to a stream, but even that doesn't load them all into memory as it basically creates a one-shot `Iterable` that just returns the passed in `Iterator` (this is what `() -> iterator` does).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4301#issuecomment-368830180:97,load,loading,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4301#issuecomment-368830180,2,['load'],"['load', 'loading']"
Performance,"I performed a DepthOfCoverage analysis using the latest version of GATK (4.2.0) within the docker environment. I have provided only the required arguments and files, as shown in the line below:; gatk DepthOfCoverage -R assembly-Pacbio.genome.fasta -O Coverage_Pacbio -I Alignment_sorted_Pacbio.bam -L scaffolds-Pacbio-Chr-sizes.interval_list. The execution came to an end and apparently without a problem:. 17:04:50.881 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 29, 2021 5:04:51 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:04:51.384 INFO DepthOfCoverage - ------------------------------------------------------------; 17:04:51.385 INFO DepthOfCoverage - The Genome Analysis Toolkit (GATK) v4.2.0.0; 17:04:51.385 INFO DepthOfCoverage - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:04:51.385 INFO DepthOfCoverage - Executing as root@d84100edcb97 on Linux v5.4.0-77-generic amd64; 17:04:51.385 INFO DepthOfCoverage - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 17:04:51.386 INFO DepthOfCoverage - Start Date/Time: June 29, 2021 5:04:50 PM GMT; 17:04:51.386 INFO DepthOfCoverage - ------------------------------------------------------------; 17:04:51.386 INFO DepthOfCoverage - ------------------------------------------------------------; 17:04:51.387 INFO DepthOfCoverage - HTSJDK Version: 2.24.0; 17:04:51.387 INFO DepthOfCoverage - Picard Version: 2.25.0; 17:04:51.387 INFO DepthOfCoverage - Built for Spark Version: 2.4.5; 17:04:51.387 INFO DepthOfCoverage - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:04:51.387 INFO DepthOfCoverage - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:04:51.387 INFO DepthOfCoverage - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:04",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7332:2,perform,performed,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7332,2,"['Load', 'perform']","['Loading', 'performed']"
Performance,"I ran BWA-MEM on 1m read pairs. Here's a comparison for pure BWA-MEM versus the Spark version, using two threads in each case. (The Spark version had a single executor which was allocated two cores via the `--executor-cores` argument.) . ```; $BWA mem -K 100000 -v 3 -t 2 human_g1k_v37.fasta ERR000589_1.1m.fastq ERR000589_2.1m.fastq > ERR242035.1m.sam 2> foo.err; # Real time: 666.345 sec; CPU: 1267.746 sec. ./gatk-launch BwaSpark \; --threads 2 \; --ref hdfs://bottou01.sjc.cloudera.com/user/$USER/bwa/human_g1k_v37.fasta \; --input hdfs:///user/$USER/bwa/ERR000589.1m.bam \; --output hdfs:///user/$USER/bwa/ERR000589-aligned.1m.bam \; -- \; --sparkRunner SPARK \; --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 1 \; --executor-cores 2 \; --executor-memory 15G \; --archives jbwa-native.tar#jbwa-native \; --conf 'spark.executor.extraLibraryPath=jbwa-native'; BwaSpark - Total time to run tool: 932s; BwaSpark - Time to download reference: 32s ; BwaSpark - Time in bwa: 1575s; ```. So BwaSpark was 40% slower overall in terms of elapsed time. Of course, this was only using a single executor, so if we used multiple executors the elapsed time could be made proportionately shorter (and this a valid use case for some). In terms of CPU time in the BWA algorithm itself, it was about 24% slower (1575/1268). The latter could be improved by optimizing the JNI path (e.g. by passing several arrays of primitives or Strings corresponding to the fields in a `ShortRead`, rather than a single array of `ShortRead`s and then translating it). There are probably other improvements to make too, but I think the summary is that this approach is feasible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219080628:1361,optimiz,optimizing,1361,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219080628,1,['optimiz'],['optimizing']
Performance,"I ran CalibrateDragstrModel on one of the NYGC 1000G crams (which should be Functionally Equivalent with ours) and got the error: `A reference must be supplied that includes the reference sequence for chr12` I did pass a reference to the tool, but couldn't get it to run until I set the samjdk.reference_fasta black magic (at @droazen 's suggestion) in the java invocation. Huge stack trace:; java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1005); 	at org.broadinstitute.hellbender.utils.Utils.runInParallel(Utils.java:1479); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsParallel(CalibrateDragstrModel.java:473); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:152); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1057); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caus",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7060:942,concurren,concurrent,942,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060,1,['concurren'],['concurrent']
Performance,"I ran IndexFeatureFile on a VCF with a valid header but no variant features. IndexFeatureFile crashes due to something regarding the progress meter. This might be a good place to output one of your helpful `USER ERROR` messages. . Thanks!. ```; acesnik@DESKTOP$ gatk/gatk IndexFeatureFile --feature-file bad.vcf; Using GATK jar gatk/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar gatk/gatk-package-4.0.0.0-local.jar IndexFeatureFile --feature-file bad.vcf; 00:17:06.701 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:gatk/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 00:17:06.843 INFO IndexFeatureFile - ------------------------------------------------------------; 00:17:06.843 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.0.0.0; 00:17:06.844 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:17:06.845 INFO IndexFeatureFile - Executing as acesnik@DESKTOP-NTA5PMC on Linux v4.4.0-43-Microsoft amd64; 00:17:06.845 INFO IndexFeatureFile - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 00:17:06.846 INFO IndexFeatureFile - Start Date/Time: January 26, 2018 12:17:06 AM GMT; 00:17:06.846 INFO IndexFeatureFile - ------------------------------------------------------------; 00:17:06.846 INFO IndexFeatureFile - ------------------------------------------------------------; 00:17:06.847 INFO IndexFeatureFile - HTSJDK Version: 2.13.2; 00:17:06.847 INFO IndexFeatureFile - Picard Version: 2.17.2; 00:17:06.848 INFO IndexFeatureFile - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:17:06.849 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:17:06.849 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 00:17:06.850 INFO IndexFeatureFile - HTSJ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4269:657,Load,Loading,657,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4269,1,['Load'],['Loading']
Performance,"I ran `FindBreakpointEvidenceSpark` and did some high-level checks to see if there are any opportunities for performance improvements. (cc @tedsharpe @cwhelan). This is the command line I ran. (Earlier I had run more executors with smaller memory settings, but the job didn't complete then.); ; ```bash; ./gatk-launch FindBreakpointEvidenceSpark \; -I hdfs:///user/$USER/broad-svdev-test-data/data/NA12878_PCR-_30X.bam \; -O hdfs:///user/$USER/broad-svdev-test-data/assembly \; --exclusionIntervals hdfs:///user/$USER/broad-svdev-test-data/reference/GRCh37.kill.intervals \; --kmersToIgnore hdfs:///user/$USER/broad-svdev-test-data/reference/Homo_sapiens_assembly38.dups \; -- \; --sparkRunner SPARK --sparkMaster yarn-client --sparkSubmitCommand spark2-submit\; --driver-memory 16G \; --num-executors 5 \; --executor-cores 7 \; --executor-memory 25G; ```. What does FindBreakpointEvidenceSpark do, from the perspective of Spark?. * [runTool] filter out secondary and supplementary alignments; * [getMappedQNamesSet] filter out duplicate reads, reads that failed vendor checks, unmapped reads; * Job 0 [ReadMetadata] mapPartitions to find partition stats; * Job 1 [getIntervals] filter and multiple map partitions to find breakpoint intervals ; * Job 2 [removeHighCoverageIntervals] mapPartitionsToPair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you want",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:109,perform,performance,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884,1,['perform'],['performance']
Performance,"I ran a few Bayesian optimizations, mostly focusing on various subsets/ranges of the read-to-haplotype and haplotype-to-reference SW parameters. I also varied whether I used part/all of chr22 and chose either F1/sensitivity as the optimization target. These experiments show that varying the SW parameters can definitely move the needle in terms of performance, even after variant normalization. For example, here are the SNP precision/sensitivity curves for an optimization of F1 over about half of chr22:. ![snp_f1](https://user-images.githubusercontent.com/11076296/96470421-44490900-11fc-11eb-96f6-83a6833b2b1f.png). And the same for indels:. ![non_snp_f1](https://user-images.githubusercontent.com/11076296/96470468-5034cb00-11fc-11eb-86a7-bbf81bc10122.png). This all raises the question of what the best optimization strategy should be. It looks like the SW parameters aren't the limiting factor for sensitivity (I would guess that might be MQ or other read filters) but can probably help improve precision. So we may want to relax some filters or optimize them jointly. We may also want to look at optimizations that include filtering, as I mention above.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712237919:21,optimiz,optimizations,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712237919,7,"['optimiz', 'perform']","['optimization', 'optimizations', 'optimize', 'performance']"
Performance,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests [failed](https://travis-ci.org/broadinstitute/gatk/builds/422089722), including a few of the Funcotator ones:. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance. These may be test artifacts, but we should audit the Funcotator cache access patterns and see if this is actually causing thrashing that affects performance. Since the FeatureCache caching strategy assumes queries will be forward-only, it might be an indication that Funcotator performance could be improved by either turning off caching or using an alternative cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5143:259,cache,cached,259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143,5,"['cache', 'perform']","['cache', 'cached', 'performance']"
Performance,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5148:259,cache,cached,259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148,5,"['cache', 'perform']","['cache', 'cached', 'performance']"
Performance,"I rebased this, and responded to code review comments:. - updated comments; - reverted GenotypeGVCFs change; - reverted changing the default lookahead for VariantWalker side inputs. I think changing the default lookahead for VariantWalker side inputs to the new, smaller value will hurt performance for tools like VQSR. I did some crude timing tests using the FeatureDataSource default (1000 bases) proposed in this branch, and the current default (100,000 bases). The following are total times as reported by Gradle for serial runs of the VQSR integration tests:. With 1000 base lookahead:; 1m40s; 1m29s; 1m29s; 1m25s. With 100,000 base lookahead:; 1m29s; 1m17s; 1m16s; 1m18s. Back to 1000 base lookahead again:; 1m36s; 1m26s; 1m26s; 1m29s. It seems pretty consistently slower with the smaller lookahead.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3480#issuecomment-417360848:287,perform,performance,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3480#issuecomment-417360848,1,['perform'],['performance']
Performance,"I refactored some of the stream methods to address @akiezun's (well-founded, as running HaplotypeCaller showed) performance concerns. That, along with @lbergelson's suggestion to lazily evaluate error messages, leaves HaplotypeCaller's performance unaffected. Tests are passing locally but Travis is giving me a mysterious error:. > compileTestJavaerror: error reading /home/travis/.gradle/caches/modules-2/files-2.1/commons-httpclient/commons-httpclient/3.1/964cd74171f427720480efdec40a7c7f6e58426a/commons-httpclient-3.1.jar; error in opening zip file; > error: error reading /home/travis/.gradle/caches/modules-2/files-2.1/commons-httpclient/commons-httpclient/3.1/964cd74171f427720480efdec40a7c7f6e58426a/commons-httpclient-3.1.jar; cannot read zip file. I am fully rebased onto master, including @akiezun's PR from this afternoon. @droazen and @lbergelson do you have insights?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-232447710:112,perform,performance,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-232447710,4,"['cache', 'perform']","['caches', 'performance']"
Performance,"I reproduced various out of memory errors in a Linux VM with 4G of RAM, both with the `IntelInflaterDeflaterIntegrationTest` enabled and disabled. Most resulted in the kernel killing the Java process, like this one (from `dmesg`):; ```; [38425.759992] Out of memory: Kill process 10295 (java) score 747 or sacrifice child; [38425.759998] Killed process 10295 (java) total-vm:7885212kB, anon-rss:3250892kB, file-rss:0kB; ```. Some were caught by the JVM, like this one:; ```; #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 90177536 bytes for committing reserved memory.; # Possible reasons:; # The system is out of physical RAM or swap space; # In 32 bit mode, the process size limit was hit; # Possible solutions:; # Reduce memory load on the system; # Increase physical memory or swap space; # Check if swap backing store is full; # Use 64 bit Java on a 64 bit OS; # Decrease Java heap size (-Xmx/-Xms); # Decrease number of Java threads; # Decrease Java thread stack sizes (-Xss); # Set larger code cache with -XX:ReservedCodeCacheSize=; # This output file may be truncated or incomplete.; #; # Out of Memory Error (os_linux.cpp:2627), pid=20484, tid=139679452493568; #; # JRE version: Java(TM) SE Runtime Environment (8.0_72-b15) (build 1.8.0_72-b15); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.72-b15 mixed mode linux-amd64 compressed oops); # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; ```. Here's my theory of what's happening. The `maxHeapSize` for test JVMs is set to 4G in `build.gradle`:; ```; maxHeapSize = ""4G""; ```. A 4G max heap size is too high for systems with 4G of RAM, because the Java heap grows until the system runs out of memory. If we decrease `maxHeapSize`, the GC should prevent the Java heap from growing too large, with the trade-off of more GC calls. I changed the `maxHeapSize` to `2G` a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2490#issuecomment-288423316:813,load,load,813,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490#issuecomment-288423316,2,"['cache', 'load']","['cache', 'load']"
Performance,"I run HaplotypeCaller twice , the former one was stopped because of unexpected power outages. I check the LOG and found the chromosome where HaplotypeCaller stopped. So i star another HaplotypeCaller(later one) with the ""-L *.intervals"", it begin from the chromosome where former HaplotypeCaller stopped.The ref genome and the parameters were all the same. However, HaplotypeCaller give different results. Note: the ref genome has 26 chromosomes :A01-A13;D01-D13. **_The former LOG:_**. nohup: ignoring input and appending output to nohup.out; 09:04:49.857 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/home/chenwei/biosoft/gatk-4.0.10.1/gatk-package-4.0.10.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:05:02.971 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.971 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.10.1; 09:05:02.971 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:05:02.972 INFO HaplotypeCaller - Executing as chenwei@localhost.localdomain on Linux v3.10.0-1160.31.1.el7.x86_64 amd64; 09:05:02.972 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_292-b10; 09:05:02.973 INFO HaplotypeCaller - Start Date/Time: August 22, 2021 9:04:49 AM CST; 09:05:02.973 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.973 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.974 INFO HaplotypeCaller - HTSJDK Version: 2.16.1; 09:05:02.974 INFO HaplotypeCaller - Picard Version: 2.18.13; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7454:586,Load,Loading,586,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454,1,['Load'],['Loading']
Performance,"I run the BaseRecalibrator,and at fisrt it can good running,after a time,I got this error; htsjdk.samtools.SAMFormatException: Invalid GZIP header; This is the log:; Using GATK jar /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx20G -Djava.io.tmpdir=./; -jar /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar BaseRecalibrator -R /data/home/wuly/source/Homo_sapiens_assembly38.fasta -I M1.bam --known-sites /data/home/wuly/source/dbsnp_146.hg38.vcf.gz --known-sites /data/home/wuly/source/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites /data/home/wuly/source/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/home/wuly/source/hapmap_3.3.hg38.vcf.gz -O M1_recal.table17:55:54.326 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compre; ssion.soMay 24, 2019 5:55:56 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:55:56.095 INFO BaseRecalibrator - ------------------------------------------------------------; 17:55:56.096 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.2.0; 17:55:56.096 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:55:56.096 INFO BaseRecalibrator - Executing as wuly@localhost.localdomain on Linux v3.10.0-957.10.1.el7.x86_64 amd64; 17:55:56.096 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 17:55:56.096 INFO BaseRecalibrator - Start Date/Time: May 24, 2019 5:55:54 PM EDT; 17:55:56.096 INFO BaseRecalibrator - ------------------------------------------------------------; 17:55:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5968:957,Load,Loading,957,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5968,1,['Load'],['Loading']
Performance,"I see same problem in `4.0.5.2`:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar GenotypeGVCFs -O rad34test.comb2.raw.g.vcf.gz -R ../../../jic_reference/alygenomes.fasta -V rad34test.comb2.raw.vcf.gz --new-qual; 22:45:47.050 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:45:47.648 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:45:47.649 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.5.2; 22:45:47.649 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:45:47.800 INFO GenotypeGVCFs - Initializing engine; 22:45:48.331 INFO FeatureManager - Using codec VCFCodec to read file file:///home/vojta/dokumenty/fakulta/botanika/arabidopsis/samples/lib_2018_06/4_joined/rad34test.comb2.raw.vcf.gz; 22:45:48.467 INFO GenotypeGVCFs - Done initializing engine; 22:45:48.555 INFO ProgressMeter - Starting traversal; 22:45:48.556 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 22:45:51.038 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 22:45:51.045 INFO GenotypeGVCFs - Shutting down engine; [2. ervence 2018 22:45:51 CEST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=528482304; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 6.911788849595091E-17,NaN; at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.getLog10PNonRef(AlleleFrequencyCalculator.java:143); at org.broadinstitute",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-401933028:422,Load,Loading,422,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-401933028,1,['Load'],['Loading']
Performance,"I see the exception on most chromosomes, here's the one for chr1:. java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:79293873 end:79293872; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:49); at org.broadinstitute.hellbender.engine.AssemblyRegion.add(AssemblyRegion.java:335); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.fillNextAssemblyRegionWithReads(AssemblyRegionIterator.java:230); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:194); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:135); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4120#issuecomment-356718095:782,load,loadNextAssemblyRegion,782,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4120#issuecomment-356718095,1,['load'],['loadNextAssemblyRegion']
Performance,"I see. . Yes. That's what I'm planning on (except that `AssemblyContigAlignmentsConfigPicker` is upstream of this unit), and here's the thought for why:; * I'd try to place the alignment picking step in a single place as much as possible, this makes improvements to the alignment picking/filtering step easier; * the size-based filter can be tuned, even by an CLI argument, this would affect the number of segments in the CPX logic, and the alt_arrangment annotations, and the simple variants re-interpreted by `CpxVariantReInterpreterSpark`, but it won't affect the alt haplotype sequence, which IMO is what really is important. ; * I'm developing a downstream variant filter, which hopefully can cut down the false-positives. And for the question of ""why 2 instead of 1"", I think what you are suggesting is to change; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 2;; if (one.getSizeOnRead() >= MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; to; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 1;; if (one.getSizeOnRead() > MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; Am i right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353:342,tune,tuned,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353,1,['tune'],['tuned']
Performance,"I started with Mark Duplicates and found that on the 76MB BAM file in resources, the walker took ~20 seconds, while local Spark took ~120s (with all cores). However, when I changed GATKRegistrator to use SAMRecordToGATKReadAdapterSerializer, local Spark took ~36s (all cores) or ~55s (1 core). Also, this is not writing a single file, since https://github.com/broadinstitute/gatk/issues/1015 has not been fixed yet. So we should definitely use the better serializer - is there any reason not to do that now?. Spark performance running locally with a single core on small files is probably not going to match the walker implementations since the local runner is not tuned for that use case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1047#issuecomment-157095589:515,perform,performance,515,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1047#issuecomment-157095589,2,"['perform', 'tune']","['performance', 'tuned']"
Performance,"I still got the same error with version 4.1.9.0. . > Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/orange/reed/nhouse/Raw_seqs/SEQ9_samples/tmp; 11:30:50.248 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:30:50.478 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 26, 2020 11:30:50 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:30:50.791 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.791 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.9.0; 11:30:50.792 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:30:50.792 INFO CombineGVCFs - Executing as nwijewardena@c3a-s8.ufhpc on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 11:30:50.792 INFO CombineGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_31-b13; 11:30:50.792 INFO CombineGVCFs - Start Date/Time: October 26, 2020 11:30:50 AM EDT; 11:30:50.793 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.793 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.794 INFO CombineGVCFs - HTSJDK Version: 2.23.0; 11:30:50.794 INFO CombineGVCFs - Picard Version: 2.23.3; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:30:50.795 INFO CombineGVCFs - Deflater: IntelDeflater; 11:30:50.795 INFO CombineGVCFs - Inflater: IntelInflater; 11:30:50",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716640444:326,Load,Loading,326,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716640444,1,['Load'],['Loading']
Performance,"I suspect it will also make things substantially slower. It will end up doing essentially a full sort of the bam file I believe. Since we really only need local sorting to fix the problem this is a bit overkill. If this is a bottleneck we can likely improve it, but it would be more work than just flipping the single flag.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1865#issuecomment-224902018:225,bottleneck,bottleneck,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1865#issuecomment-224902018,1,['bottleneck'],['bottleneck']
Performance,"I suspect it's a different issue than #4062, but probably some similar library incompatibility issue. Unfortunately the stack trace doesn't have enough information in it. We should consider rewriting the error message for this so that we are sure to have the first cause reported, not just the `Could not load genomicsdb native library` message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-356984584:305,load,load,305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-356984584,1,['load'],['load']
Performance,I suspect this was caused by the race condition in #2050 ; will close now - reopen if seen after #2053 is merged,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2039#issuecomment-235660642:33,race condition,race condition,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2039#issuecomment-235660642,1,['race condition'],['race condition']
Performance,"I talked to comms and we agreed that a ""mitochondria-mode"" argument to Mutect2 was the right balance of clarity (you're really running Mutect2 not a wrapper) and simplicity (you don't need a laundry list of arguments to change which mode you're in if you just want to run with optimized defaults). . @ldgauthier @davidbenjamin @takutosato @rcmajovski Could you please take another look? Removing the wrapper tools has cleaned up the code so there are fewer changes now. I also changed TLOD to LOD in this version, but I'm happy to take that out and have that be future work if anyone is worried about it being a breaking change.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-428195077:277,optimiz,optimized,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-428195077,1,['optimiz'],['optimized']
Performance,"I terms of comparing implementations in this particular instance, the entire BAM file is also being scanned, so there is no additional limitation in using Parquet. In general, though, there are some additional complications for typical Hadoop data sets due to their distributed nature. There are multiple methods for accelerating lookups, depending on the particular application and what kind of latency you need. A few of them:; - Data set partitioning in the style of Hive, where you split your data set into a directory hierarchy that's based on the values of one of the columns. This is like building an index, and should be done on cols that feature in lot of predicates. In the quince repo that we're using for ingest, we are partitioning the data based on genome locus. This makes it easy to access the data only from the locus of interest.; - Parquet supports predicate pushdown and column stats on its row chunks, so the more you homogenize the data (e.g., by sorting), the more likely it is you can skip large blocks of data.; - The parquet file format supports the concept of indexing (though I don't think it is implemented yet in any of the packages that read/write it); - For particular applications, you can also use a different storage backend like HBase or Kudu that allow very rapid point/range queries at scale. I believe GEL is planning on trying out HBase for some of their applications; - The Hammerbacher lab and the ADAM folks are also working on tools for BAM file indexing and visualization that scales on Hadoop. I think the projects are cycledash/pileup.js and mango, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1067#issuecomment-152446541:396,latency,latency,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1067#issuecomment-152446541,1,['latency'],['latency']
Performance,"I think all of our dataflow / spark code is at least almost entirely using `GATKRead`. GATKRead is designed to not provide access to the header because it's not available from a google `Read` backed `GATKRead`. It sounds like there is some information that `Read` includes that is missing from a headerless `SAMRecord`. I think we could audit the `SAMRecordToGATKReadAdaptor` to find any places it touches the header and then cache that information in the adaptor before stripping the header. We don't need to add back in the headers at any point, because we provide library functions to perform any header related operation with a provided header.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141107659:426,cache,cache,426,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141107659,2,"['cache', 'perform']","['cache', 'perform']"
Performance,"I think issues with gCNV postprocessing initially stemmed from things like 1) taking large sample x shard transposes (which Cromwell may have had trouble with at some point), 2) localizing more files than necessary for each sample, 3) introduction of lexicographical globbing bugs, etc. I think various people have tried to address this via things like bundling, which we ended up reverting in favor of transposing. Unfortunately, I'm afraid I've lost the plot on this after so long; and not sure I ever had it---is there any documentation (e.g., of things like how bundling improved performance) elsewhere that might help us decide what's left to be done? Seems like I had some more coherent thoughts in https://github.com/broadinstitute/gatk/pull/6607#issuecomment-632303744. As before, if any remaining issues like the call caching mentioned above would best be addressed at the Cromwell level, I think it's at least worth an investigation of what might be involved.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-900624782:584,perform,performance,584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-900624782,1,['perform'],['performance']
Performance,"I think it would be good to profile and compare a variety of approaches for dealing with the reference data. In the dr_read_preprocessing_pipeline_skeleton branch we are taking the approach of sharding the reference into fixed-size chunks, and using GA4GH API calls (rather than files) to populate each chunk. We currently have no idea how well this will perform, though we suspect it will be better than copying the entire reference to each worker. @tomwhite Would it be possible at this point to profile the two options you mentioned (""broadcast the entire reference sequence to all nodes in the cluster"" vs. ""streaming reads from HDFS"") with a real human reference like GRCh37 so that we can make an informed choice between them?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/567#issuecomment-112850702:355,perform,perform,355,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-112850702,1,['perform'],['perform']
Performance,"I think that this is part of wider need to for dependency-injection in tools; often the initialize() method might be loaded with instantiation of components that themselves require some user argument inputs.. can this be done in a more declarative fashion? . For example... HC, UG or GenotypeGVCFs the have annotationEngine or genotypingEngine components that are explicitly initialized in initialize() what if the engine is responsible to instantiate them and add the appropriate arguments to the command line which are declared in the corresponding classes rather than in the tool (or a synthetic argument collection class)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/96#issuecomment-69810912:117,load,loaded,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/96#issuecomment-69810912,1,['load'],['loaded']
Performance,"I think the `Optimized` version deals with avoiding replicating reads in a more nuanced manner, but if I understand correctly, it doesn't seem to me to avoid the shuffle. It looks like it essentially ignores the concept of data locality entirely, and potentially transfers a lot of data over the network. (Equivalent to performing a shuffle on a sorted file in HDFS.). IMO, the current ""shuffle"" implementation is already a ""Spark-y"" way to do it, but with multiple inefficiencies:; - Reads/variants are keyed to their corresponding shards, replicating reads if they cross over shard boundaries. This necessitates an `aggregateByKey` operation that potentially reshuffles the entire data set at the end to deal with neighboring shards that could be hashed to different machines.; - The impl uses `cogroup` and `groupByKey`, which require materializing all values for a key in memory (which could be large). Best to avoid these if possible.; - And related to the previous issue, the join strategy for reads and variants is basically a cross-product-and-filter, which is not very efficient, especially considering that the data can be ordered. I think the best implementation here would steal JP's method of sharding the reads/variants, but make use of `repartitionAndSortWithinPartition`, which lets you specify what partition to use and also sorts all the values within a given partition. This means that we could employ a sort-merge on each partition, and only scan through the datasets once after shuffling them. Do you already have an impl for doing a sort-merge of `Locatable`s? These can be a bit tricky. I wrote one for the `ShuffleRegionJoin` impl in ADAM, but there are semantic differences that would make it less efficient to use. (Specifically, it would require the `aggregateByKey` operation and also creating `SimpleInterval`-style objects from a separate model.). Finally, I would also add the ability to specify which join strategy to use separately for the reference bases and the vari",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1007#issuecomment-151721602:13,Optimiz,Optimized,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1007#issuecomment-151721602,2,"['Optimiz', 'perform']","['Optimized', 'performing']"
Performance,"I think the root cause is the method ensureCapacity of GenotypesCache is not synchronized. So when multiple task threads run into this method, the new added cache is not fully initialized.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8961#issuecomment-2306051902:157,cache,cache,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8961#issuecomment-2306051902,1,['cache'],['cache']
Performance,"I think there are two points to this issue: 1. validation or no-validation and 2. need to pass GenomeLocParser around... . Is 1. about performance? otherwise we prefer to have validated locations, right? and If it is about performance I think it should be shown that it really makes a difference to remove those checks. About, 2., can be solved by being able to recover the sequence-dictonary/reference object (called Reference from this point on) from a genome loc and then you can ask it for a new genome-loc if the appropriate methods are added instead of depending on that annoying middle man called GenomeLocParser. I would say that is unlikely to be in the situation where you want to create a GenomeLoc out of the blur without having already a reference to another GenomeLoc or Reference object available. It is an issue if GenomeLoc holds on to a reference to the Reference object? (or the instantiating class is a inner class of the Reference?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/100#issuecomment-69802695:135,perform,performance,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100#issuecomment-69802695,2,['perform'],['performance']
Performance,"I think this is happening because were trying to serialize the class loader sun.misc.Launcher$AppClassLoader), which appears to be reached through the graph by way of via https://github.com/damiencarol/jsr203-hadoop/blob/master/src/main/java/hdfs/jsr203/HadoopFileSystem.java#L82. We probably need to short circuit that with a custom serializer for one of these:. Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager). See, for instance, https://github.com/dbpedia/distributed-extraction-framework/issues/9.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-668654169:69,load,loader,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-668654169,1,['load'],['loader']
Performance,"I think this issue is very important to me. When I deal with the CAP data (high capture cfDNA data), GATK4.1.0.8's performance is much better than GATK4.2 ...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1174725963:115,perform,performance,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1174725963,1,['perform'],['performance']
Performance,"I think we are also waiting for FC to update, so that NIO can be call cached. Not sure what the status is on that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-461938277:70,cache,cached,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-461938277,1,['cache'],['cached']
Performance,I think we discovered at some point that IntervalSkipList has worse performance than htsjdk `OverlapDetector`. We should probably be deprecating and removing it. #3608,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3541#issuecomment-331950525:68,perform,performance,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3541#issuecomment-331950525,1,['perform'],['performance']
Performance,"I think we should wait for the NIO-based fix rather than merging this, which would be useless for our performance work.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2333#issuecomment-271720196:102,perform,performance,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2333#issuecomment-271720196,1,['perform'],['performance']
Performance,I think we should write most readable code for now and optimize with a profiler later. The regexp machinery in Java libraries definitely does a better job optimizing the state machine than a person can.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/380#issuecomment-94477514:55,optimiz,optimize,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/380#issuecomment-94477514,2,['optimiz'],"['optimize', 'optimizing']"
Performance,"I think we're having a very similar issue as #7639, though we're using v4.2.6.1. We're running GenotypeGVCFs with a command similar to:; ```; java8 -Xmx120g -Xms120g -Xss2m \; -jar GenomeAnalysisTK4.jar GenotypeGVCFs \; -R 128_Mmul_10.fasta \; --variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/92a51eab-e10f-103a-8ff9-f8f3fc866871/WGS_v2_db03_500.gdb \; -O /home/exacloud/gscratch/prime-seq/workDir/4570d7bc-e1e5-103a-8ff9-f8f3fc866871/Job493.work/WGS_v2_db03_500.temp.vcf.gz \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 \; --max-alternate-alleles 6 \; --genomicsdb-max-alternate-alleles 9 \; --force-output-intervals WGS-WXS.whitelist.v2.3.sort.merge.bed \; -L X:117601765-123544428 \; --only-output-calls-starting-in-intervals \; --genomicsdb-shared-posixfs-optimizations; ```. the exception is:; ```; 09 Jul 2022 14:35:24,661 DEBUG: 	java.lang.IllegalStateException: Genotype has no likelihoods: [29446 T*|C GQ 60 DP 10 AD 8,0 {PGT=0|1, PID=117659384_CAGATCGATGGATCT_C, PS=117659384, SB=[1, 7, 0, 2]}]; 09 Jul 2022 14:35:24,667 DEBUG: 		at org.broadinstitute.hellbender.utils.GenotypeUtils.computeDiploidGenotypeCounts(GenotypeUtils.java:89); 09 Jul 2022 14:35:24,673 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.ExcessHet.calculateEH(ExcessHet.java:96); 09 Jul 2022 14:35:24,679 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.ExcessHet.annotate(ExcessHet.java:84); 09 Jul 2022 14:35:24,684 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.addInfoAnnotations(VariantAnnotatorEngine.java:355); 09 Jul 2022 14:35:24,690 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:334); 09 Jul 2022 14:35:24,696 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:306); 09 Jul 2022 14:35:24,701 DEBUG: 		at org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7933:296,cache,cachedData,296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7933,2,"['cache', 'optimiz']","['cachedData', 'optimizations']"
Performance,"I thought I figured this out but I haven't. I'm looking at sample1.md.bam file header, but I can't figure it out. ```. for BAM in sample1.md.bam ; do samtools view -H $BAM; > header.sam; done; @HD	VN:1.5	SO:coordinate; @SQ	SN:1	LN:4918979; @SQ	SN:2	LN:4844472; @SQ	SN:3	LN:4079167; @SQ	SN:4	LN:3923705; @SQ	SN:5	LN:3948441; @SQ	SN:6	LN:3778736; @SQ	SN:7	LN:2058334; @SQ	SN:8	LN:1833124; @PG	ID:hisat2	PN:hisat2	VN:2.1.0	CL:""/usr/local/apps/eb/HISAT2/2.1.0-foss-2016b/bin/hisat2-align-s --wrapper basic-0 -p 8 --no-spliced-alignment -I 100 -x ./ref_tran.dir/ref_tran -S sample1.sam -1 e1_R1.fastq -2 e1_R2.fastq""; @PG	ID:MarkDuplicates	VN:2.16.0-SNAPSHOT	CL:MarkDuplicates INPUT=[sample1.bam] OUTPUT=sample1.md.bam METRICS_FILE=sample1.md.metrics.txt MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 TAG_DUPLICATE_SET_MEMBERS=false REMOVE_SEQUENCING_DUPLICATES=false TAGGING_POLICY=DontTag CLEAR_DT=true ADD_PG_TAG_TO_READS=true REMOVE_DUPLICATES=false ASSUME_SORTED=false DUPLICATE_SCORING_STRATEGY=SUM_OF_BASE_QUALITIES PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates READ_NAME_REGEX=<optimized capture of last three ':' separated fields as numeric values> OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 MAX_OPTICAL_DUPLICATE_SET_SIZE=300000 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false	PN:MarkDuplicates. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5372#issuecomment-434156611:1174,optimiz,optimized,1174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5372#issuecomment-434156611,1,['optimiz'],['optimized']
Performance,"I tried clearing my caches and rebuilding, but I resolve everything. I noticed that our artifactory website looks much different today than it did yesterday. I wonder if it was down temporarily for an update. Maybe try again now? Unless they put it behind the firewall which would be a disaster... can you access https://artifactory.broadinstitute.org/?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641:20,cache,caches,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641,1,['cache'],['caches']
Performance,"I tried from a GCE instance and got **6m12s** for the gsutil copy, and **5m21s** for the NIO code. So we know that using the NIO code would match the gsutil performance (in this case, about 100MB/s). . As to the original question of how this translates to Spark performance, well, this test just fails to prove that GCS/NIO are too slow; more investigation is needed. . The cluster we're using has 10 machines so it may be able to run up to 10x faster than this single-machine test, ie. only about 30s to load the data via NIO. Of course, the program does more than that, and we still have to demonstrate that we're not going to bottleneck the GCS servers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-227905621:157,perform,performance,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-227905621,4,"['bottleneck', 'load', 'perform']","['bottleneck', 'load', 'performance']"
Performance,"I tried the FilterSamReads command using the Picard jar file and the output bam file to stdout had no issues; Command:; `java -jar ~/picard/build/libs/picard.jar FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET > picard_stdoutbam.bam`; Log:; ```; 10:33:09.327 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/gbrandt/picard/build/libs/picard.jar!/com/intel/gkl/native/libgkl_compression.dylib; INFO	2021-02-23 10:33:09	FilterSamReads	Filtering [presorted=true] subsampled.bam -> OUTPUT=stdout [sortorder=coordinate]; INFO	2021-02-23 10:33:09	SAMFileWriterFactory	Unknown file extension, assuming BAM format when writing file: file:///dev/stdout; INFO	2021-02-23 10:33:09	FilterSamReads	6 SAMRecords written to stdout; ```; Checking the file:; `gunzip -c -d -f picard_stdoutbam.bam | head -n 5`; No issues:; ```; BAM?2@HD	VN:1.6	SO:coordinate; @SQ	SN:1	LN:249250621	AS:NCBI-Build-37	SP:Homo sapiens	UR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:2	LN:243199373	AS:NCBI-Build-37	SP:Homo sapiens	UR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:3	LN:198022430	AS:NCBI-Build-37	SP:Homo sapiens	UR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:4	LN:191154276	AS:NCBI-Build-37	SP:Homo sapiens	UR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7080#issuecomment-784427228:382,Load,Loading,382,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7080#issuecomment-784427228,1,['Load'],['Loading']
Performance,I tried the latest GATK release and also reported errors.; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR7_gvcf_database -G StandardAnnotation --genomicsdb-shared-posixfs-optimizations true -O new_ALL_MATERIALS_chr7.g.vcf.gz; 17:49:50.404 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 17:49:50.653 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:49:51.271 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.273 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.6.1; 17:49:51.273 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:49:51.273 INFO GenotypeGVCFs - Executing as gaoshibin@comput6 on Linux v3.10.0-693.el7.x86_64 amd64; 17:49:51.274 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 17:49:51.274 INFO GenotypeGVCFs - Start Date/Time: 2022522 054950; 17:49:51.274 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.275 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.276 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 17:49:51.276 INFO GenotypeGVCFs - Picard Version: 2.27.1; 17:49:51.276 INFO GenotypeGVCFs - Built for Spark Version: 2.4.5; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_S,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135301848:507,optimiz,optimizations,507,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135301848,2,"['Load', 'optimiz']","['Loading', 'optimizations']"
Performance,"I tried to ERR000589 process data with BwaSpark. The bam file size is 1.3G. The average time spent is about 25 min (5 nodes).; However it would only cost 5 min in processing same data if I tried to use original C bwa with 32 threads.; Base on this observation, I have several questions list as follow:; 1. If there is anything wrong with my params?; 2. For each Partition, is BwaSpark running in multi-thread mode?; 3. How to control the number of the bwa threads inside BwaSpark?. P.S. ; The running command is:; ./gatk-launch BwaSpark -I hdfs:///user/XX/ERR000589/ERR000589.bam -O hdfs:///user/XX/ERR000589/ERR000589_bwa.bam -R hdfs:///user/xx/refs/ucsc.hg19.fasta --bwamemIndexImage ~/data/ref/ucsc.hg19.img -disableSequenceDictionaryValidation true -- --sparkRunner SPARK --sparkMaster <master_url> --executor-cores 1 --total-executor-cores 16 --executor-memory 4G. I tried to further adjust the following parameters,; --executor-cores --total-executor-cores --executor-memory --driver-memory; but none of these took less time than 16 min. Besides, I alsow tried to run it in local mode, while it won't end successfully. It seems that CPU was in endless waiting. I guess it occupied so much memory that the swap space is in use? Pic 1 shows the memory consumed while running ; This time, the command is:; ./gatk-launch BwaSpark -I hdfs:///user/XX/ERR000589/ERR000589.bam -O hdfs:///user/xx/ERR000589/ERR000589.bwa.bam -R /software/home/xx/data/ref/ucsc.hg19.fasta \ --bwamemIndexImage ~/data/ref/ucsc.hg19.img -disableSequenceDictionaryValidation true -- --sparkRunner SPARK --sparkMaster local[*] --total-executor-cores 8 --executor-memory 20G --driver-memory 30G. BTW, the testing environment is:; CPU 2 X 8 physical core; node: 5 ; network: GBE; memory: 64G; ![memory](https://cloud.githubusercontent.com/assets/22440517/25929003/19a38210-3632-11e7-8dfc-c18f75604de6.png); ![cpu](https://cloud.githubusercontent.com/assets/22440517/25929008/205497ac-3632-11e7-8aa9-9c459d99371d.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2705:396,multi-thread,multi-thread,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2705,1,['multi-thread'],['multi-thread']
Performance,"I used to have it in there, but once I got copy performance up I assumed it was just clutter. I need to make a minor update to the scripts anyway (option to run the debug program instead of the full pipeline) so I can put back the option to not copy fastq files as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4332#issuecomment-362655529:48,perform,performance,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4332#issuecomment-362655529,1,['perform'],['performance']
Performance,"I using following command to filter my vcf file:. ```; gatk --java-options ""-Xmx4g"" FilterMutectCalls -O Filtered.vcf -V Try.vcf.gz; ```. And failed with following info:. ```; Using GATK jar /root/miniconda2/envs/aginome-rna/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /root/miniconda2/envs/aginome-rna/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar FilterMutectCalls -O Filtered.vcf -V Try.vcf.gz; 09:54:21.606 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/root/miniconda2/envs/aginome-rna/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:54:28.491 INFO FilterMutectCalls - ------------------------------------------------------------; 09:54:28.492 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.0.0; 09:54:28.492 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:54:28.492 INFO FilterMutectCalls - Executing as root@e19ded81d0c5 on Linux v4.15.0-55-generic amd64; 09:54:28.493 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 09:54:28.493 INFO FilterMutectCalls - Start Date/Time: August 20, 2019 9:54:21 AM UTC; 09:54:28.493 INFO FilterMutectCalls - ------------------------------------------------------------; 09:54:28.493 INFO FilterMutectCalls - ------------------------------------------------------------; 09:54:28.494 INFO FilterMutectCalls - HTSJDK Version: 2.18.2; 09:54:28.494 INFO FilterMutectCalls - Picard Version: 2.18.25; 09:54:28.494 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:54:28.494 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:54:28.494 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:54:28.495 I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6102:637,Load,Loading,637,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6102,1,['Load'],['Loading']
Performance,"I verified that the intel deflater was being loaded in the executors by checking their logs. Not all executors actually do any work on the cluster I was using, so check one that has output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1785#issuecomment-215285682:45,load,loaded,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1785#issuecomment-215285682,1,['load'],['loaded']
Performance,"I vote no -- the danger (and need to worry about the danger!) is not worth the relatively small gain. `HaplotypeCaller` might very possibly create cyclical object references, so I don't think this is merely hypothetical. I recommend we stick to safe optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1734#issuecomment-212134553:250,optimiz,optimizations,250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1734#issuecomment-212134553,1,['optimiz'],['optimizations']
Performance,"I vote to close. As per my earlier remark, performance reading from GCS buckets (at least in the case highlighted in this ticket) is just fine. The root cause was that the comparison was forgetting about the 6min it takes to place the GCS file onto the cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-265000498:43,perform,performance,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-265000498,1,['perform'],['performance']
Performance,I want to establish how well the current async reading and writing of reads and variants works and in what circumstances it should be used. Analysis here is a matrix:; - reads vs variants; - read vs write; - for variants: wide vs narrow; - for variants: compressed vs uncompressed; - performance on multiple cores vs overhead when pinned to 1 core,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1606:284,perform,performance,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1606,1,['perform'],['performance']
Performance,I was able to reproduce this problem by running 1000 concurrent read streams for a while. This allowed me to verify that my fix indeed resolves the problem (at least for my repro). I have sent a [PR](https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2083) to gcloud for review.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-303211426:53,concurren,concurrent,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-303211426,1,['concurren'],['concurrent']
Performance,"I was assuming that this would be fixed by #1433, but it only fixed the inverse of this problem. It's possible now to load HDFS files from the local runner using the full namenode path . i.e. ; `hdfs://dataflow01.broadinstitute.org/user/louisb/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam`, . Loading files with the sparkRunner and yarn-client is still failing. . We're getting a new error now though. ```; java.lang.IllegalArgumentException: unknown SAM format, cannot create RecordReader: file:/local/dev/akiezun/bin/gatk/src/test/resources/org/broadinstitute/hellbender/tools/valid.bam; at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:181); at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:151); at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:124); at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:65); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(T",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1417#issuecomment-174655123:118,load,load,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1417#issuecomment-174655123,2,"['Load', 'load']","['Loading', 'load']"
Performance,"I was investigating an issue where the depth is reported lower than expected at a given site. The default value of `--minimum-mapping-quality` is `20`, so I tried `1`, `20,` and `60`. Both values `1` and `60` give _higher_ depth (`INFO.DP`) than `20`, which is very counter-intuitive. The read pairs are overlapping, so I tried the `--do-not-correct-overlapping-quality` option, which caused this bias to go away. I'd still don't understand why increasing and decreasing the minimum mapping quality makes a difference, but it is likely to do with overlapping read pairs. ```bash; $ gatk HaplotypeCaller \; -I in.bam \; -L chr7:145945238-145945238 \; -stand-call-conf 0 \; --disable-optimizations \; --force-active -O out.vcf \; --reference /path/to/ucsc.hg19.fasta \; --minimum-mapping-quality <value>;; $ gatk --version; ...; The Genome Analysis Toolkit (GATK) v4.2.0.0; HTSJDK Version: 2.24.0; Picard Version: 2.25.0; ```. (I tried this `4.1.4.0`). `--minimum-mapping-quality 1`:; ```bash; chr7	145945238	.	A	G	7534.06	.	AC=2;AF=1.00;AN=2;DP=247;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=58.06;QD=31.52;SOR=1.050	GT:AD:DP:GQ:PL	1/1:0,239:239:99:7548,716,0; ```. `--minimum-mapping-quality 20`:; ```bash; chr7	145945238	.	A	G	267.64	.	AC=1;AF=0.500;AN=2;BaseQRankSum=2.838;DP=14;ExcessHet=3.0103;FS=6.264;MLEAC=1;MLEAF=0.500;MQ=59.06;MQRankSum=0.000;QD=22.30;ReadPosRankSum=2.208;SOR=2.022	GT:AD:DP:GQ:PL	0/1:3,9:12:28:275,0,28; ```. `--minimum-mapping-quality 60`:; ```bash; chr7	145945238	.	A	G	7150.06	.	AC=2;AF=1.00;AN=2;DP=224;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=32.06;SOR=1.008	GT:AD:DP:GQ:PL	1/1:0,223:223:99:7164,668,0. ```. <details>; <summary>test.bam</summary>. ```; @HD	VN:1.6	SO:coordinate; @SQ	SN:chr1	LN:249250621; @SQ	SN:chr2	LN:243199373; @SQ	SN:chr3	LN:198022430; @SQ	SN:chr4	LN:191154276; @SQ	SN:chr5	LN:180915260; @SQ	SN:chr6	LN:171115067; @SQ	SN:chr7	LN:159138663; @SQ	SN:chr8	LN:146364022; @SQ	SN:chr9	LN:141213431; @SQ	SN:chr10	LN:135534747; @SQ	SN:c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7124:682,optimiz,optimizations,682,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7124,1,['optimiz'],['optimizations']
Performance,"I was looking `ReadWindowWalker` and I found it very interesting for iterate over windows, but although it is similar I still think that is not solving the same problem as the `SlidingWindowWalker` for two reasons:; 1. `ReadWindowWalker` requires reads to construct the `ReadWindow`, and it is not general for both reads and variants. My main idea behind the `SlidingWindowWalker` was to perform operation over windows along the genome (or requested intervals) for any kind of source provided to the walker.; 2. On the other hand, the approach to generate the sliding windows in my implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:388,perform,perform,388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775,1,['perform'],['perform']
Performance,"I was looking into this because it is useful for me, and I have found that there is going to be redundancy between the `VariantAnnotatorEngine`code and the plugin. Here a couple of suggestions after trying to implement something in this regard time ago:. * Remove/deprecate the private class `AnnotationManager` in favor of the plugin. The current code is performing reflection operations by itself, and this can cause some problems.; * Refactor the `VariantAnnotatorEngine` constructors in favor of a constructor from the barclay plugin and a list of annotations to apply, to avoid the `AnnotationManager` implementation.; * Remove/deprecate static methods for creating an annotator engine (`ofAllMinusExcluded` and `ofSelectedMinusExcluded`) in favor of handling this in the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3287#issuecomment-316077922:356,perform,performing,356,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3287#issuecomment-316077922,1,['perform'],['performing']
Performance,"I was mistaken about this not being faster - I was using a counting function that Spark can optimise by pulling onto the map side so that the records don't go through the shuffle. I changed this to simply dump the processed reads so they have to go through the shuffle, and I got the following timings when processing a 121GB BAM file.; - With shuffle: 27 min; - No shuffle (two scans over input): 24.7 min (8% saving); - No shuffle (one scan over input): 17 min (37% saving). The version that does two scans is faster, but not hugely so. Removing a scan is possible, but requires the use of a sequence dictionary to find the end points of contigs. I've done this in the latest version of my branch (https://github.com/broadinstitute/gatk/compare/tw_overlap_partitioner), but there are more edge cases to test. Before I do this, however, it would be worth trying this approach with the Haplotype Caller to see if it works, and if it is appreciably faster. If the number of reads is filtered significantly so only a fraction go through the shuffle, then the performance gains will be smaller, and may not in fact be worth the increase in code complexity. @droazen, what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-249590040:1057,perform,performance,1057,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-249590040,1,['perform'],['performance']
Performance,"I was outputting to .vcf.gz. . I reran the command to output to just. vcf and it runs without error:; ```; /gatk-launch FilterByOrientationBias --artifactModes 'G/T' -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P ~/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk FilterByOrientationBias --artifactModes G/T -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; 01:16:16.916 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.4.3.jar!/com/intel/gkl/native/libgkl_compression.dylib; [June 6, 2017 1:16:16 AM EDT] FilterByOrientationBias --output test_filterbyorientationbias.vcf --preAdapterDetailFile /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics --artifactModes G/T --variant /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891:928,Load,Loading,928,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891,1,['Load'],['Loading']
Performance,I was running : java -jar /Users/mac/Downloads/picard-2.jar AddOrReplaceReadGroups I=/Users/mac/Desktop/NGS-/SRR6369642-pe.bam O=/Users/mac/Desktop/NGS-/SRR6369642-pe-RG.bam RGID=C7BDWACXX.5 RGLB=Lmj_A445_EP+3.2_run1 RGPL=Illumina RGPU=C7BDWACXX.5 RGSM=NO8162944 . I get :00:13:06.733 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/mac/Downloads/picard-2.jar!/com/intel/gkl/native/libgkl_compression.dylib; [Tue Sep 08 00:13:07 WEST 2020] AddOrReplaceReadGroups INPUT=/Users/mac/Desktop/NGS-/SRR6369642-pe.bam OUTPUT=/Users/mac/Desktop/NGS-/SRR6369642-pe-RG.bam RGID=C7BDWACXX.5 RGLB=Lmj_A445_EP+3.2_run1 RGPL=Illumina RGPU=C7BDWACXX.5 RGSM=NO8162944 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false; [Tue Sep 08 00:13:07 WEST 2020] Executing as mac@MacBook-Air-de-mac.local on Mac OS X 10.15.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_65-b17; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.23.0; INFO	2020-09-08 00:13:08	AddOrReplaceReadGroups	Created read-group ID=C7BDWACXX.5 PL=Illumina LB=Lmj_A445_EP+3.2_run1 SM=NO8162944. INFO	2020-09-08 00:13:08	AddOrReplaceReadGroups	Seen many non-increasing record positions. Printing Read-names as well.; fatal error . the first time is was sorting ana indexing when I do it again I get this error how should I fix it !!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6797:312,Load,Loading,312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6797,1,['Load'],['Loading']
Performance,"I was thinking more carefully on this and another option is create methods in `ReadPileup` to fix the overlaps after construction and/or getBaseCounts without overlaps. This won't break the behaviour of LIBS and it is up to the user to change overlaps. But for performance issues, I would like to have a variable in `ReadPileup` for track if the overlaps are corrected/fixed, to avoid recomputation. I can implement this in a different PR or in this one if the basic idea behind this one is not accepted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235993555:261,perform,performance,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235993555,1,['perform'],['performance']
Performance,"I will start working on this off of the work that currently resides in #6034. The proposal will be to perform KBestHaplotype finding for multiple source/sink vertexes and then perform smith waterman on the resulting ""dangling"" haplotypes that are created in order to recover the probable dangling sequence. Hopefully the number of haplotypes will have been brought down by enough that this operation will be tolerable in terms of cost.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5957#issuecomment-511024376:102,perform,perform,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5957#issuecomment-511024376,2,['perform'],['perform']
Performance,"I wondered how much of the time was due to parsing the VCF file. To test this, I used Kryo to serialize the `IntervalsSkipList` to a binary blob, then tried loading the binary blob directly. This reduced the load time from around 6 minutes to 4.7 minutes - so some speed improvement, but not a lot. See https://github.com/broadinstitute/gatk/tree/tw_known_sites_perf_kryo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-412897602:157,load,loading,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-412897602,2,['load'],"['load', 'loading']"
Performance,"I would guess that we actually do get the requested r-backports 1.1.10, but since this is an R package rather than a python package, the python package version check performed by the failing test is not applicable (i.e., you cannot `import r-backports` in python).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6955#issuecomment-726977652:166,perform,performed,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6955#issuecomment-726977652,1,['perform'],['performed']
Performance,"I would like to eventually port the indel realignment pipeline, but I don't know if I will have time. @sooheelee, maybe you can tell me if people is interested on it? I think that it is still important for Pool-Seq data, where haplotype-based calling is not usually performed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3084#issuecomment-307734736:266,perform,performed,266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3084#issuecomment-307734736,1,['perform'],['performed']
Performance,"I would like to have a look to #3447 and review it because it is quite important for downstream projects. But GitHub just show a fancy unicorn saying that ""This page is taking way too long to load"". . I wonder if this is a problem only for me, and if something can be done to be able to review before accepting it. Do you have any idea of why this is happening? @droazen or @jonn-smith, could you help me here? I would like to review before it gets in, for have minimal effects in downstream projects like mine...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3945:192,load,load,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3945,1,['load'],['load']
Performance,"I wouldn't be surprised if I could optimize the implementation of the likelihoods model quite a bit. Just having a slightly looser threshold for convergence, for example.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-449499104:35,optimiz,optimize,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-449499104,1,['optimiz'],['optimize']
Performance,"I'd also note that in the example command that @tomwhite is running uses just 4GB of memory on the driver while using a broadcast join strategy, which I believe is the default strategy used by BQSR. The [GZIP'ed dbSnp build 138 VCF in the hg18 GATK bundle](ftp://ftp.broadinstitute.org/bundle/hg18/) is ~1.4GB, and the hg18 2bit file is probably going to be ballpark 1GB of data. That's a lot of data to collect onto a driver with just 4GB of memory, so I wouldn't be surprised if the driver is OOMing. @lbergelson you'd mentioned [above](https://github.com/broadinstitute/gatk/issues/4376#issuecomment-364187885) that you haven't seen said performance regression and that it isn't showing up in the regression tests kicked off by CI. Can you confirm what driver memory settings you are using?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367150206:641,perform,performance,641,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367150206,1,['perform'],['performance']
Performance,I'll add that we do already know that the existing caching strategy massively improves Funcotator performance (by about an order of magnitude!) due to an evaluation that @lbergelson did.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5143#issuecomment-416986210:98,perform,performance,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143#issuecomment-416986210,1,['perform'],['performance']
Performance,I'll have a look at it - maybe i'll write a naive first cut for later optimization,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/104#issuecomment-77039616:70,optimiz,optimization,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/104#issuecomment-77039616,1,['optimiz'],['optimization']
Performance,I'll kick off a test run on FC soon (the queue is currently a bit backed up)...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5490#issuecomment-450968381:41,queue,queue,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5490#issuecomment-450968381,1,['queue'],['queue']
Performance,"I'll open a ticket for ref conf performance optimization, but I'm keen to get a ""draft"" of the MT joint calling pipeline into 4.1.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-451466938:32,perform,performance,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-451466938,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"I'm getting a similar error. any solutions?; ```; 17:14:13.170 INFO FuncotateSegments - The following datasources support funcotation on segments:; 17:14:13.171 INFO FuncotateSegments - Gencode 34 CANONICAL; 17:14:13.209 INFO FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode. Performing conversion.; 17:14:13.209 WARN FuncotatorEngine - WARNING: You are using B37 as a reference. Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases. There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references.; 17:14:13.411 INFO ProgressMeter - Starting traversal; 17:14:13.412 INFO ProgressMeter - Current Locus Elapsed Minutes Features Processed Features/Minute; 17:14:15.391 INFO FuncotateSegments - Shutting down engine; [September 11, 2022 5:14:15 PM GMT] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.30 minutes.; Runtime.totalMemory()=1752170496; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:917445 end:911649; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2939); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914); at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-1243013314:310,Perform,Performing,310,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-1243013314,1,['Perform'],['Performing']
Performance,"I'm getting the same issue on GATK 4.1.9.0 FilterAlignmentArtifacts. This bug has been present for 1 year. Has this been fixed?; Note: There is no work-around because FilterAlignmentArtifacts does not have a --smith-waterman option. Here is my error:; ```; 20:12:42.724 WARN FilterAlignmentArtifacts - . [1m[31m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 20:12:42.725 INFO FilterAlignmentArtifacts - Initializing engine; 20:12:48.403 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-secure-024a1aae-a4f9-4025-aa93-f759f93a8203/50383670-4607-4e59-9bfc-4db970980f0e/Mutect2/773a91ea-25be-4d49-b97c-16527076250c/call-Filter/cacheCopy/TN-20-36-filtered.vcf; 20:12:50.117 INFO FilterAlignmentArtifacts - Done initializing engine; 20:12:51.042 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 20:12:51.099 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 20:12:51.100 INFO IntelPairHmm - Available threads: 14; 20:12:51.100 INFO IntelPairHmm - Requested threads: 4; 20:12:51.100 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 20:12:51.100 INFO ProgressMeter - Starting traversal; 20:12:51.100 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 20:20:25.766 INFO ProgressMeter - chr3:104142090 7.6 1000 132.0; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007efc9818177e, pid=24, tid=0x00007f13b3c76700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # C [libgkl_smithwaterman18",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098:820,cache,cacheCopy,820,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098,2,"['Load', 'cache']","['Loading', 'cacheCopy']"
Performance,"I'm having a similar issue on gatk 4.2.6.1; ```; 17:14:13.170 INFO FuncotateSegments - The following datasources support funcotation on segments:; 17:14:13.171 INFO FuncotateSegments - Gencode 34 CANONICAL; 17:14:13.209 INFO FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode. Performing conversion.; 17:14:13.209 WARN FuncotatorEngine - WARNING: You are using B37 as a reference. Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases. There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references.; 17:14:13.411 INFO ProgressMeter - Starting traversal; 17:14:13.412 INFO ProgressMeter - Current Locus Elapsed Minutes Features Processed Features/Minute; 17:14:15.391 INFO FuncotateSegments - Shutting down engine; [September 11, 2022 5:14:15 PM GMT] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.30 minutes.; Runtime.totalMemory()=1752170496; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:917445 end:911649; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2939); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914); at o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7676#issuecomment-1252518062:309,Perform,Performing,309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676#issuecomment-1252518062,1,['Perform'],['Performing']
Performance,"I'm not sure what proportion of users leverage the incremental import functionality...it wasn't available when GenomicsDBImport was first made available, but has been around for ~3 years now. As for workspaces with whole chromosomes -- there is no requirement or performance benefits to using whole chromosomes. As you say, subsetting a chromosome to smaller regions will work and make the import and query parallelizable. (if you remember where the advice about whole chromosomes came from, let us know. That might be something that needs to be updated/clarified). Many small contigs does add overhead to import though and, till recently, multiple contigs couldn't be imported together (i.e., each contig would have it's own folder under the GenomicsDB workspace - which gets inefficient with many small contigs). For WGS, probably the best way to create the GenomicsDBImport interval list is to split based on where there are consecutive N's in the reference genome (maybe using [Picard](https://broadinstitute.github.io/picard/command-line-overview.html#ScatterIntervalsByNs)) and/or regions that you are blacklisting. I think you suggested that some of the blacklisted regions were especially gnarly - maybe ploidy or high alternate allele count? - depending on the frequency of those, we may save a bit on space/memory requirements. That may address your concern about overlap between variants and import intervals. In general, any variant that starts in a specified import interval will show up in a query to that workspace. I'm not sure if the blacklist regions contain any variants that start within but extend beyond the blacklist -- those may not show up if the regions are split up in this way.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1212486548:263,perform,performance,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1212486548,1,['perform'],['performance']
Performance,I'm posting an experimental version of this branch for performance purposes. This is no longer in a state where it should be merged.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396645011:55,perform,performance,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396645011,1,['perform'],['performance']
Performance,"I'm pretty sure this is a hadoop-bam issue, but I'm finding that any BAM produced by bwa (VN 0.7.16a-r1181) will not load in Spark. The BAM loads successfully in ValidateSamFile (although it throws errors because there are no RGs). Running it through AddOrReplaceReadGroups makes the error go away. Attempting to load from local disk gives the following error:. `htsjdk.samtools.SAMFormatException: Does not seem like a BAM file; 	at org.seqdoop.hadoop_bam.BAMSplitGuesser.<init>(BAMSplitGuesser.java:88); 	at org.seqdoop.hadoop_bam.BAMInputFormat.addProbabilisticSplits(BAMInputFormat.java:228); 	at org.seqdoop.hadoop_bam.BAMInputFormat.getSplits(BAMInputFormat.java:155); 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.getSplits(AnySAMInputFormat.java:252); 	at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:121); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	at scala.Option.getOrE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3488:117,load,load,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3488,3,['load'],"['load', 'loads']"
Performance,"I'm still not very familiar with the way people have used extensions of; Locatable, or how indexing can be used to boost performance. The stuff I've; worked on is a bit of a corner case, and I didn't write much of the; infrastructure, I've been tacking on features. For now I've just been keeping the gzipped text files from the UCSC; browser. They're tab delimited with two header lines, the first basically; giving info about context of the data (it's genome data for the , hg38) and; the second being a description of the columns (each being of form; tract_name.column_name). There's nothing at all sophisticated about this; format, but it's pretty generalizable and easy to parse (and create). An; example; >; > # hgIntegrator: database=hg38 region=genome Wed Apr 18 11:15:34 2018; >; > #gap.chrom gap.chromStart gap.chromEnd gap.type; >; > chr1 0 10000 telomere; >; > chr1 207666 257666 contig; >; > chr1 297968 347968 contig; >; > chr1 535988 585988 contig; >; > chr1 2702781 2746290 scaffold; >; >; For what it's worth, your description of your approach sounds like a; sensible one to me.; I am concerned about the size of the data and how we'd access it. I've; chosen the tracts I have because they are small enough to jam into; resources. On Tue, May 1, 2018 at 8:06 AM samuelklee <notifications@github.com> wrote:. > @TedBrookings <https://github.com/TedBrookings> which formats are you; > using, in particular?; >; > In the CNV package, I've taken pains to unify how tabular data are; > represented in Java, depending on whether each record is Locatable or; > whether the collection of records can be associated with a sample name or; > sequence dictionary. This allows us to represent records that extend; > Locatable with multidimensional numerical or non-numerical annotations; > along with some metadata (sample name and sequence dictionary) with a; > minimum of boilerplate. There are also base methods for producing interval; > trees, etc.; >; > However, this unification effort was a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551:121,perform,performance,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551,1,['perform'],['performance']
Performance,"I'm trying to genotype 2388 whole genome samples of a wild species. This species has a large genome and lots of diversity. I've created my genomicsDB for my combined set and genotypeGVCF gets stuck when trying to make the VCF. ; I've been giving it 120G of ram, 32 cores and 30 minutes and it only prints out the first variable 12 sites (corresponding to about 800bp of the genome) to the VCF . I understand that is certainly going to be slow, and I'm prepared to heavily parallelize it, but this is currently unusable to me. Is there any way to speed it up?. Here's my command:. ```; /gatk/gatk-launch --java-options ""-Xmx120G"" GenotypeGVCFs \; -R /home/user/bin/ref/reference.fa \; --intervals $contig \; -V gendb://${chr}_$pos \; -O /scratch/wild_gwas/$genomicsdb/${chr}_$pos.tmp.vcf.gz \; --seconds-between-progress-updates 5 --verbosity DEBUG; ```; Here's standard out:. > 21:13:04.092 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/build/install/gatk/lib/gkl-0.8.2.jar!/com/intel/gkl/native/libgkl_compression.so; > 21:13:04.108 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/gowens/libgkl_compression3380966567685792416.so; > 21:13:04.218 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:13:04.219 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.0.0; > 21:13:04.219 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; > 21:13:04.219 INFO GenotypeGVCFs - Executing as user@cdr806.int.cedar.computecanada.ca on Linux v3.10.0-693.5.2.el7.x86_64 amd64; > 21:13:04.219 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-2ubuntu1.16.04.3-b11; > 21:13:04.219 INFO GenotypeGVCFs - Start Date/Time: January 15, 2018 9:13:04 PM UTC; > 21:13:04.219 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:13:04.220 INFO GenotypeGVCFs - -------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:918,Load,Loading,918,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Load'],['Loading']
Performance,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2033:95,perform,perform,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033,2,"['load', 'perform']","['loading', 'perform']"
Performance,"I'm using HaplotypeCaller (`gatk-package-4.beta.5`) to analyse the control medulloblastoma sample from the ICGC benchmark https://www.nature.com/articles/ncomms10001. Reads are aligned to `GRCh37` (from the Broad bundle, without decoy sequences) using `bwa mem`. Analysis is performed within the [bcbio-nextgen](https://github.com/chapmanb/bcbio-nextgen), which splits input into chunks by chromosome for parallelisation. For some reason, the chunk corresponding to the chromosome `GL000216.1` makes HaplotypeCaller crash with the error `IllegalArgumentException: contig must be non-null and not equal to *, and start must be >= 1`. I isolated the `gatk-launch` command, and narrowed down the reproducible example to these 2 reads:; ```; H239:179:C1K3VACXX:8:2116:11771:72429 161 GL000216.1 19 23 70S31M 4 49141708 0 ATTCCCTTACATTCGGATTGATACTATTAAAATCACTTACTCTTCCTTACATTCCATTCCATCCGGGCTGTTCCATTTCATTCTATTACACTCCACTCAAT ?1:=D>?B?CC:?A,<C;:AEGC<+AC+++2+:3*1:*11999*:099?<?0?99BBG*9?D*?##################################### NM:i:2 MD:Z:25T2C2 AS:i:25 XS:i:20 RG:Z:MB_normal_50x MQ:i:0 ms:i:1911 mc:i:49141802 MC:Z:11S30M5D60M; HWI-7001436:66:C3FYFACXX:5:1216:4411:82080 65 GL000216.1 27 57 101M 9 72653232 0 CATTCTATTACACTCCATTCCATTTCTATCCATTCCATTCCATTCTATTCCATTCCACTTGGGTCGATTCAATTCCATTCCATTCTATCCCTTCCATTCCA CCCFFFFFHHHHHJJJJJIJJJJJJIJJJJHIJJJJJJJJJJJJJJJJJJJJJJJJJIJIJIJGGIJIJJJJJJJJJJJJJJJJJJGJHHHHHHFFFFFFD NM:i:7 MD:Z:24C2T14A16C1T21C4T12 AS:i:66 XS:i:36 RG:Z:MB_normal_50x MQ:i:0 ms:i:3858 mc:i:72653218 MC:Z:14S45M1D33M9; ```; And one nucleotide target region:; ```; GL000216.1 87 88; ```. These BAM and BED files are attached here: [GL000216.1_87_gatk_debug.zip](https://github.com/broadinstitute/gatk/files/1477532/GL000216.1_87_gatk_debug.zip). Running command below:; ```; gatk-launch HaplotypeCaller \; -R /data/projects/punim0010/local/share/bcbio/genomes/Hsapiens/GRCh37/seq/GRCh37.fa \ ; -I GL000216.1_start_49__read_84_88.bam \; -L GL000216.1_87-88.bed \; --output out.vcf.gz; ```. Gives",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3845:275,perform,performed,275,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3845,1,['perform'],['performed']
Performance,"I'm working to migrate GATK3's VariantEval to GATK4. This includes this code ~line 319:. // Load the sample list, using an intermediate tree set to sort the samples; final Set<String> allSampleNames = SampleUtils.getSamplesFromCommandLineInput(vcfSamples);; sampleNamesForEvaluation.addAll(new TreeSet<String>(SampleUtils.getSamplesFromCommandLineInput(vcfSamples, SAMPLE_EXPRESSIONS)));; isSubsettingSamples = ! sampleNamesForEvaluation.containsAll(allSampleNames);. Is there a GATK4 equivalent to SampleUtils.getSamplesFromCommandLineInput()? The basic features this has are: 1) Given a list of string or filepaths, it parses everything into a set, 2) Filters the list based on SAMPLE_EXPRESSIONS (which is less hard to do). Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5039:92,Load,Load,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5039,1,['Load'],['Load']
Performance,"I'm worried that we're losing the point of caching these. Do we have any idea how important this is to performance/ how many of the calls fall in the 0-9,999 range?. Would it be better to consider replacing with a threadsafe cache that actually does caching rather than a selection of precomputed values? . We could potentially use one of the [guava caches](https://code.google.com/p/guava-libraries/wiki/CachesExplained) if we actually need large values cached.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1395#issuecomment-166951570:103,perform,performance,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1395#issuecomment-166951570,5,"['Cache', 'cache', 'perform']","['CachesExplained', 'cache', 'cached', 'caches', 'performance']"
Performance,"I've also seen UnknownHostException: metadata, which seems like it's probably related. My favorite part about that exception is `This is likely because code is not running on Google Compute Engine.`. ```; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleT",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:215,concurren,concurrent,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,5,['concurren'],['concurrent']
Performance,"I've been looking at 2bit performance today, comparing ADAM release version 0.20.0 to release version 0.23.0 and to git HEAD (0.24.0-SNAPSHOT), in various use cases, and do not see any performance differences. @tomwhite `loadReferenceFile` in ADAM only supports local 2bit files, what happens in between the file in `gs://` cloud storage to when you load it via ADAM APIs?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367159861:26,perform,performance,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367159861,4,"['load', 'perform']","['load', 'loadReferenceFile', 'performance']"
Performance,"I've created a third party JAR that depends on GATK4 (4.beta.2). I believe I just hit a similar issue; however, what's odd is that it seems intermittent (though i have n=2 on this tool so far):. [October 2, 2017 6:10:01 AM PDT] com.github.discvrseq.walkers.BackportLiftedVcf done. Elapsed time: 6.27 minutes.; Runtime.totalMemory()=45813334016; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at com.github.discvrseq.walkers.BackportLiftedVcf.apply(BackportLiftedVcf.java:156); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinsti",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-333579182:422,Load,LoadSnappy,422,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-333579182,1,['Load'],['LoadSnappy']
Performance,I've created https://github.com/broadinstitute/gatk/issues/2625 to separately address a possible performance issue introduced by this branch.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2605#issuecomment-297483331:97,perform,performance,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2605#issuecomment-297483331,1,['perform'],['performance']
Performance,"I've found at least one optimization that may be invalid if SW parameters are changed (also noted by @yfarjoun https://github.com/broadinstitute/gatk/commit/4ccbaddc069539fa6e3ac2690469c1e0a4b03ccb#r31525284). I'm going to remove this for now, but will leave a TODO that it could be reintroduced if appropriate checks on the SW parameters are performed. Not sure if there are any other similar assumptions lurking...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-707870344:24,optimiz,optimization,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-707870344,2,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,"I've had ConcurrentModificationExceptions several times when mocked ReferenceMultiSource objects are both mocked and broadcast ([for example](https://travis-ci.org/broadinstitute/gatk/jobs/275982454)). Mocks appear to be mutated, even when the mocked object is immutable, so serialization can fail. This replaces the mock object with a faked one.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3586:9,Concurren,ConcurrentModificationExceptions,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3586,1,['Concurren'],['ConcurrentModificationExceptions']
Performance,"I've had this issue on MacOS X and was able to install the environment successfully by removing the open-mp and mkl lines from the yaml:; ```; - intel-openmp=2018.0.0; - mkl=2018.0.1; - mkl-service=1.1.2; ``` ; Then you may need to remove the partially installed environment with:; ```; conda remove --name gatk --all; ```; Then you can run:; ```; conda create -n gatk -f ./scripts/gatkcondaenv.yml; ```; Hopefully, the tools will run without openmp and mkl, but I'm sure we are taking a performance hit so we should figure out what is the right channel to point to for these packages. @erniebrau have you ever had these isssues?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4822#issuecomment-393235457:488,perform,performance,488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4822#issuecomment-393235457,1,['perform'],['performance']
Performance,"I've implemented the Gaussian-kernel binary-segmentation algorithm from this paper: https://hal.inria.fr/hal-01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM m",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:274,perform,performance,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666,1,['perform'],['performance']
Performance,"I've never used these, but https://github.com/jvm-profiling-tools could potentially be a source for java profilers. From reading a bit about hprof, it seems to add a lot of overhead and has questionable accuracy. About workspaces with lots of contigs/smaller contigs -- the performance issue there is mostly during import. In your experiment above to subset the workspace, did the subsetted workspace return faster for SelectVariants? Or use less memory? I'm a bit surprised if so since your query is restricted to just that single array anyway. Regarding @jjfarrell's suggestion of ReblockGVCFs -- I can't speak to any loss of precision, etc there but I would be curious to see if you could run some of your input GVCFs through that, just to see how much smaller they seem to get.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1211399696:274,perform,performance,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1211399696,1,['perform'],['performance']
Performance,"I've opened PR #1004 for this. Would you like to take a look @droazen? I haven't tried this on a cluster, but I guess you can run it side-by-side with JP's optimization.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/994#issuecomment-148692643:156,optimiz,optimization,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/994#issuecomment-148692643,1,['optimiz'],['optimization']
Performance,I've performed a release of 4.0.9.0 using the poms generated with https://github.com/broadinstitute/gatk/pull/5224 and a bit of manual work.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5212#issuecomment-424502910:5,perform,performed,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5212#issuecomment-424502910,1,['perform'],['performed']
Performance,"I've run into an error using a certain BAM file I created for testing. Possibly relevant: I also tried running it through PrintReads - all reads were filtered out by the WellFormedReadFilter because they do not have read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2219:854,load,load,854,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219,1,['load'],['load']
Performance,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2535:557,Cache,Caches,557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535,1,['Cache'],['Caches']
Performance,"I've tested this version in both GVCF and non-GVCF mode, and with and without -L intervals, and it seems to work reasonably. The output, as far as I can tell, is very close to the GATK3 output, but not an exact match. Looking in IGV I suspect that we might have a few boundary artifacts from the new traversal, but I'll have to do more work to confirm this. I've made an effort in the past few days to get the annotations, filtering, etc., closer to GATK3, and this has helped a lot. I have not tested all of the more exotic HC arguments, however, and it's very likely that a few are not completely functional. . Hooking up the native `PairHmm` resulted in a ~3x speedup and hopefully made us more competitive with GATK3 in performance -- I'll post some benchmarking numbers here later. The `isActive()` machinery was ported, but is not currently used (ie., we consider all sites as active for now). At one point I attempted a standalone tool to do the `isActive()` step, but the overhead of going through the reads twice and writing down a huge sites-level file with the results from the individual loci caused me to abandon that idea. Still, we may want to experiment more with a streamlined version of isActive() in the future if we can't get the performance we want from the current implementation. `ReadWindowWalker` has an entry point to plug in a future `isActive()` check if we choose to add one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1567#issuecomment-194847622:724,perform,performance,724,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1567#issuecomment-194847622,2,['perform'],['performance']
Performance,"I've tried the latest GATK today. The error message changed. Please help. Thanks. Using GATK jar /omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --output-file-format VCF --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38; 10:24:13.971 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:14.182 INFO Funcotator - ------------------------------------------------------------; 10:24:14.183 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.4.0-0.0.2; 10:24:14.183 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:24:14.183 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 10:24:14.184 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 10:24:14.184 INFO Funcotator - Start Date/Time: April 28, 2018 10:24:13 AM ICT; 10:24:14.184 INFO Funcotator - ------------------------------------------------------------; 10:24:14.184 INFO Funcotator - ------------------------------------------------------------; 10:24:14.185 INFO Funcotator - HTSJDK Version: 2.14.3; 10:24:14.185 INFO Funcotator - Picard Version: 2.18.2; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:24:14.186 INFO Fu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363:653,Load,Loading,653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363,1,['Load'],['Loading']
Performance,"I've uploaded the vcf file for you to test. command line:; ```shell; disk/juntong/software/gatk-4.0.7.0/gatk SelectVariants -V /disk/juntong/huada/V300029595_results/mutect_outputs/V300029595.merged.vcf.gatk.somatic.vcf.gz -O /disk/juntong/huada/V300029595_results/mutect_outputs/V300029595.merged.vcf.gatk.somatic.vcf.gatk.somatic.indel.vcf.gz -select-type INDEL; ```. log:; ```; Using GATK jar /disk/juntong/software/gatk-4.0.7.0/gatk-package-4.0.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /disk/juntong/software/gatk-4.0.7.0/gatk-package-4.0.7.0-local.jar SelectVariants -V /disk/juntong/huada/V300029595_results/mutect_outputs/V300029595.merged.vcf.gatk.somatic.vcf.gz -O /disk/juntong/huada/V300029595_results/mutect_outputs/V300029595.merged.vcf.gatk.somatic.vcf.gatk.somatic.indel.vcf.gz -select-type INDEL; 05:06:54.800 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/disk/juntong/software/gatk-4.0.7.0/gatk-package-4.0.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 05:06:55.409 INFO SelectVariants - ------------------------------------------------------------; 05:06:55.409 INFO SelectVariants - The Genome Analysis Toolkit (GATK) v4.0.7.0; 05:06:55.409 INFO SelectVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 05:06:55.410 INFO SelectVariants - Executing as juntong@train1 on Linux v3.10.0-1062.1.1.el7.x86_64 amd64; 05:06:55.410 INFO SelectVariants - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_222-b10; 05:06:55.410 INFO SelectVariants - Start Date/Time: November 6, 2019 5:06:54 AM EST; 05:06:55.410 INFO SelectVariants - ------------------------------------------------------------; 05:06:55.410 INFO SelectVariants - ------------------------------------------------------------; 05:06:55.411 INFO SelectVariants - HTSJDK Version: 2.16.0; 05:06:55.411 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6254:1323,Load,Loading,1323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6254,1,['Load'],['Loading']
Performance,"IBBLE : false; >; > 16:17:05.844 INFO HaplotypeCaller - Deflater: JdkDeflater; >; > 16:17:05.844 INFO HaplotypeCaller - Inflater: JdkInflater; >; > 16:17:05.844 INFO HaplotypeCaller - GCS max retries/reopens: 20; >; > 16:17:05.844 INFO HaplotypeCaller - Requester pays: disabled; >; > 16:17:05.845 INFO HaplotypeCaller - Initializing engine; >; > 16:17:05.928 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater; >; > 16:17:05.932 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater; >; > 16:17:06.503 INFO FeatureManager - Using codec VCFCodec to read file file:///home/robert/test/snps.vcf; >; > 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals; >; > 16:17:06.551 INFO HaplotypeCaller - Done initializing engine; >; > 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:4985,Load,Loading,4985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['Load'],['Loading']
Performance,"IIRC, the main commit in ADAM that would effect BQSR or HC was https://github.com/bigdatagenomics/adam/commit/1eed8e8e464f8f92a6e87afc1d334e751423e810#diff-abb5cd690409453a589fa8aadbfd7151 which _improved_ the performance of extracting regions from a 2 bit file. I run GATK4 HC pretty regularly with this change and haven't seen performance issues on datasets ranging in size from small targeted datasets up to WGS. Anywho, let me know if there's anything I can do to help debug the perf issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366078617:210,perform,performance,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366078617,2,['perform'],['performance']
Performance,"IMO I would only port LoglessPairHMM for a first iteration ... base on the premature optimization approach it should be Log10PairHMM as the former is an optimization of the later but I believe it makes a huge difference. . I second @droazen, I think JNI ones should be added later to get the performance back. About the GraphBased approach... I never got it to fly as it could as I was pulled to do other stuff. I think it should not be ported for now anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/543#issuecomment-108655203:85,optimiz,optimization,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/543#issuecomment-108655203,3,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ----------------------------,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4686:1496,Queue,Queue,1496,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686,1,['Queue'],['Queue']
Performance,"ION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/centos/gatk-4.beta.5/gatk-package-4.beta.5-spark.jar PrintReadsSpark -I /home/centos/storage/NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam -O /home/centos/storage/output.bam --sparkMaster spark://192.168.1.110:7077; 05:27:50.924 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 05:27:51.034 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/centos/gatk-4.beta.5/gatk-package-4.beta.5-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [October 3, 2017 5:27:51 AM UTC] PrintReadsSpark --output /home/centos/storage/output.bam --input /home/centos/storage/NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam --sparkMaster spark://192.168.1.110:7077 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 3, 2017 5:27:51 AM UTC] Executing as centos@master.novalocal on Linux 3.10.0-514.10.2.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11; Version: 4.beta.5; 05:27:52.642 INFO Print",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3651:1616,Load,Loading,1616,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3651,1,['Load'],['Loading']
Performance,"ITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : fals; 08:27:10.888 INFO Mutect2 - Deflater: IntelDeflate; 08:27:10.889 INFO Mutect2 - Inflater: IntelInflate; 08:27:10.889 INFO Mutect2 - GCS max retries/reopens: 2; 08:27:10.889 INFO Mutect2 - Requester pays: disable; 08:27:10.889 INFO Mutect2 - Initializing engin; 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.<init>(ReferenceConfidenceModel.java:116); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticReferenceConfidenceModel.<init>(SomaticReferenceConfidenceModel.java:38); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.<init>(Mutect2Engine.java:149); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.onTraversalStart(Mutect2.java:286); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:982); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:2937,multi-thread,multi-threaded,2937,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['multi-thread'],['multi-threaded']
Performance,"I_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 11:43:32.131 INFO SortSam - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:43:32.131 INFO SortSam - Defaults.REFERENCE_FASTA : null; 11:43:32.131 INFO SortSam - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 11:43:32.131 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:43:32.131 INFO SortSam - Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:43:32.131 INFO SortSam - Deflater IntelDeflater; 11:43:32.131 INFO SortSam - Initializing engine; 11:43:32.131 INFO SortSam - Done initializing engine; 11:43:42.134 INFO SortSam - Shutting down engine; [December 7, 2016 11:43:42 AM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=1890058240; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.br",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2299:1540,Load,LoadSnappy,1540,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2299,2,['Load'],['LoadSnappy']
Performance,"If @akiezun is going to pick this up, he should start from where I left off. If it gets bumped to Beta, I'd be happy to keep it in my queue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/591#issuecomment-157571184:134,queue,queue,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/591#issuecomment-157571184,1,['queue'],['queue']
Performance,"If I make a tool fail, e.g. a bad argument. the process exit status is 0 making difficult to track failure in including scripts, SGE and (perhaps?) Queue?. Failure should result in a non-zero exit status.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/342:148,Queue,Queue,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/342,1,['Queue'],['Queue']
Performance,"If I understand correctly, are you saying that the pseudocode for `loadIntervals` in case of `IntervalSetRule.UNION` is:. ```; initialize cumulative interval list to empty list; for (intervalString in intervals) {; parse the next interval; *recalculate* cumulative intervals = union(next interval, cumulative intervals); // as opposed to cumulative intervals.add(next interval); }; ```; which scales quadratically?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3788#issuecomment-341729635:67,load,loadIntervals,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3788#issuecomment-341729635,1,['load'],['loadIntervals']
Performance,"If a `VariantWalker` driving variant is indexed with tribble but does not have an sequence dictionary in the header, the dictionary is loaded from the index. Nevertheless, this is a truncated dictionary because the end coordinate for each chromosome is the last variant in that contig. Thus, even if a proper interval for the genome is provided (regarding the reference sequence), the program throw an user error exception. This could be reproduced with the following test in `ExampleVariantWalkerIntegrationTest`:. ``` java; @Test; public void testExampleVariantWalkerInvalidDictionary() throws IOException {; final IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -L 1:200-1125"" +; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -V "" + TEST_DATA_DIRECTORY + ""example_variants.vcf"" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"", Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleVariantWalkerIntegrationTest_output.txt""));; testSpec.executeTest(""testExampleVariantWalker_UndefinedContigLengthsInDictionary"", this);; }; ```. The thrown exceptions is the following:. ``` java; java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: A USER ERROR has occurred: Badly formed genome loc: Failed to parse Genome Location string: 1:200-1125; ```. This comes from the overrided method `VariantWalker.getBestAvailableSequenceDictionary()`, which prefers the one from the driving variant (in this case, the one which comes from the index), not using the one from the reference/reads if available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2081:135,load,loaded,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2081,1,['load'],['loaded']
Performance,"If a tool requires a reference, for example, it should be able to indicate so via an annotation, instead of manually checking the inherited argument value for null. The engine should perform the check on behalf of the tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/120:183,perform,perform,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/120,1,['perform'],['perform']
Performance,"If gatk-launch is run so that it invokes a local jar directly without the generated launch script, it fails to properly pass system properties. This will cause confusing bugs and performance issues. It was discovered in due to #2300. This effects all users using our packaged jars available on the website since they don't include the gradle generated wrapper script.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316:179,perform,performance,179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316,1,['perform'],['performance']
Performance,"If the GKL can't be loaded on a particular platform/architecture, it should fall back to the Java implementations transparently and without error. If it's not doing so, that's a bug.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1985#issuecomment-231369127:20,load,loaded,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985#issuecomment-231369127,1,['load'],['loaded']
Performance,"If these events were indeed not CNLOH, as we discussed, then I don't think we should merge this. Perhaps we should take a step back and answer definitively whether simply blacklisting common germline regions is enough to replicate/obviate most of the postprocessing. Should be straightforward to run an evaluation with and without blacklisting---and hopefully our truth data accurately reflects whether blacklisting is desirable. If tagging/filtering rare germline is still a concern, then I'd say the next step is to see whether simply changing segmentation parameters to artificially decrease resolution and/or simple length-based filtering suffices. Finally, simple filtering based on CR-AF as described above could be implemented. If the normal is available, we can make IS_NORMAL calls simply based on the overlap of the ModelSegments posteriors (with corresponding qualities). If not, then some heuristic determination of the normal state from the tumor alone as in Marton's caller could be performed. This would combine the IS_NORMAL calling and filtering steps into one simple tool. The output could be a tagged/filtered ModelSegments .seg file and the corresponding VCF.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-458551250:997,perform,performed,997,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-458551250,1,['perform'],['performed']
Performance,"If you run AlignAssembledContigsSpark with an incorrect BWA index version (ie. one that was generated with BWA index 0.7.12 or previous), you get executor logs that trace back to an ""IOException: File system is closed"" error, which is very misleading. I believe that this happens because Spark retries the tasks multiple times after they have failed, and in the subsequent tries the filesystem is in a bad state. It would be nice if we could either catch this error earlier, or check to make sure that the reference is compatible before trying to load it somehow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2123:547,load,load,547,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123,1,['load'],['load']
Performance,Implement Spark correctness tests on gatk-jenkins separate from performance tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2288:64,perform,performance,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288,1,['perform'],['performance']
Performance,Implement automated performance benchmarks (in Jenkins or similar),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5261:20,perform,performance,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5261,1,['perform'],['performance']
Performance,"Implements two new tools and updates some methods for a revamp of the `CombineBatches` cross-batch integration module in [gatk-sv](https://github.com/broadinstitute/gatk-sv). - `SVStratify` - tool for splitting out a VCF by variant class. Users pass in a configuration table (see tool documentation for an example) specifying one or more stratification groups classified by SVTYPE, SVLEN range, and reference context(s). The latter are specified as a set of interval lists using `--context-name` and `--context-intervals` arguments. All variants are matched with their respective group which is annotated in the `STRAT` INFO field. Optionally, the output can be split into multiple VCFs by group, which is a very useful functionality that currently can't be done efficiently with common commands/toolkits.; - `GroupedSVCluster` - a hybrid tool combining functionality from `SVStratify` with `SVCluster` to perform intra-stratum clustering. This tool is critical for fine-tuned clustering of specific variants types within certain reference contexts. For example, small variants in simple repeats tend to have lower breakpoint accuracy and are typically ""reclustered"" during call set refinement with looser clustering criteria.; - `SVStratificationEngine` - new class for performing stratification.; - Updates to breakpoint refinement in `CanonicalSVCollapser` that should improve breakpoint accuracy, particularly in larger call sets. Raw evidence support and variant quality are now considered when choosing a representative breakpoint for a group of clustered SVs.; - Added `FlagFieldLogic` type for customizing how `BOTHSIDE_PASS` and `HIGH_SR_BACKGROUND` INFO flags are collapsed during clustering.; - `RD_CN` is now used as a backup if `CN` is not available when determining carrier status for sample overlap.; - Removed no-sort option in favor of spooled sorting.; - Bug fix: support for empty EVIDENCE info fields; - Bug fix: in one of the JointGermlineCnvDefragmenter tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8990:906,perform,perform,906,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8990,3,"['perform', 'tune']","['perform', 'performing', 'tuned']"
Performance,Improve GRADLE build Performance,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7561:21,Perform,Performance,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7561,1,['Perform'],['Performance']
Performance,Improve M2 ref conf compute performance,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5561:28,perform,performance,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5561,1,['perform'],['performance']
Performance,Improve PathSeq performance on samples with large numbers of non-host reads,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5780:16,perform,performance,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5780,1,['perform'],['performance']
Performance,Improve Travis CI build Performance,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7418:24,Perform,Performance,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7418,1,['Perform'],['Performance']
Performance,Improve performance of HaplotypeCallerSpark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5263:8,perform,performance,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5263,1,['perform'],['performance']
Performance,Improve performance of JBWA native code,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1857:8,perform,performance,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1857,1,['perform'],['performance']
Performance,Improve performance of MarkDuplicatesSpark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1100:8,perform,performance,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1100,1,['perform'],['performance']
Performance,Improve performance of htsjdk CRAM code,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5205:8,perform,performance,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5205,1,['perform'],['performance']
Performance,Improve the performance of forward-backward algorithm by caching emission likelihoods,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2896:12,perform,performance,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2896,1,['perform'],['performance']
Performance,"In GATK3, this check is performed by the `BadCigarFilter`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/373#issuecomment-93023779:24,perform,performed,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/373#issuecomment-93023779,1,['perform'],['performed']
Performance,"In `CigarUtils.calculateCigar(final byte[] refSeq, final byte[] altSeq)` we have the following optimization to check for exact equality in order to avoid expensive Smith-Waterman alignment:. ```; if (Arrays.equals(refSeq, altSeq)){; return eg 101M; }; ```. We could similarly optimize a broader class of alt haplotypes by ruling out indels and returning M-only Cigars. To do this, line up the two sequences and see if there are a small number of non-contiguous mismatches. If so, each mismatch is a substitution. This optimization would shave about 5% off the runtime of both Mutect2 and HaplotypeCaller.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5459:95,optimiz,optimization,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5459,3,['optimiz'],"['optimization', 'optimize']"
Performance,"In `InfiniteRandomMatingPopulationModel` the methods `singleSampleLikelihoods`, `multiSampleHeterogeneousPloidyModelLikelihoods`, and `multiSampleHomogeneousPloidyModelLikelihoods` do almost exactly the same thing with almost identical code. It does not appear that the assumptions of a single sample or homogeneous ploidy are used for any optimizations, unless you count not repeating the ploidy getter. @lbergelson am I missing something?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1684:340,optimiz,optimizations,340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1684,1,['optimiz'],['optimizations']
Performance,"In a nutshell, `GenotypeLikelihoodsAllelePair GenotypeLikelihoods.getAllelePair(final int PLindex)` returns the cached alleles corresponding to the PLindex. This cache is diploid because the method that initializes it, `GenotypeLikelihoodsAllelePair[] GenotypeLikelihoods.calculatePLcache(final int altAlleles)`, uses a diploid number of likelihoods ( `numLikelihoods(1 + altAlleles, 2)`) and adds an allele pair each PLindex, instead of the ploidy number of allele indices. If the methods are generalized, they should return `ArrayList<int>` instead of `GenotypeLikelihoodsAllelePair`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1382#issuecomment-167845181:112,cache,cached,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1382#issuecomment-167845181,2,['cache'],"['cache', 'cached']"
Performance,"In addition, I haven't been very through about how the random number generator is interacting with the multi-threading part, so just to be cautious: https://stackoverflow.com/questions/22313552/contention-in-concurrent-use-of-java-util-random",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5146#issuecomment-419549217:103,multi-thread,multi-threading,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146#issuecomment-419549217,2,"['concurren', 'multi-thread']","['concurrent-use-of-java-util-random', 'multi-threading']"
Performance,"In doing continued profiling of the HaplotypeCaller GVCF mode I have observed that somewhere in the range of 12% of our overall runtime (after i've made my other optimizations) is spent in `VariantContextBuilder.make()` upon further investigation I have noticed that we are currently building a VariantContext object for each pileup in `ReferenceConfidenceModel.calculateReferenceConfidence()`. This means that we are building a unique VariantContext object for essentially every spot on the genome. VariantContext object building represents a significant overhead in terms of validation and construction and memory usage. I suspect that if we were to create some reduced object without as much overhead we could save ourselves a lot of trouble time and memory merging these things. Unfortunately I think the merging of these context objects happens in the GVCF writer which means it won't be a trivial change to make to the engine. Perhaps it is worth investigating what can be done to this code, as it represents another size-able chunk of speedup if we can squash it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5618:162,optimiz,optimizations,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5618,1,['optimiz'],['optimizations']
Performance,"In doing some performance evaluation work for some other HaplotypeCaller work I have noticed that there is apparently a performance regression on the order of perhaps 10-20% of runtime. Running locally I find that running over the same section of a WGS chromosome 15 on the current master 78a9ecd3123fdb77acf3dd7a73b0c12bf4602a1c vs the release 4.1.5.0 i get the following results: . Master: ; real	12m19.765s; user	13m49.276s; sys	0m8.571s; 4.1.5.0: ; real	9m50.558s; user	11m11.924s; sys	0m10.193s. Doing some very cursory digging it would appear that the culprit is in the HMM adjacent code being slowed down. (Note the relative runtime of HMM vs SW) ; Master: ; <img width=""822"" alt=""Screen Shot 2020-04-23 at 1 28 52 PM"" src=""https://user-images.githubusercontent.com/16102845/80130392-80115780-8566-11ea-8f2b-a6978ac71d39.png"">. 4.1.5.0: ; <img width=""850"" alt=""Screen Shot 2020-04-23 at 1 28 33 PM"" src=""https://user-images.githubusercontent.com/16102845/80130396-80115780-8566-11ea-9a1e-1e923bef47a5.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6567:14,perform,performance,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6567,2,['perform'],['performance']
Performance,"In general our germline tools are designed for short variants. I don't think any of them will handle a millions long indel well or at all. The SV or CNV tools sound like a better fit although I'm not sure exactly if they cover your use case exactly. Typically we process short variants and long variants like this separately. . We should be detecting this variant up front on when loading into genomicsDB if it's going to be problematic to retrieve it, and we should be giving a better error message. I don't think we'll be able to handle it through GenotypeGVCFs in any helpful way though. (The best I can imagine it doing is passing it through ungenotyped.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1376029770:381,load,loading,381,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1376029770,1,['load'],['loading']
Performance,"In order for the variant calling pipeline to be able to scale to process the 68K genomes in the next version of gnomAD and beyond, we need to limit the amount of genotype data we localize. For the filtering portion of the pipeline we could greatly improve performance using a ""sites-only"" query from GenomicsDB. The result could be in BCF format (as I believe it is now) and should include the first 8 fields of the VCF line (CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO) but should not return format-level/genotype data.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3688:256,perform,performance,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3688,1,['perform'],['performance']
Performance,"In order of priority:. 1) The ability to query and/or stream intervals for locatable collections might reduce the overhead of file localization in the germline workflows---even though we only run GermlineCNVCaller on a subset of intervals in any particular shard, we localize the entire read-count file. This could be enabled in the parent class to benefit all locatable files, but since it will probably require indexing, we should use only when necessary.; 2) Memory requirements for some tools could be reduced by avoiding intermediate creation of an internally held list, streaming it directly instead.; 3) NIO streaming of entire files to/from buckets could be easily added to the relevant CSV/HDF5 read/write classes. Apart from the first issue, I don't think this really adds much, since the largest files are only ~1GB (and most seg files are much smaller) and are typically cheap to localize for single samples. See also #3976, #4004, #4717, and #5715 for context. I think we should first demonstrate if the first issue is really the dominating cost in the germline pipeline. If not, we should first focus on optimizing inference. The other issues are much lower priority.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5716:1118,optimiz,optimizing,1118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5716,1,['optimiz'],['optimizing']
Performance,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:1391,load,load,1391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495,1,['load'],['load']
Performance,"In particular, can we improve the performance of `--strict` mode to the point where it could become the default?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5263#issuecomment-460399213:34,perform,performance,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5263#issuecomment-460399213,1,['perform'],['performance']
Performance,"In running and re-running GvsPrepareCallset.wdl, one past run did not use compressed references, so that is always used with call caching is turned on (which it is by default), even though the dataset has reingested compressed references since then. This is the exact scenario that GetBQTableLastModifiedDatetime was created for  database-based tasks that we want to be able to call cache accurately. Integration run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ea2ecb01-f35f-441a-ba08-1e7938da2ebe (single failure is for ExtractFilterTask.GvsCreateFilterSet.BigQuery Query Scanned ""The relative difference between these is 0.0507051, which is greater than the allowed tolerance (0.05)"")",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8667:384,cache,cache,384,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8667,1,['cache'],['cache']
Performance,"In scripts/gatkcondaenv.yml.template, the following line produces an error upon the initial attempt to create the conda environment:. `anaconda::tensorflow=1.12.0=mkl_py36h69b6ba0_0`. It seems that the up to date dependency should be:. `anaconda::tensorflow=1.12.0=mkl_py36h2b2bbaf_0`. To replicate, perform a clean clone, with no conda environment created yet, and then:. `$ ./gradlew localDevCondaEnv`. This will fail with:. `ResolvePackageNotFound: - anaconda::tensorflow==1.12.0=mkl_py36h69b6ba0_0 `",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6369:300,perform,perform,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6369,1,['perform'],['perform']
Performance,"In some preliminary testing I've done it looks like using native Hadoop libraries can speed up tools running in Spark local mode. In private Spark tools under development (which travers a WGS BAM and then performing several shuffles) I have seen speedups of up to 40% (~ 46 minutes -> 26 minutes). An initial test of `MarkDuplicatesSpark` using a 30GB bam file gave me a 9% speedup (logs are below). It might be good to investigate making this easier for users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746:205,perform,performing,205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746,2,"['load', 'perform']","['load', 'performing']"
Performance,"In the classic GATK, walkers had the option to be multi-thread in two different ways:. * `NanoSchedulable` for thread-safe `map()` calls.; * `TreeReducible` for thread-safe `map()` and `reduce()` calls. Because now the new framework's walkers have only one `apply()` function, maybe the previous design is not applicable. Nevertheless, it will be useful to implement a way to allows a tool to apply the function in a multi-thread way. Is there any plan to implement something similar in GATK4?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345:50,multi-thread,multi-thread,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345,2,['multi-thread'],['multi-thread']
Performance,In the discussion in this branch https://github.com/broadinstitute/gatk/pull/6351#pullrequestreview-1430841832 we were tripped up by the fact that the Carrot tests were showing a slight (between 5 and 7% on the aggregated `$ time` command output across 50 shards) runtime regression in the current version of HaplotypeCaller compared with a misconfigured older version of the tool. Specifically the faster older version was broadinstitute/gatk-nightly:2022-03-04-4.2.5.0-9-gb097f75c5-NIGHTLY-SNAPSHOT which was before the Java 17 migration (which is a high likelihood culprit form the past year). . Somebody should spend a few hours with a profiler to make sure there isn't some obvious culprit. . Here is the command that Carrot was running:; ```HaplotypeCaller \; -R /cromwell_root/dsp-methods-carrot-data/test_data/haplotypecaller_tests/Homo_sapiens_assembly38.fasta \; -I gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/chm1_chm13_hiseqx_sm_hf3mo.bam \; -L /cromwell_root/dsde-methods-carrot-prod-cromwell/VariantCallingCarrotOrchestrated/9886a710-334a-41eb-a495-6968d322730a/call-CHMSampleHeadToHead/VariantCallingCarrot/63594353-145d-4c4a-a713-352ad41ff3e6/call-ScatterIntervalList/cacheCopy/glob-cb4648beeaff920acb03de7603c06f98/10scattered.interval_list \; -O CHM113.g.vcf.gz \; -contamination 0.0 \; -G StandardAnnotation -G StandardHCAnnotation -G AS_StandardAnnotation \; \; \; \; -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \; -ERC GVCF \; ```; And a shard where a significant slowdown was observed spanned the region `chr3:55313816` -> `chr3:113699078` which should hopefully provide a good starting point for anybody investigating this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8327:1201,cache,cacheCopy,1201,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8327,1,['cache'],['cacheCopy']
Performance,"In the previous version of GATK, a multi-sample pileup (already stratified by sample) was handled with a different class to be more efficient while performing operations by sample or re-splitting. This was done in a very complicated way, using [`PileupElementTracker`](https://github.com/broadgsa/gatk/blob/0b73e380436aaa5a41fb3aab97ab651207669f47/public/gatk-utils/src/main/java/org/broadinstitute/gatk/utils/pileup/PileupElementTracker.java) with different implementations of a `ReadBackedPileup` interface. Instead of using a `List<PileupElement>` internally, `ReadPileup` could have an `ElementTracker` field, that could implement splitting, sorting by position and other operations to improve the efficiency. This may solve issues like https://github.com/broadinstitute/gatk/issues/2245. In addition, it may improve the performance for a `LocusWalker` that needs the reads by sample, because the `LocusIteratorByState` already split by sample name the reads and pass them with a map.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2309:148,perform,performing,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2309,2,['perform'],"['performance', 'performing']"
Performance,"In the process of designing correctness tests for `MarkDuplicatesSpark`, @davidadamsphd has come up with a potential set of optimizations to `MarkDuplicatesSpark` that have the potential to improve performance by an order of magnitude. The task here is to meet with @davidadamsphd, get access to and understand his optimizations, and port them to the main `MarkDuplicatesSpark` tool (along with any other optimizations you feel are appropriate).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1100:124,optimiz,optimizations,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1100,4,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"In the process of working to generify the `KBestHaplotypeFinder` it has come to my attention that the method `KbestHaplotypeFinder.removeCyclesAndVerticesThatDontLeadToSinks()` seems inefficient. Specifically it calls out to `findGuiltyVerticesAndEdgesToRemoveCycles()` which is a recursive method that attempts to search every child path through the graph and determine which ones either don't lead to sinks or loop back on themselves, adding rule breaking edges to a list. The performance cost seems like it would relate to the size of the graph which would have a bigger impact on the non-seq graph generated after #5922. Currently no information is preserved between recursive calls meaning that much work will be repeated (for instance a dynamic approach could save the work of revisiting already visited edges). This may be complicated somewhat by the pruning mechanism this method uses to generate parent vertexes. . This is low priority since this method will likely be entirely replaced by a new approach in #5923 later anyway.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5954:479,perform,performance,479,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5954,1,['perform'],['performance']
Performance,"In this branch are a number of improvements and changes that form the baseline for the current ongoing evaluation of the DRAGEN/GATK pipeline. This represents the joint work of both msyelf and @vruano. The major improvements in this branch are as follows:; - `EstimateDragstrModelParameters` tool for estimating the per-sample/per-STRType errors for use in the HMM gap open/gap close penalties as well as the necessary changes to the PairHMM loading code in order to adjust the model appropriately.; - Support for using the DragstrParams and flat SNP priors to compute genotype posteriors and the support for using them in the selection of genotypes as well as for computing the QUAL score. ; - Base Quality Dropout (BQD) model which penalizes variants with low average base quality scores among genotyped reads and reads that were otherwise excluded from the genotyper. A number of additional arguments to expose internal behaviors in the readThreadingAssembler and HaplotypeCaller have been made in order to support threading more lowBQ reads through to the genotyper. ; - Foreign Read Detection (FRD) model which uses an adjusted mapping quality score as well as read strandedness information to penalize reads that are likely to have originated from somewhere else on the genome. A number of additional arguments and behaviors have been exposed in order to preserve lower mapping quality reads in the HaplotypeCaller in service.; - Dynamic Read Disqualification, allows for longer/lower base quality reads to be less likely to be rejected by eliminating the hard cap on quality scores and further adjusting the limit based on the average base quality for bases in the read. . Design decisions that I would direct the reviewers attention to as they correspond to potentially dangerous/controversial changes:; - Because FRD/BQD require low quality ends to be included in the models for genotyping, I have added the option to softclipLowQualityEnds (as opposed to their current treatment which involv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6634:442,load,loading,442,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6634,1,['load'],['loading']
Performance,"In this case, it may make sense to insert an in-memory cache layer between Spark and Isilon for holding those intermediate datasets. For example, if Alluxio (Tachyon) is used, the benefits of locality could still be preserved and the I/O pressure to network as well as to Isilon storage will be largely released. GATK pipeline would achieve memory speed data sharing across different computational stages. The following is one of study cases:; [Making the Impossible Possible with Tachyon: Accelerate Spark Jobs from Hours to Seconds](https://dzone.com/articles/Accelerate-In-Memory-Processing-with-Spark-from-Hours-to-Seconds-With-Tachyon)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1508#issuecomment-188793060:55,cache,cache,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1508#issuecomment-188793060,1,['cache'],['cache']
Performance,"Included is a JProfiler screenshot of where it is spending all of its time. As you can see below, it has not even run any of the ```LocusWalker.apply(...)``` code.; ```; lichtens@OncobuntuMk3:~/IdeaProjects/hellbender-protected$ java -jar build/libs/gatk-protected.jar Pileup -I ~/broad_oncotator_configs/hcc_purity/SM-74NEG.bam -O ~/broad_oncotator_configs/pileup.out -L /home/lichtens/broad_oncotator_configs/allchr.1kg.phase3.v5a.snp.maf10.biallelic.recode.fixed.prune5.trim1M.test.interval_list; 15:04:36.262 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/home/lichtens/IdeaProjects/hellbender-protected/build/libs/gatk-protected-all-0556d5b-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 15:04:36.344 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [January 25, 2017 3:04:36 PM EST] org.broadinstitute.hellbender.tools.walkers.qc.Pileup --output /home/lichtens/broad_oncotator_configs/pileup.out --intervals /home/lichtens/broad_oncotator_configs/allchr.1kg.phase3.v5a.snp.maf10.biallelic.recode.fixed.prune5.trim1M.test.interval_list --input /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bam --showVerbose false --outputInsertLength false --maxDepthPerSample 0 --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [January 25, 2017 3:04:36 PM EST] Executing as lichtens@OncobuntuMk3 on Linux 3.19.0-30-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_51-b16; Version: Version:0556d5b-SNAPSHOT; 15:04:36.348 INFO Pileup - Defaults.BUFFER_SIZE : 131072; 15:04:36.348 INFO Pileup - Defaults.COMP",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2356:544,load,load,544,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356,2,['load'],"['load', 'loaded']"
Performance,"Increasing the vcf size would give us a better sense of how the broadcast scales with input size -- I don't think you have enough data points to draw any conclusions there yet. Also, you could increase the memory per executor to allow the larger broadcasts to work -- point is to figure out whether the network/bittorrent protocol is a bottleneck.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-210726420:336,bottleneck,bottleneck,336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-210726420,1,['bottleneck'],['bottleneck']
Performance,"Inflater; 16:51:50.649 INFO HaplotypeCaller - GCS max retries/reopens: 20; 16:51:50.649 INFO HaplotypeCaller - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 16:51:50.649 INFO HaplotypeCaller - Initializing engine; 16:51:51.056 INFO FeatureManager - Using codec BEDCodec to read file file:///home/vlad/tmp/debug_gatk/bad_87-88.bed; 16:51:51.063 INFO IntervalArgumentCollection - Processing 1 bp from intervals; 16:51:51.068 INFO HaplotypeCaller - Done initializing engine; 16:51:51.075 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 16:51:51.293 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 16:51:51.509 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 16:51:51.762 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/vlad/bcbio/anaconda/share/gatk4-4.0b5-0/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_utils.so; 16:51:51.764 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/vlad/bcbio/anaconda/share/gatk4-4.0b5-0/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 16:51:51.795 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 16:51:51.796 INFO IntelPairHmm - Available threads: 32; 16:51:51.796 INFO IntelPairHmm - Requested threads: 4; 16:51:51.796 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 16:51:51.815 INFO ProgressMeter - Starting traversal; 16:51:51.815 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 16:51:51.881 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 16:51:51.881 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 16:51:5",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3845:8332,Load,Loading,8332,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3845,1,['Load'],['Loading']
Performance,Integrate GenomicsDB load VCFs java interface,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2087:21,load,load,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2087,1,['load'],['load']
Performance,"Integration run [in progress](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/544fb86b-ffb7-447c-b380-fdefce10be99). Does away with the `STARTED` and `FINISHED` sample load statuses to more explicitly record what work has actually been done for a sample: `REFERENCES_LOADED`, `VARIANTS_LOADED` or `HEADERS_LOADED`. Legacy `FINISHED` and `STARTED` states are recognized and handled appropriately (short circuiting data load and being ignored, respectively).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8674:192,load,load,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8674,2,['load'],['load']
Performance,Integration tests look okay (there's a Python one that says to skip if it's in docker):; - org.broadinstitute.hellbender.utils.python.StreamingPythonExecutorIntegrationTest#testRequirePythonEnvironment; Unit tests:; - org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest#testLikelihoodsFromHaplotypes; - org.broadinstitute.hellbender.utils.io.IOUtilsUnitTest#testUnsuccessfulCanReadFileCheck (intended to be skipped); - fixed ; No variant calling tests ignored; No python tests ignored; No R tests ignored. The PairHMM one returns:; ```; 03:44:48.410 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga7585161099923450811.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); ```; I don't know if that's expected or not -- maybe yes because it's looking for an FPGA?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5339#issuecomment-592086345:601,load,load,601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5339#issuecomment-592086345,1,['load'],['load']
Performance,Intel deflater doesn't always perform well on smaller data at compression level 1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3413:30,perform,perform,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3413,1,['perform'],['perform']
Performance,"Intel-optimized TF 1.9, AVX support",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5142:6,optimiz,optimized,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5142,1,['optimiz'],['optimized']
Performance,Intel-optimized tensorflow package specified in the conda env doesn't work on osx-64,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6325:6,optimiz,optimized,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6325,1,['optimiz'],['optimized']
Performance,"IntelliJ seems to be having a bit of difficulting loading GATK these days:; > build.gradle error (413,0); > Received fatal alert: protocol_version",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2642#issuecomment-298437516:50,load,loading,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2642#issuecomment-298437516,1,['load'],['loading']
Performance,"Interesting, it's definitely possible it's coming from one of the other buckets. I don't think we have fine grained control over WHICH bucket we attempt to read requester pays status from, so it's possible if it's enabled it's necessary to have that permission on every bucket. It's annoying that the error message doesn't say which reader is performing the access. Is there a longer stack trace available?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7492#issuecomment-934908586:343,perform,performing,343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492#issuecomment-934908586,1,['perform'],['performing']
Performance,"Interesting, that's somewhat disturbing news, I wonder if we're paying for ssd's without actually being able to use them... It's also possible there's a different setting that's configuring the ssd's to be used for shuffle output. . We should investigate this further and 1) see if setting spark.local.dir makes a performance difference 2) ask the dataproc team about this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283418564:314,perform,performance,314,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283418564,1,['perform'],['performance']
Performance,"Interesting. At some point we do expect to see diminishing returns when adding more cores, because the parts that parellelize poorly will start to dominate, but I would hope that it's with more than 16 cores... . One thing we've seen is that performance can be harmed by using to many cores / executor. We've seen problems due to lock contention within spark on executors that have more than ~8 cores. I might try running 4 executors with 8 cores each and seeing if that's an improvement vs 2 executors with 16 cores each. . We're currently heavily re-writing MarkDuplicatesSpark which may be an improvement in the near future.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4516#issuecomment-371856207:242,perform,performance,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4516#issuecomment-371856207,1,['perform'],['performance']
Performance,"Interesting. Sorry this is causing so much trouble. From one of your above comments I wasn't clear if the solution using `--conf 'spark.submit.deployMode=cluster'` work correctly or not. . Is it possible that it's correct behavior for it to fail with the linkage error? According to the [mapr doc](https://maprdocs.mapr.com/52/DevelopmentGuide/c-loading-mapr-native-library.html) that command causes it to expect the application to load the library itself, but GATK by default doesn't have a copy of MAPR and won't load it on it's own. Have you included the mapr library somehow into the gatk jar? Or is it provided to spark some other way? I don't really know how maprfs works and how it interacts with hadoop paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350315653:346,load,loading-mapr-native-library,346,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350315653,3,['load'],"['load', 'loading-mapr-native-library']"
Performance,IntervalUtils::loadIntervals gives bad error on missing .interval_list file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6956:15,load,loadIntervals,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6956,1,['load'],['loadIntervals']
Performance,Investigate if using Spark Datasets (and Spark SQL) in MarkDuplicatesSpark improves performance and/or simplifies the code.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6103:84,perform,performance,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6103,1,['perform'],['performance']
Performance,Investigate performance of PoN of NA12878 replicates.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3152:12,perform,performance,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3152,1,['perform'],['performance']
Performance,Investigate performance of enum vs byte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/115:12,perform,performance,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/115,1,['perform'],['performance']
Performance,Investigate performance regression in ApplyBQSR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4618:12,perform,performance,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4618,1,['perform'],['performance']
Performance,Is it a real performance gain?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/312#issuecomment-82519315:13,perform,performance,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/312#issuecomment-82519315,1,['perform'],['performance']
Performance,"Is there a script that can be tuned to stop putting the legend of VQSR plots on top of the plot?; It is masking important information as shown below.; When this is using ggplot2, the legend could be captured and plotted as a separate grid object next to the plot (for instance). ![example](https://i.imgur.com/53rbF0c.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6585:30,tune,tuned,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6585,1,['tune'],['tuned']
Performance,Is there a way to have java load a config file as system properties on startup?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267124998:28,load,load,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267124998,1,['load'],['load']
Performance,"Is your GenomicDB named as your contig ? If so it may produce an error if your working directory is the Genomic DB folder, see my comment here : . https://gatk.broadinstitute.org/hc/en-us/community/posts/26901826831899-GenotypeGVCFs-issue-when-using-include-non-variant-sites-parameter. In addition the option -L needs to be set in in both the GenomicDBimport step and the GenotypeGVCFs step, see below : . `singularity exec --bind /nvme/disk0/lecellier_data:/nvme/disk0/lecellier_data /home/hdenis/gatk_latest.sif gatk --java-options ""-Xmx15g -Xms4g"" GenomicsDBImport --genomicsdb-workspace-path ""${INDIR}GenomicDB/${CONTIG_NAME}"" --batch-size 50 -L $CONTIG --sample-name-map ""${INDIR}aspat_gvcf_clean.sample_map"" --tmp-dir /nvme/disk0/lecellier_data/WGS_GBR_data/tmp --reader-threads 7 --genomicsdb-shared-posixfs-optimizations true --bypass-feature-reader true `. `singularity exec --bind /nvme/disk0/lecellier_data:/nvme/disk0/lecellier_data /home/hdenis/gatk_latest.sif gatk --java-options ""-Xmx58g"" GenotypeGVCFs -R $REF_3 -V ""gendb://${INDIR}GenomicDB/${CONTIG_NAME}"" -O ""${OUTDIR}aspat_clean_${CONTIG_NAME}.vcf.gz"" --tmp-dir /nvme/disk0/lecellier_data/WGS_GBR_data/tmp --include-non-variant-sites true -L $CONTIG --only-output-calls-starting-in-intervals true`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2212079735:816,optimiz,optimizations,816,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2212079735,1,['optimiz'],['optimizations']
Performance,Issue solved after updating to the recent version and using the `--genomicsdb-shared-posixfs-optimizations true` flag.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7007#issuecomment-748076928:93,optimiz,optimizations,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7007#issuecomment-748076928,1,['optimiz'],['optimizations']
Performance,Issues that need to be addressed as part of this ticket:; - support for arguments defined in filter classes; - PluginManager -- to port or not to port?; - performance issues with searching the classpath; - separating out the readmetrics code from the code that applies filters,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6#issuecomment-69800773:155,perform,performance,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6#issuecomment-69800773,1,['perform'],['performance']
Performance,"It all depends on the network you are running from, as noted in the following discussion:. https://github.com/googlegenomics/utils-java/issues/9#issuecomment-60502722. So the closer you are to the data such as through GCE, and launching a GCE instance from a [relatively similar zone](https://cloud.google.com/compute/docs/regions-zones/regions-zones) such as (`us-east1-b`, `us-east1-c`, `us-east1-d`) the quicker the result. Sometimes the setup time to launch the instance might take some time as well. I don't have a setup as the Broad to run the same test and determine what might be happening, but I just re-ran the following test on an external (non-GCE) cluster and below are the results for a 1.46 GB file, which seem to come closer to @jean-philippe-martin's most recent results (and projected using my throughput, a 34.56 GB file would take about 13 min 38 sec, but not 55 min):. ``` Bash; $ gsutil ls -l gs://pgp-harvard-data-public/hu011C57/GS000018120-DID/GS000015172-ASM/GS01669-DNA_B05/ASM/REF/coverageRefScore-chr1-GS000015172-ASM.tsv.bz2. 1563675749 2014-04-24T20:26:25Z gs://pgp-harvard-data-public/hu011C57/GS000018120-DID/GS000015172-ASM/GS01669-DNA_B05/ASM/REF/coverageRefScore-chr1-GS000015172-ASM.tsv.bz2; TOTAL: 1 objects, 1563675749 bytes (1.46 GiB). $; $ time(gsutil cp -L transfer_statistics.txt gs://pgp-harvard-data-public/hu011C57/GS000018120-DID/GS000015172-ASM/GS01669-DNA_B05/ASM/REF/coverageRefScore-chr1-GS000015172-ASM.tsv.bz2 . ). Copying gs://pgp-harvard-data-public/hu011C57/GS000018120-DID/GS000015172-ASM/GS01669-DNA_B05/ASM/REF/coverageRefScore-chr1-GS000015172-ASM.tsv.bz2...; Downloading ..././coverageRefScore-chr1-GS000015172-ASM.tsv.bz2: 372.81 MiB/372.81 MiB; Downloading ..././coverageRefScore-chr1-GS000015172-ASM.tsv.bz2: 372.81 MiB/372.81 MiB; Downloading ..././coverageRefScore-chr1-GS000015172-ASM.tsv.bz2: 372.81 MiB/372.81 MiB; Downloading ..././coverageRefScore-chr1-GS000015172-ASM.tsv.bz2: 372.81 MiB/372.81 MiB; WARNING: Found no hashes to v",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-227913893:812,throughput,throughput,812,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-227913893,1,['throughput'],['throughput']
Performance,"It does seem that way. I don't think it's `serialVersionUID`, that usually manifests as failures to deserialize rather than class cast problems. I think it's likely an issue with 2 different class loaders loading that same class (based on discussion in #1386.) Possibly something analogous to https://issues.apache.org/jira/browse/SPARK-1403, but on yarn.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1315#issuecomment-188423698:197,load,loaders,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315#issuecomment-188423698,2,['load'],"['loaders', 'loading']"
Performance,"It doesn't iterate over the overlapping intervals, it just checks whether any overlap at all. I've created a branch that uses `OverlapDetector` here: https://github.com/tomwhite/Hadoop-BAM/tree/intervals-optimization, which you can use to check if the changes address the performance issues you were seeing. . It would be good if `OverlapDetector` could take a `Locatable` so we can pass in `SAMRecord` directly and not create a new `Interval` object. Having an `overlaps()` method that returns a boolean would be nice too. I'm not sure how unmapped reads that have a coordinate set are handled. They may need a special case, as in https://github.com/HadoopGenomics/Hadoop-BAM/compare/master...tomwhite:intervals-optimization#diff-60ad8bad94dec0448bd32dcfebfdd6f4L234.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209888970:204,optimiz,optimization,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209888970,3,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"It got slow after the big rebase before the branch was code reviewed, I believe. Some dependent class may have been changed in a way that adversely affected the optimized BQSR during the window of time between rebases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1006#issuecomment-148840243:161,optimiz,optimized,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1006#issuecomment-148840243,1,['optimiz'],['optimized']
Performance,"It is not designed to enable you to do that. It uses a scatter-gather approach for parallelism. You'll want to submit multiple tasks that each process a disjoint interval of the genome. Personally, I have wondered how Mutect2 performs for variants at the borders of interval boundaries.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7688#issuecomment-1046371449:226,perform,performs,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7688#issuecomment-1046371449,1,['perform'],['performs']
Performance,"It just seems a bit scary to be mucking about with start/end indexes over multiple pages when we don't have to be. I suspect we may be potentially losing out on some optimizations if we're creating new requests for each page instead of following up on the original one, although that's just speculation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/869#issuecomment-135861754:166,optimiz,optimizations,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/869#issuecomment-135861754,1,['optimiz'],['optimizations']
Performance,"It loads all reads for each shard at a time, but it could be lazier and load only 1-2 assembly regions' worth of reads.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3516:3,load,loads,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3516,2,['load'],"['load', 'loads']"
Performance,"It looks like Hadoop-BAM uses Hadoop's TotalOrderPartitioner to do the sort in parallel (i.e. using more than one reducer) in such a way as to have a total ordering, so that reads in partition _i_ are all less than those in _i+1_. Then the output files are concatenated. This will need some alterations to get it working in Spark, since TotalOrderPartitioner relies on the distributed cache (which Spark doesn't have). I can take a look at this if you like, @akiezun.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1094#issuecomment-157406851:385,cache,cache,385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1094#issuecomment-157406851,1,['cache'],['cache']
Performance,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into /home/travis/site-library; (as lib is unspecified); Error: (converted from warning) dependencies rlang, vctrs are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6072:65,cache,cache,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072,1,['cache'],['cache']
Performance,"It looks like at one point GATK 3.8 was available via the [homebrew science tap](https://github.com/ilovezfs/homebrew-science/blob/master/gatk.rb). I've tried adding their formula to my homebrew formula folder and installing via ``` brew install gatk.rb``` but there's a ton of errors. ```Updating Homebrew...; ==> Downloading https://github.com/broadgsa/gatk-protected/archive/3.8-1.tar.gz; Already downloaded: /Users/timothystiles/Library/Caches/Homebrew/gatk-3.8-1.tar.gz; ==> mvn package -Dmaven.repo.local=${PWD}/repo; Last 15 lines from /Users/timothystiles/Library/Logs/Homebrew/gatk/01.mvn:; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR]; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR]; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR]; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4164#issuecomment-358697586:441,Cache,Caches,441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4164#issuecomment-358697586,1,['Cache'],['Caches']
Performance,It looks like it made the tests substantially slower.... I'm not totally clear on why. Maybe because it has to re-optimize code every time it restarts the jvm.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6093#issuecomment-521372663:114,optimiz,optimize,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6093#issuecomment-521372663,1,['optimiz'],['optimize']
Performance,"It looks like the conda env recently started resolving h5py to v3.1.0, which in turn appears to be incompatible with the keras version we're using, causing the CNNScoreVariants integration tests to fail when keras tries to load the model file (see https://github.com/tensorflow/tensorflow/issues/44467). This PR pins the version to the version used by the last build I could find that succeeded, which is 2.10.0.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6955:223,load,load,223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6955,1,['load'],['load']
Performance,"It looks like the errors are all of the flavor:. ```; Unable to load Maven meta-data from https://artifactory.broadinstitute.org/artifactory/libs-snapshot/com/github/samtools/htsjdk/2.9.1-34-gd7bae17-SNAPSHOT/maven-metadata.xml.; > Could not GET 'https://artifactory.broadinstitute.org/artifactory/libs-snapshot/com/github/samtools/htsjdk/2.9.1-34-gd7bae17-SNAPSHOT/maven-metadata.xml'. ; Received status code 401 from server: Unauthorized; ```; Perhaps Maven's temporarily in a bad mood, we'll have to try again later.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-306264180:64,load,load,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-306264180,1,['load'],['load']
Performance,"It looks like the tests are running very slowly because of a performance regression due to the changes in `IntegrationTestSpec.assertEqualTextFiles`. You can see this on tests that should be unaffected by the core changes in this PR, like the VQSR integration tests, which have no cloud/bucket dependency and usually take about [1 minute](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_24775.2/tests/test/classes/org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibratorIntegrationTest.html), but took [25 minutes](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_24563.2/tests/test/classes/org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibratorIntegrationTest.html) with this branch. The same thing happens when the VQSR tests are run locally with this branch; all the time is spent in `assertEqualTextFiles`. @jean-philippe-martin I don't want to push to this branch without your ok, but reverting the first few lines of `assertEqualTextFiles` seems to fix the problem locally. (Separately, I will do some profiling to figure out for posterity sake why that change had such a dramatic affect ).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-462772143:61,perform,performance,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-462772143,1,['perform'],['performance']
Performance,It looks like we need to update mockito. ; https://storage.googleapis.com/hellbender-test-logs/build_reports/master_27538.13/tests/test/index.html. ```. java.lang.IllegalArgumentException: Unknown Java version: 11; 	at net.bytebuddy.ClassFileVersion.ofJavaVersion(ClassFileVersion.java:135); 	at net.bytebuddy.ClassFileVersion$VersionLocator$ForJava9CapableVm.locate(ClassFileVersion.java:357); 	at net.bytebuddy.ClassFileVersion.ofThisVm(ClassFileVersion.java:147); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection$Dispatcher$CreationAction.run(ClassInjector.java:301); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection$Dispatcher$CreationAction.run(ClassInjector.java:290); 	at java.base/java.security.AccessController.doPrivileged(Native Method); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection.<clinit>(ClassInjector.java:70); 	at net.bytebuddy.dynamic.loading.ClassLoadingStrategy$Default$InjectionDispatcher.load(ClassLoadingStrategy.java:184); 	at net.bytebuddy.dynamic.TypeResolutionStrategy$Passive.initialize(TypeResolutionStrategy.java:79); 	at net.bytebuddy.dynamic.DynamicType$Default$Unloaded.load(DynamicType.java:4456); 	at org.mockito.internal.creation.bytebuddy.SubclassBytecodeGenerator.mockClass(SubclassBytecodeGenerator.java:115); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator$1.call(TypeCachingBytecodeGenerator.java:37); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator$1.call(TypeCachingBytecodeGenerator.java:34); 	at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:138); 	at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:346); 	at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:161); 	at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:355); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator.mockClass(TypeCachingBytecodeGenerator.java:32); 	at org.mockito.internal.creation.bytebuddy.SubclassB,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532377836:493,load,loading,493,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532377836,5,['load'],"['load', 'loading']"
Performance,"It seems like it's taking perceptibly longer time to start the gatk. This was an annoyance with GATK3 and we should see if there's anything we can do for gatk4 to avoid it. . `time gatk-launch PrintReads --help` reports ~1.2 seconds to run. . There's no significant overhead from gatk-launch, so all the time is in our own loading process. It's probably time spent searching the class path like it was in gatk3 but I haven't done any profiling to be sure. . The `4.alpha.1` milestone launches in ~0.9 seconds, so we've definitely been adding some extra time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2127:323,load,loading,323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2127,1,['load'],['loading']
Performance,"It seems like the default shard padding of 1000 used by clients of the new `SparkSharder` is likely to be larger than we need, and reducing it could have a big effect on performance (particularly now that we overestimate the read extent of each partition by roughly the amount of padding). Let's profile and see what the performance impact of decreasing the shard padding is on real data, and come up with a default value more tailored to typical read lengths.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2250:170,perform,performance,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2250,2,['perform'],['performance']
Performance,"It seems like the problem was that the researcher was starting with a coordinate-sorted bam, whereas `MarkDuplicatesSpark` requires a name-sorted bam for good performance. @jamesemery feel free to close once you're satisfied that this is resolved, and once we've made whatever additional documentation clarifications are warranted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5670#issuecomment-463756512:159,perform,performance,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5670#issuecomment-463756512,1,['perform'],['performance']
Performance,"It seems plausible to me, though, that the Google auth library may have been patched to perform checks that it wasn't performing previously. Maybe our project permissions have always been mis-configured :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330940762:88,perform,perform,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330940762,2,['perform'],"['perform', 'performing']"
Performance,"It was a feature which we would have loved, but alas this isn't the case. We also relied on ```-ct``` to get percent of bases depending on their coverage (ex. 20x) which has now been dropped in GATK 4+ versions. We came across another tool called [mosdepth](https://github.com/brentp/mosdepth). When compared to DepthOfCoverage - ; - It uses multithreading (albeit only for deflation, so no performance gains when going beyond 4 threads).; - It gives coverage for exome within 5 minutes, and even faster when we don't need the per base coverage output.; - Per base coverage output can be skipped using ```-x``` the output of this matches closely to output from DepthOfCoverage. Do keep in mind, DepthOfCoverage also supports this skipping when using the parameter ```--omitDepthOutputAtEachBase``` which saves massively on I/O and cuts processing time from 50 minutes per sample to 40 minutes per sample. If you do decide to give it a try, we have some tips and suggestions - ; - The tool generates multiple output files. If looking for total coverage, check the last line of file ```output.mosdepth.summary.txt```; - If looking for percent of bases covered at target read depth, this information is present in file ```output.mosdepth.region.dist.txt```. If your target read depth is 20x, you can search this file with ```grep -P ""total\t20\t""``` and the third column should be the percentage (with only one decimal); - By using ```-d4``` switch, they claim the above percentage granularity increases to 4 decimal points.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7890#issuecomment-1153478071:391,perform,performance,391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7890#issuecomment-1153478071,1,['perform'],['performance']
Performance,It was cursed with a bad cache.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5120#issuecomment-423018400:25,cache,cache,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5120#issuecomment-423018400,1,['cache'],['cache']
Performance,"It won't be able to run any faster than BWA mem does with a similar number of cores, since it is essentially just running bwamem. It's potentially faster as part of a spark pipeline so you can load and process data once instead of saving the data to disk and reloading it repeatedly. . The complete list of spark configuration parameters is available on the [spark docs](https://spark.apache.org/docs/3.5.0/configuration.html). Many of them are not relevant in local mode. From what I understand the local mode is going to execute as a single executor with the number of cores specified in the `local[#]` block ( or the total number of system threads if it's set to `*`) It will use the available memory that java is configured with. I'm pretty sure it's ignoring the memory and configuration parameters you've set. Those will be relevant if you configure a stand alone spark cluster (potentially one running exclusively on your local machine). . Our spark tools are not being actively developed for the most part. We've moved away from them to use single threaded tools widely sharded and managed by cromwell. The additional complexity of the spark environment made it hard to see much benefit when most of the tools are embarassingly parallel and easily shardable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8897#issuecomment-2214866066:193,load,load,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8897#issuecomment-2214866066,1,['load'],['load']
Performance,"It would be nice to take stdout as the input. For example, when it is necessary to pass raw VCF from caller to CNNScoreVariants. In the current version, an error is produced. ```; zcat /home/platon/Dissertation/Exp/ngs_test/no_filtered.vcf.gz | gatk CNNScoreVariants \; -V /dev/stdin \; -R /home/platon/Dissertation/Exp/ngs_test/homo_sapiens/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz \; -O /home/platon/Dissertation/Exp/Output; ```. ```; Using GATK jar /home/platon/miniconda3/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/platon/miniconda3/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar CNNScoreVariants -V /dev/stdin -R /home/platon/Dissertation/Exp/ngs_test/homo_sapiens/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz -O /home/platon/Dissertation/Exp/Output; 18:04:27.033 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/platon/miniconda3/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 26, 2020 6:04:27 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:04:27.246 INFO CNNScoreVariants - ------------------------------------------------------------; 18:04:27.246 INFO CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.1.8.1; 18:04:27.246 INFO CNNScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:04:27.246 INFO CNNScoreVariants - Executing as platon@platon-VivoBook-ASUSLaptop-X712FA-X712FA on Linux v5.4.0-42-generic amd64; 18:04:27.246 INFO CNNScoreVariants - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 18:04:27.247 INFO CNNScoreVariants - Start Date/Time: 26  2020 . 18:04:27 MSK; 18:04:27.247 INFO CNNScoreVariants - -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6749:1116,Load,Loading,1116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6749,1,['Load'],['Loading']
Performance,It would be useful to many people if we could load FASTQ directly in BwaSpark instead of requiring uBam. . Potential options for doing so are:. 1. Use existing ADAM support for FASTQ. 2. Add support to disq,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5020:46,load,load,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5020,1,['load'],['load']
Performance,It would help performance if we used constants instead of arrays in PairHMM. I'll investigate.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2036#issuecomment-288540961:14,perform,performance,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2036#issuecomment-288540961,1,['perform'],['performance']
Performance,"It's currently not clear what the best setting for DEFAULT_READSHARD_SIZE is, or how important it is to performance / results. We should find out.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4298:104,perform,performance,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4298,1,['perform'],['performance']
Performance,"It's not a surprise that a local disk is faster than a remote one, but the; magnitude of the difference is a lot more than I would expect. I remember; at the time using direct GCS access to get the best possible performance in; the bit I was working on, but I don't remember exactly how much of a; difference it made. From my desktop it takes 2m25s to download the whole file, so the ~6min; difference seems really excessive, something is broken. One thing to look; into is whether the sharding is working correctly (are we getting the; correct number of parallel downloads?). Presumably this code is using the HDFS adapter. It'll be interesting to; compare vs the NIO version (and then the optimized NIO version once I write; it).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213094600:212,perform,performance,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213094600,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,It's not likely that the reference loading issue will be fixed this month @huangk3. I'm answering in @lbergelson stead as he's out for a few weeks. This is on the team's radar so there is some chance that it could be but we are unable to say for sure.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-251757030:35,load,loading,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-251757030,1,['load'],['loading']
Performance,"It's on my list. Pretty near the bottom, but it's there. Runtime is probably getting up near an hour for big jobs. The memory requirements are horrific, because we load all the variants into memory and then we don't even use them all! For the biggest cohorts we use 104GB. I wish I was joking. If sklearn can minibatch GMMs then that would be amazing. We use a maximum of 2.5M variants for training and number of annotations/dimensions is O(10). The smallest exome cohort would train with about 80,000 variants with about 3GB of memory. That being said this definitely isn't the biggest cost contributor for joint calling, and hopefully all the sporadic failures have been hammered out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6425#issuecomment-594061198:164,load,load,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6425#issuecomment-594061198,1,['load'],['load']
Performance,"It's possible that GenomicsDB was also optimized for diploids and thusly slower for other ploidies because a lot of the code was ported from GATK. I think `new-qual` is more likely to help, but you can certainly try CombineGVCFs. I don't have a lot of good benchmarks, honestly, but maybe @lbergelson does. GATK3 GenotypeGVCFs for 20,000 human samples took 113.54 hours for about 1.9Mbp, but that was on a CombinedGVCF already extracted from GenomicsDB. With that many samples there are a lot of multi-allelics so that part should be similar to your data. The java options were `java -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -Xmx11500m`, so still less memory than your task seems to need. And without `new-qual`. It also seems to be much faster than your run, but it's hard to say how runtime should scale with the number of samples.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4467#issuecomment-370477452:39,optimiz,optimized,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4467#issuecomment-370477452,1,['optimiz'],['optimized']
Performance,"It's possible to override on the command line. . I tested it on our cluster and tools seemed to run fine. Removing the ""spark.driver.userClassPathFirst=true` causes problems on our cluster but the executor doesn't seem to make any difference. I'm worried we'll run into a problem where we need this to be both true and false to avoid 2 different simultaneous errors. My understanding is that the errors are happening because one class is being loaded by 2 different class loaders, but I don't understand how to fix that directly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1518#issuecomment-190795541:444,load,loaded,444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1518#issuecomment-190795541,2,['load'],"['loaded', 'loaders']"
Performance,It's some sort of race condition; ```; org.apache.spark.SparkException: Job aborted.; 	at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1081); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1000); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:991); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:823); 	at org.disq_bio.disq.impl.formats.vcf.VcfSink.save(VcfSink.java:80); 	at org.disq_bio.disq.HtsjdkVariantsRddStorage.write(HtsjdkVariantsRddStorage.java:156); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariantsSingle(VariantsSparkSink.java:134); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariants(VariantsSparkSink.java:110); 	a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:18,race condition,race condition,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['race condition'],['race condition']
Performance,"It's useful to have a baseline during performance benchmarking to answer the question ""are we better than a naive shuffle?"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1203#issuecomment-288522230:38,perform,performance,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1203#issuecomment-288522230,1,['perform'],['performance']
Performance,"Its the same optimizations for level as we discussed before, no new optimizations. The previous merge did not include all the optimizations (error on my part)... hence I have to do a new PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4379#issuecomment-364265035:13,optimiz,optimizations,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4379#issuecomment-364265035,3,['optimiz'],['optimizations']
Performance,Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zTW9kZWxCYWNrZW5kLmphdmE=) | `100.000% <> ()` | |; | [...sr/scalable/modeling/VariantAnnotationsScorer.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zU2NvcmVyLmphdmE=) | `64.706% <> (-13.072%)` | :arrow_down: |; | [...able/ExtractVariantAnnotationsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9uc0ludGVncmF0aW9uVGVzdC5qYXZh) | `98.214% <> (+1.548%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `73.529% <45.161%> (-4.248%)` | :arrow_down: |; | [...stering/BayesianGaussianMixtureModelPosterior.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jbHVzdGVyaW5nL0JheWVzaWFuR2F1c3NpYW5NaXh0dXJlTW9kZWxQb3N0ZXJpb3IuamF2YQ==) | `58.333% <58.333%> ()` | |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_cam,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333:3585,scalab,scalable,3585,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333,1,['scalab'],['scalable']
Performance,JDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:38:05.914 INFO HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:38:05.914 INFO HaplotypeCallerSpark - Deflater: IntelDeflater; 09:38:05.915 INFO HaplotypeCallerSpark - Inflater: IntelInflater; 09:38:05.915 INFO HaplotypeCallerSpark - GCS max retries/reopens: 20; 09:38:05.915 INFO HaplotypeCallerSpark - Requester pays: disabled; 09:38:05.915 WARN HaplotypeCallerSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: HaplotypeCallerSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 09:38:05.915 INFO HaplotypeCallerSpark - Initializing engine; 09:38:05.915 INFO HaplotypeCallerSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 20/08/15 09:38:06 INFO SparkContext: Running Spark version 2.4.5; 09:38:06.440 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 20/08/15 09:38:06 INFO SparkContext: Submitted application: HaplotypeCallerSpark; 20/08/15 09:38:06 INFO SecurityManager: Changing view acls to: xc278; 20/08/15 09:38:06 INFO SecurityManager: Changing modify acls to: xc278; 20/08/15 09:38:06 INFO SecurityManager: Changing view acls groups to:; 20/08/15 09:38:06 INFO SecurityManager: Changing modify acls groups to:; 20/08/15 09:38:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(xc278); groups with view permissions: Set(); users with modify permissions: Set(xc278); groups with modify permissions: Set(); 20/08/15 09:38:06 INFO Utils: Successfully started service 'sparkDriver' on port 33339.; 20/08/15 09:38:06 INFO SparkEnv: Registering MapOutputTracker; 20/08/15 09:38:06 INFO SparkEnv: Registering BlockManagerMaster; 20/08/15 09:38:06 INFO BlockManagerMasterEndpoint: Using org.apach,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6750:4141,load,load,4141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6750,1,['load'],['load']
Performance,Jar on Maven central updated - please clear any cached jars,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-295584988:48,cache,cached,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-295584988,1,['cache'],['cached']
Performance,"Java implementation of segmentation is now in the sl_wgs_segmentation dev branch, with a few simple unit tests. I'll expand on these and add tests for denoising in the future, but for now we have a working revised pipeline up through segmentation. The CLI is simply named ModelSegments (since my thinking is that it could eventually replace ACNV). I ran it on some old denoised exomes. Runtime is <10s, comparable to CBS. Here's a particularly noisy exome:. CBS found 1398 segments:; ![cbs](https://user-images.githubusercontent.com/11076296/30165095-cdf6251a-93ac-11e7-91fb-dcc8f48fe07f.png). Kernel segmentation with a penalty given by a = 1, b = 0 found 1018 segments:; ![kern](https://user-images.githubusercontent.com/11076296/30165106-dbbe0b40-93ac-11e7-99ec-5d58d8417d8b.png). Kernel segmentation with a penalty given by a = b = 1 (which is probably a reasonable default penalty, at least based on asymptotic theoretical arguments) reduced this to 270 segments :; ![kern-smooth](https://user-images.githubusercontent.com/11076296/30165113-e2b545a8-93ac-11e7-97a9-a692e43ebbdf.png). The number of segments can similarly be controlled in WGS. WGS runtime is ~7min for 250bp bins, ~30s of which is TSV reading, and there is one more spot in my implementation that could stand a bit of optimization, which might bring the runtime down. In contrast, I kicked off CBS 45 minutes ago, and it's still running... @LeeTL1220 this is probably ready to hand off to you for some WDL writing and preliminary evaluation. ; Although I can't guarantee that there aren't bugs, I ran about ~80 exomes with no problem. We can talk later today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936:1289,optimiz,optimization,1289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936,1,['optimiz'],['optimization']
Performance,"JavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 13:13:11.050 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:13:11.275 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:13:12.999 INFO CountReadsSpark - ------------------------------------------------------------; 13:13:13.000 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:13:13.000 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:13:13.000 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:13:13.001 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:13:13.001 INFO CountReadsSpark - Start Date/Time: December 21, 2018 1:13:11 PM EST; 13:13:13.001 INFO CountReadsSpark - ------------------------------------------------------------; 13:13:13.001 INFO CountReadsSpark - ------------------------------------------------------------; 13:13:13.003 INFO Co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:1900,Load,Loading,1900,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['Load'],['Loading']
Performance,"JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:5040,concurren,concurrent,5040,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,1,['concurren'],['concurrent']
Performance,"Just a quick test of `--splitMultiallelics` looks good:; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --maxIndelSize 250 -O zeta_snippet_leftalign_maxindelsize250_splitmultiallelics.vcf.gz --splitMultiallelics; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --maxIndelSize 250 -O zeta_snippet_leftalign_maxindelsize250_splitmultiallelics.vcf.gz --splitMultiallelics; 17:52:19.004 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 05, 2018 5:52:19 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 17:52:19.130 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 17:52:19.131 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-24-gb43bc27-SNAPSHOT; 17:52:19.131 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:52:19.131 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 17:52:19.131 INFO LeftAlignAndTrimVariants ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418893971:811,Load,Loading,811,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418893971,1,['Load'],['Loading']
Performance,Just adding a note here that `FeatureCache` should eventually be refactored to use the simplified Interval class (when it exists) to track cache boundaries and compute overlap.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630:139,cache,cache,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630,1,['cache'],['cache']
Performance,"Just for future reference, note that comments in `testVariantRecalibratorSNPMaxAttempts` are also incorrect or out of date. The test passes even if you limit it to one attempt. ```; // For this test, we deliberately *DON'T* sample a single random int as above; this causes; // the tool to require 4 attempts to acquire enough negative training data to succeed; ```. So again, the tests were already ""broken."" But still, rather than attempt to fix them, I think it's best to follow the principle of not changing both production and test code to the extent that it is possible in this scenario. We've already updated enough exact-match expected results to make me a bit uncomfortable!. Someone else may want to tackle fixing the tests in a separate push, but I think it makes sense for me to focus on avoiding these sorts of issues when writing tests for the new tools. EDIT: For the record, I confirmed that the undesired behavior in this test that the RNG hack was trying to avoid was fixed (and hence, the test was ""broken"") in #6425. Probably wasn't noticed because this is the only non-exact-match test and the test isn't strict enough to check that attempts 1-3 fail, it only checks that we succeed by attempt 4. Again, someone else may feel free to examine the actual coverage of this test and whether it's safe to remove it and/or clean up all the duct tape---but at some point, it becomes difficult to tell which pieces of duct tape are load bearing!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1064236628:1444,load,load,1444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1064236628,1,['load'],['load']
Performance,"Just tested this locally outside of the Docker. I do not see the WARN:; ```; WMCF9-CB5:shlee$ gatk40110 LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O 13_tumor-artifact-prior-table.tsv ; Using GATK jar /Applications/genomicstools/gatk/gatk-4.0.11.0/gatk-package-4.0.11.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /Applications/genomicstools/gatk/gatk-4.0.11.0/gatk-package-4.0.11.0-local.jar LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O 13_tumor-artifact-prior-table.tsv; 12:16:19.960 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Applications/genomicstools/gatk/gatk-4.0.11.0/gatk-package-4.0.11.0-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; Nov 26, 2018 12:16:20 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 12:16:20.176 INFO LearnReadOrientationModel - ------------------------------------------------------------; 12:16:20.177 INFO LearnReadOrientationModel - The Genome Analysis Toolkit (GATK) v4.0.11.0; 12:16:20.177 INFO LearnReadOrientationModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:16:20.177 INFO LearnReadOrientationModel - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 12:16:20.177 INFO LearnReadOrientationModel - Java run",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441721615:816,Load,Loading,816,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441721615,1,['Load'],['Loading']
Performance,"Just to add - GenomicsDB relies on the filesystem for integrity checks just like any other file on the filesystem. We could add integrity checks at a micro chunk-level, but that would be at the expense of performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-713055866:205,perform,performance,205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-713055866,1,['perform'],['performance']
Performance,"Just to chime in with some possibly-not-relevant experience, I had pretty good luck in my past life distributing references (the BWA index, in my case) with Hadoop's distributed cache mechanism (which I guess is similar to broadcasting in Spark?). It would sort of saturate the network, though, when a big job was starting up, and I guess the characteristics of the network you're running on could make a difference.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/567#issuecomment-112857196:178,cache,cache,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-112857196,1,['cache'],['cache']
Performance,"Just wanted to add - I just tried installing the GATK docker as described here: https://gatk.broadinstitute.org/hc/en-us/articles/360035889991--How-to-Run-GATK-in-a-Docker-container. As I'd think that all software dependencies and whatnot should be fine. However, I still get the same error message:. /gatk/./gatk --java-options ""-Xmx25g"" SplitNCigarReads \; > -R Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam \; > --tmp-dir /gatk/my_data/temp -O thing.bam; Using GATK jar /gatk/gatk-package-4.1.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx25g -jar /gatk/gatk-package-4.1.3.0-local.jar SplitNCigarReads -R Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam --tmp-dir /gatk/my_data/temp -O thing.bam. 21:12:14.158 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 02, 2023 9:12:16 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:12:16.383 INFO SplitNCigarReads - ------------------------------------------------------------; 21:12:16.384 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.1.3.0; 21:12:16.384 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:12:16.384 INFO SplitNCigarReads - Executing as root@9d399eec0e24 on Linux v5.19.0-32-generic amd64; 21:12:16.384 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 21:12:16.384 INFO SplitNCigarReads - Start Date/Time: March 2, 2023 9:12:14 PM UTC; 21:12:16.385 INFO SplitNCigarReads - ------------------------------------------------------------; 21:12:16.385 INFO SplitNCigarReads - ----------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452564826:928,Load,Loading,928,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452564826,1,['Load'],['Loading']
Performance,K Version: 2.21.0; 10:29:22.409 INFO Mutect2 - Picard Version: 2.21.2; 10:29:22.409 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:29:22.409 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:29:22.409 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:29:22.409 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:29:22.409 INFO Mutect2 - Deflater: IntelDeflater; 10:29:22.409 INFO Mutect2 - Inflater: IntelInflater; 10:29:22.409 INFO Mutect2 - GCS max retries/reopens: 20; 10:29:22.409 INFO Mutect2 - Requester pays: disabled; 10:29:22.409 INFO Mutect2 - Initializing engine; 10:29:22.609 INFO IntervalArgumentCollection - Processing 170805979 bp from intervals; 10:29:22.613 INFO Mutect2 - Done initializing engine; 10:29:22.622 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/usr/share/java/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 10:29:22.624 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/usr/share/java/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 10:29:22.625 INFO IntelSmithWaterman - Using CPU-supported AVX-512 instructions; 10:29:22.625 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 10:29:22.631 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/usr/share/java/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 10:29:22.660 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 10:29:22.660 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 10:29:22.660 INFO IntelPairHmm - Available threads: 40; 10:29:22.660 INFO IntelPairHmm - Requested threads: 4; 10:29:22.660 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 10:29:22.688 INFO ProgressMeter - Starting traversal; 10:29:22.688 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Proc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7032:3505,Load,Loading,3505,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7032,1,['Load'],['Loading']
Performance,Kc fix rr load bug,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7550:10,load,load,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7550,1,['load'],['load']
Performance,KeyReadsSpark.java:43); at org.broadinstitute.hellbender.tools.spark.pipelines.KeyReadsSpark.lambda$runTool$72eaf22$1(KeyReadsSpark.java:28); at org.broadinstitute.hellbender.tools.spark.pipelines.KeyReadsSpark$$Lambda$11/1228804001.call(Unknown Source); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1002); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1002); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:219); at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```. This start offset is 2176858951 (142662628213169L>>>16) - i.e. around 2GB in. I've managed to reproduce with a local program now. This reveals the following problem:. ```; Caused by: java.lang.IllegalArgumentException: Unrecognized CigarOperator: 11; at htsjdk.samtools.CigarOperator.binaryToEnum(CigarOperator.java:143); at htsjdk.samtools.BinaryCigarCodec.binaryCigarToCigarElement(BinaryCigarCodec.java:87); at htsjdk.samtools.BinaryCigarCodec.decode(BinaryCigarCodec.java:63); at htsjdk.samtools.BAMRecord.getCigar(BAMRecord.java:243); at htsjdk.samtools.SAMRecord.getUnclippedStart(SAMRecord.java:482); at org.seqdoop.hadoop_bam.TestBAMInputFormat.getUnclippedStart(TestBAMInputFormat.java:130); at org.seqdoop.hadoop_bam.TestBAMInputFormat.getStrandedUnclippedStart(TestBAMInputFormat.java:122); at,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1098#issuecomment-156150350:1580,concurren,concurrent,1580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1098#issuecomment-156150350,1,['concurren'],['concurrent']
Performance,Known sites optimization on Spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5103:12,optimiz,optimization,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5103,1,['optimiz'],['optimization']
Performance,"L : 2; 03:56:44.708 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 03:56:44.708 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 03:56:44.708 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 03:56:44.708 INFO HaplotypeCaller - Deflater: IntelDeflater; 03:56:44.708 INFO HaplotypeCaller - Inflater: IntelInflater; 03:56:44.709 INFO HaplotypeCaller - GCS max retries/reopens: 20; 03:56:44.709 INFO HaplotypeCaller - Requester pays: disabled; 03:56:44.709 INFO HaplotypeCaller - Initializing engine; 03:56:45.204 INFO FeatureManager - Using codec BEDCodec to read file file:///data/b37.chr13.bed; 03:56:45.276 INFO IntervalArgumentCollection - Processing 595907 bp from intervals; 03:56:45.305 INFO HaplotypeCaller - Done initializing engine; 03:56:45.324 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 03:56:45.349 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 03:56:45.351 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 03:56:45.373 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 03:56:45.375 INFO IntelPairHmm - Available threads: 8; 03:56:45.375 INFO IntelPairHmm - Requested threads: 4; 03:56:45.375 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 03:56:45.415 INFO ProgressMeter - Starting traversal; 03:56:45.416 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 03:56:46.180 WARN VariantAnnotatorEngine - Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given.; 03:56:46.210 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position 13:32911888 and possibly s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8149:4932,Load,Loading,4932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149,1,['Load'],['Loading']
Performance,"L : 2; 03:58:33.932 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 03:58:33.932 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 03:58:33.932 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 03:58:33.933 INFO HaplotypeCaller - Deflater: IntelDeflater; 03:58:33.933 INFO HaplotypeCaller - Inflater: IntelInflater; 03:58:33.933 INFO HaplotypeCaller - GCS max retries/reopens: 20; 03:58:33.933 INFO HaplotypeCaller - Requester pays: disabled; 03:58:33.934 INFO HaplotypeCaller - Initializing engine; 03:58:34.384 INFO FeatureManager - Using codec BEDCodec to read file file:///data/b37.chr13.bed; 03:58:34.461 INFO IntervalArgumentCollection - Processing 595907 bp from intervals; 03:58:34.491 INFO HaplotypeCaller - Done initializing engine; 03:58:34.509 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 03:58:34.532 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 03:58:34.536 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 03:58:34.580 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 03:58:34.582 INFO IntelPairHmm - Available threads: 8; 03:58:34.582 INFO IntelPairHmm - Requested threads: 4; 03:58:34.582 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 03:58:34.623 INFO ProgressMeter - Starting traversal; 03:58:34.623 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 03:58:35.812 INFO HaplotypeCaller - 58 read(s) filtered by: ((((((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter) AND NotDuplicateReadFilter) AND PassesVendorQualityCheckReadFilter) A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8149:11244,Load,Loading,11244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149,1,['Load'],['Loading']
Performance,"LEAC=2;MLEAF=1.00;MQ=60.00;QD=29.08;SOR=1.179 GT:AD:DP:F1R2:F2R1:GQ:PL 1/1:0,13:13:0,4:0,9:39:392,39,0; 13 32929232 . A G 168.60 . AC=1;AF=0.500;AN=2;BaseQRankSum=1.335;DP=11;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=15.33;ReadPosRankSum=-1.442;SOR=0.446 GT:AD:DP:F1R2:F2R1:GQ:PL; 0/1:5,6:11:3,3:2,3:99:176,0,121; 13 32929387 . T C 208.98 . AC=2;AF=1.00;AN=2;DP=7;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.85;SOR=1.609 GT:AD:DP:F1R2:F2R1:GQ:PL 1/1:0,7:7:0,2:0,5:21:223,21,0; ```. Execution log:; ```; Using GATK jar /gatk/gatk-package-4.1.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.1.0-local.jar HaplotypeCaller --input sample.bam --annotation OrientationBiasReadCounts --intervals b37.chr13.bed --reference hs37d5.fa --output sample.vcf.gz; 03:58:32.017 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2023 3:58:33 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 03:58:33.929 INFO HaplotypeCaller - ------------------------------------------------------------; 03:58:33.930 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.1.0; 03:58:33.930 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:58:33.930 INFO HaplotypeCaller - Executing as root@91e458b8c2fc on Linux v5.10.76-linuxkit amd64; 03:58:33.930 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 03:58:33.931 INFO HaplotypeCaller - Start Date/Time: January 6, 2023 3:58:31 AM UTC; 03:58:33.931 INFO HaplotypeCaller - ----------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8149:8945,Load,Loading,8945,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149,1,['Load'],['Loading']
Performance,LGTM!. Have any numbers on the performance improvement? It wasn't easy to see an old run before Jasix that would be a good 1-to-1 comparison.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8133#issuecomment-1355651843:31,perform,performance,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8133#issuecomment-1355651843,1,['perform'],['performance']
Performance,Latest results show that GCS buckets perform even better than fine. Closing this issue.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-282843205:37,perform,perform,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-282843205,1,['perform'],['perform']
Performance,"LearnReadOrientationModel loads tables for one ref context at a time, reducing memory demands 32x",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8639:26,load,loads,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8639,1,['load'],['loads']
Performance,"Let us take an example. Suppose, we configure GenomicsDB with 3 column partitions - 0-10, 10-100, 100-300 and want to run GenomicsDBImport tool with an interval [0,100]. In this case, the import tool will only write contigs between 0 and 100 into first two partitions (according to the loader JSON file). Is this what you had in mind? The command line will look like:; $ gatk-launch GenomicsDBImport -L 0-100 --loaderJSONfile loader.json --streamIdJSONFile stream.json. This can definitely be done. However, the client still needs to know the column partitions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277372931:286,load,loader,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277372931,3,['load'],"['loader', 'loaderJSONfile']"
Performance,"Let's discuss further before you get too far along. The design of the Collections code was intended to ensure that very strict file formats are adhered to within the CNV pipeline. Making it more flexible to accommodate TSVs with arbitrary column headers, relax requirements for sequence dictionaries, etc. undermines that goal. There are also two other issues to consider:. 1) It looks like @jonn-smith has also been putting considerable effort into building a TSV framework for Funcotator. Perhaps CombineSegmentBreakpoints should consider using that framework instead, if it is more appropriate. We can also discuss bringing the CNV pipeline over into that framework, but this should definitely wait until after release. The end goal is for CNV team to spend as little time as possible writing or maintaining any code related to TSV parsing. 2) @mbabadi has put together some python evaluation code for the new gCNV, which makes use of the IntervalTree python package and PyVCF to accomplish some things that are very similar to what CombineSegmentBreakpoints is doing. Perhaps we could implement a similar approach purely in Java by making use of the IntervalTree implementation in htsjdk. I think for now we should treat CombineSegmentBreakpoints as a one-off tool to be used for internal validations. After release, we should design a more generic evaluation tool. This tool could take as input multiple collections of annotated locatables, with a few rigidly defined formats allowed (e.g., VCF, CNV Collection TSVs, perhaps some TSVs from other tools, etc.), with one designated as ground truth. The regions for evaluation could also be specified via -L (since it is possible this might not completely specified by the ground-truth collection). The appropriate intersections and lookups could then be performed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352860616:1807,perform,performed,1807,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352860616,1,['perform'],['performed']
Performance,"Let's talk about groupReadPairs, pairedReads, and unpairedReads. The groupBy method called by groupReadPairs is very expensive both in time and in memory. It's a full hash shuffle of GATKReads (time expensive), that results in a gazillion 1- and 2-element Lists (memory expensive). So you certainly don't want to do it twice. But the way pairedReads and unpairedReads is set up, you *will* do it twice if you want to process both paired and unpaired reads. (And even if you aren't, someone else might try to use this code to do so.). So my first suggestion is that you remove the call to groupReadPairs from pairedReads and unpairedReads, and let a user groupReadPairs once, and reuse the resulting JavaPairRDD to process paired and unpaired reads. My second suggestion is quite a bit more complicated, but I think it would result in far better performance. I'll sketch it out here, and then I can explain it further in person, if it's a direction you'd like to pursue. The first step is to create a JavaRDD<GATKRead> in which all pairs sharing a template name are in the same partition (but without grouping them). To do that, you temporarily boost the input JavaRDD into a JavaPairRDD<String,GATKRead> by extracting the read name as a key. Then you repartition (to do the shuffle). Then you map back to an ordinary JavaRDD<GATKRead> by keeping just the value. (Note: if the BAM has queryname sort order, you can just skip this step entirely.); Now you can do a mapPartition operation to filter for paired or unpaired reads: Iterate over the reads in the partition, and keep a hash map of [name -> read] of reads that have not yet found mates. To filter for paired reads, whenever you find the name of the current read already in the table, just emit the current read and the read in the map as a pair, and delete the read from the map (you're done with that name -- this keeps the table smaller). To filter for unpaired reads, just delete any map entry that you successfully look up, and insert any ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2664#issuecomment-299955039:845,perform,performance,845,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2664#issuecomment-299955039,1,['perform'],['performance']
Performance,Lets see what the performance difference is. It's pretty awkward to pass raw bytes around.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/115:18,perform,performance,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/115,1,['perform'],['performance']
