quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,filename,wiki,url,total_similar,target_keywords,target_matched_words
Availability," ""F"" | 7 | 3 | 10 | 81 | -5 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Notes; -----. The number of partitions in the new table is equal to the number of; partitions containing the first `n` rows. Parameters; ----------; n : int; Number of rows to include. Returns; -------; :class:`.Table`; Table limited to the first `n` rows.; """""". return Table(ir.TableHead(self._tir, n)). [docs] @typecheck_method(n=int); def tail(self, n) -> 'Table':; """"""Subset table to last `n` rows. Examples; --------; Subset to the last three rows:. >>> table_result = table1.tail(3); >>> table_result.count(); 3. Notes; -----. The number of partitions in the new table is equal to the number of; partitions containing the last `n` rows. Parameters; ----------; n : int; Number of rows to include. Returns; -------; :class:`.Table`; Table including the last `n` rows.; """""". return Table(ir.TableTail(self._tir, n)). [docs] @typecheck_method(p=numeric, seed=nullable(int)); def sample(self, p, seed=None) -> 'Table':; """"""Downsample the table by keeping each row with probability ``p``. Examples; --------. Downsample the table to approximately 1% of its rows. >>> table1.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+; >>> small_table1 = table1.sample(0.75, seed=0); >>> small_table1.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:89902,Down,Downsample,89902,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['Down'],['Downsample']
Availability," 'id1', 'ID2': 'id2'}). Use a file with an ""old_id"" and ""new_id"" column to rename samples:. >>> mapping_table = hc.import_table('data/sample_mapping.txt'); >>> mapping_dict = {row.old_id: row.new_id for row in mapping_table.collect()}; >>> vds_result = vds.rename_samples(mapping_dict). :param dict mapping: Mapping from old to new sample IDs. :return: Dataset with remapped sample IDs.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.renameSamples(mapping); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(num_partitions=integral,; shuffle=bool); def repartition(self, num_partitions, shuffle=True):; """"""Increase or decrease the number of variant dataset partitions. **Examples**. Repartition the variant dataset to have 500 partitions:. >>> vds_result = vds.repartition(500). **Notes**. Check the current number of partitions with :py:meth:`.num_partitions`. The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. When a variant dataset with :math:`M` variants is first imported, each of the :math:`k` partition will contain about :math:`M/k` of the variants. Since each partition has some computational overhead, decreasing the number of partitions can improve performance after significant filtering. Since it's recommended to have at least 2 - 4 partitions per core, increasing the number of partitions can allow one to take advantage of more cores. Partitions are a core concept of distributed computation in Spark, see `here <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__ for details. With ``shuffle=True``, Hail does a full shuffle of the data and creates equal sized partitions. With ``shuffle=False``, Hail combines existing partitions to avoid a full shuffle. These algorithms correspond to the ``repartition`` and ``coalesce`` commands in Spark, res",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:191822,avail,available,191822,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['avail'],['available']
Availability," (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR). - HemiX -- in non-PAR of X, m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:157439,error,error,157439,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability," (0 + 1) / 1]. After filter, 250/284 samples remain. Next is genotype QC. It’s a good idea to filter out genotypes where the reads aren’t where they should be: if we find a genotype called homozygous reference with >10% alternate reads, a genotype called homozygous alternate with >10% reference reads, or a genotype called heterozygote without a ref / alt balance near 1:1, it is likely to be an error.; In a low-depth dataset like 1KG, it is hard to detect bad genotypes using this metric, since a read ratio of 1 alt to 10 reference can easily be explained by binomial sampling. However, in a high-depth dataset, a read ratio of 10:100 is a sure cause for concern!. [32]:. ab = mt.AD[1] / hl.sum(mt.AD). filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))). fraction_filtered = mt.aggregate_entries(hl.agg.fraction(~filter_condition_ab)); print(f'Filtering {fraction_filtered * 100:.2f}% entries out of downstream analysis.'); mt = mt.filter_entries(filter_condition_ab). [Stage 34:> (0 + 1) / 1]. Filtering 3.60% entries out of downstream analysis. [ ]:. Variant QC is a bit more of the same: we can use the variant_qc function to produce a variety of useful statistics, plot them, and filter. [33]:. mt = hl.variant_qc(mt). [34]:. mt.row.describe(). --------------------------------------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; rsid: str,; qual: float64,; filters: set<str>,; info: struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: array<int32>,; MLEAF: array<float64>,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; QD: float64,; ReadPosRankSum: float64,; set: str; },; variant_qc: struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:16517,down,downstream,16517,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['down'],['downstream']
Availability," (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height=400)). 2-D histogram; For visualizing relationships between variables in large datasets (where scatter plots may be less informative since they highlight outliers), the histogram_2d() function will create a heatmap with the number of observations in each section of a 2-d grid based on two variables. [10]:. p = hl.plot.histogram2d(pca_scores.scores[0], pca_scores.scores[1]); show(p). Q-Q (Quantile-Quantile); The qq() function requires either a Python type or a Hail field containing p-values to be plotted. This function also allows for downsampling. [11]:. p = hl.plot.qq(gwas.p_value, n_divisions=None); p2 = hl.plot.qq(gwas.p_value, n_divisions=75). show(gridplot([p, p2], ncols=2, width=400, height=400)). Manhattan; The manhattan() function requires a Hail field containing p-values. [12]:. p = hl.plot.manhattan(gwas.p_value); show(p). We can also pass in a dictionary of fields that we would like to show up as we hover over a data point, and choose not to downsample if the dataset is relatively small. [13]:. hover_fields = dict([('alleles', gwas.alleles)]); p = hl.plot.manhattan(gwas.p_value, hover_fields=hover_fields, n_divisions=None); show(p). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/08-plotting.html:6792,down,downsampling,6792,docs/0.2/tutorials/08-plotting.html,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html,2,['down'],"['downsample', 'downsampling']"
Availability," (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating infor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:103569,error,error,103569,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability," (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e21=(; (p**3) * ((X - 1) / X) * ((X - 2) / X) * (T / (T - 1)) * (T / (T - 2)); + (q**3) * ((Y - 1) / Y) * ((Y - 2) / Y) * (T / (T - 1)) * (T / (T - 2)); + (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e22=1,; ). dataset = dataset.checkpoint(hl.utils.new_temp_file()). expectations = dataset.aggregate_rows(; hl.struct(; e00=hl.agg.sum(dataset._e00),; e10=hl.agg.sum(dataset._e10),; e20=hl.agg.sum(dataset._e20),; e11=hl.agg.sum(dataset._e11),; e21=hl.agg.sum(dataset._e21),; e22=hl.agg.sum(dataset._e22),; ); ). IS_HOM_REF = BlockMatrix.from_entry_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_te",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:6850,checkpoint,checkpoint,6850,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,2,['checkpoint'],['checkpoint']
Availability," (table1.group_by(table1.SEX); ... .aggregate(mean_height_data = hl.agg.mean(table1.HT))); >>> table3.show(). Join tables together inside an annotation expression:; >>> table2 = table2.key_by('ID'); >>> table1 = table1.annotate(B = table2[table1.ID].B); >>> table1.show(). Attributes. globals; Returns a struct expression including all global fields. key; Row key struct. row; Returns a struct expression of all row-indexed fields, including keys. row_value; Returns a struct expression including all non-key row-indexed fields. Methods. add_index; Add the integer index of each row as a new row field. aggregate; Aggregate over rows into a local value. all; Evaluate whether a boolean expression is true for all rows. annotate; Add new fields. annotate_globals; Add new global fields. anti_join; Filters the table to rows whose key does not appear in other. any; Evaluate whether a Boolean expression is true for at least one row. cache; Persist this table in memory. checkpoint; Checkpoint the table to disk by writing and reading. collect; Collect the rows of the table into a local list. collect_by_key; Collect values for each unique key into an array. count; Count the number of rows in the table. describe; Print information about the fields in the table. distinct; Deduplicate keys, keeping exactly one row for each unique key. drop; Drop fields from the table. expand_types; Expand complex types into structs and arrays. explode; Explode rows along a field of type array or set, copying the entire row for each element. export; Export to a text file. filter; Filter rows conditional on the value of each row's fields. flatten; Flatten nested structs. from_pandas; Create table from Pandas DataFrame. from_spark; Convert PySpark SQL DataFrame to a table. group_by; Group by a new key for use with GroupedTable.aggregate(). head; Subset table to first n rows. index; Expose the row values as if looked up in a dictionary, indexing with exprs. index_globals; Return this table's global variables",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:3676,checkpoint,checkpoint,3676,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['checkpoint'],['checkpoint']
Availability," **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR). - HemiX -- in non-PAR of X, male child; - HemiY -- in non-PAR of Y, male child; - Auto ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:157486,error,errors,157486,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:29671,error,error,29671,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['error'],['error']
Availability," - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`~.LocusExpression.in_autosome`. - Auto -- in autosome or in PAR or female child; - HemiX -- in non-PAR of X and male child; - HemiY -- in non-PAR of Y and male child. `Any` refers to the set \{ HomRef, Het, HomVar, NoCall \} and `~`; denotes complement in this set. +------+---------+---------+--------+----------------------------+; | Code | Dad | Mom | Kid | Copy State | Implicated |; +======+=========+===",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:7149,error,errors,7149,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability," - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`~.LocusExpression.in_autosome`. - Auto -- in autosome or in PAR or female child; - HemiX -- in non-PAR of X and male child; - HemiY -- in non-PAR of Y and male child. `Any` refers to the set \{ HomRef, Het, HomVar, NoCall \} and `~`; denotes complement in this set. +------+---------+---------+--------+----------------------------+; | Code | Dad | Mom | Kid | Copy State | Implicated |; +======+=========+=========+========+============+===============+; | 1 | HomVar | HomVar | Het | Auto | Dad, Mom, Kid |; +------+---------+---------+--------+------------+---------------+; | 2 | HomRef | HomRef | Het | Auto | Dad",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:7333,error,error,7333,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['error']
Availability," ----------; item : :class:`str`; Field name. Returns; -------; :class:`.SetExpression`; A set formed by getting the given field for each struct in; this set; """""". return self.map(lambda x: x[item]). [docs]class DictExpression(Expression):; """"""Expression of type :class:`.tdict`. >>> d = hl.literal({'Alice': 43, 'Bob': 33, 'Charles': 44}); """""". @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(DictExpression, self).__init__(x, type, indices, aggregations); assert isinstance(type, tdict); self._kc = coercer_from_dtype(type.key_type); self._vc = coercer_from_dtype(type.value_type). [docs] @typecheck_method(item=expr_any); def __getitem__(self, item):; """"""Get the value associated with key `item`. Examples; --------. >>> hl.eval(d['Alice']); 43. Notes; -----; Raises an error if `item` is not a key of the dictionary. Use; :meth:`.DictExpression.get` to return missing instead of an error. Parameters; ----------; item : :class:`.Expression`; Key expression. Returns; -------; :class:`.Expression`; Value associated with key `item`.; """"""; if not self._kc.can_coerce(item.dtype):; raise TypeError(; ""dict encountered an invalid key type\n"" "" dict key type: '{}'\n"" "" type of 'item': '{}'"".format(; self.dtype.key_type, item.dtype; ); ); return self._index(self.dtype.value_type, self._kc.coerce(item)). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns whether a given key is present in the dictionary. Examples; --------. >>> hl.eval(d.contains('Alice')); True. >>> hl.eval(d.contains('Anne')); False. Parameters; ----------; item : :class:`.Expression`; Key to test for inclusion. Returns; -------; :class:`.BooleanExpression`; ``True`` if `item` is a key of the dictionary, ``False`` otherwise.; """"""; if not self._kc.can_coerce(item.dtype):; raise TypeError(; ""'DictExpression.contains' encountered an invalid key type\n""; "" dict key typ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:37056,error,error,37056,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['error'],['error']
Availability," ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """""". valid_regions = {'us', 'us-central1', 'europe-west1'}; if region not in valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {valid_clouds}.'; ). datasets = get_datasets_metadata(); names = set([dataset for dataset in datasets]); if name not in names:; raise ValueError(f'{name} is not a dataset available in the' f' repository.'). versions = set(dataset['version'] for dataset in datasets[name]['versions']); if version not in versions:; raise ValueError(; f'Version {version!r} not available for dataset' f' {name!r}.\n' f'Available versions: {versions}.'; ). reference_genomes = set(dataset['reference_genome'] for dataset in datasets[name]['versions']); if reference_genome not in reference_genomes:; raise ValueError(; f'Reference genome build {reference_genome!r} not'; f' available for dataset {name!r}.\n'; f'Available reference genome builds:'; f' {reference_genomes}.'; ). clouds = set(k for dataset in datasets[name]['versions'] for k in dataset['url'].keys()); if cloud not in clouds:; raise ValueError(f'Cloud platform {cloud!r} not available for dataset {name}.\nAvailable platforms: {clouds}.'). regions = set(k for dataset in datasets[name]['versions'] for k in dataset['url'][cloud].keys()); if region not in regions:; raise ValueError(; f'Region {region!r} not available for dataset'; f' {name!r} on cloud platform {cloud!r}.\n'; f'Available regions: {regions}.'; ). path = [; dataset['url'][cloud][region]; for dataset in datasets[name]['versions']; if all([dataset['version'] == version, dataset['reference_genome'] ==",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:3213,Avail,Available,3213,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,1,['Avail'],['Available']
Availability," / failure_prob, hl.range(0, 5)). def compute_single_error(s, failure_prob=failure_prob):; return hl.sqrt(hl.log(2 / failure_prob) * s / 2). def compute_global_error(s):; return hl.rbind(compute_grid_size(s), lambda p: 1 / p + compute_single_error(s, failure_prob / p)). if all_quantiles:; return hl.rbind(cdf, lambda cdf: hl.rbind(compute_sum(cdf), compute_global_error)); else:; return hl.rbind(cdf, lambda cdf: hl.rbind(compute_sum(cdf), compute_single_error)). def _error_from_cdf_python(cdf, failure_prob, all_quantiles=False):; """"""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :obj:`dict`; Result of :func:`.approx_cdf` aggregator, evaluated to a python dict; failure_prob: :obj:`float`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :obj:`float`; Upper bound on error of quantile estimates.; """"""; import math. s = 0; for i in builtins.range(builtins.len(cdf._compaction_counts)):; s += cdf._compaction_counts[i] << (2 * i); s = s / (cdf.ranks[-1] ** 2). def update_grid_size(p):; return 4 * math.sqrt(math.log(2 * p / failure_prob) / (2 * s)). def compute_grid_size(s):; p = 1 / failure_prob; for _ in builtins.range(5):; p = update_grid_size(p); return p. def compute_single_error(s, failure_prob=failure_prob):; return math.sqrt(math.log(2 / failure_prob) * s / 2). if s == 0:; # no compactions ergo no error; return 0; elif all_quantiles:; p = compute_grid_size(s); return 1 / p + compute_single_error(s, failure_prob / p); else:; return compute_single_error(s, failure_prob). [docs]@typecheck(t=hail_type); def missing(t: Union[HailType, str]):; """"""Creates an expression representing a missing value of a specified type. Examples; --------. >>> hl.eval(hl.missing(hl.tarray(hl.tstr))); None. >>> hl.eval(hl.missing('array<str>')); None. Notes; ----",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:7032,error,error,7032,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['error'],['error']
Availability," 0 10; +-----------; 0 1. In summary:. GT: Downcode filtered alleles to reference.; AD: The filtered alleles’ columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms 25,5,10,20 to 40,20.; DP: No change.; PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded genotype, and shift so the overall minimum PL is 0.; GQ: The second-lowest PL (after shifting). Expression Variables; The following symbols are in scope for expr:. v (Variant): Variant; va: variant annotations; aIndex (Int): the index of the allele being tested. The following symbols are in scope for annotation:. v (Variant): Variant; va: variant annotations; aIndices (Array[Int]): the array of old indices (such that aIndices[newIndex] = oldIndex and aIndices[0] = 0). Parameters:; expr (str) – Boolean filter expression involving v (variant), va (variant annotations), ; and aIndex (allele index); annotation (str) – Annotation modifying expression involving v (new variant), va (old variant annotations),; and aIndices (maps from new to old indices); subset (bool) – If true, subsets PL and AD, otherwise downcodes the PL and AD.; Genotype and GQ are set based on the resulting PLs.; keep (bool) – If true, keep variants matching expr; filter_altered_genotypes (bool) – If true, genotypes that contain filtered-out alleles are set to missing.; max_shift (int) – maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered.; keepStar (bool) – If true, keep variants where the only allele left is a * allele. Returns:Filtered variant dataset. Return type:VariantDataset. filter_genotypes(expr, keep=True)[source]¶; Filter genotypes based on expression.; Examples; Filter genotypes by allele balance dependent on genotype call:; >>> vds_result = vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ' +; ... '((g.isHomRef() && ab <= 0.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:51597,down,downcodes,51597,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['down'],['downcodes']
Availability," 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). def call_hemi(kid_pp, parent, parent_pp, kid_ad_ratio):; p_data_given_dn = parent_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (parent_pp[1] + parent_pp[2]) * kid_pp[2] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (parent.DP) < min_dp_ratio) | (kid_ad_ratio < min_child_ab), failure); .when((hl.sum(parent.AD) == 0), failure); .when(parent.AD[1] / hl.sum(parent.AD) > max_parent_ab, failure); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.3, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). de_novo_call = (; hl.case();",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:32101,failure,failure,32101,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['failure'],['failure']
Availability," 1000kb 1 0.2.; The list of pruned variants returned by Hail and PLINK will differ because Hail mean-imputes missing values and tests pairs of variants in a different order than PLINK.; Be sure to provide enough disk space per worker because ld_prune() persists up to 3 copies of the data to both memory and disk.; The amount of disk space required will depend on the size and minor allele frequency of the input data and the prune parameters r2 and window. The number of bytes stored in memory per variant is about nSamples / 4 + 50. Warning; The variants in the pruned set are not guaranteed to be identical each time ld_prune() is run. We recommend running ld_prune() once and exporting the list of LD pruned variants using; export_variants() for future use. Parameters:; r2 (float) – Maximum \(R^2\) threshold between two variants in the pruned set within a given window.; window (int) – Width of window in base-pairs for computing pair-wise \(R^2\) values.; memory_per_core (int) – Total amount of memory available for each core in MB. If unsure, use the default value.; num_cores (int) – The number of cores available. Equivalent to the total number of workers times the number of cores per worker. Returns:Variant dataset filtered to those variants which remain after LD pruning. Return type:VariantDataset. linreg(y, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0)[source]¶; Test each variant for association using linear regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run linear regression per variant using a phenotype and two covariates stored in sample annotations:; >>> vds_result = vds.linreg('sa.pheno.height', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The linreg() method computes, for each variant, statistics of; the \(t\)-test for the genotype coefficient of the linear function; of best fit from sample genotype and covariates to quantitative; phenotype or case-control status",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:77755,avail,available,77755,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['avail'],['available']
Availability," 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This method does not perform small sample size correction.; The q_stat return value is not the \(Q\) statistic from the paper. We m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:74310,error,errors,74310,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability," 3))),; _e10=(; 4 * (p**3) * q * ((X - 1) / X) * ((X - 2) / X) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); + 4 * p * (q**3) * ((Y - 1) / Y) * ((Y - 2) / Y) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); ),; _e20=(; (p**4) * ((X - 1) / X) * ((X - 2) / X) * ((X - 3) / X) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); + (q**4) * ((Y - 1) / Y) * ((Y - 2) / Y) * ((Y - 3) / Y) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); + 4 * (p**2) * (q**2) * ((X - 1) / X) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); ),; _e11=(; 2 * (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + 2 * p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e21=(; (p**3) * ((X - 1) / X) * ((X - 2) / X) * (T / (T - 1)) * (T / (T - 2)); + (q**3) * ((Y - 1) / Y) * ((Y - 2) / Y) * (T / (T - 1)) * (T / (T - 2)); + (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e22=1,; ). dataset = dataset.checkpoint(hl.utils.new_temp_file()). expectations = dataset.aggregate_rows(; hl.struct(; e00=hl.agg.sum(dataset._e00),; e10=hl.agg.sum(dataset._e10),; e20=hl.agg.sum(dataset._e20),; e11=hl.agg.sum(dataset._e11),; e21=hl.agg.sum(dataset._e21),; e22=hl.agg.sum(dataset._e22),; ); ). IS_HOM_REF = BlockMatrix.from_entry_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:6230,checkpoint,checkpoint,6230,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,2,['checkpoint'],['checkpoint']
Availability," 4, 5, or 6 fields below, depending on; the `statistics` parameter:. - `i` (``col_key.dtype``) -- First sample. (key field); - `j` (``col_key.dtype``) -- Second sample. (key field); - `kin` (:py:data:`.tfloat64`) -- Kinship estimate, :math:`\widehat{\phi_{ij}}`.; - `ibd2` (:py:data:`.tfloat64`) -- IBD2 estimate, :math:`\widehat{k^{(2)}_{ij}}`.; - `ibd0` (:py:data:`.tfloat64`) -- IBD0 estimate, :math:`\widehat{k^{(0)}_{ij}}`.; - `ibd1` (:py:data:`.tfloat64`) -- IBD1 estimate, :math:`\widehat{k^{(1)}_{ij}}`. Here ``col_key`` refers to the column key of the source matrix table,; and ``col_key.dtype`` is a struct containing the column key fields. There is one row for each pair of distinct samples (columns), where `i`; corresponds to the column of smaller column index. In particular, if the; same column key value exists for :math:`n` columns, then the resulting; table will have :math:`\binom{n-1}{2}` rows with both key fields equal to; that column key value. This may result in unexpected behavior in downstream; processing. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; min_individual_maf : :obj:`float`; The minimum individual-specific minor allele frequency.; If either individual-specific minor allele frequency for a pair of; individuals is below this threshold, then the variant will not; be used to estimate relatedness for the pair.; k : :obj:`int`, optional; If set, `k` principal component scores are computed and used.; Exactly one of `k` and `scores_expr` must be specified.; scores_expr : :class:`.ArrayNumericExpression`, optional; Column-indexed expression of principal component scores, with the same; source as `call_expr`. All array values must have the same positive length,; corresponding to the number of principal components, and all scores must; be non-missing. Exactly one of `k` and `scores_expr` must be specified.; min_kinship : :obj:`float`, optional; If set, pairs of samples with kinship lower than `min_kinship` ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:10668,down,downstream,10668,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['down'],['downstream']
Availability," 5).map(lambda _: hl.rand_bool(0.5))). Aggregate to compute the fraction True per element:; >>> ht.aggregate(hl.agg.array_agg(lambda element: hl.agg.fraction(element), ht.arr)) ; [0.54, 0.55, 0.46, 0.52, 0.48]. Notes; This function requires that all values of array have the same length. If; two values have different lengths, then an exception will be thrown.; The f argument should be a function taking one argument, an expression of; the element type of array, and returning an expression including; aggregation(s). The type of the aggregated expression returned by; array_agg() is an array of elements of the return type of f. Parameters:. f – Aggregation function to apply to each element of the exploded array.; array (ArrayExpression) – Array to aggregate. Returns:; ArrayExpression. hail.expr.aggregators.downsample(x, y, label=None, n_divisions=500)[source]; Downsample (x, y) coordinate datapoints. Parameters:. x (NumericExpression) – X-values to be downsampled.; y (NumericExpression) – Y-values to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the distribution of values. In; particular, for any value x = values(i) in the summary, we estimate that; there are ranks(i) values strictly les",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:33335,down,downsampled,33335,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['down'],['downsampled']
Availability," 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93800,fault,fault,93800,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability," 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This method does not perform small sample size correction.; The q_stat return value is not the \(Q\) statistic from the paper. We match the output; of the SKAT R package which returns \(\tilde{Q}\):. \[\tilde{Q} = \frac{Q}{2}\]. Parameters:. group (Expression) – Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Flo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:74570,fault,fault,74570,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['fault'],['fault']
Availability," : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns; -------; :class:`.Table`; """""". ht = hl.import_table(; path,; min_partitions=min_partitions,; comment='#',; no_header=True,; types={'f3': hl.tint, 'f4': hl.tint, 'f5': hl.tfloat, 'f7': hl.tint},; missing='.',; delimiter='\t',; force_bgz=force_bgz,; force=force,; ). ht = ht.rename({; 'f0': 'seqname',; 'f1': 'source',; 'f2': 'feature',; 'f3': 'start',; 'f4': 'end',; 'f5': 'score',; 'f6': 'strand',; 'f7': 'frame',; 'f8': 'attribute',; }). def parse_attributes(unparsed_attributes):; def parse_attribute(attribute):; key_and_value = attribute.split(' '); key = key_and_value[0]; value = key_and_value[1]; return (key, value.replace('""|;\\$', '')). return hl.dict(unparsed_attributes.split('; ').map(parse_attribute)). ht = ht.annotate(attribute=parse_attributes(ht['attribute'])). ht = ht.checkpoint(new_temp_file()). attributes = ht.aggregate(hl.agg.explode(lambda x: hl.agg.collect_as_set(x), ht['attribute'].keys())). ht = ht.transmute(**{x: hl.or_missing(ht['attribute'].contains(x), ht['attribute'][x]) for x in attributes if x}). if reference_genome:; if reference_genome.name == 'GRCh37':; ht = ht.annotate(; seqname=hl.case(); .when((ht['seqname'] == 'M') | (ht['seqname'] == 'chrM'), 'MT'); .when(ht['seqname'].startswith('chr'), ht['seqname'].replace('^chr', '')); .default(ht['seqname']); ); else:; ht = ht.annotate(; seqname=hl.case(); .when(ht['seqname'].startswith('HLA'), ht['seqname']); .when(ht['seqname'].startswith('chrHLA'), ht['seqname'].replace('^chr', '')); .when(ht['seqname'].startswith('chr'), ht['seqname']); .default('chr' + ht['seqname']); ); if skip_invalid_contigs:; valid_contigs = hl.literal(set(reference_genome.contigs)); ht = ht.filter(valid_contigs.contains(ht['seqname'])); ht = ht.transmute(; interval=hl.locus_interval(; ht['seqname'],; ht['start'],; ht['end'],; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html:5057,checkpoint,checkpoint,5057,docs/0.2/_modules/hail/experimental/import_gtf.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html,2,['checkpoint'],['checkpoint']
Availability," <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Formatter.html#syntax>`__; for valid format specifiers and arguments. Missing values are printed as ``'null'`` except when using the; format flags `'b'` and `'B'` (printed as ``'false'`` instead). Parameters; ----------; f : :class:`.StringExpression`; Java `format string <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Formatter.html#syntax>`__.; args : variable-length arguments of :class:`.Expression`; Arguments to format. Returns; -------; :class:`.StringExpression`; """""". return _func(""format"", hl.tstr, f, hl.tuple(args)). [docs]@typecheck(x=expr_float64, y=expr_float64, tolerance=expr_float64, absolute=expr_bool, nan_same=expr_bool); def approx_equal(x, y, tolerance=1e-6, absolute=False, nan_same=False):; """"""Tests whether two numbers are approximately equal. Examples; --------; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters; ----------; x : :class:`.NumericExpression`; y : :class:`.NumericExpression`; tolerance : :class:`.NumericExpression`; absolute : :class:`.BooleanExpression`; If True, compute ``abs(x - y) <= tolerance``. Otherwise, compute; ``abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022)``.; nan_same : :class:`.BooleanExpression`; If True, then ``NaN == NaN`` will evaluate to True. Otherwise,; it will return False. Returns; -------; :class:`.BooleanExpression`; """""". return _func(""approxEqual"", hl.tbool, x, y, tolerance, absolute, nan_same). def _shift_op(x, y, op):; assert op in ('<<', '>>', '>>>'); t = x.dtype; if t == hl.tint64:; word_size = 64; zero = hl.int64(0); else:; word_size = 32; zero = hl.int32(0). indices, aggregations = unify_all(x, y); return hl.bind(; lambda x, y: (; hl.case(); .when(y >= word_size, hl.sign(x) if op == '>>' else zero); .when(y >= 0, construct_expr(ir.ApplyBinaryPrimOp(op, x._ir, y._ir), t, indices, aggregations)); .or_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:177045,toler,tolerance,177045,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['toler'],['tolerance']
Availability," = []; for e in exprs:; analyze(caller, e, indices, broadcast=False); if not e._ir.is_nested_field:; raise ExpressionException(; f""{caller!r} expects keyword arguments for complex expressions\n""; f"" Correct: ht = ht.key_by('x')\n""; f"" Correct: ht = ht.key_by(ht.x)\n""; f"" Correct: ht = ht.key_by(x = ht.x.replace(' ', '_'))\n""; f"" INCORRECT: ht = ht.key_by(ht.x.replace(' ', '_'))""; ). name = e._ir.name; final_key.append(name). if not is_top_level_field(e):; bindings.append((name, e)); else:; existing_key_fields.append(name). final_key.extend(named_exprs); bindings.extend(named_exprs.items()); check_collisions(caller, final_key, indices, override_protected_indices=override_protected_indices); return final_key, dict(bindings). def check_keys(caller, name, protected_key):; from hail.expr.expressions import ExpressionException. if name in protected_key:; msg = (; f""{caller!r}: cannot overwrite key field {name!r} with annotate, select or drop; ""; f""use key_by to modify keys.""; ); error('Analysis exception: {}'.format(msg)); raise ExpressionException(msg). def get_select_exprs(caller, exprs, named_exprs, indices, base_struct):; from hail.expr.expressions import ExpressionException, analyze, to_expr. exprs = [indices.source[e] if isinstance(e, str) else e for e in exprs]; named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}; select_fields = indices.protected_key[:]; protected_key = set(select_fields); insertions = {}. final_fields = select_fields[:]. def is_top_level_field(e):; return e in indices.source._fields_inverse. for e in exprs:; if not e._ir.is_nested_field:; raise ExpressionException(; f""{caller!r} expects keyword arguments for complex expressions\n""; f"" Correct: ht = ht.select('x')\n""; f"" Correct: ht = ht.select(ht.x)\n""; f"" Correct: ht = ht.select(x = ht.x.replace(' ', '_'))\n""; f"" INCORRECT: ht = ht.select(ht.x.replace(' ', '_'))""; ); analyze(caller, e, indices, broadcast=False). name = e._ir.name; check_keys(caller, name, protected_key); final_fields.ap",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:12563,error,error,12563,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,2,['error'],['error']
Availability," = hl.bind(lambda x: x / hl.sum(x), kid_linear_pl). dad_linear_pl = 10 ** (-dad.PL / 10); dad_pp = hl.bind(lambda x: x / hl.sum(x), dad_linear_pl). mom_linear_pl = 10 ** (-mom.PL / 10); mom_pp = hl.bind(lambda x: x / hl.sum(x), mom_linear_pl). kid_ad_ratio = kid.AD[1] / hl.sum(kid.AD); dp_ratio = kid.DP / (dad.DP + mom.DP). def call_auto(kid_pp, dad_pp, mom_pp, kid_ad_ratio):; p_data_given_dn = dad_pp[0] * mom_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (dad_pp[1] * mom_pp[0] + dad_pp[0] * mom_pp[1]) * kid_pp[1] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (dad.DP + mom.DP) < min_dp_ratio) | ~(kid_ad_ratio >= min_child_ab), failure); .when((hl.sum(mom.AD) == 0) | (hl.sum(dad.AD) == 0), failure); .when(; (mom.AD[1] / hl.sum(mom.AD) > max_parent_ab) | (dad.AD[1] / hl.sum(dad.AD) > max_parent_ab), failure; ); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:30482,failure,failure,30482,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['failure'],['failure']
Availability," = hl.nd.array(scores_table.collect(_localize=False).map(lambda x: x.__scores)). # Define NaN for missing values, otherwise cannot convert expr to block matrix; nan = hl.float64(float('NaN')). # Create genotype matrix, set missing GT entries to NaN; mt = mt.select_entries(__gt=call_expr.n_alt_alleles()).unfilter_entries(); gt_with_nan_expr = hl.or_else(hl.float64(mt.__gt), nan); if not block_size:; block_size = BlockMatrix.default_block_size(); g = BlockMatrix.from_entry_expr(gt_with_nan_expr, block_size=block_size); g = g.checkpoint(new_temp_file('pc_relate_bm/g', 'bm')); sqrt_n_samples = hl.nd.array([hl.sqrt(g.shape[1])]). # Recover singular values, S0, as vector of column norms of pc_scores if necessary; if compute_S0:; S0 = (pc_scores ** hl.int32(2)).sum(0).map(lambda x: hl.sqrt(x)); else:; S0 = hl.nd.array(eigens).map(lambda x: hl.sqrt(x)); # Set first entry of S to sqrt(n), for intercept term in beta; S = hl.nd.hstack((sqrt_n_samples, S0))._persist(); # Recover V from pc_scores with inv(S0); V0 = (pc_scores * (1 / S0))._persist(); # Set all entries in first column of V to 1/sqrt(n), for intercept term in beta; ones_normalized = hl.nd.full((V0.shape[0], 1), (1 / S[0])); V = hl.nd.hstack((ones_normalized, V0)). # Compute matrix of regression coefficients for PCs (beta), shape (k, m); beta = BlockMatrix.from_ndarray(((1 / S) * V).T, block_size=block_size) @ g.T; beta = beta.checkpoint(new_temp_file('pc_relate_bm/beta', 'bm')). # Compute matrix of individual-specific AF estimates (mu), shape (m, n); mu = 0.5 * (BlockMatrix.from_ndarray(V * S, block_size=block_size) @ beta).T; # Replace entries in mu with NaN if invalid or if corresponding GT is missing (no contribution from that variant); mu = mu._apply_map2(; lambda _mu, _g: hl.if_else(_bad_mu(_mu, min_individual_maf) | hl.is_nan(_g), nan, _mu),; g,; sparsity_strategy='NeedsDense',; ); mu = mu.checkpoint(new_temp_file('pc_relate_bm/mu', 'bm')). # Compute kinship matrix (phi), shape (n, n); # Where mu is NaN (missi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:19509,Recover,Recover,19509,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,1,['Recover'],['Recover']
Availability," = right.localize_entries('right_entries', 'right_cols'). ht = left_t.join(right_t, how='outer'); ht = ht.annotate_globals(; left_keys=hl.group_by(; lambda t: t[0],; hl.enumerate(ht.left_cols.map(lambda x: hl.tuple([x[f] for f in left.col_key])), index_first=False),; ).map_values(lambda elts: elts.map(lambda t: t[1])),; right_keys=hl.group_by(; lambda t: t[0],; hl.enumerate(ht.right_cols.map(lambda x: hl.tuple([x[f] for f in right.col_key])), index_first=False),; ).map_values(lambda elts: elts.map(lambda t: t[1])),; ); ht = ht.annotate_globals(; key_indices=hl.array(ht.left_keys.key_set().union(ht.right_keys.key_set())); .map(lambda k: hl.struct(k=k, left_indices=ht.left_keys.get(k), right_indices=ht.right_keys.get(k))); .flatmap(; lambda s: hl.case(); .when(; hl.is_defined(s.left_indices) & hl.is_defined(s.right_indices),; hl.range(0, s.left_indices.length()).flatmap(; lambda i: hl.range(0, s.right_indices.length()).map(; lambda j: hl.struct(k=s.k, left_index=s.left_indices[i], right_index=s.right_indices[j]); ); ),; ); .when(; hl.is_defined(s.left_indices),; s.left_indices.map(lambda elt: hl.struct(k=s.k, left_index=elt, right_index=hl.missing('int32'))),; ); .when(; hl.is_defined(s.right_indices),; s.right_indices.map(lambda elt: hl.struct(k=s.k, left_index=hl.missing('int32'), right_index=elt)),; ); .or_error('assertion error'); ); ); ht = ht.annotate(; __entries=ht.key_indices.map(; lambda s: hl.struct(left_entry=ht.left_entries[s.left_index], right_entry=ht.right_entries[s.right_index]); ); ); ht = ht.annotate_globals(; __cols=ht.key_indices.map(; lambda s: hl.struct(; **{f: s.k[i] for i, f in enumerate(left.col_key)},; left_col=ht.left_cols[s.left_index],; right_col=ht.right_cols[s.right_index],; ); ); ); ht = ht.drop('left_entries', 'left_cols', 'left_keys', 'right_entries', 'right_cols', 'right_keys', 'key_indices'); return ht._unlocalize_entries('__entries', '__cols', list(left.col_key)). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/full_outer_join_mt.html:5391,error,error,5391,docs/0.2/_modules/hail/experimental/full_outer_join_mt.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/full_outer_join_mt.html,2,['error'],['error']
Availability," = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmethod; def get_running():; """"""Return the running Hail context in this Python session. **Example**. .. doctest::; :options: +SKIP. >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. :return: Current Hail context.; :rtype: :class:`.HailContext`; """""". return Env.hc(). @property; def version(self):; """"""Return the version of Hail associated with this HailContext. :rtype: str; """"""; return self._jhc.version(). [docs] @handle_py4j; @typecheck_method(regex=strlike,; path=oneof(strlike, listof(strlike)),; max_count=integral); def grep(self, regex, path, max_count=100):; """"""Grep big files, like, really fast. **Examples**. Print all lines containing the string ``hello`` in *file.txt*:. >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in *file1.txt* and *file2.txt*:. >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). **Background**. :py:meth:`~hail.HailContext.grep` mimics the basic functionality of Unix ``grep`` in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regula",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:4011,recover,recover,4011,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['recover'],['recover']
Availability," @ b); sqrtW = hl.sqrt(mu * (1 - mu)); q, r = hl.nd.qr(X * sqrtW.T.reshape(-1, 1)); h = (q * q).sum(1); coef = r[:m, :m]; residual = y - mu; dep = q[:, :m].T @ ((residual + (h * (0.5 - mu))) / sqrtW); delta_b_struct = hl.nd.solve_triangular(coef, dep.reshape(-1, 1), no_crash=True); exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution.reshape(-1). max_delta_b = nd_max(hl.abs(delta_b)). return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.annotate(n_iterations=0, log_lkhd=0, converged=False, exploded=False); return hl.experimental.loop(fit, dtype, 1, b). def _firth_test(null_fit, X, y, max_iterations, tolerance) -> StructExpression:; firth_improved_null_fit = _firth_fit(null_fit.b, X, y, max_iterations=max_iterations, tolerance=tolerance); dof = 1 # 1 variant. def cont(firth_improved_null_fit):; initial_b_full_model = hl.nd.hstack([firth_improved_null_fit.b, hl.nd.array([0.0])]); firth_fit = _firth_fit(initial_b_full_model, X, y, max_iterations=max_iterations, tolerance=tolerance). def cont2(firth_fit):; firth_chi_sq = 2 * (firth_fit.log_lkhd - firth_improved_null_fit.log_lkhd); firth_p = hl.pchisqtail(firth_chi_sq, dof). blank_struct = hl.struct(; beta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:45819,toler,tolerance,45819,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['toler'],['tolerance']
Availability," Database; Other Resources. Hail. Docs »; Tutorials. View page source. Tutorials¶; To take Hail for a test drive, go through our tutorials. These can be viewed here in the documentation,; but we recommend instead that you run them yourself with Jupyter.; Download the Hail distribution from our getting started page, and follow; the instructions there to set up the Hail. Inside the unzipped distribution folder, you’ll find; a tutorials/ directory. cd to this directory and run jhail to start the notebook; server, then click a notebook to begin!. Hail Overview¶; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the; functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by population stratification. Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language¶; This notebook starts with the basics of the Hail expression language, and builds up practical experience; with the type system, syntax, and functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice, dice, filter, and query genetic data. Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate¶; This notebook uses the Hail expression language to query, filter, and annotate the same thousand genomes; dataset from the overview. We also cover how to compute aggregate statis",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials-landing.html:2098,down,download,2098,docs/0.1/tutorials-landing.html,https://hail.is,https://hail.is/docs/0.1/tutorials-landing.html,1,['down'],['download']
Availability," Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR). - HemiX -- in non-PAR of X, male child; - HemiY -- in non-PAR of Y, male child; - Auto -- otherwise (in autosome or PAR, or female child). Any refers to :math:`\{ HomRef, Het, HomVar, NoCall \}` and ! denotes complement in this set. +--------+------------+------------+----------+------------------+; |Code | Dad | Mom | Kid | Copy State |; +========+============+============+==========+==================+; | 1 | HomVar | HomVar | Het | Auto |; +--------+------------+------------+----------+------------------+; | 2 | HomRef | HomRef ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:157945,error,error,157945,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability," Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compres",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:77797,error,error,77797,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability," For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:11675,error,errors,11675,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['error'],['errors']
Availability," Google Cloud Storage to a local file:; >>> hadoop_copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') . Notes; Try using hadoop_open() first, it’s simpler, but not great; for large data! For example:; >>> with hadoop_open('gs://my_bucket/results.csv', 'r') as f: ; ... pandas_df.to_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers). Parameters:. src (str) – Source file URI.; dest (str) – Destination file URI. hail.utils.hadoop_exists(path)[source]; Returns True if path exists. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_is_file(path)[source]; Returns True if path both exists and is a file. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_is_dir(path)[source]; Returns True if path both exists and is a directory. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_stat(path)[source]; Returns information about the file or directory at a given path.; Notes; Raises an error if path does not exist.; The resulting dictionary contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; dict. hail.utils.hadoop_ls(path)[source]; Returns information about files at path.; Notes; Raises an error if path does not exist.; If path is a file, returns a list with one element. If path is a; directory, returns an element for each file contained in path (does not; search recursively).; Each dict element of the result list contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; list [dict]. hail.utils.hadoop_scheme_supported(scheme)[sour",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:7577,error,error,7577,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,1,['error'],['error']
Availability," Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see `here <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only **unphased** and **diploid** genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed. Before importing, ensure that:. - The sample file has the same number of samples as the BGEN file.; - No duplicate sample IDs are present. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns <sec-hadoop-glob>`. .. _gpfilters:. **Genotype probability (``gp``) representation**:. The following modifications are made to genotype probabilities in BGEN v1.1 files:. - Since genotype probabilities are understood to define a probability distribution, :py:meth:`~hail.HailContext.import_bgen` automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the ``tolerance`` parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains. - :py:meth:`~hail.HailContext.import_bgen` normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. **Annotations**. :py:meth:`~hail.HailContext.import_bgen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .bgen files to import.; :type path: str or list of str. :param float tolerance: If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1. :param sample_file: Sample file.; :type sample_file:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:6856,toler,tolerance,6856,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['toler'],['tolerance']
Availability," MM^T. Note that the only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in :py:meth:`~hail.VariantDataset.grm` is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. :param bool force_block: Force using Spark's BlockMatrix to compute kinship (advanced). :param bool force_gramian: Force using Spark's RowMatrix.computeGramian to compute kinship (advanced). :return: Realized Relationship Matrix for all samples.; :rtype: :py:class:`KinshipMatrix`; """"""; return KinshipMatrix(self._jvdf.rrm(force_block, force_gramian)). [docs] @handle_py4j; @typecheck_method(other=vds_type,; tolerance=numeric); def same(self, other, tolerance=1e-6):; """"""True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values. **Examples**. This will return True:. >>> vds.same(vds). **Notes**. The ``tolerance`` parameter sets the tolerance for equality when comparing floating-point fields. More precisely, :math:`x` and :math:`y` are equal if. .. math::. \abs{x - y} \leq tolerance * \max{\abs{x}, \abs{y}}. :param other: variant dataset to compare against; :type other: :class:`.VariantDataset`. :param float tolerance: floating-point tolerance for equality. :rtype: bool; """""". return self._jvds.same(other._jvds, tolerance). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(root=strlike,; keep_star=bool); def sample_qc(self, root='sa.qc', keep_star=False):; """"""Compute per-sample QC metrics. .. include:: requireTGenotype.rst. **Annotations**. :py:meth:`~hail.VariantDataset.sample_qc` computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; ``sa.qc.<identifier>`` (or ``<root>.<identifier>`` if a non-default root was passed):. +---------------------------+--------+----------------------------------------------------------+; | Name | Type | Description |",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:196420,toler,tolerance,196420,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['toler'],['tolerance']
Availability," Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the job’s command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; This method can be called more than once. It’s behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command.; To declare a resource file of type JobResourceFile, use either; the get attribute syntax of job.{identifier} or the get item syntax of; job[‘identifier’]. If an object for that identifier doesn’t exist,; then one will be created automatically (only allowed in the; command() method). The identifier name can be any valid Python; identifier such as ofile5000.; All JobResourceFile are temporary files and must be written to; a permanent location using Batch.write_output() if the output; needs to be saved.; Only resources can be referred to in commands. Referencing a; batch.Batch or Job will result in an error. Parameters:; command (str) – A bash command. Return ty",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:2000,echo,echo,2000,docs/batch/api/batch/hailtop.batch.job.BashJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html,2,['echo'],['echo']
Availability," TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the GENESIS Bioconductor package .; pc_relate() differs from the reference; implementation in a couple key ways:. the principal components analysis does not use an unrelated set of; individuals; the estimators do not perform small sample correction; the algorithm does not provide an option to use population-wide; allele frequency estimates; the algorithm does not provide an option to not use “overall; standardization” (see R pcrelate documentation). Notes; The block_size controls memory usage and parallelism. If it is large; enough to hold an entire sample-by-sample matrix of 64-bit doubles in; memory, then only one Spark worker node can be used to compute matrix; operations. If it is too small, communication overhead will begin to; dominate the computation’s time. The author has found that on Google; Dataproc (where each core has about 3.75GB of memory), setting; block_size larger than 512 tends to cause memory exhaustion errors.; The minimum allele frequency filter is applied per-pair: if either of; the two individual’s individual-specific minor allele frequency is below; the threshold, then the variant’s contribution to relatedness estimates; is zero.; Under the PC-Relate model, kinship, [ phi_{ij} ], ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection.; Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, [ k^{(2)}_{ij} ],; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs.; Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation; “Third degree relatives” are those pairs sharing; [ 2^{-3} = 12.5 %",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:135609,error,errors,135609,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability," Test whether other is a proper subset of the set (other <= set and other != set). Parameters:; other (SetExpression) – Set expression of the same type. Returns:; BooleanExpression – True if other is a proper subset of the set. False otherwise. __le__(other)[source]; Test whether every element in the set is in other. Parameters:; other (SetExpression) – Set expression of the same type. Returns:; BooleanExpression – True if every element in the set is in other. False otherwise. __lt__(other)[source]; Test whether the set is a proper subset of other (set <= other and set != other). Parameters:; other (SetExpression) – Set expression of the same type. Returns:; BooleanExpression – True if the set is a proper subset of other. False otherwise. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __or__(other)[source]; Return the union of the set and other.; Examples; >>> hl.eval(s1 | s2); {1, 2, 3, 5}. Parameters:; other (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements present in either set. __sub__(other)[source]; Return the difference of the set and other.; Examples; >>> hl.eval(s1 - s2); {2}. >>> hl.eval(s2 - s1); {5}. Parameters:; other (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements in the set that are not in other. __xor__(other)[source]; Return the symmetric difference of the set and other.; Examples; >>> hl.eval(s1 ^ s2); {2, 5}. Parameters:; other (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements present in either the set or other but not both. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.SetExpression.html:3251,error,error,3251,docs/0.2/hail.expr.SetExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html,1,['error'],['error']
Availability," This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alon",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:95771,fault,fault,95771,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability," Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; GWAS Tutorial. View page source. GWAS Tutorial; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association test, and demonstrate the need to control for confounding caused by population stratification. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:1527,avail,available,1527,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['avail'],['available']
Availability," ValueError(; f'{name} not found in annotation database,'; f' you may list all known dataset names'; f' with available_datasets'; ); return self.__by_name[name]. def _annotate_gene_name(self, rel: Union[TableRows, MatrixRows]) -> Tuple[str, Union[TableRows, MatrixRows]]:; """"""Annotate row lens with gene name if annotation dataset is gene; keyed. Parameters; ----------; rel : :class:`TableRows` or :class:`MatrixRows`; Row lens of relational object to be annotated. Returns; -------; :class:`tuple`; """"""; gene_field = Env.get_uid(); gencode = self.__by_name['gencode'].index_compatible_version(rel.key); return gene_field, rel.annotate(**{gene_field: gencode.gene_name}). def _check_availability(self, names: Iterable) -> None:; """"""Check if datasets given in `names` are available in the annotation; database instance. Parameters; ----------; names : :obj:`iterable`; Names to check.; """"""; unavailable = [x for x in names if x not in self.__by_name.keys()]; if unavailable:; raise ValueError(f'datasets: {unavailable} not available' f' in the {self.region} region.'). [docs] @typecheck_method(rel=oneof(table_type, matrix_table_type), names=str); def annotate_rows_db(self, rel: Union[Table, MatrixTable], *names: str) -> Union[Table, MatrixTable]:; """"""Add annotations from datasets specified by name to a relational; object. List datasets with :attr:`~.available_datasets`. An interactive query builder is available in the; `Hail Annotation Database documentation; </docs/0.2/annotation_database_ui.html>`_. Examples; --------; Annotate a :class:`.MatrixTable` with ``gnomad_lof_metrics``:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') # doctest: +SKIP. Annotate a :class:`.Table` with ``clinvar_gene_summary``, ``CADD``,; and ``DANN``:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> ht = db.annotate_rows_db(ht, 'clinvar_gene_summary', 'CADD', 'DANN') # doctest: +SKIP. Notes; -----. If a dataset is g",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:14272,avail,available,14272,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['avail'],['available']
Availability," We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alon",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:79812,fault,fault,79812,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability," _templates/experimental.rst. Examples; --------; Import a text table and construct a rows-only matrix table:. >>> table = hl.import_table('data/variant-lof.tsv'); >>> table = table.transmute(**hl.parse_variant(table['v'])).key_by('locus', 'alleles'); >>> sites_mt = hl.MatrixTable.from_rows_table(table). Notes; -----; All fields in the table become row-indexed fields in the; result. Parameters; ----------; table : :class:`.Table`; The table to be converted. Returns; -------; :class:`.MatrixTable`; """"""; col_values_uid = Env.get_uid(); entries_uid = Env.get_uid(); return (; table.annotate_globals(**{col_values_uid: hl.empty_array(hl.tstruct())}); .annotate(**{entries_uid: hl.empty_array(hl.tstruct())}); ._unlocalize_entries(entries_uid, col_values_uid, []); ). [docs] @typecheck_method(p=numeric, seed=nullable(int)); def sample_rows(self, p: float, seed=None) -> 'MatrixTable':; """"""Downsample the matrix table by keeping each row with probability ``p``. Examples; --------; Downsample the dataset to approximately 1% of its rows. >>> small_dataset = dataset.sample_rows(0.01). Notes; -----; Although the :class:`MatrixTable` returned by this method may be; small, it requires a full pass over the rows of the sampled object. Parameters; ----------; p : :obj:`float`; Probability of keeping each row.; seed : :obj:`int`; Random seed. Returns; -------; :class:`.MatrixTable`; Matrix table with approximately ``p * n_rows`` rows.; """""". if not 0 <= p <= 1:; raise ValueError(""Requires 'p' in [0,1]. Found p={}"".format(p)). return self.filter_rows(hl.rand_bool(p, seed)). [docs] @typecheck_method(p=numeric, seed=nullable(int)); def sample_cols(self, p: float, seed=None) -> 'MatrixTable':; """"""Downsample the matrix table by keeping each column with probability ``p``. Examples; --------; Downsample the dataset to approximately 1% of its columns. >>> small_dataset = dataset.sample_cols(0.01). Parameters; ----------; p : :obj:`float`; Probability of keeping each column.; seed : :obj:`int`; Ran",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:127991,Down,Downsample,127991,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['Down'],['Downsample']
Availability, a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram infers min and max values automatically.; (#11317) Add support; for alpha aesthetic and identity position to; geom_histogram. Version 0.2.83; Release 2022-02-01. Bug fixes. (#11268) Fixed; log argument in hail.plot.histogram.; (#11276) Fixed; log argument in hail.plot.pdf.; (#11256) Fixed; memory leak in LD Prune. New features. (#11274) Added; geom_col to hail.ggplot. hailctl dataproc. (#11280) Updated;,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:53261,error,error,53261,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability," already downsampled the full thousand genomes dataset to include more common variants than we’d expect by chance.; In Hail, the association tests accept column fields for the sample phenotype and covariates. Since we’ve already got our phenotype of interest (caffeine consumption) in the dataset, we are good to go:. [38]:. gwas = hl.linear_regression_rows(y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.row.describe(). [Stage 41:> (0 + 1) / 1]. --------------------------------------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; n: int32,; sum_x: float64,; y_transpose_x: float64,; beta: float64,; standard_error: float64,; t_stat: float64,; p_value: float64; }; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f0460f91d00>; Index:; ['row']; --------------------------------------------------------. Looking at the bottom of the above printout, you can see the linear regression adds new row fields for the beta, standard error, t-statistic, and p-value.; Hail makes it easy to visualize results! Let’s make a Manhattan plot:. [39]:. p = hl.plot.manhattan(gwas.p_value); show(p). This doesn’t look like much of a skyline. Let’s check whether our GWAS was well controlled using a Q-Q (quantile-quantile) plot. [40]:. p = hl.plot.qq(gwas.p_value); show(p). Confounded!; The observed p-values drift away from the expectation immediately. Either every SNP in our dataset is causally linked to caffeine consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate this phenotype. This leads to a stratified distribution of the phenotype. The solution is to include ancestry as a covariate in our regression.; The linear_regression_rows function can also take column fields to use as covariates. We already annotated our samples with reported ancestry, but it is good to be skeptical of these labels due to human error. Gen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:19859,error,error,19859,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['error'],['error']
Availability," and stores temporary intermediate files in “gs://my-bucket/temporary-files”.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1864,echo,echo,1864,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,2,['echo'],['echo']
Availability," and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job object",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:3901,echo,echo,3901,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability," and the alternate alleles (ALT field) are; the subsequent elements.; filters (tset of tstr) – Set containing all filters applied to a; variant.; rsid (tstr) – rsID of the variant.; qual (tfloat64) – Floating-point number in the QUAL field.; info (tstruct) – All INFO fields defined in the VCF header; can be found in the struct info. Data types match the type specified; in the VCF header, and if the declared Number is not 1, the result; will be stored as an array. Entry Fields; import_vcf() generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elem",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:45024,down,downstream,45024,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['down'],['downstream']
Availability," annotations in Step 4, depending on whether \(\delta\) is set or fit. Annotation; Type; Value. global.lmmreg.useML; Boolean; true if fit by ML, false if fit by REML. global.lmmreg.beta; Dict[String, Double]; map from intercept and the given covariates expressions to the corresponding fit \(\beta\) coefficients. global.lmmreg.sigmaG2; Double; fit coefficient of genetic variance, \(\hat{\sigma}_g^2\). global.lmmreg.sigmaE2; Double; fit coefficient of environmental variance \(\hat{\sigma}_e^2\). global.lmmreg.delta; Double; fit ratio of variance component coefficients, \(\hat{\delta}\). global.lmmreg.h2; Double; fit narrow-sense heritability, \(\hat{h}^2\). global.lmmreg.nEigs; Int; number of eigenvectors of kinship matrix used to fit model. global.lmmreg.dropped_variance_fraction; Double; specified value of dropped_variance_fraction. global.lmmreg.evals; Array[Double]; all eigenvalues of the kinship matrix in descending order. global.lmmreg.fit.seH2; Double; standard error of \(\hat{h}^2\) under asymptotic normal approximation. global.lmmreg.fit.normLkhdH2; Array[Double]; likelihood function of \(h^2\) normalized on the discrete grid 0.01, 0.02, ..., 0.99. Index i is the likelihood for percentage i. global.lmmreg.fit.maxLogLkhd; Double; (restricted) maximum log likelihood corresponding to \(\hat{\delta}\). global.lmmreg.fit.logDeltaGrid; Array[Double]; values of \(\mathrm{ln}(\delta)\) used in the grid search. global.lmmreg.fit.logLkhdVals; Array[Double]; (restricted) log likelihood of \(y\) given \(X\) and \(\mathrm{ln}(\delta)\) at the (RE)ML fit of \(\beta\) and \(\sigma_g^2\). These global annotations are also added to hail.log, with the ranked evals and \(\delta\) grid with values in .tsv tabular form. Use grep 'lmmreg:' hail.log to find the lines just above each table.; If Step 5 is performed, lmmreg() also adds four linear regression variant annotations. Annotation; Type; Value. va.lmmreg.beta; Double; fit genotype coefficient, \(\hat\beta_0\). va.lmmreg.sigma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:94259,error,error,94259,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability," annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """""". _valid_key_properties: ClassVar = {'gene', 'unique'}; _valid_regions: ClassVar = {'us', 'us-central1', 'europe-west1'}; _valid_clouds: ClassVar = {'gcp', 'aws'}; _valid_combinations: ClassVar = {('us', 'aws'), ('us-central1', 'gcp'), ('europe-west1', 'gcp')}. def __init__(; self,; *,; region: str = 'us-central1',; cloud: str = 'gcp',; url: Optional[str] = None,; config: Optional[dict] = None,; ):; if region not in DB._valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid regions are {DB._valid_regions}.'; ); if cloud not in DB._valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {DB._valid_clouds}.'; ); if (region, cloud) not in DB._val",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:10269,avail,available,10269,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['avail'],['available']
Availability," as:. >>> vds_result = vds.linreg('if (sa.pheno.isFemale) sa.pheno.age else (2 * sa.pheno.age + 10)'). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation ``sa.fam.isCase`` added by importing a FAM file with case-control phenotype, case is 1 and control is 0. The standard least-squares linear regression model is derived in Section; 3.2 of `The Elements of Statistical Learning, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__. See; equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 2` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; genotype and intercept. **Annotations**. With the default root, the following four variant annotations are added. - **va.linreg.beta** (*Double*) -- fit genotype coefficient, :math:`\hat\beta_1`; - **va.linreg.se** (*Double*) -- estimated standard error, :math:`\widehat{\mathrm{se}}`; - **va.linreg.tstat** (*Double*) -- :math:`t`-statistic, equal to :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; - **va.linreg.pval** (*Double*) -- :math:`p`-value. :param str y: Response expression. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of linear regression. :param bool use_dosages: If true, use dosages genotypes rather than hard call genotypes. :param int min_ac: Minimum alternate allele count. :param float min_af: Minimum alternate allele frequency. :return: Variant dataset with linear regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.linreg(y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages, min_ac, min_af); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; y=strlike,; covariates=listof(s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:101397,error,error,101397,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability," association testing for; sequencing data with the sequence kernel association test. Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; Generate a dataset with a phenotype noisily computed from the genotypes:; >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = (hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)) > 0.5). Test if the phenotype is significantly associated with the genotype:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 1.78e+02 | 1.68e-04 | 0 |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper’s suggested weights which are derived from the; allele frequency.; >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real da",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:72779,fault,fault,72779,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['fault'],['fault']
Availability," backend (Optional[ServiceBackend]) – Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str]) – The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None]) – The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameter’s value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool) – If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool) – If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str]) – DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version of map(). async_submit; Aysncio compatible version of BatchPoolExecutor.submit(). map; Call fn on cloud machines with arguments from iterables. shutdown; Allow temporary resources to be cleaned up. submit; Call fn on a cloud machine with all remaining arguments and keyword arguments. async async_map(fn, iterables, timeout=None, chunksize=1); Aysncio compatible version of map(). Return type:; AsyncGenerator[int, None]. async async_submit(unapplied, *args, **kwargs); Aysncio compatible version of BatchPoolExecutor.submit(). Return type:; BatchPoolFuture. map(fn, *iterables, timeout=None, chunksize=1); Call fn on cloud machines with arguments from iterables.; This function returns a ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:3283,down,down,3283,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,2,['down'],['down']
Availability," bad_fields = []; for field in entry_fields:; if field in expected_field_types and entry_fields[field].dtype != expected_field_types[field]:; bad_fields.append((field, entry_fields[field].dtype, expected_field_types[field])). if bad_fields:; msg = '\n '.join([f""'{x[0]}'\tfound: {x[1]}\texpected: {x[2]}"" for x in bad_fields]); raise TypeError(""'split_multi_hts': Found invalid types for the following fields:\n "" + msg). update_entries_expression = {}; if 'GT' in entry_fields:; update_entries_expression['GT'] = hl.downcode(split.GT, split.a_index); if 'DP' in entry_fields:; update_entries_expression['DP'] = split.DP; if 'AD' in entry_fields:; update_entries_expression['AD'] = hl.or_missing(; hl.is_defined(split.AD), [hl.sum(split.AD) - split.AD[split.a_index], split.AD[split.a_index]]; ); if 'PL' in entry_fields:; pl = hl.or_missing(; hl.is_defined(split.PL),; (; hl.range(0, 3).map(; lambda i: hl.min(; (; hl.range(0, hl.triangle(split.old_alleles.length())); .filter(; lambda j: hl.downcode(; hl.unphased_diploid_gt_index_call(j), split.a_index; ).unphased_diploid_gt_index(); == i; ); .map(lambda j: split.PL[j]); ); ); ); ),; ); if 'GQ' in entry_fields:; update_entries_expression['PL'] = pl; update_entries_expression['GQ'] = hl.or_else(hl.gq_from_pl(pl), split.GQ); else:; update_entries_expression['PL'] = pl; elif 'GQ' in entry_fields:; update_entries_expression['GQ'] = split.GQ. if 'PGT' in entry_fields:; update_entries_expression['PGT'] = hl.downcode(split.PGT, split.a_index); if 'PID' in entry_fields:; update_entries_expression['PID'] = split.PID; return split.annotate_entries(**update_entries_expression).drop('old_locus', 'old_alleles'). [docs]@typecheck(call_expr=expr_call); def genetic_relatedness_matrix(call_expr) -> BlockMatrix:; r""""""Compute the genetic relatedness matrix (GRM). Examples; --------. >>> grm = hl.genetic_relatedness_matrix(dataset.GT). Notes; -----; The genetic relationship matrix (GRM) :math:`G` encodes genetic correlation; between each pair of sa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:124897,down,downcode,124897,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['down'],['downcode']
Availability," based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""; if mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl.is_defined(mt.PL),; hl.bind(; lambda unnorm: unnorm - hl.min(unnorm),; hl.range(0, hl.triangle(mt.alleles.length())).map(; lambda newi: hl.bind(; lambda newc: mt.PL[; hl.call(mt.new_to_old[newc[0]], mt.new_to_old[newc[1]]).unphased_diploid_gt_index(); ],; hl.unphased_diploid_gt_index_call(newi),; ); ),; ),; hl.missing(tarray(tint32)),; ); return mt.annotate_entries(; GT=hl.unphased_diploid_gt_index_call(hl.argmin(newPL, unique=True)),; AD=hl.if_else(; hl.is_defined(mt.AD),; hl.range(0, mt.alleles.length()).map(lambda newi: mt.AD[mt.new_to_old[newi]])",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:162583,down,downcodes,162583,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['down'],['downcodes']
Availability," be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:76920,failure,failures,76920,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['error', 'failure']","['error', 'failures']"
Availability," call is matched, then; `then` is returned. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0 and self._when_missing_case is None:; return then; self._unify_type(then.dtype); return self._finish(then). [docs] def or_missing(self):; """"""Finish the switch statement by returning missing. Notes; -----; If no value from a :meth:`~.SwitchBuilder.when` call is matched, then; the result is missing. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_missing' cannot be called without at least one 'when' call""); from hail.expr.functions import missing. return self._finish(missing(self._ret_type)). [docs] @typecheck_method(message=expr_str); def or_error(self, message):; """"""Finish the switch statement by throwing an error with the given message. Notes; -----; If no value from a :meth:`.SwitchBuilder.when` call is matched, then an; error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_error' cannot be called without at least one 'when' call""); error_expr = construct_expr(ir.Die(message._ir, self._ret_type), self._ret_type); return self._finish(error_expr). [docs]class CaseBuilder(ConditionalBuilder):; """"""Class for chaining multiple if-else statements. Examples; --------. >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; -----; All expressions appearing as the `then` parameters to; :meth:`~hail.expr.builders.CaseBuilder.when` or; :meth:`~hail.expr.builders.CaseBuilder.default` method calls must be the; same type. Parameters; ----------; missing_false: :obj:`.bool`; Treat missing predicates as ``False``. See Also; --",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/builders.html:4966,error,error,4966,docs/0.2/_modules/hail/expr/builders.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/builders.html,2,['error'],['error']
Availability," data in textual representations into a Hail; MatrixTable. Finally, it is possible to create a Hail Table; from a pandas DataFrame with Table.from_pandas(). import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_matrix_table(paths[, row_fields, ...]); Import tab-delimited file(s) as a MatrixTable. import_lines(paths[, min_partitions, ...]); Import lines of file(s) as a Table of strings. Genetics; Hail has several functions to import genetics-specific file formats into Hail; MatrixTable or Table objects:. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. import_plink(bed, bim, fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_bed(path[, reference_genome, ...]); Import a UCSC BED file as a Table. import_bgen(path, entry_fields[, ...]); Import BGEN file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). import_gen(path[, sample_file, tolerance, ...]); Import GEN file(s) as a MatrixTable. import_fam(path[, quant_pheno, delimiter, ...]); Import a PLINK FAM file into a Table. import_locus_intervals(path[, ...]); Import a locus interval list as a Table. Export. General purpose; Some of the most widely-used export functionality is found as class methods; on the Table and Expression objects:. Table.export(): Used to write a Table to a text table (TSV).; Expression.export(): Used to write an expression to a text file. For; one-dimensional expressions (table row fields, matrix table row or column fields),; this is very similar to Table.export(). For two-dimensional expressions; (entry expressions on matrix tables), a text matrix representation that can be; imported with import_matrix_table() will be produced.; Table.to_pandas(): Used to convert a Hail table to a pandas; DataFrame. Genetics; Hail can export to some of the genetics-specific file formats:. export_vcf(dataset, output[, ...]); Export a MatrixTable ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:2848,toler,tolerance,2848,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['toler'],['tolerance']
Availability," eigenvalues,; # presumably because a good estimate of the Generalized Chi-Sqaured CDF is not significantly; # affected by chi-squared components with very tiny weights.; threshold = 1e-5 * eigenvalues.sum() / eigenvalues.shape[0]; w = hl.array(eigenvalues).filter(lambda y: y >= threshold); genchisq_data = hl.pgenchisq(; ht.Q,; w=w,; k=hl.nd.ones(hl.len(w), dtype=hl.tint32),; lam=hl.nd.zeros(hl.len(w)),; mu=0,; sigma=0,; min_accuracy=accuracy,; max_iterations=iterations,; ); ht = ht.select(; 'size',; # for reasons unknown, the R implementation calls this expression the Q statistic (which is; # *not* what they write in the paper); q_stat=ht.Q / 2,; # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples', 'null_fit'). [docs]@typecheck(; key_expr=expr_any,; weight_expr=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; logistic=oneof(bool, sized_tupleof(nullable(int), nullable(float))),; max_size=int,; accuracy=numeric,; iterations=int,; ); def skat(; key_expr,; weight_expr,; y,; x,; covariates,; logistic: Union[bool, Tuple[int, float]] = False,; max_size: int = 46340,; accuracy: float = 1e-6,; iterations: int = 10000,; ) -> Table:; r""""""Test each keyed group of rows for association by linear or logistic; SKAT test. Examples; --------. Test each gene for association using the linear sequence kernel association; test:. >>> skat_table = hl.skat(key_expr=burden_ds.gene,; ... weight_expr=burden_ds.weight,; ... y=burden_ds.burden.pheno,; ... x=burden_ds.GT.n_alt_alleles(),; ... covariates=[1, burden_ds.burden.cov1, burden_ds.burden.cov2]). .. caution::. By default, the Davies algorithm iterat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:101166,fault,fault,101166,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability," evaluate the cumulative distribution function (CDF).; w : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A weight for each non-central chi-square term.; k : :obj:`list` of :obj:`int` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tint32`; A degrees of freedom parameter for each non-central chi-square term.; lam : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A non-centrality parameter for each non-central chi-square term. We use `lam` instead; of `lambda` because the latter is a reserved word in Python.; mu : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; sigma : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; max_iterations : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is eith",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:70446,error,error,70446,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['error'],['error']
Availability," fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table. Returns; -------; :class:`.Table`; """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""). y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). # _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_names = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field_names, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # Handle filtering columns with missing values:; mt = mt.filter_cols(hl.arr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:57190,toler,tolerance,57190,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,6,['toler'],['tolerance']
Availability," fields. row; Returns a struct expression of all row-indexed fields, including keys. row_key; Row key struct. row_value; Returns a struct expression including all non-key row-indexed fields. Methods. add_col_index; Add the integer index of each column as a new column field. add_row_index; Add the integer index of each row as a new row field. aggregate_cols; Aggregate over columns to a local value. aggregate_entries; Aggregate over entries to a local value. aggregate_rows; Aggregate over rows to a local value. annotate_cols; Create new column-indexed fields by name. annotate_entries; Create new row-and-column-indexed fields by name. annotate_globals; Create new global fields by name. annotate_rows; Create new row-indexed fields by name. anti_join_cols; Filters the table to columns whose key does not appear in other. anti_join_rows; Filters the table to rows whose key does not appear in other. cache; Persist the dataset in memory. checkpoint; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. choose_cols; Choose a new set of columns from a list of old column indices. collect_cols_by_key; Collect values for each unique column key into arrays. cols; Returns a table with all column fields in the matrix. compute_entry_filter_stats; Compute statistics about the number and fraction of filtered entries. count; Count the number of rows and columns in the matrix. count_cols; Count the number of columns in the matrix. count_rows; Count the number of rows in the matrix. describe; Print information about the fields in the matrix table. distinct_by_col; Remove columns with a duplicate row key, keeping exactly one column for each unique key. distinct_by_row; Remove rows with a duplicate row key, keeping exactly one row for each unique key. drop; Drop fields. entries; Returns a matrix in coordinate table form. explode_cols; Explodes a column field of type array or set, copying the entire column for each element. explode_rows; Expl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:3468,checkpoint,checkpoint,3468,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['checkpoint'],['checkpoint']
Availability," file stored in Google Cloud Storage:; >>> with hadoop_open('gs://my-bucket/notes.txt') as f: ; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:; >>> with hadoop_open('gs://my-bucket/notes.txt', 'w') as f: ; ... f.write('result1: %s\n' % result1); ... f.write('result2: %s\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:; >>> from struct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: ; ... print(unpack('<f', bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). Caution; These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use hadoop_copy(); to move your file to a distributed file system. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hail.utils.hadoop_copy(src, dest)[source]; Copy a file through the Hadoop filesystem API.; Supports distributed file systems like hdfs, gs, and s3.; Examples; Copy a file from Google Cloud Storage to a local file:; >>> hadoop_copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') . Notes; Try using hadoop_open() first, it’s simpler, but not great; for large data! For example:; >>> with hadoop_open('gs://my_bucket/results.csv', '",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:5854,error,error,5854,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,1,['error'],['error']
Availability," focus on building images. Installation; You can install Docker by following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; https://docs.docker.com/engine/reference/builder/. Building Images; To create a Docker image, use; docker build -t us-docker.pkg.dev/<my-project>/<my-image>:<tag> -f Docke",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/docker_resources.html:1809,down,download,1809,docs/batch/docker_resources.html,https://hail.is,https://hail.is/docs/batch/docker_resources.html,1,['down'],['download']
Availability," following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .bgen files to import.; :type path: str or list of str. :param float tolerance: If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1. :param sample_file: Sample file.; :type sample_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :return: Variant dataset imported from .bgen file.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importBgens(jindexed_seq_args(path), joption(sample_file),; tolerance, joption(min_partitions)); return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; sample_file=nullable(strlike),; tolerance=numeric,; min_partitions=nullable(integral),; chromosome=nullable(strlike)); def import_gen(self, path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None):; """"""Import .gen file(s) as variant dataset. **Examples**. Read a .gen file and a .sample file and write to a .vds file:. >>> (hc.import_gen('data/example.gen', sample_file='data/example.sample'); ... .write('output/gen_example1.vds')). Load multiple files at the same time with :ref:`Hadoop glob patterns <sec-hadoop-glob>`:. >>> (hc.import_gen('data/example.chr*.gen', sample_file='data/example.sample'); ... .write('output/gen_example2.vds')). **Notes**. For more information on the .gen file format, see `here <http://www.stats.ox.ac.uk/%7Emarchini/software/gwas/file_format.html#mozTocId40300>`__. To ensure that the .gen file(s) and .sample file are correctly prepared for import:. - If there are only 5 columns before the start of the genotype probability data (chromosome field is missing), you must specify the chromosome u",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:8250,toler,tolerance,8250,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,2,['toler'],['tolerance']
Availability," format flags `'b'` and `'B'` (printed as ``'false'`` instead). Parameters; ----------; f : :class:`.StringExpression`; Java `format string <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Formatter.html#syntax>`__.; args : variable-length arguments of :class:`.Expression`; Arguments to format. Returns; -------; :class:`.StringExpression`; """""". return _func(""format"", hl.tstr, f, hl.tuple(args)). [docs]@typecheck(x=expr_float64, y=expr_float64, tolerance=expr_float64, absolute=expr_bool, nan_same=expr_bool); def approx_equal(x, y, tolerance=1e-6, absolute=False, nan_same=False):; """"""Tests whether two numbers are approximately equal. Examples; --------; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters; ----------; x : :class:`.NumericExpression`; y : :class:`.NumericExpression`; tolerance : :class:`.NumericExpression`; absolute : :class:`.BooleanExpression`; If True, compute ``abs(x - y) <= tolerance``. Otherwise, compute; ``abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022)``.; nan_same : :class:`.BooleanExpression`; If True, then ``NaN == NaN`` will evaluate to True. Otherwise,; it will return False. Returns; -------; :class:`.BooleanExpression`; """""". return _func(""approxEqual"", hl.tbool, x, y, tolerance, absolute, nan_same). def _shift_op(x, y, op):; assert op in ('<<', '>>', '>>>'); t = x.dtype; if t == hl.tint64:; word_size = 64; zero = hl.int64(0); else:; word_size = 32; zero = hl.int32(0). indices, aggregations = unify_all(x, y); return hl.bind(; lambda x, y: (; hl.case(); .when(y >= word_size, hl.sign(x) if op == '>>' else zero); .when(y >= 0, construct_expr(ir.ApplyBinaryPrimOp(op, x._ir, y._ir), t, indices, aggregations)); .or_error('cannot shift by a negative value: ' + hl.str(x) + f"" {op} "" + hl.str(y)); ),; x,; y,; ). def _bit_op(x, y, op):; if x.dtype == hl.tint32 and y.dtype == hl.tint32:; t = hl.tint32; else:; t = hl.tint64;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:177289,toler,tolerance,177289,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['toler'],['tolerance']
Availability," functions. locus(contig, pos[, reference_genome]); Construct a locus expression from a chromosome and position. locus_from_global_position(global_pos[, ...]); Constructs a locus expression from a global position and a reference genome. locus_interval(contig, start, end[, ...]); Construct a locus interval expression. parse_locus(s[, reference_genome]); Construct a locus expression by parsing a string or string expression. parse_variant(s[, reference_genome]); Construct a struct with a locus and alleles by parsing a string. parse_locus_interval(s[, reference_genome, ...]); Construct a locus interval expression by parsing a string or string expression. variant_str(*args); Create a variant colon-delimited string. call(*alleles[, phased]); Construct a call expression. unphased_diploid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_strand_ambiguous(ref, alt); Returns True if the alleles are strand ambiguous. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:1669,down,downcode,1669,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['down'],['downcode']
Availability," given by:. \[\widehat{k^{(2)}_{ij}} \coloneqq; \frac{\sum_{s \in S_{ij}}X_{is} X_{js}}{\sum_{s \in S_{ij}}; \widehat{\sigma^2_{is}} \widehat{\sigma^2_{js}}}\]; The estimator for identity-by-descent zero is given by:. \[\widehat{k^{(0)}_{ij}} \coloneqq; \begin{cases}; \frac{\text{IBS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}}; \widehat{\mu_{is}}^2(1 - \widehat{\mu_{js}})^2; + (1 - \widehat{\mu_{is}})^2\widehat{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}\]; The estimator for identity-by-descent one is given by:. \[\widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}\]; Note that, even if present, phase information is ignored by this method.; The PC-Relate method is described in “Model-free Estimation of Recent; Genetic Relatedness”. Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the GENESIS Bioconductor package .; pc_relate() differs from the reference implementation in a few; ways:. if k is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples.; the estimators do not perform small sample correction; the algorithm does not provide an option to use population-wide; allele frequency estimates; the algorithm does not provide an option to not use “overall; standardization” (see R pcrelate documentation). Under the PC-Relate model, kinship, \(\phi_{ij}\), ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection.; Parent-child and sibling pairs both have kinship",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:17635,avail,available,17635,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['avail'],['available']
Availability," group representing a mapping of identifier to input resource files. run; Execute a batch. select_jobs; Select all jobs in the batch whose name matches pattern. write_output; Write resource file or resource file group to an output destination. static from_batch_id(batch_id, *args, **kwargs); Create a Batch from an existing batch id.; Notes; Can only be used with the ServiceBackend.; Examples; Create a batch object from an existing batch id:; >>> b = Batch.from_batch_id(1) . Parameters:; batch_id (int) – ID of an existing Batch. Return type:; Batch. Returns:; A Batch object that can append jobs to an existing batch. new_bash_job(name=None, attributes=None, shell=None); Initialize a BashJob object with default memory, storage,; image, and CPU settings (defined in Batch) upon batch creation.; Examples; Create and execute a batch b with one job j that prints “hello world”:; >>> b = Batch(); >>> j = b.new_bash_job(name='hello', attributes={'language': 'english'}); >>> j.command('echo ""hello world""'); >>> b.run(). Parameters:. name (Optional[str]) – Name of the job.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead. Return type:; BashJob. new_job(name=None, attributes=None, shell=None); Alias for Batch.new_bash_job(). Return type:; BashJob. new_python_job(name=None, attributes=None); Initialize a new PythonJob object with default; Python image, memory, storage, and CPU settings (defined in Batch); upon batch creation.; Examples; Create and execute a batch b with one job j that prints “hello alice”:; b = Batch(default_python_image='hailgenetics/python-dill:3.9-slim'). def hello(name):; return f'hello {name}'. j = b.new_python_job(); output = j.call(hello, 'alice'). # Write out the str representation of result to a file. b.write_output(output.as_str(), 'hello.txt'). b.run(). Notes; The image to use for Python jobs can be specified by default_python_image; when constructing a Ba",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:6205,echo,echo,6205,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['echo'],['echo']
Availability," grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lme",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:125112,error,errors,125112,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability, hail.expr.functions). bit_and() (in module hail.expr.functions). bit_count() (in module hail.expr.functions). bit_lshift() (in module hail.expr.functions). bit_not() (in module hail.expr.functions). bit_or() (in module hail.expr.functions). bit_rshift() (in module hail.expr.functions). bit_xor() (in module hail.expr.functions). block_size (hail.linalg.BlockMatrix property). BlockMatrix (class in hail.linalg). bool() (in module hail.expr.functions). BooleanExpression (class in hail.expr). C. cache() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). calculate_phenotypes() (in module hail.experimental.ldscsim). Call (class in hail.genetics). call() (in module hail.expr.functions). call_stats() (in module hail.expr.aggregators). CallExpression (class in hail.expr). case() (in module hail.expr.functions). CaseBuilder (class in hail.expr.builders). cdf() (in module hail.plot). ceil() (hail.linalg.BlockMatrix method). (in module hail.expr.functions). checkpoint() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). (hail.vds.VariantDataset method). chi_squared_test() (in module hail.expr.functions). choose_cols() (hail.MatrixTable method). citation() (in module hail). coalesce() (in module hail.expr.functions). cochran_mantel_haenszel_test() (in module hail.expr.functions). col (hail.MatrixTable property). col_key (hail.MatrixTable property). col_value (hail.MatrixTable property). collect() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). (hail.expr.BooleanExpression method). (hail.expr.CallExpression method). (hail.expr.CollectionExpression method). (hail.expr.DictExpression method). (hail.expr.Expression method). (hail.expr.Float32Expression method). (hail.expr.Float64Expression method). (hail.expr.Int32Expression method). (hail.expr.Int64Expression method). (hail.expr.IntervalExpression method). (hail.expr.LocusExpression method). (hail.expr.NDArrayExpression method). (hail.expr.,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genindex.html:13252,checkpoint,checkpoint,13252,docs/0.2/genindex.html,https://hail.is,https://hail.is/docs/0.2/genindex.html,1,['checkpoint'],['checkpoint']
Availability," height=800, collect_all=None, n_divisions=500, missing_label='NA')[source]; Create a Quantile-Quantile plot. (https://en.wikipedia.org/wiki/Q-Q_plot); If no label or a single label is provided, then returns bokeh.plotting.figure; Otherwise returns a bokeh.models.layouts.Column containing:; - a bokeh.models.widgets.inputs.Select dropdown selection widget for labels; - a bokeh.plotting.figure containing the interactive qq plot; Points will be colored by one of the labels defined in the label using the color scheme defined in; the corresponding entry of colors if provided (otherwise a default scheme is used). To specify your color; mapper, check the bokeh documentation; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions.; Hovering on points will display their coordinates, labels and any additional fields specified in hover_fields. Parameters:. pvals (NumericExpression) – List of x-values to be plotted.; label (Expression or Dict[str, Expression]]) – Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title (str, optional) – Title of the scatterplot.; xlabel (str, optional) – X-axis label.; ylabel (str, optional) – Y-axis label.; size (int) – Size of markers in screen space units.; legend (bool) – Whether or not to show the legend in the resulting figure.; hover_fields (Dict[str, Expression], optional) – Extra fields to be displayed when hovering over a point o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:10563,down,down,10563,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['down'],['down']
Availability," hl.import_matrix_table by caching header line computation. Bug fixes. (#12115) When using; use_new_shuffle=True, fix a bug when there are more than 2^31; rows; (#12074) Fix bug; where hl.init could silently overwrite the global random seed.; (#12079) Fix bug in; handling of missing (aka NA) fields in grouped aggregation and; distinct by key.; (#12056) Fix; hl.export_vcf to actually create tabix files when requested.; (#12020) Fix bug in; hl.experimental.densify which manifested as an AssertionError; about dtypes. Version 0.2.97; Released 2022-06-30. New Features. (#11756); hb.BatchPoolExecutor and Python jobs both now also support async; functions. Bug fixes. (#11962) Fix error; (logged as (#11891)); in VCF combiner when exactly 10 or 100 files are combined.; (#11969) Fix; import_table and import_lines to use multiple partitions when; force_bgz is used.; (#11964) Fix; erroneous “Bucket is a requester pays bucket but no user project; provided.” errors in Google Dataproc by updating to the latest; Dataproc image version. Version 0.2.96; Released 2022-06-21. New Features. (#11833); hl.rand_unif now has default arguments of 0.0 and 1.0. Bug fixes. (#11905) Fix; erroneous FileNotFoundError in glob patterns; (#11921) and; (#11910) Fix file; clobbering during text export with speculative execution.; (#11920) Fix array; out of bounds error when tree aggregating a multiple of 50; partitions.; (#11937) Fixed; correctness bug in scan order for Table.annotate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:47944,error,errors,47944,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability," if one; exists). If either `elem` or `array` is missing, the result is missing. Examples; --------. >>> a = hl.array([0, 2, 4, 8]). >>> hl.eval(hl.binary_search(a, -1)); 0. >>> hl.eval(hl.binary_search(a, 1)); 1. >>> hl.eval(hl.binary_search(a, 10)); 4. """"""; c = coercer_from_dtype(array.dtype.element_type); if not c.can_coerce(elem.dtype):; raise TypeError(; f""'binary_search': cannot search an array of type {array.dtype} for a value of type {elem.dtype}""; ); elem = c.coerce(elem); return hl.switch(elem).when_missing(hl.missing(hl.tint32)).default(_lower_bound(array, elem)). @typecheck(s=expr_str); def _escape_string(s):; return _func(""escapeString"", hl.tstr, s). @typecheck(left=expr_any, right=expr_any, tolerance=expr_float64, absolute=expr_bool); def _values_similar(left, right, tolerance=1e-6, absolute=False):; assert left.dtype == right.dtype; return (is_missing(left) & is_missing(right)) | (; (is_defined(left) & is_defined(right)) & _func(""valuesSimilar"", hl.tbool, left, right, tolerance, absolute); ). @typecheck(coords=expr_array(expr_array(expr_float64)), radius=expr_float64); def _locus_windows_per_contig(coords, radius):; rt = hl.ttuple(hl.tarray(hl.tint32), hl.tarray(hl.tint32)); return _func(""locus_windows_per_contig"", rt, coords, radius). [docs]@typecheck(a=expr_array(), seed=nullable(builtins.int)); def shuffle(a, seed: Optional[builtins.int] = None) -> ArrayExpression:; """"""Randomly permute an array. Example; -------. >>> hl.reset_global_randomness(); >>> hl.eval(hl.shuffle(hl.range(5))); [4, 0, 2, 1, 3]. Parameters; ----------; a : :class:`.ArrayExpression`; Array to permute.; seed : :obj:`int`, optional; Random seed. Returns; -------; :class:`.ArrayExpression`; """"""; return sorted(a, key=lambda _: hl.rand_unif(0.0, 1.0)). [docs]@typecheck(path=builtins.str, point_or_interval=expr_any); def query_table(path, point_or_interval):; """"""Query records from a table corresponding to a given point or range of keys. Notes; -----; This function does not dispatch t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:185128,toler,tolerance,185128,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['toler'],['tolerance']
Availability," in Hardy-Weinberg equilibrium and is further motivated in the paper Patterson, Price and Reich, 2006. (The resulting amplification of signal from the low end of the allele frequency spectrum will also introduce noise for rare variants; common practice is to filter out variants with minor allele frequency below some cutoff.) The factor \(1/m\) gives each sample row approximately unit total variance (assuming linkage equilibrium) so that the diagonal entries of the GRM are approximately 1. Equivalently,. \[G_{ik} = \frac{1}{m} \sum_{j=1}^m \frac{(C_{ij}-2p_j)(C_{kj}-2p_j)}{2 p_j (1-p_j)} \]. Returns:Genetic Relatedness Matrix for all samples. Return type:KinshipMatrix. hardcalls()[source]¶; Drop all genotype fields except the GT field. Important; The genotype_schema() must be of type TGenotype in order to use this method. A hard-called variant dataset is about two orders of magnitude; smaller than a standard sequencing dataset. Use this; method to create a smaller, faster; representation for downstream processing that only; requires the GT field. Returns:Variant dataset with no genotype metadata. Return type:VariantDataset. ibd(maf=None, bounded=True, min=None, max=None)[source]¶; Compute matrix of identity-by-descent estimations. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; To calculate a full IBD matrix, using minor allele frequencies computed; from the variant dataset itself:; >>> vds.ibd(). To calculate an IBD matrix containing only pairs of samples with; PI_HAT in [0.2, 0.9], using minor allele frequencies stored in; va.panel_maf:; >>> vds.ibd(maf='va.panel_maf', min=0.2, max=0.9). Notes; The implementation is based on the IBD algorithm described in the PLINK; paper.; ibd() requires the dataset to be; bi-allelic (otherwise run split_multi() or otherwise run filter_multi()); and does not perform LD pruning. Linkage disequilibrium may bias the; result so consider filtering variants first.; The resulting KeyTable ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:66085,down,downstream,66085,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['down'],['downstream']
Availability," into a single file using the concatenate() function and then; write the concatenated results file to a permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven’t already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we’ll append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesn’t exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:8714,checkpoint,checkpointed,8714,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpointed']
Availability," j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the concatenate() function and then; write the concatenated results file to a permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven’t already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we’ll append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesn’t exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running j",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:8523,checkpoint,checkpoints,8523,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoints']
Availability," key tables. :param str variant_keys: Variant annotation path for the TArray or TSet of keys associated to each variant. :param bool single_key: if true, ``variant_keys`` is interpreted as a single (or missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree); def mendel_errors(self, pedigree):; """"""Find Mendel errors; count per variant, individual and nuclear; family. .. include:: requireTGenotype.rst. **Examples**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are mod",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:154447,error,errors,154447,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability," key/value pairs in the dictionary. key_set; Returns the set of keys in the dictionary. keys; Returns an array with all keys in the dictionary. map_values; Transform values of the dictionary according to a function. size; Returns the size of the dictionary. values; Returns an array with all values in the dictionary. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Get the value associated with key item.; Examples; >>> hl.eval(d['Alice']); 43. Notes; Raises an error if item is not a key of the dictionary. Use; DictExpression.get() to return missing instead of an error. Parameters:; item (Expression) – Key expression. Returns:; Expression – Value associated with key item. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns whether a given key is pres",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.DictExpression.html:2120,error,error,2120,docs/0.2/hail.expr.DictExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html,1,['error'],['error']
Availability," kid_linear_pl). dad_linear_pl = 10 ** (-dad.PL / 10); dad_pp = hl.bind(lambda x: x / hl.sum(x), dad_linear_pl). mom_linear_pl = 10 ** (-mom.PL / 10); mom_pp = hl.bind(lambda x: x / hl.sum(x), mom_linear_pl). kid_ad_ratio = kid.AD[1] / hl.sum(kid.AD); dp_ratio = kid.DP / (dad.DP + mom.DP). def call_auto(kid_pp, dad_pp, mom_pp, kid_ad_ratio):; p_data_given_dn = dad_pp[0] * mom_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (dad_pp[1] * mom_pp[0] + dad_pp[0] * mom_pp[1]) * kid_pp[1] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (dad.DP + mom.DP) < min_dp_ratio) | ~(kid_ad_ratio >= min_child_ab), failure); .when((hl.sum(mom.AD) == 0) | (hl.sum(dad.AD) == 0), failure); .when(; (mom.AD[1] / hl.sum(mom.AD) > max_parent_ab) | (dad.AD[1] / hl.sum(dad.AD) > max_parent_ab), failure; ); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). def call_hemi(kid_pp, parent",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:30519,failure,failure,30519,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['failure'],['failure']
Availability," logit(); floor(); ceil(); sqrt(); sign(); min(); nanmin(); max(); nanmax(); mean(); median(); product(); sum(); cumulative_sum(); argmin(); argmax(); corr(); uniroot(); binary_search(). String functions; format(); json(); parse_json(); hamming(); delimit(); entropy(); parse_int(); parse_int32(); parse_int64(); parse_float(); parse_float32(); parse_float64(). Statistical functions; chi_squared_test(); fisher_exact_test(); contingency_table_test(); cochran_mantel_haenszel_test(); dbeta(); dchisq(); dnorm(); dpois(); hardy_weinberg_test(); binom_test(); pchisqtail(); pgenchisq(); pnorm(); pT(); pF(); ppois(); qchisqtail(); qnorm(); qpois(). Random functions; Setting a seed; Reproducibility across sessions. Genetics functions; locus(); locus_from_global_position(); locus_interval(); parse_locus(); parse_variant(); parse_locus_interval(); variant_str(); call(); unphased_diploid_gt_index_call(); parse_call(); downcode(); triangle(); is_snp(); is_mnp(); is_transition(); is_transversion(); is_insertion(); is_deletion(); is_indel(); is_star(); is_complex(); is_strand_ambiguous(); is_valid_contig(); is_valid_locus(); contig_length(); allele_type(); numeric_allele_type(); pl_dosage(); gp_dosage(); get_sequence(); mendel_error_code(); liftover(); min_rep(); reverse_complement(). Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. null(t); Deprecated in favor of missing(). is_missing(expre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/index.html:2518,down,downcode,2518,docs/0.2/functions/index.html,https://hail.is,https://hail.is/docs/0.2/functions/index.html,1,['down'],['downcode']
Availability," mean-imputed over these columns.; As in the example, the intercept covariate 1 must be included; explicitly if desired. Notes; This method provides a scalable implementation of the score-based; variance-component test originally described in; Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test.; Row weights must be non-negative. Rows with missing weights are ignored. In; the R package skat—which assumes rows are variants—default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field AF, one can use the expression:; >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response y must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively.; The resulting Table provides the group’s key (id), thenumber of; rows in the group (size), the variance component score q_stat, the SKAT; p-value, and a fault flag. For the toy example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC; 3; 4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:80310,fault,fault,80310,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['fault'],['fault']
Availability," most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; split_multi_hts(), which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See also; split_multi_hts(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If T",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:85405,down,downcoding,85405,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['down'],['downcoding']
Availability, must be a single entry-indexed field; (not a list of fields).; The phenotype field that keys the table returned by; ld_score_regression() will have values corresponding to the; column keys of the input matrix table. This function returns a Table with one row per set of summary; statistics passed to the chi_sq_exprs argument. The following; row-indexed fields are included in the table:. phenotype (tstr) – The name of the phenotype. The; returned table is keyed by this field. See the notes below for; details on the possible values of this field.; mean_chi_sq (tfloat64) – The mean chi-squared; test statistic for the given phenotype.; intercept (Struct) – Contains fields:. estimate (tfloat64) – A point estimate of the; intercept \(1 + Na\).; standard_error (tfloat64) – An estimate of; the standard error of this point estimate. snp_heritability (Struct) – Contains fields:. estimate (tfloat64) – A point estimate of the; SNP-heritability \(h_g^2\).; standard_error (tfloat64) – An estimate of; the standard error of this point estimate. Warning; ld_score_regression() considers only the rows for which both row; fields weight_expr and ld_score_expr are defined. Rows with missing; values in either field are removed prior to fitting the LD score; regression model. Parameters:. weight_expr (Float64Expression) – Row-indexed expression for the LD scores used to derive; variant weights in the model.; ld_score_expr (Float64Expression) – Row-indexed expression for the LD scores used as covariates; in the model.; chi_sq_exprs (Float64Expression or list of) – Float64Expression; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions for chi-squared; statistics resulting from genome-wide association; studies (GWAS).; n_samples_exprs (NumericExpression or list of) – NumericExpression; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions indicating the number of; samples used in the studies that generated the test; statistics suppli,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:13490,error,error,13490,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['error'],['error']
Availability," ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; BlockMatrix.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; BlockMatrix – Persisted block matrix. classmethod random(n_rows, n_cols, block_size=None, seed=None, gaussian=True)[source]; Creates a block matrix with standard normal or uniform random entries.; Examples; Create a block matrix with 10 rows, 20 columns, and standard normal entries:; >>> bm = BlockMatrix.random(10, 20). Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:25496,redundant,redundant,25496,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['redundant'],['redundant']
Availability," no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:38592,toler,tolerance,38592,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['toler'],['tolerance']
Availability," now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0.2.69. Added the option to specify either remote_tmpdir or bucket when using the ServiceBackend. Version 0.2.68. Fixed copying a directory from GCS when using the LocalBackend; Fixed writing files to GCS when the bucket name starts with a “g” or an “s”; Fixed the error “Argument list too long” when using the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:4731,error,errors,4731,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['error'],['errors']
Availability," obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). results = []. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint_batching.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=Fals",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:15287,checkpoint,checkpoint,15287,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability," of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; dr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:12067,error,error,12067,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['error'],['error']
Availability," of allele indices.; phased (bool) – If True, preserve the order of alleles. Returns:; CallExpression. hail.expr.functions.unphased_diploid_gt_index_call(gt_index)[source]; Construct an unphased, diploid call from a genotype index.; Examples; >>> hl.eval(hl.unphased_diploid_gt_index_call(4)); Call(alleles=[1, 2], phased=False). Parameters:; gt_index (int or Expression of type tint32) – Unphased, diploid genotype index. Returns:; CallExpression. hail.expr.functions.parse_call(s)[source]; Construct a call expression by parsing a string or string expression.; Examples; >>> hl.eval(hl.parse_call('0|2')); Call(alleles=[0, 2], phased=True). Notes; This method expects strings in the following format:. ploidy; Phased; Unphased. 0; |-; -. 1; |i; i. 2; i|j; i/j. 3; i|j|k; i/j/k. N; i|j|k|...|N; i/j/k/.../N. Parameters:; s (str or StringExpression) – String to parse. Returns:; CallExpression. hail.expr.functions.downcode(c, i)[source]; Create a new call by setting all alleles other than i to ref; Examples; Preserve the third allele and downcode all other alleles to reference.; >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters:. c (CallExpression) – A call.; i (Expression of type tint32) – The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns:; CallExpression. hail.expr.functions.triangle(n)[source]; Returns the triangle number of n.; Examples; >>> hl.eval(hl.triangle(3)); 6. Notes; The calculation is n * (n + 1) / 2. Parameters:; n (Expression of type tint32). Returns:; Expression of type tint32. hail.expr.functions.is_snp(ref, alt)[source]; Returns True if the alleles constitute a single nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters:. ref (StringExpression) – Refe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:11351,down,downcode,11351,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,2,['down'],['downcode']
Availability," of genotype and age:. >>> ds_ann = ds.annotate_rows(linreg =; ... hl.agg.linreg(ds.pheno.blood_pressure,; ... [1,; ... ds.GT.n_alt_alleles(),; ... ds.pheno.age,; ... ds.GT.n_alt_alleles() * ds.pheno.age])). Warning; -------; As in the example, the intercept covariate ``1`` must be included; **explicitly** if desired. Notes; -----; In relation to; `lm.summary <https://stat.ethz.ch/R-manual/R-devel/library/stats/html/summary.lm.html>`__; in R, ``linreg(y, x = [1, mt.x1, mt.x2])`` computes; ``summary(lm(y ~ x1 + x2))`` and; ``linreg(y, x = [mt.x1, mt.x2], nested_dim=0)`` computes; ``summary(lm(y ~ x1 + x2 - 1))``. More generally, `nested_dim` defines the number of effects to fit in the; nested (null) model, with the effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; - `beta` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated regression coefficient for each covariate.; - `standard_error` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated standard error for each covariate.; - `t_stat` (:class:`.tarray` of :py:data:`.tfloat64`):; t-statistic for each covariate.; - `p_value` (:class:`.tarray` of :py:data:`.tfloat64`):; p-value for each covariate.; - `multiple_standard_error` (:py:data:`.tfloat64`):; Estimated standard deviation of the random error.; - `multiple_r_squared` (:py:data:`.tfloat64`):; Coefficient of determination for nested models.; - `adjusted_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; numb",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:51671,error,error,51671,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['error'],['error']
Availability," of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.row_key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_matrix_table(tmp2).add_row_index(uid).key_rows_by(uid); ht.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions).drop(uid); else:; # checkpoint rather than write t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:108005,resilien,resilient-distributed-datasets-rdds,108005,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['resilien'],['resilient-distributed-datasets-rdds']
Availability," or VariantDataset) – Dataset.; min_af – Minimum reference allele frequency to filter variants.; max_af – Maximum reference allele frequency to filter variants.; min_dp – Minimum sequencing depth to filter variants.; max_dp – Maximum sequencing depth to filter variants.; min_gq – Minimum genotype quality to filter variants; ref_AF – Reference AF expression. Necessary when the sample size is below 10,000. Returns:; Table. hail.methods.mendel_errors(call, pedigree)[source]; Find Mendel errors; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, ke",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:48286,error,errors,48286,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,4,['error'],['errors']
Availability," or ``'aws'``. Returns; -------; :class:`.DatasetVersion` if available on cloud platform, else ``None``.; """"""; assert 'url' in doc, doc; assert 'version' in doc, doc; assert 'reference_genome' in doc, doc; if cloud in doc['url']:; return DatasetVersion(doc['url'][cloud], doc['version'], doc['reference_genome']); else:; return None. @staticmethod; def get_region(name: str, versions: List['DatasetVersion'], region: str) -> List['DatasetVersion']:; """"""Get versions of a :class:`.Dataset` in the specified region, if they; exist. Parameters; ----------; name : :obj:`str`; Name of dataset.; versions : :class:`list` of :class:`.DatasetVersion`; List of DatasetVersion objects where the value for :attr:`.url`; is a :obj:`dict` containing key: value pairs, like ``region: url``.; region : :obj:`str`; Region from which to access data, available regions given in; :attr:`hail.experimental.DB._valid_regions`. Returns; -------; available_versions : :class:`list` of :class:`.DatasetVersion`; List of available versions of a class:`.Dataset` for region.; """"""; available_versions = []; for version in versions:; if version.in_region(name, region):; version.url = version.url[region]; available_versions.append(version); return available_versions. def __init__(self, url: Union[dict, str], version: Optional[str], reference_genome: Optional[str]):; self.url = url; self.version = version; self.reference_genome = reference_genome. def in_region(self, name: str, region: str) -> bool:; """"""Check if a :class:`.DatasetVersion` object is accessible in the; desired region. Parameters; ----------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset exists in the specified region.; """"""; current_version = self.version; available_regions = [k for k in self.url.keys()]; valid_region = region in available_regions; if not valid",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:2976,avail,available,2976,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['avail'],['available']
Availability," p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone. - s2 : :obj:`.tfloat64`, the variance of the residuals, :math:`\sigma^2` in the paper. - null_fit:. - b : :obj:`.tndarray` vector of coefficients. - score : :obj:`.tndarray` vector of score statistics. - fisher : :obj:`.tndarray` matrix of fisher statistics. - mu : :obj:`.tndarray` the expected value under the null model. - n_iterations : :obj:`.tint32` the number of iterations before termination. - log_lkhd : :obj:`.tfloat64` the log-likelihood of the final iteration. - converged : :obj:`.tbool` True if the null model converged. - exploded : :obj:`.tbool` True if the null model failed to converge due to numerical; explosion. """"""; mt = matrix_ta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:96402,fault,fault,96402,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability," partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:22243,error,errors,22243,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability," path, max_count=100):; """"""Grep big files, like, really fast. **Examples**. Print all lines containing the string ``hello`` in *file.txt*:. >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in *file1.txt* and *file2.txt*:. >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). **Background**. :py:meth:`~hail.HailContext.grep` mimics the basic functionality of Unix ``grep`` in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regular expressions at `RegExr <http://regexr.com/>`__. :param str regex: The regular expression to match. :param path: The files to search.; :type path: str or list of str. :param int max_count: The maximum number of matches to return.; """""". self._jhc.grep(regex, jindexed_seq_args(path), max_count). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; tolerance=numeric,; sample_file=nullable(strlike),; min_partitions=nullable(integral)); def import_bgen(self, path, tolerance=0.2, sample_file=None, min_partitions=None):; """"""Import .bgen file(s) as variant dataset. **Examples**. Importing a BGEN file as a VDS (assuming it has already been indexed). >>> vds = hc.import_bgen(""data/example3.bgen"", sample_file=""data/example3.sample""). **Notes**. Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see `here <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only **unphased** and **diploid** genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed. Before importing, ensure that:. - The sample file has the same number of samples as the BGEN file.; - No duplicate sample IDs are present. To load multiple files at the same time, use :ref:`Hadoop Glob Pa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:5410,toler,tolerance,5410,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,2,['toler'],['tolerance']
Availability," permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven’t already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we’ll append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesn’t exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:8778,checkpoint,checkpointed,8778,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpointed']
Availability," probability (``gp``) representation:; The following modifications are made to genotype probabilities in BGEN v1.1 files:. Since genotype probabilities are understood to define a probability distribution, import_bgen() automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the tolerance parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains.; import_bgen() normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. Annotations; import_bgen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .bgen files to import.; tolerance (float) – If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1.; sample_file (str or None) – Sample file.; min_partitions (int or None) – Number of partitions. Returns:Variant dataset imported from .bgen file. Return type:VariantDataset. import_gen(path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None)[source]¶; Import .gen file(s) as variant dataset.; Examples; Read a .gen file and a .sample file and write to a .vds file:; >>> (hc.import_gen('data/example.gen', sample_file='data/example.sample'); ... .write('output/gen_example1.vds')). Load multiple files at the same time with Hadoop glob patterns:; >>> (hc.import_gen('data/example.chr*.gen', sample_file='data/example.sample'); ... .write('output/gen_example2.vds')). Notes; For more information on the .gen file format, see here.; To ensure that the .gen file(s) and .sample file are correctly prepared for import:. If there are only 5",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:10257,toler,tolerance,10257,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['toler'],['tolerance']
Availability," range [0, 1]. Got {}"".format(min_match)). if isinstance(x.dtype, tlocus):; rg = x.dtype.reference_genome; method_name = ""liftoverLocus""; rtype = tstruct(result=tlocus(dest_reference_genome), is_negative_strand=tbool); else:; rg = x.dtype.point_type.reference_genome; method_name = ""liftoverLocusInterval""; rtype = tstruct(result=tinterval(tlocus(dest_reference_genome)), is_negative_strand=tbool). if not rg.has_liftover(dest_reference_genome.name):; raise TypeError(; """"""Reference genome '{}' does not have liftover to '{}'.; Use 'add_liftover' to load a liftover chain file."""""".format(rg.name, dest_reference_genome.name); ). expr = _func(method_name, rtype, x, to_expr(min_match, tfloat64)); if not include_strand:; expr = expr.result; return expr. [docs]@typecheck(; f=func_spec(1, expr_float64),; min=expr_float64,; max=expr_float64,; max_iter=builtins.int,; epsilon=builtins.float,; tolerance=builtins.float,; ); def uniroot(f: Callable, min, max, *, max_iter=1000, epsilon=2.2204460492503131e-16, tolerance=1.220703e-4):; """"""Finds a root of the function `f` within the interval `[min, max]`. Examples; --------. >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; -----; `f(min)` and `f(max)` must not have the same sign. If no root can be found, the result of this call will be `NA` (missing). :func:`.uniroot` returns an estimate for a root with accuracy; `4 * epsilon * abs(x) + tolerance`. 4*EPSILON*abs(x) + tol. Parameters; ----------; f : function ( (arg) -> :class:`.Float64Expression`); Must return a :class:`.Float64Expression`.; min : :class:`.Float64Expression`; max : :class:`.Float64Expression`; max_iter : `int`; The maximum number of iterations before giving up.; epsilon : `float`; The scaling factor in the accuracy of the root found.; tolerance : `float`; The constant factor in approximate accuracy of the root found. Returns; -------; :class:`.Float64Expression`; The root of the function `f`.; """""". # Based on:; # https://github.com/wch/r-source/blob/e5b21d0397",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:172505,toler,tolerance,172505,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['toler'],['tolerance']
Availability," rearrange these fields if; necessary. The following new fields are generated:. - `old_locus` (``locus``) -- The old locus, before filtering and computing; the minimal representation.; - `old_alleles` (``array<str>``) -- The old alleles, before filtering and; computing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are filtered are missing.; - `new_to_old` (``array<int32>``) -- An array that maps new allele index to; the old allele index. Its length is the same as the modified `alleles`; field. **Downcode algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; :func:`.split_multi_hts`. The downcode algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, comb",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:159403,down,downcode,159403,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['down'],['downcode']
Availability," requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_ag",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:73695,error,error,73695,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability," self.block_size + (; self._last_col_block_width if stop_bcol == self._n_block_cols else self.block_size; ). return self[start_row:stop_row, start_col:stop_col]. @typecheck_method(b=oneof(np.ndarray, block_matrix_type)); def __matmul__(self, b):; """"""Matrix multiplication: a @ b. Parameters; ----------; b: :class:`numpy.ndarray` or :class:`BlockMatrix`. Returns; -------; :class:`.BlockMatrix`; """"""; if isinstance(b, np.ndarray):; b = BlockMatrix(_to_bmir(b, self.block_size)). if self.n_cols != b.n_rows:; raise ValueError(f'incompatible shapes for matrix multiplication: {self.shape} and {b.shape}'). return BlockMatrix(BlockMatrixDot(self._bmir, b._bmir)). [docs] @typecheck_method(b=oneof(np.ndarray, block_matrix_type), splits=int, path_prefix=nullable(str)); def tree_matmul(self, b, *, splits, path_prefix=None):; """"""Matrix multiplication in situations with large inner dimension. This function splits a single matrix multiplication into `split_on_inner` smaller matrix multiplications,; does the smaller multiplications, checkpoints them with names defined by `file_name_prefix`, and adds them; together. This is useful in cases when the multiplication of two large matrices results in a much smaller matrix. Parameters; ----------; b: :class:`numpy.ndarray` or :class:`BlockMatrix`; splits: :obj:`int` (keyword only argument); The number of smaller multiplications to do.; path_prefix: :class:`str` (keyword only argument); The prefix of the path to write the block matrices to. If unspecified, writes to a tmpdir. Returns; -------; :class:`.BlockMatrix`; """"""; if isinstance(b, np.ndarray):; b = BlockMatrix(_to_bmir(b, self.block_size)). if self.n_cols != b.n_rows:; raise ValueError(f'incompatible shapes for matrix multiplication: {self.shape} and {b.shape}'). if path_prefix is None:; path_prefix = new_temp_file(""tree_matmul_tmp""). if splits != 1:; inner_brange_size = int(math.ceil(self._n_block_cols / splits)); split_points = [*list(range(0, self._n_block_cols, inner_brange_size)), ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:49079,checkpoint,checkpoints,49079,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['checkpoint'],['checkpoints']
Availability," separator. If the; entry field name is empty, the separator is omitted.; The table inherits the globals from the matrix table.; Examples; Consider a matrix table with the following schema:; Global fields:; 'batch': str; Column fields:; 's': str; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; Entry fields:; 'GT': call; 'GQ': int32; Column key:; 's': str; Row key:; 'locus': locus<GRCh37>; 'alleles': array<str>. and three sample IDs: A, B and C. Then the result of; make_table():; >>> ht = mt.make_table() . has the original row fields along with 6 additional fields,; one for each sample and entry field:; Global fields:; 'batch': str; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'A.GT': call; 'A.GQ': int32; 'B.GT': call; 'B.GQ': int32; 'C.GT': call; 'C.GQ': int32; Key:; 'locus': locus<GRCh37>; 'alleles': array<str>. n_partitions()[source]; Number of partitions.; Notes; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. Partitions are a; core concept of distributed computation in Spark, see here; for details. Returns:; int – Number of partitions. naive_coalesce(max_partitions)[source]; Naively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> dataset_result = dataset.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; MatrixTable – Matrix table with at most max_partitions partitions. persist(storage_level='MEMORY_AND_D",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:48612,avail,available,48612,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['avail'],['available']
Availability," side is smaller than or equal to the right side. __lt__(other); Less-than comparison.; Examples; >>> hl.eval(x < 5); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is smaller than the right side. __mod__(other); Compute the left modulo the right number.; Examples; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other)[source]; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 4",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Int32Expression.html:3408,error,error,3408,docs/0.2/hail.expr.Int32Expression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html,1,['error'],['error']
Availability," slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr.write('Done!\n'). Downloading data (~50M) from Google Storage...; Download finished!; Extracting...; Done!. We will read a dataset from disk, and print some summary statistics; about it to re-familiarize ourselves. In [4]:. vds = hc.read('data/1kg.vds'); vds.summa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:1832,down,download,1832,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,2,['down'],"['download', 'downloads']"
Availability," something like this:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db([; 'va.cadd'; ]); ). pprint(vds.variant_schema). This code would return the following schema:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; cadd: Struct{; RawScore: Double,; PHRED: Double; }; }. Database Query¶; Select annotations by clicking on the checkboxes in the documentation, and the appropriate Hail command will be generated; in the panel below.; Use the “Copy to clipboard” button to copy the generated Hail code, and paste the command into your; own Hail script. Database Query. Copy to clipboard. vds = ( hc .read('my.vds') .split_multi(); .annotate_variants_db([ ... ]); ). Documentation¶; These annotations have been collected from a variety of publications and their accompanying datasets (usually text files). Links to; the relevant publications and raw data downloads are included where applicable. Important Notes¶. Multiallelic variants¶; Annotations in the database are keyed by biallelic variants. For some annotations, this means Hail’s split_multi() method; has been used to split multiallelic variants into biallelics. Warning; It is recommended to run split_multi() on your VDS before using annotate_variants_db(). You can use; annotate_variants_db() without first splitting multiallelic variants, but any multiallelics in your VDS will not be annotated.; If you first split these variants, the resulting biallelic variants may then be annotated by the database. VEP annotations¶; VEP annotations are included in this database under the root va.vep. To add VEP annotations, the annotate_variants_db(); method runs Hail’s vep() method on your VDS. This means that your cluster must be properly initialized as described in the; Running VEP section in this discussion post. Warning; If you want to add VEP annotations to your VDS, make sure to add ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/annotationdb.html:2135,down,downloads,2135,docs/0.1/annotationdb.html,https://hail.is,https://hail.is/docs/0.1/annotationdb.html,1,['down'],['downloads']
Availability," source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a CaseBuilder. Parameters:. condition (BooleanExpression); then (Expression). Returns:; CaseBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html:1769,error,error,1769,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html,2,['error'],['error']
Availability," splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See also; split_multi_hts(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table. hail.methods.split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False)[source]; Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema.; struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; MatrixTable.annotate_entries().; Examples; >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; This method assumes ds contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:86322,error,error,86322,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['error']
Availability," term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probability p = Prob(\(Z < x\)) with \(Z\); a normal random variable. Defaults to a standard normal random variable. Parameters:. x (float or Expression of type tfloat64); mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:20866,fault,fault,20866,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['fault'],['fault']
Availability," than or equal to the right side. __lt__(other)[source]; Less-than comparison.; Examples; >>> hl.eval(x < 5); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is smaller than the right side. __mod__(other)[source]; Compute the left modulo the right number.; Examples; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other)[source]; Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None)[source]; Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other)[source]; Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other)[source]; Divide two numbers.; Examples; >>> hl.eval(x / ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NumericExpression.html:3521,error,error,3521,docs/0.2/hail.expr.NumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html,1,['error'],['error']
Availability," that a chromosome field will be of type tstr. Setting; impute=True and types={'Chromosome': hl.tstr} solves this problem. Parameters:. paths (str or list of str) – Files to import.; key (str or list of str) – Key fields(s).; min_partitions (int or None) – Minimum number of partitions.; no_header (bool) – If True`, assume the file has no header and name the N fields f0,; f1, … fN (0-indexed).; impute (bool) – If True, Impute field types from the file.; comment (str or list of str) – Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; delimiter (str) – Field delimiter regex.; missing (str or list [str]) – Identifier(s) to be treated as missing.; types (dict mapping str to HailType) – Dictionary defining field types.; quote (str or None) – Quote character.; skip_blank_lines (bool) – If True, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter (str, optional) – Line filter regex. A partial match results in the line being removed; from the file. Applies before find_replace, if both are defined.; find_replace ((str, str)) – Line substitution regex. Functions like re.sub, but obeys the exact; semantics of Java’s; String.replaceAll.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field (str, optional) – If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns. Re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:35717,error,error,35717,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['error'],['error']
Availability," the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ when using the; LocalBackend; Fixed executing a PythonJob when using the LocalBackend. Version 0.2.65. Added PythonJob; Added new Job.memory inputs lowmem, standard, and highmem corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; Job.storage is now interpreted as the desired extra storage mounted at /io in addition to the default root filesystem / when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core jobs and 2.5Gi for 0.5 core jobs.; Changed how we bill for storage when using the ServiceBackend by decoupling storage requests from CPU and memory requests.; Added new worker types when using the ServiceBackend and automatically select the cheapest worker type based on a job’s CPU and memory requests. Version 0.2.58. Added concatenate and plink_merge functions that use tree aggregation when merging.; BatchPoolExecutor now raises an informative error message for a variety of “system” errors, such as missing container images. Version 0.2.56. Fix LocalBackend.run() succeeding when intermediate command fails. Version 0.2.55. Attempts are now sorted by attempt time in the Batch Service UI. Version 0.2.53. Implement and document BatchPoolExecutor. Version 0.2.50. Add requester_pays_project as a new parameter on batches. Version 0.2.43. Add support for a user-specified, at-most-once HTTP POST callback when a Batch completes. Version 0.2.42. Fixed the documentation for job memory and storage requests to have default units in bytes. Previous. © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:6728,error,error,6728,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,2,['error'],"['error', 'errors']"
Availability," the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\\beta_0 + \\beta_1 \, \mathrm{gt} + \\beta_2 \, \mathrm{age} + \\beta_3 \, \mathrm{isFemale} + \\varepsilon), \quad \\varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid; function <https://en.wikipedia.org/wiki/Sigmoid_function>`__, the; genotype :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; :math:`\mathrm{isFemale}` is coded as 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitt",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:141737,error,error,141737,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability," the allele being tested. The following symbols are in scope for ``annotation``:. - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; - ``aIndices`` (*Array[Int]*): the array of old indices (such that ``aIndices[newIndex] = oldIndex`` and ``aIndices[0] = 0``). :param str expr: Boolean filter expression involving v (variant), va (variant annotations), ; and aIndex (allele index). :param str annotation: Annotation modifying expression involving v (new variant), va (old variant annotations),; and aIndices (maps from new to old indices). :param bool subset: If true, subsets PL and AD, otherwise downcodes the PL and AD.; Genotype and GQ are set based on the resulting PLs. :param bool keep: If true, keep variants matching expr. :param bool filter_altered_genotypes: If true, genotypes that contain filtered-out alleles are set to missing. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :param bool keepStar: If true, keep variants where the only allele left is a ``*`` allele. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.filterAlleles(expr, annotation, filter_altered_genotypes, keep, subset, max_shift,; keep_star); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(expr=strlike,; keep=bool); def filter_genotypes(self, expr, keep=True):; """"""Filter genotypes based on expression. **Examples**. Filter genotypes by allele balance dependent on genotype call:. >>> vds_result = vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ' +; ... '((g.isHomRef() && ab <= 0.1) || ' +; ... '(g.isHet() && ab >= 0.25 && ab <= 0.75) || ' +; ... '(g.isHomVar() && ab >= 0.9))'). **Notes**. ``expr`` is in genotype context so the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``v`` (*Variant*): :ref:`variant`; - ``sa``: sample annotations; - ``v",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:65553,error,error,65553,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability," the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:2063,down,downloaded,2063,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['down'],['downloaded']
Availability," the bottom left quadrant. In [34]:. plt.scatter(df[""sa.qc.dpMean""], df[""sa.qc.callRate""],; alpha=0.1); plt.xlabel('Mean DP'); plt.ylabel('Call Rate'); plt.xlim(0, 20); plt.axhline(0.97, c='k'); plt.axvline(4, c='k'); plt.show(). It’s easy to filter when we’ve got the cutoff values decided:. In [35]:. vds = vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); print('After filter, %d/1000 samples remain.' % vds.num_samples). After filter, 843/1000 samples remain. Next is genotype QC. To start, we’ll print the post-sample-QC call rate.; It’s actually gone up since the initial summary - dropping low-quality; samples disproportionally removed missing genotypes. In [36]:. call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)'); print('pre QC call rate is %.3f' % call_rate). pre QC call rate is 0.991. It’s a good idea to filter out genotypes where the reads aren’t where; they should be: if we find a genotype called homozygous reference with; >10% alternate reads, a genotype called homozygous alternate with >10%; reference reads, or a genotype called heterozygote without a ref / alt; balance near 1:1, it is likely to be an error. In [37]:. filter_condition_ab = '''let ab = g.ad[1] / g.ad.sum() in; ((g.isHomRef && ab <= 0.1) ||; (g.isHet && ab >= 0.25 && ab <= 0.75) ||; (g.isHomVar && ab >= 0.9))'''; vds = vds.filter_genotypes(filter_condition_ab). In [38]:. post_qc_call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)'); print('post QC call rate is %.3f' % post_qc_call_rate). post QC call rate is 0.955. Variant QC is a bit more of the same: we can use the; variant_qc; method to produce a variety of useful statistics, plot them, and filter. In [39]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:17574,error,error,17574,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['error'],['error']
Availability," the entry modification methods: MatrixTable.annotate_entries(),; MatrixTable.select_entries(), MatrixTable.transmute_entries().; The resulting dataset will be keyed by the split locus and alleles.; split_multi() adds the following fields:. was_split (bool) – True if this variant was originally; multiallelic, otherwise False.; a_index (int) – The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with a_index = 1 and 1:100:A:C; with a_index = 2.; old_locus (locus) – The original, unsplit locus.; old_alleles (array<str>) – The original, unsplit alleles. All other fields are left unchanged. Warning; This method assumes ds contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; split_multi_hts(), which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).ma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:84547,error,errors,84547,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability," the filtered alleles are not real so we should discard; any probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms 25,5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See pca() for more details.; Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; \(ij\) entry of the GRM \(MM^T\) is simply the dot product of rows; \(i\) and \(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:28423,down,downcode,28423,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['down'],['downcode']
Availability," the result twice. See :meth:`write_from_entry_expr` for; further documentation. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use :meth:`write_from_entry_expr` to write to external storage. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`.; """"""; path = new_temp_file(); cls.write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=mean_impute,; center=center,; normalize=normalize,; axis=axis,; block_size=block_size,; ); return cls.read(path). [docs] @classmethod; @typecheck_method(n_rows=int, n_cols=int, block_size=nullable(int), seed=nullable(int), gaussian=bool); def random(cls, n_rows, n_cols, block_size=None, seed=None, gaussian=True) -> 'BlockMatrix':; """"""Creates a block matrix with standard normal or uniform random entries. Examples; --------; Create a block matrix with",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:15084,error,error,15084,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['error'],['error']
Availability," the tsv_result to the results list. However,; tsv_result is a Python object. We use the PythonResult.as_str() method to convert the; Python object to a text file containing the str() output of the Python object.; for window in local_df_y.index.to_list():; j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the concatenate() function and then; write the concatenated results file to a permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven’t already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we’ll append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesn’t exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:8120,checkpoint,checkpoint,8120,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability," to filtered alleles); and then sets GT to the genotype with the minimum PL. Note; that if the genotype changes (as in the example), the PLs; are re-normalized (shifted) so that the most likely genotype has a PL of; 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard any; probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g., filtering alleles 1 and 2 transforms ``25,5,10,20`` to ``25,20``.; - DP: No change.; - PL: The filtered alleles' columns are eliminated and the remaining columns shifted so the minimum value is 0.; - GQ: The second-lowest PL (after shifting). **Downcode algorithm**. The downcode algorithm (``subset=False``) recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where downcodeing filtered alleles merges distinct genotypes, the minimum PL is used (since PL is on a log scale, this roughly corresponds to adding probabilities). The PLs; are then re-normalized (shifted) so that the most likely genotype has a PL of 0, and GT is set to this genotype.; If an allele is filtered, this algorithm acts similarly to :py:meth:`~hail.VariantDataset.split_multi`. The downcoding algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: The filtered alleles' columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:63160,down,downcode,63160,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['down'],['downcode']
Availability," to modify the result of the inner recursive call to triangle1(8) by; adding 9 to the result.; The second function is tail recursive: the result of triangle2(9, 0) is; the same as the result of the inner recursive call, triangle2(8, 9).; Example; To find the sum of all the numbers from n=1…10:; >>> triangle_f = lambda f, x, total: hl.if_else(x == 0, total, f(x - 1, total + x)); >>> x = hl.experimental.loop(triangle_f, hl.tint32, 10, 0); >>> hl.eval(x); 55; Let’s say we want to find the root of a polynomial equation:; >>> def polynomial(x):; … return 5 * x**3 - 2 * x - 1; We’ll use Newton’s method<https://en.wikipedia.org/wiki/Newton%27s_method>; to find it, so we’ll also define the derivative:; >>> def derivative(x):; ... return 15 * x**2 - 2. and starting at \(x_0 = 0\), we’ll compute the next step \(x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}\); until the difference between \(x_{i}\) and \(x_{i+1}\) falls below; our convergence threshold:; >>> threshold = 0.005; >>> def find_root(f, guess, error):; ... converged = hl.is_defined(error) & (error < threshold); ... new_guess = guess - (polynomial(guess) / derivative(guess)); ... new_error = hl.abs(new_guess - guess); ... return hl.if_else(converged, guess, f(new_guess, new_error)); >>> x = hl.experimental.loop(find_root, hl.tfloat, 0.0, hl.missing(hl.tfloat)); >>> hl.eval(x); 0.8052291984599675. Warning; Using arguments of a type other than numeric types and booleans can cause; memory issues if if you expect the recursive call to happen many times. Parameters:. f (function ( (marker, *args) -> Expression) – Function of one callable marker, denoting where the recursive call (or calls) is located,; and many args, the loop variables.; typ (str or HailType) – Type the loop returns.; args (variable-length args of Expression) – Expressions to initialize the loop values. Returns:; Expression – Result of the loop with args as initial loop values. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:42577,error,error,42577,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,2,['error'],['error']
Availability," tolerance`. 4*EPSILON*abs(x) + tol. Parameters; ----------; f : function ( (arg) -> :class:`.Float64Expression`); Must return a :class:`.Float64Expression`.; min : :class:`.Float64Expression`; max : :class:`.Float64Expression`; max_iter : `int`; The maximum number of iterations before giving up.; epsilon : `float`; The scaling factor in the accuracy of the root found.; tolerance : `float`; The constant factor in approximate accuracy of the root found. Returns; -------; :class:`.Float64Expression`; The root of the function `f`.; """""". # Based on:; # https://github.com/wch/r-source/blob/e5b21d0397c607883ff25cca379687b86933d730/src/library/stats/src/zeroin.c. def error_if_missing(x):; res = f(x); return case().when(is_defined(res), res).or_error(format(""'uniroot': value of f(x) is missing for x = %.1e"", x)). wrapped_f = hl.experimental.define_function(error_if_missing, 'float'). def uniroot(recur, a, b, c, fa, fb, fc, prev, iterations_remaining):; tol = 2 * epsilon * abs(b) + tolerance / 2; cb = c - b; t1 = fb / fc; t2 = fb / fa; q1 = fa / fc # = t1 / t2; pq = if_else(; a == c,; (cb * t1) / (t1 - 1.0), # linear; -t2 * (cb * q1 * (q1 - t1) - (b - a) * (t1 - 1.0)) / ((q1 - 1.0) * (t1 - 1.0) * (t2 - 1.0)),; ) # quadratic. interpolated = if_else(; (sign(pq) == sign(cb)); & (0.75 * abs(cb) > abs(pq) + tol / 2) # b + pq within [b, c]; & (abs(pq) < abs(prev / 2)), # pq not too large; pq,; cb / 2,; ). new_step = if_else((abs(prev) >= tol) & (abs(fa) > abs(fb)), interpolated, cb / 2) # try interpolation. new_b = b + if_else(new_step < 0, hl.min(new_step, -tol), hl.max(new_step, tol)); new_fb = wrapped_f(new_b). return if_else(; iterations_remaining == 0,; missing('float'),; if_else(; abs(fc) < abs(fb),; recur(b, c, b, fb, fc, fb, prev, iterations_remaining),; if_else(; (abs(cb / 2) <= tol) | (fb == 0),; b, # acceptable approximation found; if_else(; sign(new_fb) == sign(fc), # use c = b for next iteration if signs match; recur(b, new_b, b, fb, new_fb, fb, new_step, iterations_re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:173886,toler,tolerance,173886,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['toler'],['tolerance']
Availability," unexpected; output.; (#9839) Hail now; eagerly deletes temporary files produced by some BlockMatrix; operations.; (#9835) hl.any; and hl.all now also support a single collection argument and a; varargs of Boolean expressions.; (#9816); hl.pc_relate now includes values on the diagonal of kinship,; IBD-0, IBD-1, and IBD-2; (#9736) Let; NDArrayExpression.reshape take varargs instead of mandating a tuple.; (#9766); hl.export_vcf now warns if INFO field names are invalid according; to the VCF 4.3 spec. Bug fixes. (#9976) Fixed; show() representation of Hail dictionaries. Performance improvements. (#9909) Improved; performance of hl.experimental.densify by approximately 35%. Version 0.2.61; Released 2020-12-03. New features. (#9749) Add or_error; method to SwitchBuilder (hl.switch). Bug fixes. (#9775) Fixed race; condition leading to invalid intermediate files in VCF combiner.; (#9751) Fix bug where; constructing an array of empty structs causes type error.; (#9731) Fix error and; incorrect behavior when using hl.import_matrix_table with int64; data types. Version 0.2.60; Released 2020-11-16. New features. (#9696); hl.experimental.export_elasticsearch will now support; Elasticsearch versions 6.8 - 7.x by default. Bug fixes. (#9641) Showing hail; ndarray data now always prints in correct order. hailctl dataproc. (#9610) Support; interval fields in hailctl dataproc describe. Version 0.2.59; Released 2020-10-22. Datasets / Annotation DB. (#9605) The Datasets; API and the Annotation Database now support AWS, and users are; required to specify what cloud platform they’re using. hailctl dataproc. (#9609) Fixed bug; where hailctl dataproc modify did not correctly print; corresponding gcloud command. Version 0.2.58; Released 2020-10-08. New features. (#9524) Hail should; now be buildable using Spark 3.0.; (#9549) Add; ignore_in_sample_frequency flag to hl.de_novo.; (#9501) Configurable; cache size for BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:62895,error,error,62895,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability," v in named_exprs.items()}; select_fields = indices.protected_key[:]; protected_key = set(select_fields); insertions = {}. final_fields = select_fields[:]. def is_top_level_field(e):; return e in indices.source._fields_inverse. for e in exprs:; if not e._ir.is_nested_field:; raise ExpressionException(; f""{caller!r} expects keyword arguments for complex expressions\n""; f"" Correct: ht = ht.select('x')\n""; f"" Correct: ht = ht.select(ht.x)\n""; f"" Correct: ht = ht.select(x = ht.x.replace(' ', '_'))\n""; f"" INCORRECT: ht = ht.select(ht.x.replace(' ', '_'))""; ); analyze(caller, e, indices, broadcast=False). name = e._ir.name; check_keys(caller, name, protected_key); final_fields.append(name); if is_top_level_field(e):; select_fields.append(name); else:; insertions[name] = e; for k, e in named_exprs.items():; check_keys(caller, k, protected_key); final_fields.append(k); insertions[k] = e. check_collisions(caller, final_fields, indices). if final_fields == select_fields + list(insertions):; # don't clog the IR with redundant field names; s = base_struct.select(*select_fields).annotate(**insertions); else:; s = base_struct.select(*select_fields)._annotate_ordered(insertions, final_fields). assert list(s) == final_fields; return s. def check_annotate_exprs(caller, named_exprs, indices, agg_axes):; from hail.expr.expressions import analyze. protected_key = set(indices.protected_key); for k, v in named_exprs.items():; analyze(f'{caller}: field {k!r}', v, indices, agg_axes, broadcast=True); check_keys(caller, k, protected_key); check_collisions(caller, list(named_exprs), indices); return named_exprs. def process_joins(obj, exprs):; all_uids = []; left = obj; used_joins = set(). for e in exprs:; joins = e._ir.search(lambda a: isinstance(a, hail.ir.Join)); for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; if j.idx not in used_joins:; left = j.join_func(left); all_uids.extend(j.temp_vars); used_joins.add(j.idx). def cleanup(table):; remaining_uids = [uid ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:13915,redundant,redundant,13915,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,2,['redundant'],['redundant']
Availability," version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#9161) Fix bugs; calculating row_id in hl.import_matrix_table. Version 0.2.50; Released 2020-07-23. Bug fixes. (#9114) CHANGELOG:; Fixed crash when using repeated calls to hl.filter_intervals. New features. (#9101) Add; hl.nd.{concat, hstack, vstack} to concatenate ndarrays.; (#9105) Add; hl.nd.{eye, identity} to create identity matrix ndarrays.; (#9093) Add; hl.nd.inv to invert ndarrays.; (#9063) Add; BlockMatrix.tree_matmul to improve matrix multiply performance; with a large inner dimension. Version 0.2.49; Released 2020-07-08. Bug fixes. (#9058) Fixed memory; leak affecting Table.aggregate, MatrixTable.annotate_cols; aggregations, and hl.sample_qc. Version 0.2.48; Released 2020-07-07. Bug fixes. (#9029) Fix crash; when using hl.ag",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:67263,error,error,67263,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability," was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_agg; correctly. Performance Improvements. (#8413) Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% over",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:73942,error,error,73942,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,['error'],['error']
Availability," we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we’ve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways with increasing complexity to write; a pipeline that runs ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:10471,checkpoint,checkpoint,10471,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability," window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we’ve only run the jobs in groups of 10 for jobs that have no; existing checkpoi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:10350,checkpoint,checkpoint,10350,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability," with 5 components... In [49]:. pprint(pca.globals). {u'eigen': {u'PC1': 56.34707905481798,; u'PC2': 37.8109003010398,; u'PC3': 16.91974301822238,; u'PC4': 2.707349935634387,; u'PC5': 2.0851252187821174}}. In [50]:. pprint(pca.sample_schema). Struct{; Population: String,; SuperPopulation: String,; isFemale: Boolean,; PurpleHair: Boolean,; CaffeineConsumption: Int,; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; },; pca: Struct{; PC1: Double,; PC2: Double,; PC3: Double,; PC4: Double,; PC5: Double; }; }. Now that we’ve got principal components per sample, we may as well plot; them! Human history exerts a strong effect in genetic datasets. Even; with a 50MB sequencing dataset, we can recover the major human; populations. In [51]:. pca_table = pca.samples_table().to_pandas(); colors = {'AFR': 'green', 'AMR': 'red', 'EAS': 'black', 'EUR': 'blue', 'SAS': 'cyan'}; plt.scatter(pca_table[""sa.pca.PC1""], pca_table[""sa.pca.PC2""],; c = pca_table[""sa.SuperPopulation""].map(colors),; alpha = .5); plt.xlim(-0.6, 0.6); plt.xlabel(""PC1""); plt.ylabel(""PC2""); legend_entries = [mpatches.Patch(color=c, label=pheno) for pheno, c in colors.items()]; plt.legend(handles=legend_entries, loc=2); plt.show(). Now we can rerun our linear regression, controlling for the first few; principal components and sample sex. In [52]:. pvals = (common_vds; .annotate_samples_table(pca.samples_table(), expr='sa.pca = table.pca'); .linreg('sa.CaffeineConsumption', covariates=['sa.pca.PC1', 'sa.pca.PC2', 'sa.pca.PC3', 'sa.isFemale']); .query_variants('variants.map(v => va.linreg.pval).collect()')). 2018-10-18 01:27:07 Hail: INFO: Running linear regression on 843 samples with 5 covariates including inter",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:25605,recover,recover,25605,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['recover'],['recover']
Availability," x); k = len(covariates); if k == 0:; raise ValueError('_logistic_skat: at least one covariate is required.'); _warn_if_no_intercept('_logistic_skat', covariates); mt = mt._select_all(; row_exprs=dict(group=group, weight=weight), col_exprs=dict(y=y, covariates=covariates), entry_exprs=dict(x=x); ); mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])); if mt.y.dtype != hl.tbool:; mt = mt.annotate_cols(; y=(; hl.case(); .when(hl.any(mt.y == 0, mt.y == 1), hl.bool(mt.y)); .or_error(; hl.format(; f'hl._logistic_skat: phenotypes must either be True, False, 0, or 1, found: %s of type {mt.y.dtype}',; mt.y,; ); ); ); ); yvec, covmat, n = mt.aggregate_cols(; (hl.agg.collect(hl.float(mt.y)), hl.agg.collect(mt.covariates.map(hl.float)), hl.agg.count()), _localize=False; ); mt = mt.annotate_globals(yvec=hl.nd.array(yvec), covmat=hl.nd.array(covmat), n_complete_samples=n); null_fit = logreg_fit(mt.covmat, mt.yvec, None, max_iterations=null_max_iterations, tolerance=null_tolerance); mt = mt.annotate_globals(; null_fit=hl.case(); .when(null_fit.converged, null_fit); .or_error(hl.format('hl._logistic_skat: null model did not converge: %s', null_fit)); ); null_mu = mt.null_fit.mu; y_residual = mt.yvec - null_mu; mt = mt.annotate_globals(y_residual=y_residual, s2=null_mu * (1 - null_mu)); mt = mt.annotate_rows(G_row_mean=hl.agg.mean(mt.x)); mt = mt.annotate_rows(G_row=hl.agg.collect(hl.coalesce(mt.x, mt.G_row_mean))); ht = mt.rows(); ht = ht.filter(hl.all(hl.is_defined(ht.group), hl.is_defined(ht.weight))); ht = ht.group_by('group').aggregate(; weight_take=hl.agg.take(ht.weight, n=max_size + 1),; G_take=hl.agg.take(ht.G_row, n=max_size + 1),; size=hl.agg.count(),; ); ht = ht.annotate(; weight=hl.nd.array(hl.or_missing(hl.len(ht.weight_take) <= max_size, ht.weight_take)),; G=hl.nd.array(hl.or_missing(hl.len(ht.G_take) <= max_size, ht.G_take)).T,; ); ht = ht.annotate(; # Q=ht.y_residual @ (ht.G * ht.weight) @ ht.G.T @ ht.y_residual.T; Q=",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:98435,toler,tolerance,98435,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability," {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; This method can be called more than once. It’s behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command.; To declare a resource file of type JobResourceFile, use either; the get attribute syntax of job.{identifier} or the get item syntax of; job[‘identifier’]. If an object for that identifier doesn’t exist,; then one will be created automatically (only allowed in the; command() method). The identifier name can be any valid Python; identifier such as ofile5000.; All JobResourceFile are temporary files and must be written to; a permanent location using Batch.write_output() if the output; needs to be saved.; Only resources can be referred to in commands. Referencing a; batch.Batch or Job will result in an error. Parameters:; command (str) – A bash command. Return type:; BashJob. Returns:; Same job object with command appended. declare_resource_group(**mappings); Declare a resource group for a job.; Examples; Declare a resource group:; >>> b = Batch(); >>> input = b.read_input_group(bed='data/example.bed',; ... bim='data/example.bim',; ... fam='data/example.fam'); >>> j = b.new_job(); >>> j.declare_resource_group(tmp1={'bed': '{root}.bed',; ... 'bim': '{root}.bim',; ... 'fam': '{root}.fam',; ... 'log': '{root}.log'}); >>> j.command(f'plink --bfile {input} --make-bed --out {j.tmp1}'); >>> b.run() . Warning; Be careful when specifying the expressions for each file as this is Python; code that is executed with eval!. Parameters:; mappings (Dict[str, Any]) – Keywords (in the above example tmp1) are the name(s) of the; resource group(s). File names may contain arbitrary Python; expressions, which will be evaluated by Python eval. To use the; keyword as the file name, use {root} (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:2941,error,error,2941,docs/batch/api/batch/hailtop.batch.job.BashJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html,2,['error'],['error']
Availability," | 2 | 4.136 | 0.205 | 0 |; +-------+------+--------+---------+-------+; | geneB | 1 | 5.659 | 0.195 | 0 |; +-------+------+--------+---------+-------+; | geneC | 3 | 4.122 | 0.192 | 0 |; +-------+------+--------+---------+-------+. Groups larger than `max_size` appear with missing `q_stat`, `p_value`, and; `fault`. The hard limit on the number of rows in a group is 46340. Note that the variance component score `q_stat` agrees with ``Q`` in the R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of :math:`\chi^2(1)` distributions. +-------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +------+------+-----------------------------------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:105298,fault,fault,105298,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability," | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:78076,fault,fault,78076,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability," | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""; if mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl.is_defined(mt.PL),; hl.bind(; lambda unnorm: unnorm - hl.min(unnorm),; hl.range(0, hl.triangle(mt.alleles.length())).map(; lambda newi: hl.bind(; lambda newc: mt.PL[; hl.call(mt.new_to_old[newc[0]], mt.new_to_old[newc[1]]).unphased_diploid_gt_index(); ],; hl.unphased_diploid_gt_index_call(newi),; ); ),; ),; hl.missing(tarray(tint32)),; ); return mt.annotate_entries(; GT=hl.unphased_diploid_gt_index_call(hl.argmin(newPL, unique=True)),; AD=hl.if_else(; hl.is_defined(mt.AD),; h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:162495,down,downcode,162495,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['down'],['downcode']
Availability," – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:125650,error,error,125650,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['error'],['error']
Availability," “hello world”. All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:3449,echo,echo,3449,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability,""" |; +-------+----------+. Using `key` as the sole index expression is equivalent to passing all; key fields individually:. >>> table_result = table1.select(B = table2.index(table1.key).B). It is also possible to use non-key fields or expressions as the index; expressions:. >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; -----; :meth:`.Table.index` is used to expose one table's fields for use in; expressions involving the another table or matrix table's fields. The; result of the method call is a struct expression that is usable in the; same scope as `exprs`, just as if `exprs` were used to look up values of; the table in a dictionary. The type of the struct expression is the same as the indexed table's; :meth:`.row_value` (the key fields are removed, as they are available; in the form of the index expressions). Note; ----; There is a shorthand syntax for :meth:`.Table.index` using square; brackets (the Python ``__getitem__`` syntax). This syntax is preferred. >>> table_result = table1.select(B = table2[table1.ID].B). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Returns; -------; :class:`.Expression`; """"""; try:; return self._index(*exprs, all_matches=all_matches); except TableIndexKeyError as err:; raise ExpressionException(; f""Key type mismatch: cannot index table with given expressions:\n""; f"" Table key: {', '.join(str(t) for t in err.key_type.values()) or '<<<empty key>>>'}\n""; f"" Index Expressions: {', '.join(str(e.dtype) for e in err.index_expressions)}""; ). @staticmethod; def _maybe_truncate_for_flexindex(indexer, indexee_dtype):; if not len(indexee_dtype) > 0:; raise ValueError('Must have non-e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:71580,avail,available,71580,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['avail'],['available']
Availability,"""); else:; raise ValueError(""pc_relate_bm: exactly one of 'k' and 'scores_expr' "" ""must be set, found neither""). n_missing = scores_table.aggregate(agg.count_where(hl.is_missing(scores_table.__scores))); if n_missing > 0:; raise ValueError(f'Found {n_missing} columns with missing scores array.'); pc_scores = hl.nd.array(scores_table.collect(_localize=False).map(lambda x: x.__scores)). # Define NaN for missing values, otherwise cannot convert expr to block matrix; nan = hl.float64(float('NaN')). # Create genotype matrix, set missing GT entries to NaN; mt = mt.select_entries(__gt=call_expr.n_alt_alleles()).unfilter_entries(); gt_with_nan_expr = hl.or_else(hl.float64(mt.__gt), nan); if not block_size:; block_size = BlockMatrix.default_block_size(); g = BlockMatrix.from_entry_expr(gt_with_nan_expr, block_size=block_size); g = g.checkpoint(new_temp_file('pc_relate_bm/g', 'bm')); sqrt_n_samples = hl.nd.array([hl.sqrt(g.shape[1])]). # Recover singular values, S0, as vector of column norms of pc_scores if necessary; if compute_S0:; S0 = (pc_scores ** hl.int32(2)).sum(0).map(lambda x: hl.sqrt(x)); else:; S0 = hl.nd.array(eigens).map(lambda x: hl.sqrt(x)); # Set first entry of S to sqrt(n), for intercept term in beta; S = hl.nd.hstack((sqrt_n_samples, S0))._persist(); # Recover V from pc_scores with inv(S0); V0 = (pc_scores * (1 / S0))._persist(); # Set all entries in first column of V to 1/sqrt(n), for intercept term in beta; ones_normalized = hl.nd.full((V0.shape[0], 1), (1 / S[0])); V = hl.nd.hstack((ones_normalized, V0)). # Compute matrix of regression coefficients for PCs (beta), shape (k, m); beta = BlockMatrix.from_ndarray(((1 / S) * V).T, block_size=block_size) @ g.T; beta = beta.checkpoint(new_temp_file('pc_relate_bm/beta', 'bm')). # Compute matrix of individual-specific AF estimates (mu), shape (m, n); mu = 0.5 * (BlockMatrix.from_ndarray(V * S, block_size=block_size) @ beta).T; # Replace entries in mu with NaN if invalid or if corresponding GT is missing (no contrib",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:19170,Recover,Recover,19170,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,1,['Recover'],['Recover']
Availability,"""Download subset of the `Human Genome Diversity Panel; <https://www.internationalgenome.org/data-portal/data-collection/hgdp/>`__; dataset and sample annotations. Notes; -----; The download is about 30MB. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite any existing files/directories at `output_dir`.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). matrix_table_path = os.path.join(output_dir, 'HGDP.mt'); vcf_path = os.path.join(output_dir, 'HGDP.vcf.bgz'); sample_annotations_path = os.path.join(output_dir, 'HGDP_annotations.txt'); gene_annotations_path = os.path.join(output_dir, 'ensembl_gene_annotations.txt'). if (; overwrite; or not _dir_exists(fs, matrix_table_path); or not _file_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, 'HGDP.vcf.bgz'); source = resources['HGDP_matrix_table']; info(f'downloading HGDP VCF ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, resources['HGDP_matrix_table'], tmp_vcf); cluster_readable_vcf = _copy_to_tmp(fs, local_path_uri(tmp_vcf), extension='vcf.bgz'); info('importing VCF and writing to matrix table...'); hl.import_vcf(cluster_readable_vcf, min_partitions=16, reference_genome='GRCh38').write(; matrix_table_path, overwrite=True; ). tmp_sample_annot = os.path.join(tmp_dir, 'HGDP_annotations.txt'); source = resources['HGDP_annotations']; info(f'downloading HGDP annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['HGDP_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:5058,down,downloading,5058,docs/0.2/_modules/hail/utils/tutorial.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html,2,['down'],['downloading']
Availability,"""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_key = vd.row_key.dtype; if (; not isinstance(vd_row_key, hl.tstruct); or len(vd_row_key) != 2; or not vd_row_key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 or vd_col_key.types[0] != hl.tstr:; error(f""expect variant data to have a single col key of type string, found {vd_col_key}""). if 'END' not in rd.entry or rd.END.dtype != hl.tint32:; error(""expect field 'END' in entry of reference data with type int32""). if check_data:; # check cols; ref_cols = rd.col_key.collect(); var_cols = vd.col_key.collect(); if len(ref_cols) != len(var_cols):; error(; f""mismatch in number of columns: reference data has {ref_cols} columns, variant data has {var_cols} columns""; ). if ref_cols != var_cols:; first_mismatch = 0; while ref_cols[first_mismatch] == var_cols[first_mismatch]:; first_mismatch += 1; error(; f""mismatch in columns keys: ref={ref_cols[first_mismatch]}, var={var_cols[first_mismatch]} at position {first_mismatch}""; ). # check locus distinctness; n_rd_rows = rd.count_rows(); n_distinct = rd.distinct_by_row().count_rows(). if n_distinct != n_rd_rows:; error(f'reference data loci are not distinct: found {n_rd_rows} rows, but {n_distinct} distinct loci'). # check END field; end_exprs = dict(; missing_end=hl.agg.filter(hl.is_missing(rd.END), hl.agg.take((rd.row_key, rd.col_key), 5)),; end_before_position=hl.agg.filter(rd.END < rd.locus.position, hl.agg.take((r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:9638,error,error,9638,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['error'],['error']
Availability,"#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:84683,error,error,84683,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"%.3e', 0.09345332)); '9.345e-02'. >>> hl.eval(hl.format('%.4f', hl.missing(hl.tfloat64))); 'null'. >>> hl.eval(hl.format('%s %s %s', 'hello', hl.tuple([3, hl.locus('1', 2453)]), True)); 'hello (3, 1:2453) true'. Notes; -----; See the `Java documentation <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Formatter.html#syntax>`__; for valid format specifiers and arguments. Missing values are printed as ``'null'`` except when using the; format flags `'b'` and `'B'` (printed as ``'false'`` instead). Parameters; ----------; f : :class:`.StringExpression`; Java `format string <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Formatter.html#syntax>`__.; args : variable-length arguments of :class:`.Expression`; Arguments to format. Returns; -------; :class:`.StringExpression`; """""". return _func(""format"", hl.tstr, f, hl.tuple(args)). [docs]@typecheck(x=expr_float64, y=expr_float64, tolerance=expr_float64, absolute=expr_bool, nan_same=expr_bool); def approx_equal(x, y, tolerance=1e-6, absolute=False, nan_same=False):; """"""Tests whether two numbers are approximately equal. Examples; --------; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters; ----------; x : :class:`.NumericExpression`; y : :class:`.NumericExpression`; tolerance : :class:`.NumericExpression`; absolute : :class:`.BooleanExpression`; If True, compute ``abs(x - y) <= tolerance``. Otherwise, compute; ``abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022)``.; nan_same : :class:`.BooleanExpression`; If True, then ``NaN == NaN`` will evaluate to True. Otherwise,; it will return False. Returns; -------; :class:`.BooleanExpression`; """""". return _func(""approxEqual"", hl.tbool, x, y, tolerance, absolute, nan_same). def _shift_op(x, y, op):; assert op in ('<<', '>>', '>>>'); t = x.dtype; if t == hl.tint64:; word_size = 64; zero = hl.int64(0); else:; word_size = 32; zero = hl.int",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:176739,toler,tolerance,176739,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,4,['toler'],['tolerance']
Availability,"', 'Bob', 'Charlie'}). Attributes. dtype; The data type of the expression. Methods. all; Returns True if f returns True for every element. any; Returns True if f returns True for any element. filter; Returns a new collection containing elements where f returns True. find; Returns the first element where f returns True. flatmap; Map each element of the collection to a new collection, and flatten the results. fold; Reduces the collection with the given function f, provided the initial value zero. group_by; Group elements into a dict according to a lambda function. length; Returns the size of a collection. map; Transform each element of a collection. size; Returns the size of a collection. starmap; Transform each element of a collection of tuples. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. all(f)[source]; Returns True if f returns True for every element.; Examples; >>> hl.eval(a.all(lambda x: x < 10)); True. Notes; This method returns True if the collection is empty. Parameters:; f (function ( (arg) -> BooleanEx",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html:1864,error,error,1864,docs/0.2/hail.expr.CollectionExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html,1,['error'],['error']
Availability,"', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linreg('sa.phenotype') ; >>> vds.persist() . The above code does NOT persist vds. Instead, it copies vds and persists that result. ; The proper usage is this:; >>> vds = vds.pca().persist() . Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:VariantDataset. query_genotypes(exprs)[source]¶; Performs aggregation queries over genotypes, and returns Python object(s).; Examples; Compute global GQ histogr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:144367,redundant,redundant,144367,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['redundant'],['redundant']
Availability,"'https://storage.googleapis.com/hail-tutorial/1kg.vcf.bgz',; '1kg_ensembl_gene_annotations': 'https://storage.googleapis.com/hail-tutorial/ensembl_gene_annotations.txt',; 'HGDP_annotations': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_pop_and_sex_annotations.tsv',; 'HGDP_matrix_table': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_subset.vcf.bgz',; 'HGDP_ensembl_gene_annotations': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_gene_annotations.tsv',; 'movie_lens_100k': 'https://files.grouplens.org/datasets/movielens/ml-100k.zip',; }. tmp_dir: str = None. def init_temp_dir():; global tmp_dir; if tmp_dir is None:; tmp_dir = new_local_temp_dir(). def _dir_exists(fs, path):; return fs.exists(path) and fs.is_dir(path). def _file_exists(fs, path):; return fs.exists(path) and fs.is_file(path). def _copy_to_tmp(fs, src, extension=None):; dst = new_temp_file(extension=extension); fs.copy(src, dst); return dst. [docs]def get_1kg(output_dir, overwrite: bool = False):; """"""Download subset of the `1000 Genomes <http://www.internationalgenome.org/>`__; dataset and sample annotations. Notes; -----; The download is about 15M. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite any existing files/directories at `output_dir`.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). matrix_table_path = os.path.join(output_dir, '1kg.mt'); vcf_path = os.path.join(output_dir, '1kg.vcf.bgz'); sample_annotations_path = os.path.join(output_dir, '1kg_annotations.txt'); gene_annotations_path = os.path.join(output_dir, 'ensembl_gene_annotations.txt'). if (; overwrite; or not _dir_exists(fs, matrix_table_path); or not _file_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, '1kg.vcf.bgz'); source = resources['1kg_matrix_table']; info(f'downloading 1KG VCF ...\n' f' Sourc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:1907,Down,Download,1907,docs/0.2/_modules/hail/utils/tutorial.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html,1,['Down'],['Download']
Availability,"'score', or 'firth'. :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree); def mendel_errors(self, pedigree):; """"""Find Mendel errors; count per variant, individual and nuclear; family. .. include:: requireTGenotype.rst. **Examples**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:154800,error,errors,154800,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,"'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> ht = db.annotate_rows_db(ht, 'clinvar_gene_summary', 'CADD', 'DANN') . Notes; If a dataset is gene-keyed, the annotation will be a dictionary mapping; from gene name to the annotation value. There will be one entry for each; gene overlapping the given locus.; If a dataset does not have unique rows for each key (consider the; gencode genes, which may overlap; and clinvar_variant_summary,; which contains many overlapping multiple nucleotide variants), then the; result will be an array of annotation values, one for each row. Parameters:. rel (MatrixTable or Table) – The relational object to which to add annotations.; names (varargs of str",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:2256,avail,available,2256,docs/0.2/experimental/hail.experimental.DB.html,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html,1,['avail'],['available']
Availability,"(9); needs to modify the result of the inner recursive call to triangle1(8) by; adding 9 to the result.; The second function is tail recursive: the result of triangle2(9, 0) is; the same as the result of the inner recursive call, triangle2(8, 9).; Example; To find the sum of all the numbers from n=1…10:; >>> triangle_f = lambda f, x, total: hl.if_else(x == 0, total, f(x - 1, total + x)); >>> x = hl.experimental.loop(triangle_f, hl.tint32, 10, 0); >>> hl.eval(x); 55; Let’s say we want to find the root of a polynomial equation:; >>> def polynomial(x):; … return 5 * x**3 - 2 * x - 1; We’ll use Newton’s method<https://en.wikipedia.org/wiki/Newton%27s_method>; to find it, so we’ll also define the derivative:; >>> def derivative(x):; ... return 15 * x**2 - 2. and starting at \(x_0 = 0\), we’ll compute the next step \(x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}\); until the difference between \(x_{i}\) and \(x_{i+1}\) falls below; our convergence threshold:; >>> threshold = 0.005; >>> def find_root(f, guess, error):; ... converged = hl.is_defined(error) & (error < threshold); ... new_guess = guess - (polynomial(guess) / derivative(guess)); ... new_error = hl.abs(new_guess - guess); ... return hl.if_else(converged, guess, f(new_guess, new_error)); >>> x = hl.experimental.loop(find_root, hl.tfloat, 0.0, hl.missing(hl.tfloat)); >>> hl.eval(x); 0.8052291984599675. Warning; Using arguments of a type other than numeric types and booleans can cause; memory issues if if you expect the recursive call to happen many times. Parameters:. f (function ( (marker, *args) -> Expression) – Function of one callable marker, denoting where the recursive call (or calls) is located,; and many args, the loop variables.; typ (str or HailType) – Type the loop returns.; args (variable-length args of Expression) – Expressions to initialize the loop values. Returns:; Expression – Result of the loop with args as initial loop values. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:42538,error,error,42538,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['error'],['error']
Availability,"(loop_ir, ir.If):; if contains_recursive_call(loop_ir.cond):; raise TypeError(""branch condition can't contain recursive call!""); check_tail_recursive(loop_ir.cnsq); check_tail_recursive(loop_ir.altr); elif isinstance(loop_ir, ir.Let):; if contains_recursive_call(loop_ir.value):; raise TypeError(""bound value used in other expression can't contain recursive call!""); check_tail_recursive(loop_ir.body); elif isinstance(loop_ir, ir.TailLoop):; if any(contains_recursive_call(x) for n, x in loop_ir.params):; raise TypeError(""parameters passed to inner loop can't contain recursive call!""); elif not isinstance(loop_ir, ir.Recur) and contains_recursive_call(loop_ir):; raise TypeError(""found recursive expression outside of tail position!""). @typecheck(recur_exprs=expr_any); def make_loop(*recur_exprs):; if len(recur_exprs) != len(args):; raise TypeError('Recursive call in loop has wrong number of arguments'); err = None; for i, (rexpr, expr) in enumerate(zip(recur_exprs, args)):; if rexpr.dtype != expr.dtype:; if err is None:; err = 'Type error in recursive call,'; err += f'\n at argument index {i}, loop arg type: {expr.dtype}, '; err += f'recur arg type: {rexpr.dtype}'; if err is not None:; raise TypeError(err); irs = [expr._ir for expr in recur_exprs]; indices, aggregations = unify_all(*recur_exprs); return construct_expr(ir.Recur(loop_name, irs, typ), typ, indices, aggregations). uid_irs = []; loop_vars = []. for expr in args:; uid = Env.get_uid(); loop_vars.append(construct_variable(uid, expr._type, expr._indices, expr._aggregations)); uid_irs.append((uid, expr._ir)). loop_f = to_expr(f(make_loop, *loop_vars)); if loop_f.dtype != typ:; raise TypeError(f""requested type {typ} does not match inferred type {loop_f.dtype}""); check_tail_recursive(loop_f._ir); indices, aggregations = unify_all(*args, loop_f). return construct_expr(ir.TailLoop(loop_name, loop_f._ir, uid_irs), loop_f.dtype, indices, aggregations). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html:5420,error,error,5420,docs/0.2/_modules/hail/experimental/loop.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html,2,['error'],['error']
Availability,"(pheno, 'NA', lambda x: hl.if_else(x, '2', '1') if x.dtype == tbool else hl.str(x)),; }. locus = dataset.locus; a = dataset.alleles. bim_exprs = {; 'varid': expr_or_else(varid, hl.delimit([locus.contig, hl.str(locus.position), a[0], a[1]], ':')),; 'cm_position': expr_or_else(cm_position, 0.0),; }. for exprs, axis in [; (fam_exprs, dataset._col_indices),; (bim_exprs, dataset._row_indices),; (entry_exprs, dataset._entry_indices),; ]:; for name, expr in exprs.items():; analyze('export_plink/{}'.format(name), expr, axis). dataset = dataset._select_all(col_exprs=fam_exprs, col_key=[], row_exprs=bim_exprs, entry_exprs=entry_exprs). # check FAM ids for white space; t_cols = dataset.cols(); errors = []; for name in ['ind_id', 'fam_id', 'pat_id', 'mat_id']:; ids = t_cols.filter(t_cols[name].matches(r""\s+""))[name].collect(). if ids:; errors.append(f""""""expr '{name}' has spaces in the following values:\n""""""); for row in ids:; errors.append(f"""""" {row}\n""""""). if errors:; raise TypeError(""\n"".join(errors)). writer = ir.MatrixPLINKWriter(output); Env.backend().execute(ir.MatrixWrite(dataset._mir, writer)). [docs]@typecheck(; dataset=oneof(MatrixTable, Table),; output=str,; append_to_header=nullable(str),; parallel=nullable(ir.ExportType.checker),; metadata=nullable(dictof(str, dictof(str, dictof(str, str)))),; tabix=bool,; ); def export_vcf(dataset, output, append_to_header=None, parallel=None, metadata=None, *, tabix=False):; """"""Export a :class:`.MatrixTable` or :class:`.Table` as a VCF file. .. include:: ../_templates/req_tvariant.rst. Examples; --------; Export to VCF as a block-compressed file:. >>> hl.export_vcf(dataset, 'output/example.vcf.bgz'). Notes; -----; :func:`.export_vcf` writes the dataset to disk in VCF format as described in the; `VCF 4.2 spec <https://samtools.github.io/hts-specs/VCFv4.2.pdf>`__. Use the ``.vcf.bgz`` extension rather than ``.vcf`` in the output file name; for `blocked GZIP <http://www.htslib.org/doc/tabix.html>`__ compression. Note; ----; We stron",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:15698,error,errors,15698,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['error'],['errors']
Availability,"(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket – Name of the google storage bucket to mount.; mount_point – The path at which the bucket should be mounted to in the Docker; container.; read_only – If True, mount the bucket in read-only mode. Return type:; Self. Returns:; Same job object set with a bucket to mount with gcsfuse. memory(memory); Set the job’s memory requirements.; Examples; Set the job’s memory requirement to be 3Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.memory('3Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The memory expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, the values ‘lowmem’, ‘standard’,; and ‘highmem’ are also valid arguments. ‘lowmem’ corresponds to; approximately 1 Gi/core, ‘standard’ corresponds to approximately; 4 Gi/core, and ‘highmem’ corresponds to approximately 7 Gi/core.; The default value is ‘standard’. Parameters:; memory (Union[str, int, None]) – Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (‘standard’). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; E",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:5848,echo,echo,5848,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['echo'],['echo']
Availability,"(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Index into or slice the array.; Examples; Index with a single integer:; >>> hl.eval(names[1]); 'Bob'. >>> hl.eval(names[-1]); 'Charlie'. Slicing is also supported:; >>> hl.eval(names[1:]); ['Bob', 'Charlie']. Parameters:; item (slice or Expression of type tint32) – Index or slice. Returns:; Expression – Element or array slice. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. aggregate(f)[source]; Uses the aggregator library to compute a summary from an array.; This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, call_stats(). Parameters:; f – Aggregation function. Returns:; Expression. all(f); Returns True if f returns True for every element.; Examples; >>> hl.eval(a.all(lambda x: x < 10)); True. Notes; This method returns True if the collection is empty. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for every element, False otherwise. any(f); Returns True if f returns True for any element.; Examples; >>> hl.eval(a.any(lam",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:2738,error,error,2738,docs/0.2/hail.expr.ArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html,1,['error'],['error']
Availability,") Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields parameter.; (#13224); hailctl config get, set, and unset now support shell; auto-completion. Run hailctl --install-completion zsh to install; the auto-completion for zsh. You must already have completion; enab",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:25007,error,errors,25007,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,") Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as opposed to only regular python strings.; (#8198) Improved; matrix multiplication interoperation between hail; NDArrayExpression and numpy. Bug fixes. (#8279) Fix a bug; where hl.agg.approx_cdf failed inside of a group_cols_by.; (#8275) Fix bad error; message coming from mt.make_table() when keys are missing.; (#8274) Fix memory; leak in hl.export_bgen.; (#8273) Fix segfault; caused by hl.agg.downsample inside of an array_agg or; group_by. hailctl dataproc. (#8253); hailctl dataproc now supports new flags; --requester-pays-allow-all and; --requester-pays-allow-buckets. This will configure your hail; installation to be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:75688,down,downsample,75688,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['down'],['downsample']
Availability,") is related to \(\mathrm{ln}(\delta)\) through the sigmoid function. More precisely,. \[h^2 = 1 - \mathrm{sigmoid}(\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\mathrm{ln}(\delta))\]; Hence one can change variables to extract a high-resolution discretization of the likelihood function of \(h^2\) over \([0,1]\) at the corresponding REML estimators for \(\beta\) and \(\sigma_g^2\), as well as integrate over the normalized likelihood function using change of variables and the sigmoid differential equation.; For convenience, global.lmmreg.fit.normLkhdH2 records the the likelihood function of \(h^2\) normalized over the discrete grid 0.01, 0.02, ..., 0.98, 0.99. The length of the array is 101 so that index i contains the likelihood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of \(h^2\) as follows. Let \(x_2\) be the maximum likelihood estimate of \(h^2\) and let \(x_ 1\) and \(x_3\) be just to the left and right of \(x_2\). Let \(y_1\), \(y_2\), and \(y_3\) be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. \[\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\frac{1}{2 \sigma^2}\]; The standard error \(\hat{\sigma}\) is then estimated by solving for \(\sigma\).; Note that the mean and standard deviation of the (discretized or continuous) distribution held in global.lmmreg.fit.normLkhdH2 will not coincide with \(\hat{h}^2\) and \(\hat{\sigma}\), since this distribution only becomes normal in the infinite sample limit. One can visually assess normality by plotting this distributio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:103643,error,error,103643,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,")). Write two lines directly to a file in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). .. caution::. These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use :func:`.hadoop_copy`; to move your file to a distributed file system. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; # pile of hacks to preserve some legacy behavior, like auto gzip; fs = Env.fs(); if isinstance(fs, HadoopFS):; return fs.legacy_open(path, mode, buffer_size); _, ext = os.path.splitext(path); if ext in ('.gz', '.bgz'):; binary_mode = 'wb' if mode[0] == 'w' else 'rb'; file = fs.open(path, binary_mode, buffer_size); file = gzip.GzipFile(fileobj=file, mode=mode); if 'b' not in mode:; file = io.TextIOWrapp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:2498,error,error,2498,docs/0.2/_modules/hail/utils/hadoop_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html,2,['error'],['error']
Availability,")); ); weights = item_weights.values(); ranks = weights.scan(lambda acc, weight: acc + weight, 0); values = item_weights.keys(); return hl.struct(values=values, ranks=ranks, _compaction_counts=raw_cdf._compaction_counts). @typecheck(k=expr_int32, left=expr_struct(), right=expr_struct()); def _cdf_combine(k, left, right):; t = tstruct(levels=tarray(tint32), items=tarray(tfloat64), _compaction_counts=tarray(tint32)); return _func('approxCDFCombine', t, k, left, right). @typecheck(cdf=expr_struct(), failure_prob=expr_oneof(expr_float32, expr_float64), all_quantiles=bool); def _error_from_cdf(cdf, failure_prob, all_quantiles=False):; """"""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :class:`.StructExpression`; Result of :func:`.approx_cdf` aggregator; failure_prob: :class:`.NumericExpression`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :class:`.NumericExpression`; Upper bound on error of quantile estimates.; """""". def compute_sum(cdf):; s = hl.sum(; hl.range(0, hl.len(cdf._compaction_counts)).map(lambda i: cdf._compaction_counts[i] * (2 ** (2 * i))); ); return s / (cdf.ranks[-1] ** 2). def update_grid_size(p, s):; return 4 * hl.sqrt(hl.log(2 * p / failure_prob) / (2 * s)). def compute_grid_size(s):; return hl.fold(lambda p, i: update_grid_size(p, s), 1 / failure_prob, hl.range(0, 5)). def compute_single_error(s, failure_prob=failure_prob):; return hl.sqrt(hl.log(2 / failure_prob) * s / 2). def compute_global_error(s):; return hl.rbind(compute_grid_size(s), lambda p: 1 / p + compute_single_error(s, failure_prob / p)). if all_quantiles:; return hl.rbind(cdf, lambda cdf: hl.rbind(compute_sum(cdf), compute_global_error)); else:; return hl.rbind(cdf, lambda cdf: hl.rbind(compute_sum(cdf), compute_single_error)). def _error_from_cdf_pytho",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:5524,error,error,5524,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['error'],['error']
Availability,")); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters:. bucket (str) – Name of the google storage bucket to mount or the path to an Azure container in the; format of <account>/<container>.; mount_point (str) – The path at which the cloud blob storage should be mounted to in the Docker; container.; read_only (bool) – If True, mount the cloud blob storage in read-only mode. Return type:; Self. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the job’s CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where the optional suffix is m representing millicpu.; Omitting a suffix means the value is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the job’s CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None]) – Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job) – Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, moun",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:3887,echo,echo,3887,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['echo'],['echo']
Availability,"),; axes=list(indices.axes),; stray=list(unexpected_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)) for name, inds in bad_refs; ),; agg=''; if (unexpected_axes - aggregation_axes); else ""\n '{}' supports aggregation over axes {}, ""; ""so these fields may appear inside an aggregator function."".format(caller, list(aggregation_axes)),; ); ); ). if aggregations:; if aggregation_axes:; # the expected axes of aggregated expressions are the expected axes + axes aggregated over; expected_agg_axes = expected_axes.union(aggregation_axes). for agg in aggregations:; assert isinstance(agg, Aggregation); refs = get_refs(*agg.exprs); agg_axes = agg.agg_axes(). # check for stray indices; unexpected_agg_axes = agg_axes - expected_agg_axes; if unexpected_agg_axes:; # one or more out-of-scope fields; bad_refs = []; for name, inds in refs.items():; bad_axes = inds.axes.intersection(unexpected_agg_axes); if bad_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0. errors.append(; ExpressionException(; ""scope violation: '{caller}' supports aggregation over indices {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}"".format(; caller=caller,; expected=list(aggregation_axes),; axes=list(agg_axes),; stray=list(unexpected_agg_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)); for name, inds in bad_refs; ),; ); ); ); else:; errors.append(ExpressionException(""'{}' does not support aggregation"".format(caller))). for w in warnings:; warning('{}'.format(w.msg)); if errors:; for e in errors:; error('{}'.format(e.msg)); raise errors[0]. @typecheck(expression=expr_any); def eval_timed(expression):; """"""Evaluate a Hail expression, returning the result and the times taken for; each stage in the evaluation process. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; (Any, dict); Result",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:3774,error,errors,3774,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html,2,['error'],['errors']
Availability,"). a @ b; would then have shape (3, 4, 6).; Notes; The last dimension of a and the second to last dimension of b (or only dimension if b is a vector); must have the same length. The dimensions to the left of the last two dimensions of a and b (for NDArrays; of dimensionality > 2) must be equal or be compatible for broadcasting.; Number of dimensions of both NDArrays must be at least 1. Parameters:; other (numpy.ndarray NDArrayNumericExpression). Returns:; NDArrayNumericExpression or NumericExpression. __mul__(other)[source]; Positionally multiply by a ndarray or a scalar. Parameters:; other (NumericExpression or NDArrayNumericExpression) – Value or ndarray to multiply by. Returns:; NDArrayNumericExpression – NDArray of positional products. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate elements of the ndarray. Returns:; NDArrayNumericExpression – Array expression of the same type. __sub__(other)[source]; Positionally subtract a ndarray or a scalar. Parameters:; other (NumericExpression or NDArrayNumericExpression) – Value or ndarray to subtract. Returns:; NDArrayNumericExpression – NDArray of positional differences. __truediv__(other)[source]; Positionally divide by a ndarray or a scalar. Parameters:; other (NumericExpression or NDArrayNumericExpression) – Value or ndarray to divide by. Returns:; NDArrayNumericExpression – NDArray of positional quotients. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experiment",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:4400,error,error,4400,docs/0.2/hail.expr.NDArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html,1,['error'],['error']
Availability,"); None. Notes; Returns the index of the maximum value in the array.; If two or more elements are tied for maximum, then the unique parameter; will determine the result. If unique is False, then the first index; will be returned. If unique is True, then the result is missing.; If the array is empty, then the result is missing. Note; Missing elements are ignored. Parameters:. array (ArrayNumericExpression); unique (bool). Returns:; Expression of type tint32. hail.expr.functions.corr(x, y)[source]; Compute the; Pearson correlation coefficient; between x and y.; Examples; >>> hl.eval(hl.corr([1, 2, 4], [2, 3, 1])); -0.6546536707079772. Notes; Only indices where both x and y are non-missing will be included in the; calculation.; If x and y have length zero, then the result is missing. Parameters:. x (Expression of type array<tfloat64>); y (Expression of type array<tfloat64>). Returns:; Float64Expression. hail.expr.functions.uniroot(f, min, max, *, max_iter=1000, epsilon=2.220446049250313e-16, tolerance=0.0001220703)[source]; Finds a root of the function f within the interval [min, max].; Examples; >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; f(min) and f(max) must not have the same sign.; If no root can be found, the result of this call will be NA (missing).; uniroot() returns an estimate for a root with accuracy; 4 * epsilon * abs(x) + tolerance.; 4*EPSILON*abs(x) + tol. Parameters:. f (function ( (arg) -> Float64Expression)) – Must return a Float64Expression.; min (Float64Expression); max (Float64Expression); max_iter (int) – The maximum number of iterations before giving up.; epsilon (float) – The scaling factor in the accuracy of the root found.; tolerance (float) – The constant factor in approximate accuracy of the root found. Returns:; Float64Expression – The root of the function f. hail.expr.functions.binary_search(array, elem)[source]; Binary search array for the insertion point of elem. Parameters:. array (Expression of type tarray); elem (Exp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:17484,toler,tolerance,17484,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['toler'],['tolerance']
Availability,"); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; in_autosome(). Auto – in autosome or in PAR or female child; HemiX – in non-PAR of X and male child; HemiY – in non-PAR of Y and male child. Any refers to the set { HomRef, Het, HomVar, NoCall } and ~; denotes complement in this set. Code; Dad; Mom; Kid; Copy State | Implicated. 1; HomVar; HomVar; Het; Auto; Dad, Mom, Kid. 2; HomRef; HomRef; Het; Auto; Dad, Mom, Kid. 3; HomRef; ~HomRef; HomVar; Auto; Dad, Kid. 4; ~HomRef; HomRef; HomVar; Auto; Mom, Kid. 5; HomRef; HomRef; HomVar; Auto; Kid. 6; HomVar; ~HomVar; HomRef; Auto; Dad, Kid. 7; ~HomVar; HomVar; HomRef; Auto; Mom, Kid. 8; HomVar; HomVar; HomRef; Auto; Kid. 9; Any; HomVar; HomRef; HemiX; Mom, Kid. 10; Any; HomRef; HomVar; HemiX; Mom, Kid. 11; HomVar; Any; HomRef; HemiY; D",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:50635,error,error,50635,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['error']
Availability,"); ht = ht.transmute(; interval=hl.locus_interval(; ht['seqname'],; ht['start'],; ht['end'],; includes_start=True,; includes_end=True,; reference_genome=reference_genome,; ); ); else:; ht = ht.transmute(; interval=hl.interval(; hl.struct(seqname=ht['seqname'], position=ht['start']),; hl.struct(seqname=ht['seqname'], position=ht['end']),; includes_start=True,; includes_end=True,; ); ). ht = ht.key_by('interval'). return ht. [docs]@typecheck(; gene_symbols=nullable(sequenceof(str)),; gene_ids=nullable(sequenceof(str)),; transcript_ids=nullable(sequenceof(str)),; verbose=bool,; reference_genome=nullable(reference_genome_type),; gtf_file=nullable(str),; ); def get_gene_intervals(; gene_symbols=None, gene_ids=None, transcript_ids=None, verbose=True, reference_genome=None, gtf_file=None; ):; """"""Get intervals of genes or transcripts. Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable. On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz. Example; -------; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) # doctest: +SKIP. Parameters; ----------. gene_symbols : :obj:`list` of :class:`str`, optional; Gene symbols (e.g. PCSK9).; gene_ids : :obj:`list` of :class:`str`, optional; Gene IDs (e.g. ENSG00000223972).; transcript_ids : :obj:`list` of :class:`str`, optional; Transcript IDs (e.g. ENSG00000223972).; verbose : :obj:`bool`; If ``True``, print which genes and transcripts were matched in the GTF file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platf",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html:6975,avail,available,6975,docs/0.2/_modules/hail/experimental/import_gtf.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html,2,['avail'],['available']
Availability,"*kwargs). [docs] def checkpoint(self, path, **kwargs) -> 'VariantDataset':; """"""Write to `path` and then read from `path`.""""""; self.write(path, **kwargs); return read_vds(path). [docs] def n_samples(self) -> int:; """"""The number of samples present.""""""; return self.reference_data.count_cols(). @property; def reference_genome(self) -> ReferenceGenome:; """"""Dataset reference genome. Returns; -------; :class:`.ReferenceGenome`; """"""; return self.reference_data.locus.dtype.reference_genome. [docs] @typecheck_method(check_data=bool); def validate(self, *, check_data: bool = True):; """"""Eagerly checks necessary representational properties of the VDS."""""". rd = self.reference_data; vd = self.variant_data. def error(msg):; raise ValueError(f'VDS.validate: {msg}'). rd_row_key = rd.row_key.dtype; if (; not isinstance(rd_row_key, hl.tstruct); or len(rd_row_key) != 1; or not rd_row_key.fields[0] == 'locus'; or not isinstance(rd_row_key.types[0], hl.tlocus); ):; error(f""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_key = vd.row_key.dtype; if (; not isinstance(vd_row_key, hl.tstruct); or len(vd_row_key) != 2; or not vd_row_key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 or vd_col_key.types[0] != hl.tstr:; error(f""expect variant data to have a single col key of type string, found {vd_col_key}""). if 'END' not in rd.entry or rd.END.dtype != hl.tint32:; error(""expect field 'END' in entry of reference data with type int3",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:8662,error,error,8662,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['error'],['error']
Availability,", '1kg.mt'); vcf_path = os.path.join(output_dir, '1kg.vcf.bgz'); sample_annotations_path = os.path.join(output_dir, '1kg_annotations.txt'); gene_annotations_path = os.path.join(output_dir, 'ensembl_gene_annotations.txt'). if (; overwrite; or not _dir_exists(fs, matrix_table_path); or not _file_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, '1kg.vcf.bgz'); source = resources['1kg_matrix_table']; info(f'downloading 1KG VCF ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, resources['1kg_matrix_table'], tmp_vcf); cluster_readable_vcf = _copy_to_tmp(fs, local_path_uri(tmp_vcf), extension='vcf.bgz'); info('importing VCF and writing to matrix table...'); hl.import_vcf(cluster_readable_vcf, min_partitions=16).write(matrix_table_path, overwrite=True). tmp_sample_annot = os.path.join(tmp_dir, '1kg_annotations.txt'); source = resources['1kg_annotations']; info(f'downloading 1KG annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['1kg_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene_annot), gene_annotations_path); hl.hadoop_copy(local_path_uri(tmp_vcf), vcf_path); info('Done!'); else:; info('1KG files found'). [docs]def get_hgdp(output_dir, overwrite: bool = False):; """"""Download subset of the `Human Genome Diversity Panel; <https://www.internationalgenome.org/data-portal/data-collection/hgdp/>`__; dataset and sample annotations. Notes; -----; The download is about 30MB. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``Tr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:3347,down,downloading,3347,docs/0.2/_modules/hail/utils/tutorial.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html,2,['down'],['downloading']
Availability,", aggregations). @property; def dtype(self) -> HailType:; """"""The data type of the expression. Returns; -------; :class:`.HailType`. """"""; return self._type. def __bool__(self):; raise TypeError(; ""'Expression' objects cannot be converted to a 'bool'. Use 'hl.if_else' instead of Python if statements.""; ). def __len__(self):; raise TypeError(""'Expression' objects have no static length: use 'hl.len' for the length of collections""). def __contains__(self, item):; class_name = type(self).__name__; raise TypeError(f""`{class_name}` objects don't support the `in` operator.""). def __hash__(self):; return super(Expression, self).__hash__(). def __repr__(self):; return f'<{self.__class__.__name__} of type {self.dtype}>'. [docs] def __eq__(self, other):; """"""Returns ``True`` if the two expressions are equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for equality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are equal.; """"""; return self._compare_op(""=="", other). [docs] def __ne__(self, other):; """"""Returns ``True`` if the two expressions are not equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for inequality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are not equal.; """"""; return self._compare_op(""!="", other). def _to_table(self, name):; name, ds = self._to_relational(name); if isinstance(ds, hail.MatrixTable):; entries = ds.key_cols_by().entries(); entries = entr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:23430,error,error,23430,docs/0.2/_modules/hail/expr/expressions/base_expression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html,2,['error'],['error']
Availability,", and demonstrate the need to control for confounding caused by population stratification. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:2141,down,downsampling,2141,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['down'],['downsampling']
Availability,", call_stats.; (#13166) Add an; eigh ndarray method, for finding eigenvalues of symmetric; matrices (“h” is for Hermitian, the complex analogue of symmetric). Bug Fixes. (#13184) The; vds.to_dense_mt no longer densifies past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem implementations. File Format. The native file format version is now 1.7.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.118; Released 2023-06-13. New Features. (#13140) Enable; hail-az and Azure Blob Storage https URLs to contain SAS; tokens to enable bearer-auth style file access to Azure storage.; (#13129) Allow; subnet to be passed through to gcloud in hailctl. Bug Fixes. (#13126); Query-on-Batch pipelines with one partition are now retried when they; encounter transient errors.; (#13113); hail.ggplot.geom_point now displays a legend group for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakine",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:31677,error,errors,31677,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,", dest_reference_genome=reference_genome_type); def add_liftover(self, chain_file, dest_reference_genome):; """"""Register a chain file for liftover. Examples; --------; Access GRCh37 and GRCh38 using :func:`~hail.get_reference`:. >>> rg37 = hl.get_reference('GRCh37') # doctest: +SKIP; >>> rg38 = hl.get_reference('GRCh38') # doctest: +SKIP. Add a chain file from 37 to 38:. >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_liftover` to test whether a chain file has been registered. The chain file format is described; `here <https://genome.ucsc.edu/goldenpath/help/chain.html>`__. Chain files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37 to GRCh38**; gs://hail-common/references/grch37_to_grch38.over.chain.gz. **GRCh38 to GRCh37**; gs://hail-common/references/grch38_to_grch37.over.chain.gz. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; chain_file : :class:`str`; Path to chain file. Can be compressed (GZIP) or uncompressed.; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to convert to.; """""". Env.backend().add_liftover(self.name, chain_file, dest_reference_genome.name); if dest_reference_genome.name in self._liftovers:; raise KeyError(f""Liftover already exists from {self.name} to {dest_reference_genome.name}.""); if dest_reference_genome.name == self.name:; raise ValueError(f'Destination reference genome cannot have the same name as this reference {self.name}.'); self._liftovers[dest_reference_genome.name] = chain_file. [docs] @typecheck_method(global_pos=int); def locus_from_global_position(self, global_pos: int) -> 'hl.Locus':; """""" ""; Constructs a locus from a global position in reference genome.; The inverse of :meth:`.Locus.position`. Examples; --------; >>> rg = hl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:14459,down,download,14459,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,4,"['avail', 'down']","['available', 'download']"
Availability,", set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logist",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:6241,toler,tolerance,6241,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['toler'],['tolerance']
Availability,", x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:63848,toler,tolerance,63848,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['toler'],['tolerance']
Availability,",; 'pheno': expr_or_else(pheno, 'NA', lambda x: hl.if_else(x, '2', '1') if x.dtype == tbool else hl.str(x)),; }. locus = dataset.locus; a = dataset.alleles. bim_exprs = {; 'varid': expr_or_else(varid, hl.delimit([locus.contig, hl.str(locus.position), a[0], a[1]], ':')),; 'cm_position': expr_or_else(cm_position, 0.0),; }. for exprs, axis in [; (fam_exprs, dataset._col_indices),; (bim_exprs, dataset._row_indices),; (entry_exprs, dataset._entry_indices),; ]:; for name, expr in exprs.items():; analyze('export_plink/{}'.format(name), expr, axis). dataset = dataset._select_all(col_exprs=fam_exprs, col_key=[], row_exprs=bim_exprs, entry_exprs=entry_exprs). # check FAM ids for white space; t_cols = dataset.cols(); errors = []; for name in ['ind_id', 'fam_id', 'pat_id', 'mat_id']:; ids = t_cols.filter(t_cols[name].matches(r""\s+""))[name].collect(). if ids:; errors.append(f""""""expr '{name}' has spaces in the following values:\n""""""); for row in ids:; errors.append(f"""""" {row}\n""""""). if errors:; raise TypeError(""\n"".join(errors)). writer = ir.MatrixPLINKWriter(output); Env.backend().execute(ir.MatrixWrite(dataset._mir, writer)). [docs]@typecheck(; dataset=oneof(MatrixTable, Table),; output=str,; append_to_header=nullable(str),; parallel=nullable(ir.ExportType.checker),; metadata=nullable(dictof(str, dictof(str, dictof(str, str)))),; tabix=bool,; ); def export_vcf(dataset, output, append_to_header=None, parallel=None, metadata=None, *, tabix=False):; """"""Export a :class:`.MatrixTable` or :class:`.Table` as a VCF file. .. include:: ../_templates/req_tvariant.rst. Examples; --------; Export to VCF as a block-compressed file:. >>> hl.export_vcf(dataset, 'output/example.vcf.bgz'). Notes; -----; :func:`.export_vcf` writes the dataset to disk in VCF format as described in the; `VCF 4.2 spec <https://samtools.github.io/hts-specs/VCFv4.2.pdf>`__. Use the ``.vcf.bgz`` extension rather than ``.vcf`` in the output file name; for `blocked GZIP <http://www.htslib.org/doc/tabix.html>`__ compressi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:15663,error,errors,15663,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['error'],['errors']
Availability,"-+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:107035,toler,tolerance,107035,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,"-----+-------+; | 0 | 11 | 1.78e+02 | 1.68e-04 | 0 |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper’s suggested weights which are derived from the; allele frequency.; >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:73926,fault,fault,73926,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['fault'],['fault']
Availability,"------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Parameters; ----------; p : :obj:`float`; Probability of keeping each row.; seed : :obj:`int`; Random seed. Returns; -------; :class:`.Table`; Table with approximately ``p * n_rows`` rows.; """""". if not 0 <= p <= 1:; raise ValueError(""Requires 'p' in [0,1]. Found p={}"".format(p)). return self.filter(hl.rand_bool(p, seed)). [docs] @typecheck_method(n=int, shuffle=bool); def repartition(self, n, shuffle=True) -> 'Table':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> table_result = table1.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:92445,avail,available,92445,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['avail'],['available']
Availability,"------+-------+; | 0 | 11 | 1.78e+02 | 1.68e-04 | 0 |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93155,fault,fault,93155,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability,"-------+------------+----------+------------------+; | 8 | HomVar | HomVar | HomRef | Auto |; +--------+------------+------------+----------+------------------+; | 9 | Any | HomVar | HomRef | HemiX |; +--------+------------+------------+----------+------------------+; | 10 | Any | HomRef | HomVar | HemiX |; +--------+------------+------------+----------+------------------+; | 11 | HomVar | Any | HomRef | HemiY |; +--------+------------+------------+----------+------------------+; | 12 | HomRef | Any | HomVar | HemiY |; +--------+------------+------------+----------+------------------+. This method only considers children with two parents and a defined sex. PAR is currently defined with respect to reference; `GRCh37 <http://www.ncbi.nlm.nih.gov/projects/genome/assembly/grc/human/>`__:. - X: 60001 - 2699520, 154931044 - 155260560; - Y: 10001 - 2649520, 59034050 - 59363566. :param pedigree: Sample pedigree.; :type pedigree: :class:`~hail.representation.Pedigree`. :returns: Four tables with Mendel error statistics.; :rtype: (:class:`.KeyTable`, :class:`.KeyTable`, :class:`.KeyTable`, :class:`.KeyTable`); """""". kts = self._jvdf.mendelErrors(pedigree._jrep); return KeyTable(self.hc, kts._1()), KeyTable(self.hc, kts._2()), \; KeyTable(self.hc, kts._3()), KeyTable(self.hc, kts._4()). [docs] @handle_py4j; @typecheck_method(max_shift=integral); def min_rep(self, max_shift=100):; """"""; Gives minimal, left-aligned representation of alleles. Note that this can change the variant position. **Examples**. 1. Simple trimming of a multi-allelic site, no change in variant position; `1:10000:TAA:TAA,AA` => `1:10000:TA:T,A`. 2. Trimming of a bi-allelic site leading to a change in position; `1:10000:AATAA,AAGAA` => `1:10002:T:G`. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.minRep(max",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:160539,error,error,160539,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability,"--------+-------+; | 0 | 11 | 8.76e+02 | 1.23e-05 | 0 |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:77433,fault,fault,77433,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability,"-------------+------------------+; | <expr>.density | <expr>.next_year |; +----------------+------------------+; | float64 | int32 |; +----------------+------------------+; | 2.00e+03 | 2021 |; +----------------+------------------+. See also; Table.transmute(), Table.select_globals(), Table.annotate_globals(). Parameters:; named_exprs (keyword args of Expression) – Annotation expressions. Returns:; Table. union(*tables, unify=False)[source]; Union the rows of multiple tables.; Examples; Take the union of rows from two tables:; >>> union_table = table1.union(other_table). Notes; If a row appears in more than one table identically, it is duplicated; in the result. All tables must have the same key names and types. They; must also have the same row types, unless the unify parameter is; True, in which case a field appearing in any table will be included; in the result, with missing values for tables that do not contain the; field. If a field appears in multiple tables with incompatible types,; like arrays and strings, then an error will be raised. Parameters:. tables (varargs of Table) – Tables to union.; unify (bool) – Attempt to unify table field. Returns:; Table – Table with all rows from each component table. unpersist()[source]; Unpersists this table from memory/disk.; Notes; This function will have no effect on a table that was not previously; persisted. Returns:; Table – Unpersisted table. write(output, overwrite=False, stage_locally=False, _codec_spec=None)[source]; Write to disk.; Examples; >>> table1.write('output/table1.ht', overwrite=True). Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. See also; read_table(). Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output.; overwrite (bool) – If True, overwrite an existing file at the destination. write_many(output, fields, *, ov",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:78417,error,error,78417,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['error'],['error']
Availability,"----------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.dropped_variance_fraction`` | Double | specified value of ``dropped_variance_fraction`` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.evals`` | Array[Double] | all eigenvalues of the kinship matrix in descending order |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.seH2`` | Double | standard error of :math:`\\hat{h}^2` under asymptotic normal approximation |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.normLkhdH2`` | Array[Double] | likelihood function of :math:`h^2` normalized on the discrete grid ``0.01, 0.02, ..., 0.99``. Index ``i`` is the likelihood for percentage ``i``. |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.maxLogLkhd`` | Double | (restricted) maximum log likelihood corresponding to :math:`\\hat{\delta}` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logDeltaGrid`` | Array[Double] | values of :math:`",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:120369,error,error,120369,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability,"----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters; ----------; paths: :class:`str` or :obj:`list` of :obj:`str`; Files to import.; min_partitions: :obj:`int` or :obj:`None`; Minimum number of partitions.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; file_per_partition : :obj:`bool`; If ``True``, each file will be in a seperate partition. Not recommended; for most uses. Error thrown if ``True`` and `min_partitions` is less than; the number of files. Returns; -------; :class:`.Table`; Table constructed from imported data.; """""". paths = wrap_to_list(paths). if file_per_partition and min_partitions is not None:; if min_partitions > len(paths):; raise FatalError(; f'file_per_partition is True while min partitions is {min_partitions} ,which is greater'; f' than the number of files, {len(paths)}'; ). st_reader = ir.StringTableReader(paths, min_partitions, force_bgz, force, file_per_partition); table_type = hl.ttable(global_type=hl.tstruct(), row_type=hl.tstruct(file=hl.tstr, text=hl.tstr), row_key=[]); string_table = Table(ir.TableRead(st_reader, _assert_type=table_type)); return string_table. [docs]@typecheck(; paths=oneof(str, sequenceof(str)),; row_fields=dictof(str, hail_type),; row_key=oneof(str, sequenceof(str)),; entry_type=enumeration(tint32, tint64, tfloat32, tfloat64, tstr),; missing=str,; min_partitions=nullable(int),; no_header=bool,; force_bgz=bool,; sep=nullable(str),; delimiter=nul",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:66441,Error,Error,66441,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['Error'],['Error']
Availability,"----------; cdf : :obj:`dict`; Result of :func:`.approx_cdf` aggregator, evaluated to a python dict; failure_prob: :obj:`float`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :obj:`float`; Upper bound on error of quantile estimates.; """"""; import math. s = 0; for i in builtins.range(builtins.len(cdf._compaction_counts)):; s += cdf._compaction_counts[i] << (2 * i); s = s / (cdf.ranks[-1] ** 2). def update_grid_size(p):; return 4 * math.sqrt(math.log(2 * p / failure_prob) / (2 * s)). def compute_grid_size(s):; p = 1 / failure_prob; for _ in builtins.range(5):; p = update_grid_size(p); return p. def compute_single_error(s, failure_prob=failure_prob):; return math.sqrt(math.log(2 / failure_prob) * s / 2). if s == 0:; # no compactions ergo no error; return 0; elif all_quantiles:; p = compute_grid_size(s); return 1 / p + compute_single_error(s, failure_prob / p); else:; return compute_single_error(s, failure_prob). [docs]@typecheck(t=hail_type); def missing(t: Union[HailType, str]):; """"""Creates an expression representing a missing value of a specified type. Examples; --------. >>> hl.eval(hl.missing(hl.tarray(hl.tstr))); None. >>> hl.eval(hl.missing('array<str>')); None. Notes; -----; This method is useful for constructing an expression that includes missing; values, since :obj:`None` cannot be interpreted as an expression. Parameters; ----------; t : :class:`str` or :class:`.HailType`; Type of the missing expression. Returns; -------; :class:`.Expression`; A missing expression of type `t`.; """"""; return construct_expr(ir.NA(t), t). [docs]@deprecated(version=""0.2.62"", reason=""Replaced by hl.missing""); @typecheck(t=hail_type); def null(t: Union[HailType, str]):; """"""Deprecated in favor of :func:`.missing`. Creates an expression representing a missing value of a specified type. Examples; -------",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:7575,error,error,7575,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['error'],['error']
Availability,"---------; f : :class:`.StringExpression`; Java `format string <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Formatter.html#syntax>`__.; args : variable-length arguments of :class:`.Expression`; Arguments to format. Returns; -------; :class:`.StringExpression`; """""". return _func(""format"", hl.tstr, f, hl.tuple(args)). [docs]@typecheck(x=expr_float64, y=expr_float64, tolerance=expr_float64, absolute=expr_bool, nan_same=expr_bool); def approx_equal(x, y, tolerance=1e-6, absolute=False, nan_same=False):; """"""Tests whether two numbers are approximately equal. Examples; --------; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters; ----------; x : :class:`.NumericExpression`; y : :class:`.NumericExpression`; tolerance : :class:`.NumericExpression`; absolute : :class:`.BooleanExpression`; If True, compute ``abs(x - y) <= tolerance``. Otherwise, compute; ``abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022)``.; nan_same : :class:`.BooleanExpression`; If True, then ``NaN == NaN`` will evaluate to True. Otherwise,; it will return False. Returns; -------; :class:`.BooleanExpression`; """""". return _func(""approxEqual"", hl.tbool, x, y, tolerance, absolute, nan_same). def _shift_op(x, y, op):; assert op in ('<<', '>>', '>>>'); t = x.dtype; if t == hl.tint64:; word_size = 64; zero = hl.int64(0); else:; word_size = 32; zero = hl.int32(0). indices, aggregations = unify_all(x, y); return hl.bind(; lambda x, y: (; hl.case(); .when(y >= word_size, hl.sign(x) if op == '>>' else zero); .when(y >= 0, construct_expr(ir.ApplyBinaryPrimOp(op, x._ir, y._ir), t, indices, aggregations)); .or_error('cannot shift by a negative value: ' + hl.str(x) + f"" {op} "" + hl.str(y)); ),; x,; y,; ). def _bit_op(x, y, op):; if x.dtype == hl.tint32 and y.dtype == hl.tint32:; t = hl.tint32; else:; t = hl.tint64; coercer = coercer_from_dtype(t); x = coercer.coerce(x); y = coercer.coerce(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:177342,toler,tolerance,177342,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['toler'],['tolerance']
Availability,"-------. Compute sample QC metrics and remove low-quality samples:. >>> dataset = hl.sample_qc(dataset, name='sample_qc'); >>> filtered_dataset = dataset.filter_cols((dataset.sample_qc.dp_stats.mean > 20) & (dataset.sample_qc.r_ti_tv > 1.5)). Notes; -----. This method computes summary statistics per sample from a genetic matrix and stores; the results as a new column-indexed struct field in the matrix, named based on the; `name` parameter. If `mt` contains an entry field `DP` of type :py:data:`.tint32`, then the; field `dp_stats` is computed. If `mt` contains an entry field `GQ` of type; :py:data:`.tint32`, then the field `gq_stats` is computed. Both `dp_stats`; and `gq_stats` are structs with with four fields:. - `mean` (``float64``) -- Mean value.; - `stdev` (``float64``) -- Standard deviation (zero degrees of freedom).; - `min` (``int32``) -- Minimum value.; - `max` (``int32``) -- Maximum value. If the dataset does not contain an entry field `GT` of type; :py:data:`.tcall`, then an error is raised. The following fields are always; computed from `GT`:. - `call_rate` (``float64``) -- Fraction of calls not missing or filtered.; Equivalent to `n_called` divided by :meth:`.count_rows`.; - `n_called` (``int64``) -- Number of non-missing calls.; - `n_not_called` (``int64``) -- Number of missing calls.; - `n_filtered` (``int64``) -- Number of filtered entries.; - `n_hom_ref` (``int64``) -- Number of homozygous reference calls.; - `n_het` (``int64``) -- Number of heterozygous calls.; - `n_hom_var` (``int64``) -- Number of homozygous alternate calls.; - `n_non_ref` (``int64``) -- Sum of `n_het` and `n_hom_var`.; - `n_snp` (``int64``) -- Number of SNP alternate alleles.; - `n_insertion` (``int64``) -- Number of insertion alternate alleles.; - `n_deletion` (``int64``) -- Number of deletion alternate alleles.; - `n_singleton` (``int64``) -- Number of private alleles. Reference alleles are never counted as singletons, even if; every other allele at a site is non-reference.; - ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:3409,error,error,3409,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['error'],['error']
Availability,"----. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. Partitions are a; core concept of distributed computation in Spark, see `here; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. Returns; -------; int; Number of partitions.; """"""; return Env.backend().execute(ir.MatrixToValueApply(self._mir, {'name': 'NPartitionsMatrixTable'})). [docs] @typecheck_method(n_partitions=int, shuffle=bool); def repartition(self, n_partitions: int, shuffle: bool = True) -> 'MatrixTable':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:107405,avail,available,107405,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['avail'],['available']
Availability,"----; s : :class:`.StringExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""entropy"", tfloat64, s). @typecheck(x=expr_any, trunc=nullable(expr_int32)); def _showstr(x, trunc=None):; if trunc is None:; return _func(""showStr"", tstr, x); return _func(""showStr"", tstr, x, trunc). [docs]@typecheck(x=expr_any); def str(x) -> StringExpression:; """"""Returns the string representation of `x`. Examples; --------. >>> hl.eval(hl.str(hl.struct(a=5, b=7))); '{""a"":5,""b"":7}'. Parameters; ----------; x. Returns; -------; :class:`.StringExpression`; """"""; if x.dtype == tstr:; return x; else:; return _func(""str"", tstr, x). [docs]@typecheck(c=expr_call, i=expr_int32); def downcode(c, i) -> CallExpression:; """"""Create a new call by setting all alleles other than i to ref. Examples; --------; Preserve the third allele and downcode all other alleles to reference. >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters; ----------; c : :class:`.CallExpression`; A call.; i : :class:`.Expression` of type :py:data:`.tint32`; The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns; -------; :class:`.CallExpression`; """"""; return _func(""downcode"", tcall, c, i). @typecheck(pl=expr_array(expr_int32)); def gq_from_pl(pl) -> Int32Expression:; """"""Compute genotype quality from Phred-scaled probability likelihoods. Examples; --------. >>> hl.eval(hl.gq_from_pl([0, 69, 1035])); 69. Parameters; ----------; pl : :class:`.Expression` of type :class:`.tarray` of :obj:`.tint32`. Returns; -------; :class:`.Expression` of type :py:data:`.tint32`; """"""; return _func(""gqFromPL"", tint32, pl). [docs]@typecheck(n=expr_int32); def triangle(n) -> Int32Expression:; """"""Returns the triangle number of `n`. Examples; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:105309,down,downcode,105309,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['down'],['downcode']
Availability,"-18 01:26:52 Hail: INFO: Running linear regression on 843 samples with 1 covariate including intercept... Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; },; linreg: Struct{; beta: Double,; se: Double,; tstat: Double,; pval: Double; }; }. Looking at the bottom of the above printout, you can see the linear; regression adds new variant annotations for the beta, standard error,; t-statistic, and p-value. In [46]:. def qqplot(pvals, xMax, yMax):; spvals = sorted(filter(lambda x: x and not(isnan(x)), pvals)); exp = [-log(float(i) / len(spvals), 10) for i in np.arange(1, len(spvals) + 1, 1)]; obs = [-log(p, 10) for p in spvals]; plt.clf(); plt.scatter(exp, obs); plt.plot(np.arange(0, max(xMax, yMax)), c=""red""); plt.xlabel(""Expected p-value (-log10 scale)""); plt.ylabel(""Observed p-value (-log10 scale)""); plt.xlim(0, xMax); plt.ylim(0, yMax); plt.show(). Python makes it easy to make a Q-Q (quantile-quantile); plot. In [47]:. qqplot(gwas.query_variants('variants.map(v => va.linreg.pval).collect()'),; 5, 6). Confounded!¶; The observed p-values drift away from the expectation immediately.; Either every SNP in our dataset is causally linked to caffeine; consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate; this phenotype. This leads to a; stratif",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:22931,error,error,22931,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['error'],['error']
Availability,"-19. Performance. (#9264); Table.checkpoint now uses a faster LZ4 compression scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#9161) Fix bugs; calculating row_id in hl.import_matrix_table. Version 0.2.50; Released 2020-07-23. Bug fixes. (#9114) CHANGELOG:; Fixed crash when using repeated calls to hl.filter_intervals. New features. (#9101) Add; hl.nd.{concat, hstack, vstack} to concatenate ndarr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:66775,error,error,66775,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone. - s2 : :obj:`.tfloat64`, the variance of the residuals, :math:`\sigma^2` in the paper. """"""; mt = matrix_table_source('skat/x', x); k = len(covariates); if k == 0:; raise ValueError('_linear_skat: at least one covariate is required.'); _warn_if_no_intercept('_linear_skat', covariates); mt = mt._select_all(; row_exprs=dict(group=group, weight=weight), col_exprs=dict(y=y, covariates=covariates), entry_exprs=dict(x=x); ); mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])); yvec, covmat, n = mt.aggregate_cols(; (hl.agg.collect(hl.float(mt.y)), hl.agg.collect(mt.covariates.map(hl.float)), hl.agg.coun",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:80443,fault,fault,80443,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability,"-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Float64Expression) – Column-indexed response (dependent variable) expression.; x (Float64Expression) – Entry-indexed expression for input (independent variable).; covariates (list of Float64Expression) – List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size (int) – Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance (float) – The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy (float) – The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations (int) – The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns:; Table – One row per-group. The key is group. The row fields are:. group : the group parameter.; size : tint64, the number of variants in this group.; q_stat : tfloat64, the \(Q\) statistic, see Notes for why this differs from the paper.; p_value : tfloat64, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes.; fault : tint32, the fault flag from pgenchisq(). The global fields are:. n_complete_samples : tint32, the number of samples with neither a missing; phenotype nor a missing covariate.; y_residual : tint32, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone.; s2 : tfloat64, the variance of the residuals, \(\sigma^2\) in the paper.; null_fit:. b : tndar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:76429,fault,fault,76429,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['fault'],['fault']
Availability,". # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table3.snp_errors[1],; }),; hl.struct(**{; ck_name: table3.proband[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[2],; 'snp_errors': table3.snp_errors[2],; }),; ]; ); table3 = table3.explode('xs'); table3 = table3.select(**table3.xs); table3 = (; table3.group_by(ck_name, 'fam_id'); .aggregate(errors=hl.agg.sum(table3.errors), snp_errors=hl.agg.sum(table3.snp_errors)); .key_by(ck_name); ). table4 = tm.select_rows(errors=hl.agg.count_where(hl.is_defined(tm.mendel_code))).rows(). return table1, table2, table3, table4. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree); def transmission_disequilibrium_test(dataset, pedigree) -> Table:; r""""""Performs the transmission disequilibrium test on trios. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------; Compute TDT association statistics and show the first two results:. >>> pedigree = hl.Pedigree.read('data/tdt_trios.fam'); >>> tdt_table = hl.transmission_disequilibrium_test(tdt_dataset, pedigree); >>> tdt_table.show(2) # doctes",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:12333,error,errors,12333,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,". BashJob — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; BashJob; BashJob. PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BashJob. View page source. BashJob. class hailtop.batch.job.BashJob(batch, token, *, name=None, attributes=None, shell=None); Bases: Job; Object representing a single bash job to execute.; Examples; Create a batch object:; >>> b = Batch(). Create a new bash job that prints hello to a temporary file t.ofile:; >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'). Write the temporary file t.ofile to a permanent location; >>> b.write_output(j.ofile, 'hello.txt'). Execute the DAG:; >>> b.run(). Notes; This class should never be created directly by the user. Use Batch.new_job(); or Batch.new_bash_job() instead.; Methods. command; Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the job’s command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:707,echo,echo,707,docs/batch/api/batch/hailtop.batch.job.BashJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html,2,['echo'],['echo']
Availability,". Batch — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Batch. Job; BashJob; PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; Batch. View page source. Batch. class hailtop.batch.batch.Batch(name=None, backend=None, attributes=None, requester_pays_project=None, default_image=None, default_memory=None, default_cpu=None, default_storage=None, default_regions=None, default_timeout=None, default_shell=None, default_python_image=None, default_spot=None, project=None, cancel_after_n_failures=None); Bases: object; Object representing the distributed acyclic graph (DAG) of jobs to run.; Examples; Create a batch object:; >>> import hailtop.batch as hb; >>> p = hb.Batch(). Create a new job that prints “hello”:; >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:; >>> p.run(). Require all jobs in this batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str]) – Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None]) – Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respective",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:977,echo,echo,977,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['echo'],['echo']
Availability,". BatchPoolExecutor — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; BatchPoolExecutor; BatchPoolExecutor. BatchPoolFuture. Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BatchPoolExecutor. View page source. BatchPoolExecutor. class hailtop.batch.batch_pool_executor.BatchPoolExecutor(*, name=None, backend=None, image=None, cpus_per_job=None, wait_on_exit=True, cleanup_bucket=True, project=None); Bases: object; An executor which executes Python functions in the cloud.; concurrent.futures.ProcessPoolExecutor and; concurrent.futures.ThreadPoolExecutor enable the use of all the; computer cores available on a single computer. BatchPoolExecutor; enables the use of an effectively arbitrary number of cloud computer cores.; Functions provided to submit() are serialized using dill, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which submit() was; called. The Python version in the docker container will share a major and; minor verison with the local process. The image parameter overrides this; behavior.; When used as a context manager (the with syntax), the executor will wait; for all jobs to finish before finishing the with statement. This; behavior can be controlled by the wait_on_exit parameter.; This class creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:810,avail,available,810,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,2,['avail'],['available']
Availability,". Getting Started — Batch documentation. Batch; . Getting Started; Installation; Installing Batch on Mac OS X or GNU/Linux with pip; Installing the Google Cloud SDK; Try it out!. Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Getting Started. View page source. Getting Started. Installation; Batch is a Python module available inside the Hail Python package located at hailtop.batch. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; Create a conda enviroment named; hail and install the Hail python library in that environment. If conda activate doesn’t work, please read these instructions; conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; install the Google Cloud SDK. Try it out!; To try batch out, open iPython or a Jupyter notebook and run:; >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You’re now all set to run the tutorial!. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/getting_started.html:456,avail,available,456,docs/batch/getting_started.html,https://hail.is,https://hail.is/docs/batch/getting_started.html,2,"['avail', 'echo']","['available', 'echo']"
Availability,". JobResourceFile — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Resource; ResourceFile; InputResourceFile; JobResourceFile; JobResourceFile. ResourceGroup; PythonResult. Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; JobResourceFile. View page source. JobResourceFile. class hailtop.batch.resource.JobResourceFile(value, source); Bases: ResourceFile; Class representing an intermediate file from a job.; Examples; j.ofile is a JobResourceFile on the job`j`:; >>> b = Batch(); >>> j = b.new_job(name='hello-tmp'); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.run(). Notes; All JobResourceFile are temporary files and must be written; to a permanent location using Batch.write_output() if the output needs; to be saved.; Methods. add_extension; Specify the file extension to use. source. rtype:; Job. add_extension(extension); Specify the file extension to use.; Examples; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> j.ofile.add_extension('.txt'); >>> b.run(). Notes; The default file name for a JobResourceFile is the name; of the identifier. Parameters:; extension (str) – File extension to use. Return type:; JobResourceFile. Returns:; JobResourceFile – Same resource file with the extension specified. source(). Return type:; Job. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.JobResourceFile.html:745,echo,echo,745,docs/batch/api/resource/hailtop.batch.resource.JobResourceFile.html,https://hail.is,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.JobResourceFile.html,4,['echo'],['echo']
Availability,". Random Forest Model — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Clumping GWAS Results; Random Forest; Introduction; Batch Code; Imports; Random Forest Function; Format Result Function; Build Python Image; Control Code. Add Checkpointing; Add Batching of Jobs; Synopsis. Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Random Forest Model. View page source. Random Forest Model. Introduction; We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Rand",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:284,Checkpoint,Checkpointing,284,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,1,['Checkpoint'],['Checkpointing']
Availability,". Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, overwrite=False)[source]; Download subset of the 1000 Genomes; dataset and sample annotations.; Notes; The download is about 15M. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_hgdp(output_dir, overwrite=False)[source]; Download subset of the Human Genome Diversity Panel; dataset and sample annotations.; Notes; The download is about 30MB. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_movie_lens(output_dir, overwrite=False)[source]; Download public Movie Lens dataset.; Notes; The download is about 6M.; See the; MovieLens website; for more information about this dataset. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite existing files/directories at those locations. hail.utils.ANY_REGION; Built-in mutable sequence.; If no argument is given, the constructor creates a new empty list.; The argument must be an iterable if specified. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:11446,down,download,11446,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,2,['down'],['download']
Availability,". [docs] @typecheck_method(indices=sequenceof(int)); def choose_cols(self, indices: List[int]) -> 'MatrixTable':; """"""Choose a new set of columns from a list of old column indices. Examples; --------. Randomly shuffle column order:. >>> import random; >>> indices = list(range(dataset.count_cols())); >>> random.shuffle(indices); >>> dataset_reordered = dataset.choose_cols(indices). Take the first ten columns:. >>> dataset_result = dataset.choose_cols(list(range(10))). Parameters; ----------; indices : :obj:`list` of :obj:`int`; List of old column indices. Returns; -------; :class:`.MatrixTable`; """"""; n_cols = self.count_cols(); for i in indices:; if not 0 <= i < n_cols:; raise ValueError(f""'choose_cols': expect indices between 0 and {n_cols}, found {i}""); return MatrixTable(ir.MatrixChooseCols(self._mir, indices)). [docs] def n_partitions(self) -> int:; """"""Number of partitions. Notes; -----. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. Partitions are a; core concept of distributed computation in Spark, see `here; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. Returns; -------; int; Number of partitions.; """"""; return Env.backend().execute(ir.MatrixToValueApply(self._mir, {'name': 'NPartitionsMatrixTable'})). [docs] @typecheck_method(n_partitions=int, shuffle=bool); def repartition(self, n_partitions: int, shuffle: bool = True) -> 'MatrixTable':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:106508,avail,available,106508,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['avail'],['available']
Availability,". b = hl.nd.hstack([null_fit.b, hl.nd.zeros((m_diff,))]); mu = sigmoid(X @ b); score = hl.nd.hstack([null_fit.score, X1.T @ (y - mu)]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def search(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = hl.log((y * mu) + (1 - y) * (1 - mu)).sum(). next_b = b + delta_b; next_mu = sigmoid(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = X.T @ (X * (next_mu * (1 - next_mu)).reshape(-1, 1)). return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(; b=b,; score=score,; fisher=fisher,; mu=mu,; n_iterations=iteration,; log_lkhd=log_lkhd,; converged=True,; exploded=False,; ),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b, next_mu, next_score, next_fisher)); ). delta_b_struct = hl.nd.solve(fisher, score, no_crash=True); exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution; max_delta_b = nd_max(hl.abs(delta_b)); return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.annotate(n_iterations=0, log_lkhd=0, converged=False, exploded=False); return hl.experimental.loop(search, numerical_regression_fit_dtype, 1, b, mu, score, fisher). def wald_test(X, fit):; se = hl.sqrt(hl.nd.diagonal(hl.nd.inv(fit.fisher))); z = fit.b / se; p = z.map(lambda e: 2 * hl.pnorm(-hl.abs(e))); return hl.struct(; beta=fit.b[X.shape[1] - 1],; stand",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:40988,toler,tolerance,40988,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,". geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC; 3; 4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:81401,error,error,81401,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['error']
Availability,". ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; -----",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44884,toler,tolerateParseError,44884,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['toler'],['tolerateParseError']
Availability,". menu; Hail. Module code; hail.expr.expressions.expression_utils. Source code for hail.expr.expressions.expression_utils; from typing import Dict, Set. from hail.typecheck import setof, typecheck. from ...ir import MakeTuple; from ..expressions import Expression, ExpressionException, expr_any; from .indices import Aggregation, Indices. @typecheck(caller=str, expr=Expression, expected_indices=Indices, aggregation_axes=setof(str), broadcast=bool); def analyze(caller: str, expr: Expression, expected_indices: Indices, aggregation_axes: Set = set(), broadcast=True):; from hail.utils import error, warning. indices = expr._indices; source = indices.source; axes = indices.axes; aggregations = expr._aggregations. warnings = []; errors = []. expected_source = expected_indices.source; expected_axes = expected_indices.axes. if source is not None and source is not expected_source:; bad_refs = []; for name, inds in get_refs(expr).items():; if inds.source is not expected_source:; bad_refs.append(name); errors.append(; ExpressionException(; ""'{caller}': source mismatch\n""; "" Expected an expression from source {expected}\n""; "" Found expression derived from source {actual}\n""; "" Problematic field(s): {bad_refs}\n\n""; "" This error is commonly caused by chaining methods together:\n""; "" >>> ht.distinct().select(ht.x)\n\n""; "" Correct usage:\n""; "" >>> ht = ht.distinct()\n""; "" >>> ht = ht.select(ht.x)"".format(; caller=caller, expected=expected_source, actual=source, bad_refs=list(bad_refs); ); ); ). # check for stray indices by subtracting expected axes from observed; if broadcast:; unexpected_axes = axes - expected_axes; strictness = ''; else:; unexpected_axes = axes if axes != expected_axes else set(); strictness = 'strictly '. if unexpected_axes:; # one or more out-of-scope fields; refs = get_refs(expr); bad_refs = []; for name, inds in refs.items():; if broadcast:; bad_axes = inds.axes.intersection(unexpected_axes); if bad_axes:; bad_refs.append((name, inds)); elif inds.axes != expecte",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:1424,error,errors,1424,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html,2,['error'],['errors']
Availability,".2.81; Release 2021-12-20. hailctl dataproc. (#11182) Updated; Dataproc image version to mitigate yet more Log4j vulnerabilities. Version 0.2.80; Release 2021-12-15. New features. (#11077); hl.experimental.write_matrix_tables now returns the paths of the; written matrix tables. hailctl dataproc. (#11157) Updated; Dataproc image version to mitigate the Log4j vulnerability.; (#10900) Added; --region parameter to hailctl dataproc submit.; (#11090) Teach; hailctl dataproc describe how to read URLs with the protocols; s3 (Amazon S3), hail-az (Azure Blob Storage), and file; (local file system) in addition to gs (Google Cloud Storage). Version 0.2.79; Release 2021-11-17. Bug fixes. (#11023) Fixed bug; in call decoding that was introduced in version 0.2.78. New features. (#10993) New; function p_value_excess_het. Version 0.2.78; Release 2021-10-19. Bug fixes. (#10766) Don’t throw; out of memory error when broadcasting more than 2^(31) - 1 bytes.; (#10910) Filters on; key field won’t be slowed down by uses of; MatrixTable.localize_entries or Table.rename.; (#10959) Don’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:56198,down,down,56198,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['down'],['down']
Availability,".; """"""; available_versions = []; for version in versions:; if version.in_region(name, region):; version.url = version.url[region]; available_versions.append(version); return available_versions. def __init__(self, url: Union[dict, str], version: Optional[str], reference_genome: Optional[str]):; self.url = url; self.version = version; self.reference_genome = reference_genome. def in_region(self, name: str, region: str) -> bool:; """"""Check if a :class:`.DatasetVersion` object is accessible in the; desired region. Parameters; ----------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset exists in the specified region.; """"""; current_version = self.version; available_regions = [k for k in self.url.keys()]; valid_region = region in available_regions; if not valid_region:; message = (; f'\nName: {name}\n'; f'Version: {current_version}\n'; f'This dataset exists but is not yet available in the'; f' {region} region bucket.\n'; f'Dataset is currently available in the'; f' {"", "".join(available_regions)} region bucket(s).\n'; f'Reach out to the Hail team at https://discuss.hail.is/'; f' to request this dataset in your region.'; ); warnings.warn(message, UserWarning, stacklevel=1); return valid_region. def maybe_index(self, indexer_key_expr: StructExpression, all_matches: bool) -> Optional[StructExpression]:; """"""Find the prefix of the given indexer expression that can index the; :class:`.DatasetVersion`, if it exists. Parameters; ----------; indexer_key_expr : :class:`StructExpression`; Row key struct from relational object to be annotated.; all_matches : :obj:`bool`; ``True`` if `indexer_key_expr` key is not unique, indicated in; :attr:`.Dataset.key_properties` for each dataset. If ``True``, value; of `indexer_key_expr` is array of all matches. If ``False``, there; will only be single value of ex",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:4094,avail,available,4094,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['avail'],['available']
Availability,".; Column fields are stored once per column. These can contain information about the columns, or summary data calculated per column.; Entry fields are the piece that makes this structure a matrix – there is an entry for each (row, column) pair. Importing and Reading; Like tables, matrix tables can be imported from a variety of formats: VCF, (B)GEN, PLINK, TSV, etc. Matrix tables can also be read from a “native” matrix table format. Let’s read a sample of prepared 1KG data. [1]:. import hail as hl; from bokeh.io import output_notebook, show; output_notebook(). hl.utils.get_1kg('data/'). Loading BokehJS ... Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2011-0.2.133-4c60fddb171a.log; 2024-10-04 20:11:52.232 Hail: INFO: 1KG files found. [2]:. mt = hl.read_matrix_table('data/1kg.mt'); mt.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: array<int32>,; MLEAF: array<float64>,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; QD: float64,; ReadPosRankSum: float64,; set: str; }; ----------------------------------------; Entry f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/07-matrixtable.html:3019,avail,available,3019,docs/0.2/tutorials/07-matrixtable.html,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html,1,['avail'],['available']
Availability,".; If x and y have length zero, then the result is missing. Parameters:. x (Expression of type array<tfloat64>); y (Expression of type array<tfloat64>). Returns:; Float64Expression. hail.expr.functions.uniroot(f, min, max, *, max_iter=1000, epsilon=2.220446049250313e-16, tolerance=0.0001220703)[source]; Finds a root of the function f within the interval [min, max].; Examples; >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; f(min) and f(max) must not have the same sign.; If no root can be found, the result of this call will be NA (missing).; uniroot() returns an estimate for a root with accuracy; 4 * epsilon * abs(x) + tolerance.; 4*EPSILON*abs(x) + tol. Parameters:. f (function ( (arg) -> Float64Expression)) – Must return a Float64Expression.; min (Float64Expression); max (Float64Expression); max_iter (int) – The maximum number of iterations before giving up.; epsilon (float) – The scaling factor in the accuracy of the root found.; tolerance (float) – The constant factor in approximate accuracy of the root found. Returns:; Float64Expression – The root of the function f. hail.expr.functions.binary_search(array, elem)[source]; Binary search array for the insertion point of elem. Parameters:. array (Expression of type tarray); elem (Expression). Returns:; Int32Expression. Notes; This function assumes that array is sorted in ascending order, and does; not perform any sortedness check. Missing values sort last.; The returned index is the lower bound on the insertion point of elem into; the ordered array, or the index of the first element in array not smaller; than elem. This is a value between 0 and the length of array, inclusive; (if all elements in array are smaller than elem, the returned value is; the length of array or the index of the first missing value, if one; exists).; If either elem or array is missing, the result is missing.; Examples; >>> a = hl.array([0, 2, 4, 8]). >>> hl.eval(hl.binary_search(a, -1)); 0. >>> hl.eval(hl.binary_search(a, 1)); ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:18172,toler,tolerance,18172,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['toler'],['tolerance']
Availability,".; Notes; Per sample field names in the result are formed by; concatenating the sample ID with the genotype_expr left; hand side with separator. If the left hand side is empty:; `` = expr. then the dot (.) is omitted. Parameters:; variant_expr (str or list of str) – Variant annotation expressions.; genotype_expr (str or list of str) – Genotype annotation expressions.; key (str or list of str) – List of key columns.; separator (str) – Separator to use between sample IDs and genotype expression left-hand side identifiers. Return type:KeyTable. mendel_errors(pedigree)[source]¶; Find Mendel errors; count per variant, individual and nuclear; family. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:; >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped). Export all mendel errors to a text file:; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:123560,error,errors,123560,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,".; Phenotype and covariate sample annotations may also be specified using programmatic expressions without identifiers, such as:; >>> vds_result = vds.linreg('if (sa.pheno.isFemale) sa.pheno.age else (2 * sa.pheno.age + 10)'). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation sa.fam.isCase added by importing a FAM file with case-control phenotype, case is 1 and control is 0.; The standard least-squares linear regression model is derived in Section; 3.2 of The Elements of Statistical Learning, 2nd Edition. See; equation 3.12 for the t-statistic which follows the t-distribution with; \(n - k - 2\) degrees of freedom, under the null hypothesis of no; effect, with \(n\) samples and \(k\) covariates in addition to; genotype and intercept.; Annotations; With the default root, the following four variant annotations are added. va.linreg.beta (Double) – fit genotype coefficient, \(\hat\beta_1\); va.linreg.se (Double) – estimated standard error, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Double) – \(t\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Double) – \(p\)-value. Parameters:; y (str) – Response expression; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosages genotypes rather than hard call genotypes.; min_ac (int) – Minimum alternate allele count.; min_af (float) – Minimum alternate allele frequency. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. linreg3(ys, covariates=[], root='va.linreg', use_dosages=False, variant_block_size=16)[source]¶; Test each variant for association with multiple phenotypes using linear regression.; This method runs linear regression for multiple phenotypes; more efficiently than looping over linreg(). This; method is more efficient than linreg_multi_pheno(); but doesn’t implicitly filter",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:81658,error,error,81658,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,".; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; Table. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_table(). It is; possible to read the file at this path later with read_table().; Examples; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). collect(_localize=True, *, _timed=False)[source]; Collect the rows of the table into a local list.; Examples; Collect a list of all X records:; >>> all_xs = [row['X'] for row in table1.select(table1.X).collect()]. Notes; This method returns a list whose elements are of type Struct. Fields; of these structs can be accessed similarly to fields on a table, using dot; methods (struct.foo) or string indexing (struct['foo']). Warning; Using this method can cause out of memory errors. Only collect small tables. Returns:; list of Struct – List of rows. collect_by_key(name='values')[source]; Collect values for each unique key into an array. Note; Requires a keyed table. Examples; >>> t1 = hl.Table.parallelize([; ... {'t': 'foo', 'x': 4, 'y': 'A'},; ... {'t': 'bar', 'x': 2, 'y': 'B'},; ... {'t': 'bar', 'x': -3, 'y': 'C'},; ... {'t': 'quam', 'x': 0, 'y': 'D'}],; ... hl.tstruct(t=hl.tstr, x=hl.tint32, y=hl.tstr),; ... key='t'). >>> t1.show(); +--------+-------+-----+; | t | x | y |; +--------+-------+-----+; | str | int32 | str |; +--------+-------+-----+; | ""bar"" | 2 | ""B"" |; | ""bar"" | -3 | ""C"" |; | ""foo"" | 4 | ""A"" |; | ""quam"" | 0 | ""D"" |; +--------+-------+-----+. >>> t1.collect_by_key().show(); +--------+---------------------------------+; | t | values |; +--------+---------------------------------+; | str | array<struct{x: int32, y: str}> |; +--------+---------------------------------+; | ""bar"" | [(2,""B""),(-3,""C"")] |; | ""foo"" | [(4,""A"")",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:16548,error,errors,16548,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['error'],['errors']
Availability,".DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> ht = db.annotate_rows_db(ht, 'clinvar_gene_summary', 'CADD', 'DANN') . Not",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1708,avail,available,1708,docs/0.2/experimental/hail.experimental.DB.html,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html,1,['avail'],['available']
Availability,".MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'MatrixTable':; """"""; Unpersists this dataset from memory/disk. Notes; -----; This function will have no effect on a dataset that was not previously; persisted. Returns; -------; :class:`.MatrixTable`; Unpersisted dataset.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:111092,redundant,redundant,111092,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['redundant'],['redundant']
Availability,".checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd.Z2 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z1), Z1=_ibd.Z1 / (_ibd.Z0 + _ibd.Z1), Z2=hl.float(0)),; ); .default(_ibd); ). result = result.annotate(ibd=hl.struct(Z0=result.Z0, Z1=result.Z1, Z2=result.Z2)); resul",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:7755,checkpoint,checkpoint,7755,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,2,['checkpoint'],['checkpoint']
Availability,".default_block_size(); g = BlockMatrix.from_entry_expr(gt_with_nan_expr, block_size=block_size); g = g.checkpoint(new_temp_file('pc_relate_bm/g', 'bm')); sqrt_n_samples = hl.nd.array([hl.sqrt(g.shape[1])]). # Recover singular values, S0, as vector of column norms of pc_scores if necessary; if compute_S0:; S0 = (pc_scores ** hl.int32(2)).sum(0).map(lambda x: hl.sqrt(x)); else:; S0 = hl.nd.array(eigens).map(lambda x: hl.sqrt(x)); # Set first entry of S to sqrt(n), for intercept term in beta; S = hl.nd.hstack((sqrt_n_samples, S0))._persist(); # Recover V from pc_scores with inv(S0); V0 = (pc_scores * (1 / S0))._persist(); # Set all entries in first column of V to 1/sqrt(n), for intercept term in beta; ones_normalized = hl.nd.full((V0.shape[0], 1), (1 / S[0])); V = hl.nd.hstack((ones_normalized, V0)). # Compute matrix of regression coefficients for PCs (beta), shape (k, m); beta = BlockMatrix.from_ndarray(((1 / S) * V).T, block_size=block_size) @ g.T; beta = beta.checkpoint(new_temp_file('pc_relate_bm/beta', 'bm')). # Compute matrix of individual-specific AF estimates (mu), shape (m, n); mu = 0.5 * (BlockMatrix.from_ndarray(V * S, block_size=block_size) @ beta).T; # Replace entries in mu with NaN if invalid or if corresponding GT is missing (no contribution from that variant); mu = mu._apply_map2(; lambda _mu, _g: hl.if_else(_bad_mu(_mu, min_individual_maf) | hl.is_nan(_g), nan, _mu),; g,; sparsity_strategy='NeedsDense',; ); mu = mu.checkpoint(new_temp_file('pc_relate_bm/mu', 'bm')). # Compute kinship matrix (phi), shape (n, n); # Where mu is NaN (missing), set variance and centered AF to 0 (no contribution from that variant); variance = _replace_nan(mu * (1.0 - mu), 0.0).checkpoint(new_temp_file('pc_relate_bm/variance', 'bm')); centered_af = _replace_nan(g - (2.0 * mu), 0.0); phi = _gram(centered_af) / (4.0 * _gram(variance.sqrt())); phi = phi.checkpoint(new_temp_file('pc_relate_bm/phi', 'bm')); ht = phi.entries().rename({'entry': 'kin'}); ht = ht.annotate(k0=hl.missing",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:19935,checkpoint,checkpoint,19935,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['checkpoint'],['checkpoint']
Availability,".expr.functions.unphased_diploid_gt_index_call(gt_index)[source]; Construct an unphased, diploid call from a genotype index.; Examples; >>> hl.eval(hl.unphased_diploid_gt_index_call(4)); Call(alleles=[1, 2], phased=False). Parameters:; gt_index (int or Expression of type tint32) – Unphased, diploid genotype index. Returns:; CallExpression. hail.expr.functions.parse_call(s)[source]; Construct a call expression by parsing a string or string expression.; Examples; >>> hl.eval(hl.parse_call('0|2')); Call(alleles=[0, 2], phased=True). Notes; This method expects strings in the following format:. ploidy; Phased; Unphased. 0; |-; -. 1; |i; i. 2; i|j; i/j. 3; i|j|k; i/j/k. N; i|j|k|...|N; i/j/k/.../N. Parameters:; s (str or StringExpression) – String to parse. Returns:; CallExpression. hail.expr.functions.downcode(c, i)[source]; Create a new call by setting all alleles other than i to ref; Examples; Preserve the third allele and downcode all other alleles to reference.; >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters:. c (CallExpression) – A call.; i (Expression of type tint32) – The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns:; CallExpression. hail.expr.functions.triangle(n)[source]; Returns the triangle number of n.; Examples; >>> hl.eval(hl.triangle(3)); 6. Notes; The calculation is n * (n + 1) / 2. Parameters:; n (Expression of type tint32). Returns:; Expression of type tint32. hail.expr.functions.is_snp(ref, alt)[source]; Returns True if the alleles constitute a single nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:11535,down,downcode,11535,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['down'],['downcode']
Availability,".index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we’ve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the genome. The; complete code is provided here for your reference. run_rf_simple.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:11000,checkpoint,checkpoint,11000,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability,".path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the""; ""\n existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.""; ). return vds. [docs]def store_ref_block_max_length(vds_path):; """"""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS. Examples; --------; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') # doctest: +SKIP. See Also; --------; :func:`.vds.filter_intervals`, :func:`.vds.truncate_reference_blocks`. Parameters; ----------; vds_path : :obj:`str`; """"""; vds = read_vds(vds_path, _warn_no_ref_block_max_length=False). if VariantDataset.ref_block_max_length_field in vds.reference_data.globals:; warning(f""VDS at {vds_path} already contains a global annotation with the max reference block length""); return; rd = vds.reference_data; rd = rd.annotate_rows(__start_pos=rd.locus.position); fs = hl.current_backend().fs; ref_block_max_len = rd.aggregate_entries(hl.agg.max(rd.END - rd.__start_pos + 1)); with fs.open(os.path.join(vds_path, extra_ref_globals_file), 'w') as f:; json.dump({VariantDataset.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:2924,down,downstream,2924,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['down'],['downstream']
Availability,".proband_entry; dad = tm.father_entry; mom = tm.mother_entry. kid_linear_pl = 10 ** (-kid.PL / 10); kid_pp = hl.bind(lambda x: x / hl.sum(x), kid_linear_pl). dad_linear_pl = 10 ** (-dad.PL / 10); dad_pp = hl.bind(lambda x: x / hl.sum(x), dad_linear_pl). mom_linear_pl = 10 ** (-mom.PL / 10); mom_pp = hl.bind(lambda x: x / hl.sum(x), mom_linear_pl). kid_ad_ratio = kid.AD[1] / hl.sum(kid.AD); dp_ratio = kid.DP / (dad.DP + mom.DP). def call_auto(kid_pp, dad_pp, mom_pp, kid_ad_ratio):; p_data_given_dn = dad_pp[0] * mom_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (dad_pp[1] * mom_pp[0] + dad_pp[0] * mom_pp[1]) * kid_pp[1] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (dad.DP + mom.DP) < min_dp_ratio) | ~(kid_ad_ratio >= min_child_ab), failure); .when((hl.sum(mom.AD) == 0) | (hl.sum(dad.AD) == 0), failure); .when(; (mom.AD[1] / hl.sum(mom.AD) > max_parent_ab) | (dad.AD[1] / hl.sum(dad.AD) > max_parent_ab), failure; ); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:30371,failure,failure,30371,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['failure'],['failure']
Availability,".when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). def call_hemi(kid_pp, parent, parent_pp, kid_ad_ratio):; p_data_given_dn = parent_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (parent_pp[1] + parent_pp[2]) * kid_pp[2] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (parent.DP) < min_dp_ratio) | (kid_ad_ratio < min_child_ab), failure); .when((hl.sum(parent.AD) == 0), failure); .when(parent.AD[1] / hl.sum(parent.AD) > max_parent_ab, failure); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.3, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:31958,failure,failure,31958,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['failure'],['failure']
Availability,"020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in l",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:79306,error,error,79306,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"05); VariantDataset.validate now checks that all ref blocks are no; longer than the ref_block_max_length field, if it exists. Bug Fixes. (#14420) Fixes a; serious, but likely rare, bug in the Table/MatrixTable reader, which; has been present since Sep 2020. It manifests as many (around half or; more) of the rows being dropped. This could only happen when 1); reading a (matrix)table whose partitioning metadata allows rows with; the same key to be split across neighboring partitions, and 2); reading it with a different partitioning than it was written. 1); would likely only happen by reading data keyed by locus and alleles,; and rekeying it to only locus before writing. 2) would likely only; happen by using the _intervals or _n_partitions arguments to; read_(matrix)_table, or possibly repartition. Please reach; out to us if you’re concerned you may have been affected by this.; (#14330) Fixes; erroneous error in export_vcf with unphased haploid Calls.; (#14303) Fix; missingness error when sampling entries from a MatrixTable.; (#14288) Contigs may; now be compared for inquality while filtering rows. Deprecations. (#14386); MatrixTable.make_table is deprecated. Use .localize_entries; instead. Version 0.2.128; Released 2024-02-16; In GCP, the Hail Annotation DB and Datasets API have moved from; multi-regional US and EU buckets to regional US-CENTRAL1 and; EUROPE-WEST1 buckets. These buckets are requester pays which means; unless your cluster is in the US-CENTRAL1 or EUROPE-WEST1 region, you; will pay a per-gigabyte rate to read from the Annotation DB or Datasets; API. We must make this change because reading from a multi-regional; bucket into a regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:14763,error,error,14763,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"1 so that index ``i`` contains the likelihood at percentage ``i``. The values at indices 0 and 100 are left undefined. By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of :math:`h^2` as follows. Let :math:`x_2` be the maximum likelihood estimate of :math:`h^2` and let :math:`x_ 1` and :math:`x_3` be just to the left and right of :math:`x_2`. Let :math:`y_1`, :math:`y_2`, and :math:`y_3` be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. .. math::. \\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\\frac{1}{2 \sigma^2}. The standard error :math:`\\hat{\sigma}` is then estimated by solving for :math:`\sigma`. Note that the mean and standard deviation of the (discretized or continuous) distribution held in ``global.lmmreg.fit.normLkhdH2`` will not coincide with :math:`\\hat{h}^2` and :math:`\\hat{\sigma}`, since this distribution only becomes normal in the infinite sample limit. One can visually assess normality by plotting this distribution against a normal distribution with the same mean and standard deviation, or use this distribution to approximate credible intervals under a flat prior on :math:`h^2`. **Testing each variant for association**. Fixing a single variant, we define:. - :math:`v = n \\times 1` vector of genotypes, with missing genotypes imputed as the mean of called genotypes; - :math:`X_v = \\left[v | X \\right] = n \\times (1 + c)` matrix concatenating :math:`v` and :math:`X`; - :math:`\\beta_v = (\\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v) = (1 + c) \\times 1` vector of covariate coefficients. Fixing :math:`\delta` at the globa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:133641,error,error,133641,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability,"1) / Y) * ((Y - 2) / Y) * ((Y - 3) / Y) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); + 4 * (p**2) * (q**2) * ((X - 1) / X) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); ),; _e11=(; 2 * (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + 2 * p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e21=(; (p**3) * ((X - 1) / X) * ((X - 2) / X) * (T / (T - 1)) * (T / (T - 2)); + (q**3) * ((Y - 1) / Y) * ((Y - 2) / Y) * (T / (T - 1)) * (T / (T - 2)); + (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e22=1,; ). dataset = dataset.checkpoint(hl.utils.new_temp_file()). expectations = dataset.aggregate_rows(; hl.struct(; e00=hl.agg.sum(dataset._e00),; e10=hl.agg.sum(dataset._e10),; e20=hl.agg.sum(dataset._e20),; e11=hl.agg.sum(dataset._e11),; e21=hl.agg.sum(dataset._e21),; e22=hl.agg.sum(dataset._e22),; ); ). IS_HOM_REF = BlockMatrix.from_entry_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:6573,checkpoint,checkpoint,6573,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,2,['checkpoint'],['checkpoint']
Availability,"1-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailctl dataproc now supports --gcloud_configuration option. Documentation. (#7570) Hail has a; cheatsheet for Tables now. Version 0.2.27; Released 2019-11-15. New Features. (#7379) Add; delimiter argument to hl.import_matrix_table; (#7389) Add force; and force_bgz arguments to hl.experimental.import_gtf; (#7386)(#7394); Add {Table, MatrixTable}.tail.; (#7467) Added; hl.if_else as an alias for hl.cond; deprecated hl.cond.; (#7453) Add; hl.parse_int{32, 64} and hl.parse_float{32, 64}, which can; parse strings to numbers and return missing on failure.; (#7475) Add; row_join_type argument to MatrixTable.union_cols to support; outer joins on rows. Bug fixes. (#7479)(#7368)(#7402); Fix optimizer bugs.; (#7506) Updated to; latest htsjdk to resolve VCF parsing problems. hailctl dataproc. (#7460) The Spark; monitor widget now automatically collapses after a job completes. Version 0.2.26; Released 2019-10-24. New Features. (#7325) Add; string.reverse function.; (#7328) Add; string.translate function.; (#7344) Add; hl.reverse_complement function.; (#7306) Teach the VCF; combiner to handle allele specific (AS_*) fields.; (#7346) Add; hl.agg.approx_median function. Bug Fixes. (#7361) Fix AD; calculation in sparse_split_multi. Performance Improvements. (#7355) Improve; performance of IR copying. File Format. The native file format version is now 1.3.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:81992,failure,failure,81992,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['failure'],['failure']
Availability,"1401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram infers min and max values automatically.; (#11317) Add support; for alpha aesthetic and identity position to; geom_histogram. Version 0.2.83; Release 2022-02-01. Bug fixes. (#11268) Fixed; log argument in hail.plot.histogram.; (#11276) Fixed; log argument in hail.plot.pdf.; (#11256) Fixed; memory leak in LD Prune. New features. (#11274) Added; geom_col to hail.ggplot. hailctl dataproc. (#11280) Updated; dataproc image version to one not affected by log4j vulnerabilities. Version 0.2.82; Release 2022-01-24. Bug fixes. (#11209); Significantly improved usefulness and speed of Table.to_pandas,; resolved several bugs with output. New features. (#11247) Introduces; a new experimental plotting interface hail.ggplot, based on R’s; ggplot library.; (#11173) Many math; functions like hail.sqrt now automatically broadcast over; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:53754,failure,failure,53754,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['failure'],['failure']
Availability,"17. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) Fixed a combinatorial explosion in cancellation calculation in the LocalBackend; (#12917) ABS blob URIs in the form of https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH> are now supported when running in Azure. The hail-az scheme for referencing ABS blobs is now deprecated and will be removed in a future release. Version 0.2.114. (#12780) PythonJobs now handle arguments with resources nested inside dicts and lists.; (#12900) Reading data from public blobs is now supported in Azure. Version 0.2.113. (#12780) The LocalBackend now supports always_run jobs. The LocalBackend will no longer immediately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; (#12845) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:3124,error,error,3124,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['error'],['error']
Availability,"2, :]`` is a block matrix with 1 row, 10 columns,; and elements from row 2 of ``bm``. - ``bm[:3, -1]`` is a block matrix with 3 rows, 1 column,; and the first 3 elements of the last column of ``bm``. - ``bm[::2, ::2]`` is a block matrix with 5 rows, 5 columns,; and all evenly-indexed elements of ``bm``. Use :meth:`filter`, :meth:`filter_rows`, and :meth:`filter_cols` to; subset to non-slice subsets of rows and columns, e.g. to rows ``[0, 2, 5]``. **Block-sparse representation**. By default, block matrices compute and store all blocks explicitly.; However, some applications involve block matrices in which:. - some blocks consist entirely of zeroes. - some blocks are not of interest. For example, statistical geneticists often want to compute and manipulate a; banded correlation matrix capturing ""linkage disequilibrium"" between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra. To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a **block-sparse matrix**. Otherwise, we say the matrix; is block-dense. The property :meth:`is_sparse` encodes this state. Dropped blocks are not stored in memory or on :meth:`write`. In fact,; blocks that are dropped prior to an action like :meth:`export` or; :meth:`to_numpy` are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes. Block-sparse matrices may be created with; :meth:`sparsify_band`,; :meth:`sparsify_rectangles`,; :meth:`spa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:7495,down,downstream,7495,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['down'],['downstream']
Availability,"2062); hl.balding_nichols_model now supports an optional boolean; parameter, phased, to control the phasedness of the generated; genotypes. Performance improvements. (#12099) Make; repeated VCF/PLINK queries much faster by caching compiler data; structures.; (#12038) Speed up; hl.import_matrix_table by caching header line computation. Bug fixes. (#12115) When using; use_new_shuffle=True, fix a bug when there are more than 2^31; rows; (#12074) Fix bug; where hl.init could silently overwrite the global random seed.; (#12079) Fix bug in; handling of missing (aka NA) fields in grouped aggregation and; distinct by key.; (#12056) Fix; hl.export_vcf to actually create tabix files when requested.; (#12020) Fix bug in; hl.experimental.densify which manifested as an AssertionError; about dtypes. Version 0.2.97; Released 2022-06-30. New Features. (#11756); hb.BatchPoolExecutor and Python jobs both now also support async; functions. Bug fixes. (#11962) Fix error; (logged as (#11891)); in VCF combiner when exactly 10 or 100 files are combined.; (#11969) Fix; import_table and import_lines to use multiple partitions when; force_bgz is used.; (#11964) Fix; erroneous “Bucket is a requester pays bucket but no user project; provided.” errors in Google Dataproc by updating to the latest; Dataproc image version. Version 0.2.96; Released 2022-06-21. New Features. (#11833); hl.rand_unif now has default arguments of 0.0 and 1.0. Bug fixes. (#11905) Fix; erroneous FileNotFoundError in glob patterns; (#11921) and; (#11910) Fix file; clobbering during text export with speculative execution.; (#11920) Fix array; out of bounds error when tree aggregating a multiple of 50; partitions.; (#11937) Fixed; correctness bug in scan order for Table.annotate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#118",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:47667,error,error,47667,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"22-06-21. New Features. (#11833); hl.rand_unif now has default arguments of 0.0 and 1.0. Bug fixes. (#11905) Fix; erroneous FileNotFoundError in glob patterns; (#11921) and; (#11910) Fix file; clobbering during text export with speculative execution.; (#11920) Fix array; out of bounds error when tree aggregating a multiple of 50; partitions.; (#11937) Fixed; correctness bug in scan order for Table.annotate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:49084,toler,tolerance,49084,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['toler'],['tolerance']
Availability,"3. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:85158,error,error,85158,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> hl.read_table('output-many/a').show(); +-------+-------+; | a | idx |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | 1 |; | 2 | 2 |; | 3 | 3 |; | 4 | 4 |; | 5 | 5 |; | 6 | 6 |; | 7 | 7 |; | 8 | 8 |; | 9 | 9 |; +-------+-------+; >>> hl.read_table('output-many/b').describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'b': int32; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> hl.read_table('output-many/b').show(); +-------+-------+; | b | idx |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | 1 |; | 4 | 2 |; | 9 | 3 |; | 16 | 4 |; | 25 | 5 |; | 36 | 6 |; | 49 | 7 |; | 64 | 8 |; | 81 | 9 |; +-------+-------+; >>> hl.read_table('output-many/c').describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'c': str; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> hl.read_table('output-many/c').show(); +-----+-------+; | c | idx |; +-----+-------+; | str | int32 |; +-----+-------+; | ""0"" | 0 |; | ""1"" | 1 |; | ""2"" | 2 |; | ""3"" | 3 |; | ""4"" | 4 |; | ""5"" | 5 |; | ""6"" | 6 |; | ""7"" | 7 |; | ""8"" | 8 |; | ""9"" | 9 |; +-----+-------+. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. See also; read_table(). Parameters:. output (str) – Path at which to write.; fields (list of str) – The fields to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output.; overwrite (bool) – If True, overwrite an existing file at the destination. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:81354,checkpoint,checkpoint,81354,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['checkpoint'],['checkpoint']
Availability,"4 * (p**2) * (q**2) * ((X - 1) / X) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)) * (T / (T - 3)); ),; _e11=(; 2 * (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + 2 * p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e21=(; (p**3) * ((X - 1) / X) * ((X - 2) / X) * (T / (T - 1)) * (T / (T - 2)); + (q**3) * ((Y - 1) / Y) * ((Y - 2) / Y) * (T / (T - 1)) * (T / (T - 2)); + (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e22=1,; ). dataset = dataset.checkpoint(hl.utils.new_temp_file()). expectations = dataset.aggregate_rows(; hl.struct(; e00=hl.agg.sum(dataset._e00),; e10=hl.agg.sum(dataset._e10),; e20=hl.agg.sum(dataset._e20),; e11=hl.agg.sum(dataset._e11),; e21=hl.agg.sum(dataset._e21),; e22=hl.agg.sum(dataset._e22),; ); ). IS_HOM_REF = BlockMatrix.from_entry_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:6664,checkpoint,checkpoint,6664,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,2,['checkpoint'],['checkpoint']
Availability,"48]. Notes; This function requires that all values of array have the same length. If; two values have different lengths, then an exception will be thrown.; The f argument should be a function taking one argument, an expression of; the element type of array, and returning an expression including; aggregation(s). The type of the aggregated expression returned by; array_agg() is an array of elements of the return type of f. Parameters:. f – Aggregation function to apply to each element of the exploded array.; array (ArrayExpression) – Array to aggregate. Returns:; ArrayExpression. hail.expr.aggregators.downsample(x, y, label=None, n_divisions=500)[source]; Downsample (x, y) coordinate datapoints. Parameters:. x (NumericExpression) – X-values to be downsampled.; y (NumericExpression) – Y-values to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the distribution of values. In; particular, for any value x = values(i) in the summary, we estimate that; there are ranks(i) values strictly less than x, and that there are; ranks(i+1) values less than or equal to x. For any value y (not; necessarily in the summary), we estimate CDF(y) to be ranks(i), where i; is such that values(i-1) < y ≤ values",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:33530,down,downsample,33530,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['down'],['downsample']
Availability,6) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram infers min and max values automatically.; (#11317) Add support; for alpha aesthetic and identity position to; geom_histogram. Version 0.2.83; Release 2022-02-01. Bug fixes. (#11268) Fixed; log argument in hail.plot.histogram.; (#11276) Fixed; log argument in hail.plot.pdf.; (#11256) Fixed; memory leak in LD Prune. New,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:53202,error,errors,53202,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"63566. :param pedigree: Sample pedigree.; :type pedigree: :class:`~hail.representation.Pedigree`. :returns: Four tables with Mendel error statistics.; :rtype: (:class:`.KeyTable`, :class:`.KeyTable`, :class:`.KeyTable`, :class:`.KeyTable`); """""". kts = self._jvdf.mendelErrors(pedigree._jrep); return KeyTable(self.hc, kts._1()), KeyTable(self.hc, kts._2()), \; KeyTable(self.hc, kts._3()), KeyTable(self.hc, kts._4()). [docs] @handle_py4j; @typecheck_method(max_shift=integral); def min_rep(self, max_shift=100):; """"""; Gives minimal, left-aligned representation of alleles. Note that this can change the variant position. **Examples**. 1. Simple trimming of a multi-allelic site, no change in variant position; `1:10000:TAA:TAA,AA` => `1:10000:TA:T,A`. 2. Trimming of a bi-allelic site leading to a change in position; `1:10000:AATAA,AAGAA` => `1:10002:T:G`. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.minRep(max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(scores=strlike,; loadings=nullable(strlike),; eigenvalues=nullable(strlike),; k=integral,; as_array=bool); def pca(self, scores, loadings=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports prin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:161408,error,error,161408,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability,"7) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports PGT fields from GVCFs.; (#13805) Fix; (#13767).; hailctl dataproc submit now expands ~ in the --files and; --pyfiles arguments.; (#13797) Fix; (#13756). Operations; that collect large results such as to_pandas may require up to 3x; less memory.; (#13826) Fix; (#13793). Ensure; hailctl describe -u overrides the gcs_requester_pays/project; config variable.; (#13814) Fix; (#13757). Pipelines; that are memory-bound by copious use of hl.literal, such as; hl.vds.filter_intervals, require substantially less memory.; (#13894) Fix; (#13837) in which; Hail could break a Spark installation if the Hail JAR appears on the; c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:22817,error,error,22817,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"7,2,6:15:45:99,50,99,0,45,99. split_multi will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT field is downcoded once for each; alternate allele. A call for an alternate allele maps to 1 in; the biallelic variant corresponding to itself and 0; otherwise. For example, in the example above, 0/2 maps to 0/0; and 0/1. The genotype 1/2 maps to 0/1 and 0/1. The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the; sum of the other multiallelic entries. The biallelic DP is the same as the multiallelic DP. The biallelic PL entry for for a genotype g is the minimum; over PL entries for multiallelic genotypes that downcode to; g. For example, the PL for (A, T) at 0/1 is the minimum of the; PLs for 0/1 (50) and 1/2 (45), and thus 45. Fixing an alternate allele and biallelic variant, downcoding; gives a map from multiallelic to biallelic alleles and; genotypes. The biallelic AD entry for an allele is just the; sum of the multiallelic AD entries for alleles that map to; that allele. Similarly, the biallelic PL entry for a genotype; is the minimum over multiallelic PL entries for genotypes that; map to that genotype. By default, GQ is recomputed from PL. If ``propagate_gq=True``; is passed, the biallelic GQ field is simply the multiallelic; GQ field, that is, genotype qualities are unchanged. Here is a second example for a het non-ref. .. code-block:: text. A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as. .. code-block:: text. A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. **VCF Info Fields**. Hail does not split annotations in the info field. This means; that if a multiallelic site with ``info.AC`` value ``[10, 2]`` is; split, each split site will contain the same array ``[10,; 2]``. The provided allele index annotation ``va.aIndex`` can be used; to select the value cor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:209584,down,downcoding,209584,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['down'],['downcoding']
Availability,"7. >>> hl.eval(hl.pchisqtail(5, 1, lower_tail=True)); 0.9746526813225317. >>> hl.eval(hl.pchisqtail(5, 1, log_p=True)); -3.6750823266311876. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; The value at which to evaluate the CDF.; df : float or :class:`.Expression` of type :py:data:`.tfloat64`; Degrees of freedom.; ncp: float or :class:`.Expression` of type :py:data:`.tfloat64`; Noncentrality parameter, defaults to 0 if unspecified.; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; if ncp is None:; return _func(""pchisqtail"", tfloat64, x, df, lower_tail, log_p); else:; return _func(""pnchisqtail"", tfloat64, x, df, ncp, lower_tail, log_p). PGENCHISQ_RETURN_TYPE = tstruct(value=tfloat64, n_iterations=tint32, converged=tbool, fault=tint32). [docs]@typecheck(; x=expr_float64,; w=expr_array(expr_float64),; k=expr_array(expr_int32),; lam=expr_array(expr_float64),; mu=expr_float64,; sigma=expr_float64,; max_iterations=nullable(expr_int32),; min_accuracy=nullable(expr_float64),; ); def pgenchisq(x, w, k, lam, mu, sigma, *, max_iterations=None, min_accuracy=None) -> Float64Expression:; r""""""The cumulative probability function of a `generalized chi-squared distribution; <https://en.wikipedia.org/wiki/Generalized_chi-squared_distribution>`__. The generalized chi-squared distribution has many interpretations. We share here four; interpretations of the values of this distribution:. 1. A linear combination of normal variables and squares of normal variables. 2. A weighted sum of sums of squares of normally distributed values plus a normally distributed; value. 3. A weighted sum of chi-squared distributed values plus a normally distributed value. 4. A `""quadratic form"" <https://en.wik",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:64237,fault,fault,64237,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['fault'],['fault']
Availability,"71/journal.pgen.0020190>`__. (The resulting amplification of signal from the low end of the allele frequency spectrum will also introduce noise for rare variants; common practice is to filter out variants with minor allele frequency below some cutoff.) The factor :math:`1/m` gives each sample row approximately unit total variance (assuming linkage equilibrium) so that the diagonal entries of the GRM are approximately 1. Equivalently,; ; .. math::. G_{ik} = \\frac{1}{m} \\sum_{j=1}^m \\frac{(C_{ij}-2p_j)(C_{kj}-2p_j)}{2 p_j (1-p_j)} ; ; :return: Genetic Relatedness Matrix for all samples.; :rtype: :py:class:`KinshipMatrix`; """""". jkm = self._jvdf.grm(); return KinshipMatrix(jkm). [docs] @handle_py4j; @requireTGenotype; def hardcalls(self):; """"""Drop all genotype fields except the GT field. .. include:: requireTGenotype.rst. A hard-called variant dataset is about two orders of magnitude; smaller than a standard sequencing dataset. Use this; method to create a smaller, faster; representation for downstream processing that only; requires the GT field. :return: Variant dataset with no genotype metadata.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.hardCalls()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(maf=nullable(strlike),; bounded=bool,; min=nullable(numeric),; max=nullable(numeric)); def ibd(self, maf=None, bounded=True, min=None, max=None):; """"""Compute matrix of identity-by-descent estimations. .. include:: requireTGenotype.rst. **Examples**. To calculate a full IBD matrix, using minor allele frequencies computed; from the variant dataset itself:. >>> vds.ibd(). To calculate an IBD matrix containing only pairs of samples with; ``PI_HAT`` in [0.2, 0.9], using minor allele frequencies stored in; ``va.panel_maf``:. >>> vds.ibd(maf='va.panel_maf', min=0.2, max=0.9). **Notes**. The implementation is based on the IBD algorithm described in the `PLINK; paper <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950838>`__. :py:m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:81389,down,downstream,81389,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['down'],['downstream']
Availability,"7262169 | [""T"",""C""] | NA | NA | NA | NA |; +---------------+------------+-------+-------+----------+----------+. Export variants with p-values below 0.001:; >>> tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; The; transmission disequilibrium test; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. \[(t - u)^2 \over (t + u)\]; and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis.; transmission_disequilibrium_test() only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by in_autosome(), and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. Auto – in autosome or in PAR of X or female child; HemiX – in non-PAR of X and male child. Here PAR is the pseudoautosomal region; of X and Y defined by ReferenceGenome, which many variant callers; map to chromosome X. Kid; Dad; Mom; Copy State; t; u. HomRef; Het; Het; Auto; 0; 2. HomRef; HomRef; Het; Auto; 0; 1. HomRef; Het; HomRef; Auto; 0; 1. Het; Het; Het; Auto; 1; 1. Het; HomRef; Het; Auto; 1; 0. Het; Het; HomRef; Auto; 1; 0. Het; HomVar; Het; Auto; 0; 1. Het; Het; HomVar; Auto; 0; 1. HomVar; Het; Het; Auto; 2; 0. HomVar; Het; HomVar; Auto; 1; 0. HomVar; HomVar; Het; Auto; 1; 0. HomRef; HomRef; Het; HemiX; 0; 1. HomRef; HomVar; Het; HemiX; 0; 1. HomVar; HomRef; Het; HemiX; 1; 0. HomVar; HomVar; Het; HemiX; 1; 0. transmission_disequilibrium_test() produces a table with the following columns:. locus (tl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:95698,error,errors,95698,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability,"8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compile",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:72465,reliab,reliability,72465,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['reliab'],['reliability']
Availability,"9)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#6378) Fixed problem; in how entry_float_type was being handled in import_vcf. Version 0.2.15; Released 2019-06-14; After some infrastructural changes to our development process, we should; be getting back to frequent releases. hailctl; Starting in 0.2.15, pip installations of Hail come bundled with a; command- line tool, hailctl. This tool subsumes the functionality of; cloudtools, which is now deprecated. See the release thread on the; forum; for more information. New features. (#5932)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it pos",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:91843,failure,failures,91843,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['failure'],['failures']
Availability,"99 ./. 0/0:99; 1:2:G:C GT:GQ 0/1:89 0/1:99 1/1:93. Then; >>> kt = vds.make_table('v = v', ['gt = g.gt', 'gq = g.gq']). returns a KeyTable with schema; v: Variant; A.gt: Int; A.gq: Int; B.gt: Int; B.gq: Int; C.gt: Int; C.gq: Int. and values; v A.gt A.gq B.gt B.gq C.gt C.gq; 1:1:A:T 1 99 NA NA 0 99; 1:2:G:C 1 89 1 99 2 93. The above table can be generated and exported as a TSV using KeyTable export().; Notes; Per sample field names in the result are formed by; concatenating the sample ID with the genotype_expr left; hand side with separator. If the left hand side is empty:; `` = expr. then the dot (.) is omitted. Parameters:; variant_expr (str or list of str) – Variant annotation expressions.; genotype_expr (str or list of str) – Genotype annotation expressions.; key (str or list of str) – List of key columns.; separator (str) – Separator to use between sample IDs and genotype expression left-hand side identifiers. Return type:KeyTable. mendel_errors(pedigree)[source]¶; Find Mendel errors; count per variant, individual and nuclear; family. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:; >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped). Export all mendel errors to a text file:; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:123160,error,errors,123160,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,": :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht.select_globals(); return ht; mt = matrix_table_source('skat/x', x); raise_unless_entry_indexed('skat/x', x). analyze('skat/key_expr', key_expr, mt._row_indices); analyze('skat/weight_expr', weight_expr, mt._row_indices); analyze('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariate",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:107343,fault,fault,107343,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability,": array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 or vd_col_key.types[0] != hl.tstr:; error(f""expect variant data to have a single col key of type string, found {vd_col_key}""). if 'END' not in rd.entry or rd.END.dtype != hl.tint32:; error(""expect field 'END' in entry of reference data with type int32""). if check_data:; # check cols; ref_cols = rd.col_key.collect(); var_cols = vd.col_key.collect(); if len(ref_cols) != len(var_cols):; error(; f""mismatch in number of columns: reference data has {ref_cols} columns, variant data has {var_cols} columns""; ). if ref_cols != var_cols:; first_mismatch = 0; while ref_cols[first_mismatch] == var_cols[first_mismatch]:; first_mismatch += 1; error(; f""mismatch in columns keys: ref={ref_cols[first_mismatch]}, var={var_cols[first_mismatch]} at position {first_mismatch}""; ). # check locus distinctness; n_rd_rows = rd.count_rows(); n_distinct = rd.distinct_by_row().count_rows(). if n_distinct != n_rd_rows:; error(f'reference data loci are not distinct: found {n_rd_rows} rows, but {n_distinct} distinct loci'). # check END field; end_exprs = dict(; missing_end=hl.agg.filter(hl.is_missing(rd.END), hl.agg.take((rd.row_key, rd.col_key), 5)),; end_before_position=hl.agg.filter(rd.END < rd.locus.position, hl.agg.take((rd.row_key, rd.col_key), 5)),; ); if VariantDataset.ref_block_max_length_field in rd.globals:; rbml = rd[VariantDataset.ref_block_max_length_field]; end_exprs['blocks_too_long'] = hl.agg.filter(; rd.END - rd.locus.position + 1 > rbml, hl.agg.take((rd.row_key, rd.col_key), 5); ). res = rd.aggregate_entries(hl.struct(**end_exprs)). if res.missing_end:; error(; 'found records in reference data with missing END field\n '; + '",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:10091,error,error,10091,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['error'],['error']
Availability,": cdf._compaction_counts[i] * (2 ** (2 * i))); ); return s / (cdf.ranks[-1] ** 2). def update_grid_size(p, s):; return 4 * hl.sqrt(hl.log(2 * p / failure_prob) / (2 * s)). def compute_grid_size(s):; return hl.fold(lambda p, i: update_grid_size(p, s), 1 / failure_prob, hl.range(0, 5)). def compute_single_error(s, failure_prob=failure_prob):; return hl.sqrt(hl.log(2 / failure_prob) * s / 2). def compute_global_error(s):; return hl.rbind(compute_grid_size(s), lambda p: 1 / p + compute_single_error(s, failure_prob / p)). if all_quantiles:; return hl.rbind(cdf, lambda cdf: hl.rbind(compute_sum(cdf), compute_global_error)); else:; return hl.rbind(cdf, lambda cdf: hl.rbind(compute_sum(cdf), compute_single_error)). def _error_from_cdf_python(cdf, failure_prob, all_quantiles=False):; """"""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :obj:`dict`; Result of :func:`.approx_cdf` aggregator, evaluated to a python dict; failure_prob: :obj:`float`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :obj:`float`; Upper bound on error of quantile estimates.; """"""; import math. s = 0; for i in builtins.range(builtins.len(cdf._compaction_counts)):; s += cdf._compaction_counts[i] << (2 * i); s = s / (cdf.ranks[-1] ** 2). def update_grid_size(p):; return 4 * math.sqrt(math.log(2 * p / failure_prob) / (2 * s)). def compute_grid_size(s):; p = 1 / failure_prob; for _ in builtins.range(5):; p = update_grid_size(p); return p. def compute_single_error(s, failure_prob=failure_prob):; return math.sqrt(math.log(2 / failure_prob) * s / 2). if s == 0:; # no compactions ergo no error; return 0; elif all_quantiles:; p = compute_grid_size(s); return 1 / p + compute_single_error(s, failure_prob / p); else:; return compute_single_error(s, failure_prob). [docs]@typecheck(t=h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:6807,error,error,6807,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,4,['error'],['error']
Availability,"://my-bucket/df.csv', 'w') as f: # doctest: +SKIP; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). .. caution::. These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use :func:`.hadoop_copy`; to move your file to a distributed file system. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; # pile of hacks to preserve some legacy behavior, like auto gzip; fs = Env.fs(); if isinstance(fs, HadoopFS):; return fs.legacy_open(path, mod",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:2235,error,error,2235,docs/0.2/_modules/hail/utils/hadoop_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html,2,['error'],['error']
Availability,":; """"""Compute value for a single entry in dominance encoding of genotype matrix,; given the number of alternate alleles from the genotype matrix and the; estimated individual-specific allele frequency. Parameters; ----------; g : :class:`.Float64Expression`; Alternate allele count.; mu : :class:`.Float64Expression`; Estimated individual-specific allele frequency. Returns; -------; gd : :class:`.Float64Expression`; Dominance-coded entry for dominance-coded genotype matrix.; """"""; gd = (; hl.case(); .when(hl.is_nan(mu), 0.0); .when(g == 0.0, mu); .when(g == 1.0, 0.0); .when(g == 2.0, 1 - mu); .or_error('entries in genotype matrix must be 0.0, 1.0, or 2.0'); ); return gd. def _AtB_plus_BtA(A: BlockMatrix, B: BlockMatrix) -> BlockMatrix:; """"""Compute `(A.T @ B) + (B.T @ A)`, used in estimating IBD0 (k0). Parameters; ----------; A : :class:`.BlockMatrix`; B : :class:`.BlockMatrix`. Returns; -------; :class:`.BlockMatrix`; `(A.T @ B) + (B.T @ A)`; """"""; temp = (A.T @ B).checkpoint(new_temp_file()); return temp + temp.T. def _replace_nan(M: BlockMatrix, value: float) -> BlockMatrix:; """"""Replace NaN entries in a dense :class:`.BlockMatrix` with provided value. Parameters; ----------; M: :class:`.BlockMatrix`; value: :obj:`float`; Value to replace NaN entries with. Returns; -------; :class:`.BlockMatrix`; """"""; return M._map_dense(lambda x: hl.if_else(hl.is_nan(x), value, x)). @typecheck(; call_expr=expr_call,; min_individual_maf=numeric,; k=nullable(int),; scores_expr=nullable(expr_array(expr_float64)),; min_kinship=nullable(numeric),; statistics=enumeration('kin', 'kin2', 'kin20', 'all'),; block_size=nullable(int),; include_self_kinship=bool,; ); def _pc_relate_bm(; call_expr: CallExpression,; min_individual_maf: float,; *,; k: Optional[int] = None,; scores_expr: Optional[ArrayNumericExpression] = None,; min_kinship: Optional[float] = None,; statistics: str = ""all"",; block_size: Optional[int] = None,; include_self_kinship: bool = False,; ) -> Table:; assert 0.0 <= min_individu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:16393,checkpoint,checkpoint,16393,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['checkpoint'],['checkpoint']
Availability,":; raise ValueError(; ""'mendel_errors': expected 'call' to be an expression of 'MatrixTable', found {}"".format(; ""expression of '{}'"".format(source.__class__) if source is not None else 'scalar expression'; ); ). source = source.select_entries(__GT=call); dataset = require_biallelic(source, 'mendel_errors'); tm = trio_matrix(dataset, pedigree, complete_trios=True); tm = tm.select_entries(; mendel_code=hl.mendel_error_code(; tm.locus, tm.is_female, tm.father_entry['__GT'], tm.mother_entry['__GT'], tm.proband_entry['__GT']; ); ); ck_name = next(iter(source.col_key)); tm = tm.filter_entries(hl.is_defined(tm.mendel_code)); tm = tm.rename({'id': ck_name}). entries = tm.entries(). table1 = entries.select('fam_id', 'mendel_code'). t2 = tm.annotate_cols(errors=hl.agg.count(), snp_errors=hl.agg.count_where(hl.is_snp(tm.alleles[0], tm.alleles[1]))); table2 = t2.key_cols_by().cols(); table2 = table2.select(; pat_id=table2.father[ck_name],; mat_id=table2.mother[ck_name],; fam_id=table2.fam_id,; errors=table2.errors,; snp_errors=table2.snp_errors,; ); table2 = table2.group_by('pat_id', 'mat_id').aggregate(; fam_id=hl.agg.take(table2.fam_id, 1)[0],; children=hl.int32(hl.agg.count()),; errors=hl.agg.sum(table2.errors),; snp_errors=hl.agg.sum(table2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = ta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:10933,error,errors,10933,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,":`\mathrm{span}(U) = \mathcal{K}_p(AA^T, AV_0)`; * :math:`V[:, :b] = V_0`; * :math:`R\in\mathbb{R}^{b\times b}` is upper triangular; where :math:`\mathcal{K}_p(X, Y)` is the block Krylov subspace; :math:`\mathrm{span}(Y, XY, \dots, X^pY)`. Parameters; ----------; A_expr; V0; p; compute_U. Returns; -------. """"""; t = A.block_table; A_expr = A.block_expr. g_list = [V0]; G_i = V0; k = hl.eval(V0.shape[1]). for j in range(0, p):; info(f""krylov_factorization: Beginning iteration {j+1}/{p}""); G_i = t.aggregate(hl.agg.ndarray_sum(A_expr.T @ (A_expr @ G_i)), _localize=False); G_i = hl.nd.qr(G_i)[0]._persist(); g_list.append(G_i). info(""krylov_factorization: Iterations complete. Computing local QR""); V0 = hl.nd.hstack(g_list). if compute_V:; V = hl.nd.qr(V0)[0]._persist(); t = t.annotate(AV=A_expr @ V); else:; V = hl.nd.qr(V0)[0]; t = t.annotate(AV=A_expr @ V); V = None. if compute_U:; temp_file_name = hl.utils.new_temp_file(""_krylov_factorization_intermediate"", ""ht""); t = t.checkpoint(temp_file_name); AV_local = t.aggregate(hl.nd.vstack(hl.agg.collect(t.AV)), _localize=False); U, R = hl.nd.qr(AV_local)._persist(); else:; Rs = t.aggregate(hl.nd.vstack(hl.agg.collect(hl.nd.qr(t.AV)[1])), _localize=False); R = hl.nd.qr(Rs)[1]._persist(); U = None. return KrylovFactorization(U, R, V, k). def _reduced_svd(A: TallSkinnyMatrix, k=10, compute_U=False, iterations=2, iteration_size=None):; # Set Parameters; q = iterations; if iteration_size is None:; L = k + 2; else:; L = iteration_size; assert (q + 1) * L >= k; n = A.ncols. # Generate random matrix G; G = hl.rand_norm(0, 1, size=(n, L)); G = hl.nd.qr(G)[0]._persist(). fact = _krylov_factorization(A, G, q, compute_U); info(""_reduced_svd: Computing local SVD""); return fact.reduced_svd(k). @typecheck(; A=oneof(expr_float64, TallSkinnyMatrix), num_moments=int, p=nullable(int), moment_samples=int, block_size=int; ); def _spectral_moments(A, num_moments, p=None, moment_samples=500, block_size=128):; if not isinstance(A, TallSkinnyMatrix):; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:13881,checkpoint,checkpoint,13881,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['checkpoint'],['checkpoint']
Availability,":`str`. Returns; -------; :obj:`.bool`; """"""; return _fses[requester_pays_config].is_dir(path). [docs]def stat(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> FileListEntry:; """"""Returns information about the file or directory at a given path. Notes; -----; Raises an error if `path` does not exist. The resulting dictionary contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`dict`; """"""; return _fses[requester_pays_config].stat(path). [docs]def ls(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> List[FileListEntry]:; """"""Returns information about files at `path`. Notes; -----; Raises an error if `path` does not exist. If `path` is a file, returns a list with one element. If `path` is a; directory, returns an element for each file contained in `path` (does not; search recursively). Each dict element of the result list contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return _fses[requester_pays_config].ls(path). [docs]def mkdir(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Ensure files can be created whose dirname is `path`. Warning; -------. On file systems without a notion of directories, this function will do nothing. For example,; on Google Cloud Storage, this operation does",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:6123,error,error,6123,docs/0.2/_modules/hailtop/fs/fs_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html,2,['error'],['error']
Availability,":class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6553,error,error,6553,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['error']
Availability,":math:`1/m`, which gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the :math:`n \\times n` sample correlation or realized relationship matrix (RRM) :math:`K` as simply. .. math::. K = MM^T. Note that the only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in :py:meth:`~hail.VariantDataset.grm` is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. :param bool force_block: Force using Spark's BlockMatrix to compute kinship (advanced). :param bool force_gramian: Force using Spark's RowMatrix.computeGramian to compute kinship (advanced). :return: Realized Relationship Matrix for all samples.; :rtype: :py:class:`KinshipMatrix`; """"""; return KinshipMatrix(self._jvdf.rrm(force_block, force_gramian)). [docs] @handle_py4j; @typecheck_method(other=vds_type,; tolerance=numeric); def same(self, other, tolerance=1e-6):; """"""True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values. **Examples**. This will return True:. >>> vds.same(vds). **Notes**. The ``tolerance`` parameter sets the tolerance for equality when comparing floating-point fields. More precisely, :math:`x` and :math:`y` are equal if. .. math::. \abs{x - y} \leq tolerance * \max{\abs{x}, \abs{y}}. :param other: variant dataset to compare against; :type other: :class:`.VariantDataset`. :param float tolerance: floating-point tolerance for equality. :rtype: bool; """""". return self._jvds.same(other._jvds, tolerance). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(root=strlike,; keep_star=bool); def sample_qc(self, root='sa.qc', keep_star=False):; """"""Compute per-sample QC metrics. .. include:: requireTGenotype.rst. **Annotations**. :py:meth:`~hail.VariantDataset.sample_qc` computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:196170,toler,tolerance,196170,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['toler'],['tolerance']
Availability,":obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. """"""; if max_iterations is None:; max_iterations = hl.literal(10_000); if min_accuracy is None:; min_accuracy = hl.literal(1e-5); return _func(""pgenchisq"", PGENCHISQ_RETURN_TYPE, x - mu, w, k, lam, sigma, max_iterations, min_accuracy). [docs]@typecheck(x=expr_float64, mu=expr_float64, sigma=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False) -> Float64Expression:; """"""The cumulative probability function of a normal distribution with mean; `mu` and standard deviation `sigma`. Returns cumulative probability of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=Fa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:71296,fault,fault,71296,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['fault'],['fault']
Availability,"; (#13885) Hail; Query-on-Batch previously used Class A Operations for all interaction; with blobs. This change ensures that QoB only uses Class A Operations; when necessary.; (#14127); hailctl dataproc start ... --dry-run now uses shell escapes such; that, after copied and pasted into a shell, the gcloud command; works as expected.; (#14062) Fix; (#14052) which; caused incorrect results for identity by descent in Query-on-Batch.; (#14122) Ensure that; stack traces are transmitted from workers to the driver to the; client.; (#14105) When a VCF; contains missing values in array fields, Hail now suggests using; array_elements_required=False. Deprecations. (#13987) Deprecate; default_reference parameter to hl.init, users should use; hl.default_reference with an argument to set new default; references usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:20620,error,errors,20620,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now; eagerly deletes temporary files produced by some BlockMatrix; operations.; (#9835) hl.any; and hl.all now also support a single collection argument and a; varargs of Boolean expressions.; (#9816); hl.pc_relate now includes values on the diagonal of kinship,; IBD-0, IBD-1, and IBD-2; (#9736) Let; NDArrayExpression.reshape take varargs instead of mandating a tuple.; (#9766); hl.export_vcf now warns if INFO field names are invalid according; to the VCF 4.3 spec. Bug fixes. (#9976) Fixed; show() representation of Hail dictionaries. Performance improvements. (#9909) Improved; performance of hl.experimental.densify by approximately 35%. Version 0.2.61; Released 2020-12-03. New features. (#9749) Add or_error; method to SwitchBuilder (hl.switch). Bug fixes. (#9775) Fixed race; condition leading to invalid intermediate files in VCF combiner.; (#9751) Fix bug where; constructing an array of empty structs causes type error.; (#9731) Fix error and; incorrect behavior when using hl.import_matrix_table with int64; data types. Version 0.2.60; Released 2020-11-16. New features. (#9696); hl.experimental.export_elasticsearch will now support; Elasticsearch versions 6.8 - 7.x by default. Bug fixes. (#9641) Showing hail; ndarray data now always prints in correct order. hailctl dataproc. (#9610) Support; interval fields in hailctl dataproc describe. Version 0.2.59; Released 2020-10-22. Datasets / Annotation DB. (#9605) The Datasets; API and the Annotation Database now support AWS, and users are; required to specify what cloud platform they’re using. hailctl dataproc. (#9609) Fixed bug; where hailctl dataproc modify did not correctly print; corresponding gcloud command. Version 0.2.58; Released 2020-10-08. New features. (#9524) Hail should; now be buildable using Spark 3.0.; (#9549) Add; ignore_in_sample_frequency flag to hl.de_novo.; (#9501) Configurable; cache siz",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:62875,error,error,62875,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"; ),; _e11=(; 2 * (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + 2 * p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e21=(; (p**3) * ((X - 1) / X) * ((X - 2) / X) * (T / (T - 1)) * (T / (T - 2)); + (q**3) * ((Y - 1) / Y) * ((Y - 2) / Y) * (T / (T - 1)) * (T / (T - 2)); + (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e22=1,; ). dataset = dataset.checkpoint(hl.utils.new_temp_file()). expectations = dataset.aggregate_rows(; hl.struct(; e00=hl.agg.sum(dataset._e00),; e10=hl.agg.sum(dataset._e10),; e20=hl.agg.sum(dataset._e20),; e11=hl.agg.sum(dataset._e11),; e21=hl.agg.sum(dataset._e21),; e22=hl.agg.sum(dataset._e22),; ); ). IS_HOM_REF = BlockMatrix.from_entry_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.u",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:6763,checkpoint,checkpoint,6763,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,2,['checkpoint'],['checkpoint']
Availability,"; + "" requires positive bin size.""; ),; hl.float64(e - s) / nbins,; ),; ); .or_error(hl.literal(""'hist' requires positive 'bins', but bins="") + hl.str(nbins)); ). if _result_from_hist_agg_f is None:; _result_from_hist_agg_f = hl.experimental.define_function(; wrap_errors, hl.tfloat64, hl.tfloat64, hl.tint32, hl.tdict(hl.tint32, hl.tint64); ). return _result_from_hist_agg_f(start, end, bins, freq_dict). [docs]@typecheck(x=expr_float64, y=expr_float64, label=nullable(oneof(expr_str, expr_array(expr_str))), n_divisions=int); def downsample(x, y, label=None, n_divisions=500) -> ArrayExpression:; """"""Downsample (x, y) coordinate datapoints. Parameters; ----------; x : :class:`.NumericExpression`; X-values to be downsampled.; y : :class:`.NumericExpression`; Y-values to be downsampled.; label : :class:`.StringExpression` or :class:`.ArrayExpression`; Additional data for each (x, y) coordinate. Can pass in multiple fields in an :class:`.ArrayExpression`.; n_divisions : :obj:`int`; Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns; -------; :class:`.ArrayExpression`; Expression for downsampled coordinate points (x, y). The element type of the array is; :class:`.ttuple` of :py:data:`.tfloat64`, :py:data:`.tfloat64`, and :class:`.tarray` of :py:data:`.tstr`; """"""; if label is None:; label = hl.missing(hl.tarray(hl.tstr)); elif isinstance(label, StringExpression):; label = hl.array([label]); return _agg_func(; 'Downsample', [x, y, label], tarray(ttuple(tfloat64, tfloat64, tarray(tstr))), init_op_args=[n_divisions]; ). @typecheck(expr=expr_any, n=expr_int32); def _reservoir_sample(expr, n):; return _agg_func('ReservoirSample', [expr], tarray(expr.dtype), [n]). [docs]@typecheck(gp=expr_array(expr_float64)); def info_score(gp) -> StructExpression:; r""""""Compute the IMPUTE information score. Examples; --------; Calculate the info score per variant:. >>> gen_mt = hl.import_gen('data/example.gen', sample_file='data/example.sa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:44548,down,downsample,44548,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['down'],['downsample']
Availability,"; ; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). .. note::. A :py:class:`.KeyTable` keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for :py:meth:`.filter_variants_table` for an example. This is useful for; using interval files to filter a dataset. :param intervals: Interval(s) to keep or remove.; :type intervals: :class:`.Interval` or list of :class:`.Interval`. :param bool keep: Keep variants overlapping an interval if ``True``, remove variants overlapping; an interval if ``False``. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". intervals = wrap_to_list(intervals). jvds = self._jvds.filterIntervals([x._jrep for x in intervals], keep); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(variants=listof(Variant),; keep=bool); def filter_variants_list(self, variants, keep=True):; """"""Filter variants with a list of variants. **Examples**. Filter VDS down to a list of variants:. >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True); ; **Notes**. This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap with any supplied variant will not be loaded at all. This property; enables ``filter_variants_list`` to be used for reasonably low-latency queries of one; or more variants, even on large datasets. ; ; :param variants: List of variants to keep or remove.; :type variants: list of :py:class:`~hail.representation.Variant`. :param bool keep: If true, keep variants in ``variants``, otherwise remove them. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(; self.hc, self._jvds.filterVariantsList(; [TVariant()._convert_to_j(v) for v in variants], keep)). [docs] @handle_py4j; @typecheck_method(table=KeyTable,; keep=bool); def filter_variants_table(self, table, keep=True):; """"""Fil",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:75968,down,down,75968,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['down'],['down']
Availability,"; >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each job j uses an f-string; to create a temporary output file j.ofile where the output to echo is redirected.; We then use all of the output files in the sink command by creating a string; with the temporary output file names for each job. A JobResourceFile; is a Batch-specific object that inherits from str. Therefore, you can use; JobResourceFile as if they were strings, which we do with the join; command for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; We can also create a nested scatter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new Batch object b. Then for each user in ‘Alice’, ‘Bob’, and ‘Dan’; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The do_chores function takes a Batch ob",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:8231,echo,echo,8231,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability,"; A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi_hts() will create two biallelic variants (one for each; alternate allele) at the same position; A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT or PGT field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and 0/1.; The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the sum of the; other multiallelic entries.; The biallelic DP is the same as the multiallelic DP.; The biallelic PL entry for a genotype g is the minimum over PL entries; for multiallelic genotypes that downcode to g. For example, the PL for (A,; T) at 0/1 is the minimum of the PLs for 0/1 (50) and 1/2 (45), and thus 45.; Fixing an alternate allele and biallelic variant, downcoding gives a map; from multiallelic to biallelic alleles and genotypes. The biallelic AD entry; for an allele is just the sum of the multiallelic AD entries for alleles; that map to that allele. Similarly, the biallelic PL entry for a genotype is; the minimum over multiallelic PL entries for genotypes that map to that; genotype.; GQ is recomputed from PL if PL is provided and is not; missing. If not, it is copied from the original GQ.; Here is a second example for a het non-ref; A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as; A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. VCF Info Fields; Hail does not split fields in the info field. This means that if a; multiallelic site with info.AC value [10, 2] is split, each split; site will contain the same array [10, 2]. The provided allele index; field a_index can be used to select the value corresponding to the split; allele’s position:; >>> split_ds = hl.split_multi_hts(dataset); >>> split_ds = split_ds.filter_rows(split_ds.info.AC[split",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:89091,down,downcoding,89091,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['down'],['downcoding']
Availability,"; Construct a call expression by parsing a string or string expression.; Examples; >>> hl.eval(hl.parse_call('0|2')); Call(alleles=[0, 2], phased=True). Notes; This method expects strings in the following format:. ploidy; Phased; Unphased. 0; |-; -. 1; |i; i. 2; i|j; i/j. 3; i|j|k; i/j/k. N; i|j|k|...|N; i/j/k/.../N. Parameters:; s (str or StringExpression) – String to parse. Returns:; CallExpression. hail.expr.functions.downcode(c, i)[source]; Create a new call by setting all alleles other than i to ref; Examples; Preserve the third allele and downcode all other alleles to reference.; >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters:. c (CallExpression) – A call.; i (Expression of type tint32) – The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns:; CallExpression. hail.expr.functions.triangle(n)[source]; Returns the triangle number of n.; Examples; >>> hl.eval(hl.triangle(3)); 6. Notes; The calculation is n * (n + 1) / 2. Parameters:; n (Expression of type tint32). Returns:; Expression of type tint32. hail.expr.functions.is_snp(ref, alt)[source]; Returns True if the alleles constitute a single nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_mnp(ref, alt)[source]; Returns True if the alleles constitute a multiple nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_mnp('AA', 'GT')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_transition(ref, alt)[source]; Returns True if the alleles constitute a transi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:11929,down,downcoded,11929,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['down'],['downcoded']
Availability,"; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we’ll append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesn’t exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:9414,checkpoint,checkpoint,9414,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability,"; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:2288,down,downloaded,2288,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['down'],['downloaded']
Availability,"; For more information on the .gen file format, see here.; To ensure that the .gen file(s) and .sample file are correctly prepared for import:. If there are only 5 columns before the start of the genotype probability data (chromosome field is missing), you must specify the chromosome using the chromosome parameter; No duplicate sample IDs are allowed. The first column in the .sample file is used as the sample ID s.; Also, see section in import_bgen() linked here for information about Hail’s genotype probability representation.; Annotations; import_gen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .gen files to import.; sample_file (str) – The sample file.; tolerance (float) – If the sum of the genotype probabilities for a genotype differ from 1.0 by more than the tolerance, set the genotype to missing.; min_partitions (int or None) – Number of partitions.; chromosome (str or None) – Chromosome if not listed in the .gen file. Returns:Variant dataset imported from .gen and .sample files. Return type:VariantDataset. import_plink(bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quantpheno=False)[source]¶; Import PLINK binary file (BED, BIM, FAM) as variant dataset.; Examples; Import data from a PLINK binary file:; >>> vds = hc.import_plink(bed=""data/test.bed"",; ... bim=""data/test.bim"",; ... fam=""data/test.fam""). Notes; Only binary SNP-major mode files can be read into Hail. To convert your file from individual-major mode to SNP-major mode, use PLINK to read in your fileset and use the --make-bed option.; The centiMorgan position is not currently used in Hail (Column 3 in BIM file).; The ID (s) used by Hail is the individual ID (column 2 in FAM file). Warning; No duplicate individual IDs are allowed. Chromosome names (Column 1) are automa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:12124,toler,tolerance,12124,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['toler'],['tolerance']
Availability,"; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. Wh",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:22111,error,errors,22111,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"; Scatter; 2-D histogram; Q-Q (Quantile-Quantile); Manhattan. GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Plotting Tutorial. View page source. Plotting Tutorial; The Hail plot module allows for easy plotting of data. This notebook contains examples of how to use the plotting functions in this module, many of which can also be found in the first tutorial. [1]:. import hail as hl; hl.init(). from bokeh.io import show; from bokeh.layouts import gridplot. Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2012-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_1kg('data/'); mt = hl.read_matrix_table('data/1kg.mt'); table = (hl.import_table('data/1kg_annotations.txt', impute=True); .key_by('Sample')); mt = mt.annotate_cols(**table[mt.s]); mt = hl.sample_qc(mt). mt.describe(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; 'Population': str; 'SuperPopulation': str; 'isFemale': bool; 'PurpleHair': bool; 'CaffeineConsumption': int32; 'sample_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/08-plotting.html:1353,avail,available,1353,docs/0.2/tutorials/08-plotting.html,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html,1,['avail'],['available']
Availability,"; a way to checkpoint the results so we only run jobs that haven’t already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we’ll append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesn’t exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file alread",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:9093,checkpoint,checkpoint,9093,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability,"; bin_edges=hl.range(0, nbins + 1).map(lambda i: s + i * bs),; bin_freq=hl.range(0, nbins).map(lambda i: freq_dict.get(i, 0)),; n_smaller=freq_dict.get(-1, 0),; n_larger=freq_dict.get(nbins, 0),; ). def wrap_errors(s, e, nbins, freq_dict):; return (; hl.case(); .when(; nbins > 0,; hl.bind(; lambda bs: hl.case(); .when((bs > 0) & hl.is_finite(bs), result(s, nbins, bs, freq_dict)); .or_error(; ""'hist': start=""; + hl.str(s); + "" end=""; + hl.str(e); + "" bins=""; + hl.str(nbins); + "" requires positive bin size.""; ),; hl.float64(e - s) / nbins,; ),; ); .or_error(hl.literal(""'hist' requires positive 'bins', but bins="") + hl.str(nbins)); ). if _result_from_hist_agg_f is None:; _result_from_hist_agg_f = hl.experimental.define_function(; wrap_errors, hl.tfloat64, hl.tfloat64, hl.tint32, hl.tdict(hl.tint32, hl.tint64); ). return _result_from_hist_agg_f(start, end, bins, freq_dict). [docs]@typecheck(x=expr_float64, y=expr_float64, label=nullable(oneof(expr_str, expr_array(expr_str))), n_divisions=int); def downsample(x, y, label=None, n_divisions=500) -> ArrayExpression:; """"""Downsample (x, y) coordinate datapoints. Parameters; ----------; x : :class:`.NumericExpression`; X-values to be downsampled.; y : :class:`.NumericExpression`; Y-values to be downsampled.; label : :class:`.StringExpression` or :class:`.ArrayExpression`; Additional data for each (x, y) coordinate. Can pass in multiple fields in an :class:`.ArrayExpression`.; n_divisions : :obj:`int`; Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns; -------; :class:`.ArrayExpression`; Expression for downsampled coordinate points (x, y). The element type of the array is; :class:`.ttuple` of :py:data:`.tfloat64`, :py:data:`.tfloat64`, and :class:`.tarray` of :py:data:`.tstr`; """"""; if label is None:; label = hl.missing(hl.tarray(hl.tstr)); elif isinstance(label, StringExpression):; label = hl.array([label]); return _agg_func(; 'Downsample', [x, y, label], tarray(ttuple",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:44073,down,downsample,44073,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,3,"['Down', 'down']","['Downsample', 'downsample']"
Availability,"; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def from_entry_expr(; cls, entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None; ):; """"""Creates a block matrix using a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; -----; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use :meth:`write_from_entry_expr` directly to; avoid writing the result twice. See :meth:`write_from_entry_expr` for; further documentation. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use :meth:`write_from_entry_expr` to write to external storage. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:14365,error,error,14365,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['error'],['error']
Availability,"; of :math:`c_i` divided by :math:`n`. Then the base-2 Shannon entropy is; given by. .. math::. H = \sum_{i=1}^k p_i \log_2(p_i). Parameters; ----------; s : :class:`.StringExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""entropy"", tfloat64, s). @typecheck(x=expr_any, trunc=nullable(expr_int32)); def _showstr(x, trunc=None):; if trunc is None:; return _func(""showStr"", tstr, x); return _func(""showStr"", tstr, x, trunc). [docs]@typecheck(x=expr_any); def str(x) -> StringExpression:; """"""Returns the string representation of `x`. Examples; --------. >>> hl.eval(hl.str(hl.struct(a=5, b=7))); '{""a"":5,""b"":7}'. Parameters; ----------; x. Returns; -------; :class:`.StringExpression`; """"""; if x.dtype == tstr:; return x; else:; return _func(""str"", tstr, x). [docs]@typecheck(c=expr_call, i=expr_int32); def downcode(c, i) -> CallExpression:; """"""Create a new call by setting all alleles other than i to ref. Examples; --------; Preserve the third allele and downcode all other alleles to reference. >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters; ----------; c : :class:`.CallExpression`; A call.; i : :class:`.Expression` of type :py:data:`.tint32`; The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns; -------; :class:`.CallExpression`; """"""; return _func(""downcode"", tcall, c, i). @typecheck(pl=expr_array(expr_int32)); def gq_from_pl(pl) -> Int32Expression:; """"""Compute genotype quality from Phred-scaled probability likelihoods. Examples; --------. >>> hl.eval(hl.gq_from_pl([0, 69, 1035])); 69. Parameters; ----------; pl : :class:`.Expression` of type :class:`.tarray` of :obj:`.tint32`. Returns; -------; :class:`.Expression` of type :py:data:`.tint32`; """"""; return _f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:105173,down,downcode,105173,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['down'],['downcode']
Availability,"; return hl.struct(values=values, ranks=ranks, _compaction_counts=raw_cdf._compaction_counts). @typecheck(k=expr_int32, left=expr_struct(), right=expr_struct()); def _cdf_combine(k, left, right):; t = tstruct(levels=tarray(tint32), items=tarray(tfloat64), _compaction_counts=tarray(tint32)); return _func('approxCDFCombine', t, k, left, right). @typecheck(cdf=expr_struct(), failure_prob=expr_oneof(expr_float32, expr_float64), all_quantiles=bool); def _error_from_cdf(cdf, failure_prob, all_quantiles=False):; """"""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :class:`.StructExpression`; Result of :func:`.approx_cdf` aggregator; failure_prob: :class:`.NumericExpression`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :class:`.NumericExpression`; Upper bound on error of quantile estimates.; """""". def compute_sum(cdf):; s = hl.sum(; hl.range(0, hl.len(cdf._compaction_counts)).map(lambda i: cdf._compaction_counts[i] * (2 ** (2 * i))); ); return s / (cdf.ranks[-1] ** 2). def update_grid_size(p, s):; return 4 * hl.sqrt(hl.log(2 * p / failure_prob) / (2 * s)). def compute_grid_size(s):; return hl.fold(lambda p, i: update_grid_size(p, s), 1 / failure_prob, hl.range(0, 5)). def compute_single_error(s, failure_prob=failure_prob):; return hl.sqrt(hl.log(2 / failure_prob) * s / 2). def compute_global_error(s):; return hl.rbind(compute_grid_size(s), lambda p: 1 / p + compute_single_error(s, failure_prob / p)). if all_quantiles:; return hl.rbind(cdf, lambda cdf: hl.rbind(compute_sum(cdf), compute_global_error)); else:; return hl.rbind(cdf, lambda cdf: hl.rbind(compute_sum(cdf), compute_single_error)). def _error_from_cdf_python(cdf, failure_prob, all_quantiles=False):; """"""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameter",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:5643,error,error,5643,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['error'],['error']
Availability,"; split_multi() adds the; following annotations:. va.wasSplit (Boolean) – true if this variant was; originally multiallelic, otherwise false.; va.aIndex (Int) – The original index of this; alternate allele in the multiallelic representation (NB: 1; is the first alternate allele or the only alternate allele; in a biallelic variant). For example, 1:100:A:T,C splits; into two variants: 1:100:A:T with aIndex = 1 and; 1:100:A:C with aIndex = 2. Parameters:; propagate_gq (bool) – Set the GQ of output (split); genotypes to be the GQ of the input (multi-allelic) variants; instead of recompute GQ as the difference between the two; smallest PL values. Intended to be used in conjunction with; import_vcf(store_gq=True). This option will be obviated; in the future by generic genotype schemas. Experimental.; keep_star_alleles (bool) – Do not filter out * alleles.; max_shift (int) – maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. Returns:A biallelic variant dataset. Return type:VariantDataset. storage_level()[source]¶; Returns the storage (persistence) level of the variant dataset.; Notes; See the Spark documentation for details on persistence levels. Return type:str. summarize()[source]¶; Returns a summary of useful information about the dataset. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; >>> s = vds.summarize(); >>> print(s.contigs); >>> print('call rate is %.2f' % s.call_rate); >>> s.report(). The following information is contained in the summary:. samples (int) - Number of samples.; variants (int) - Number of variants.; call_rate (float) - Fraction of all genotypes called.; contigs (list of str) - List of all unique contigs found in the dataset.; multiallelics (int) - Number of multiallelic variants.; snps (int) - Number of SNP alternate alleles.; mnps (int) - Number of MNP alternate alleles.; inse",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:166167,error,error,166167,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,"; | 1 | ""cat"" |; | 2 | ""dog"" |; | 3 | ""mouse"" |; | 4 | ""rabbit"" |; +-------+----------+. Using key as the sole index expression is equivalent to passing all; key fields individually:; >>> table_result = table1.select(B = table2.index(table1.key).B). It is also possible to use non-key fields or expressions as the index; expressions:; >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; Table.index() is used to expose one table’s fields for use in; expressions involving the another table or matrix table’s fields. The; result of the method call is a struct expression that is usable in the; same scope as exprs, just as if exprs were used to look up values of; the table in a dictionary.; The type of the struct expression is the same as the indexed table’s; row_value() (the key fields are removed, as they are available; in the form of the index expressions). Note; There is a shorthand syntax for Table.index() using square; brackets (the Python __getitem__ syntax). This syntax is preferred.; >>> table_result = table1.select(B = table2[table1.ID].B). Parameters:. exprs (variable-length args of Expression) – Index expressions.; all_matches (bool) – Experimental. If True, value of expression is array of all matches. Returns:; Expression. index_globals()[source]; Return this table’s global variables for use in another; expression context.; Examples; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns:; StructExpression. join(right, how='inner', _mangle=<function Table.<lambda>>, _join_key=None)[source]; Join two tables together.; Examples; Join table1 to table2 to produce table_joined:; >>> table_joined = table1.key_by('ID').join(table2.key_by('ID')). Notes; Tables are joined at rows whose key fields have equal",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:36052,avail,available,36052,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['avail'],['available']
Availability,"<gpfilters>` for information about Hail's genotype probability representation. **Annotations**. :py:meth:`~hail.HailContext.import_gen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .gen files to import.; :type path: str or list of str. :param str sample_file: The sample file. :param float tolerance: If the sum of the genotype probabilities for a genotype differ from 1.0 by more than the tolerance, set the genotype to missing. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param chromosome: Chromosome if not listed in the .gen file.; :type chromosome: str or None. :return: Variant dataset imported from .gen and .sample files.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importGens(jindexed_seq_args(path), sample_file, joption(chromosome), joption(min_partitions),; tolerance); return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(paths=oneof(strlike, listof(strlike)),; key=oneof(strlike, listof(strlike)),; min_partitions=nullable(int),; impute=bool,; no_header=bool,; comment=nullable(strlike),; delimiter=strlike,; missing=strlike,; types=dictof(strlike, Type),; quote=nullable(char)); def import_table(self, paths, key=[], min_partitions=None, impute=False, no_header=False,; comment=None, delimiter=""\t"", missing=""NA"", types={}, quote=None):; """"""Import delimited text file (text table) as key table. The resulting key table will have no key columns, use :py:meth:`.KeyTable.key_by`; to specify keys.; ; **Example**; ; Given this file. .. code-block:: text. $ cat data/samples1.tsv; Sample	Height	Status Age; PT-1234	154.1	ADHD	24; PT-1236	160.9	Control	19; PT-1238	NA	ADHD	89; PT-1239	170.3	Control	55. The interesting thing about this table is that column ``Height`` is a floating-point number, ; and column ``Age",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:10515,toler,tolerance,10515,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['toler'],['tolerance']
Availability,"= hl.literal({1, 3, 5}). See also; CollectionExpression. Attributes. dtype; The data type of the expression. Methods. add; Returns a new set including item. contains; Returns True if item is in the set. difference; Return the set of elements in the set that are not present in set s. intersection; Return the intersection of the set and set s. is_subset; Returns True if every element is contained in set s. remove; Returns a new set excluding item. union; Return the union of the set and set s. __and__(other)[source]; Return the intersection of the set and other.; Examples; >>> hl.eval(s1 & s2); {1, 3}. Parameters:; other (SetExpression) – Set expression of the same type. Returns:; SetExpression – Set of elements present in both the set and other. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other)[source]; Test whether every element in other is in the set. Parameters:; other (SetExpression) – Set expression of the same type. Returns:; BooleanExpression – True if every element in other is in the set. False otherwise. __gt__(other)[source]; Test whether other is a proper subset of the set (other <= set and other != set). Parameters:; other (SetExpression) – Set expression of the same type. Returns:; BooleanExpression – True if other is a proper subset of the set. False otherwise. __le__(other)[source]; Test whether every element in the set is in other. Parameters:; other (SetExpression) – Set expression of the same type. Returns:; BooleanExpression – True if every element in the set is in other. False otherwise. __lt__(other)[source]; Test whether the set is a proper subset of o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.SetExpression.html:1802,error,error,1802,docs/0.2/hail.expr.SetExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.SetExpression.html,1,['error'],['error']
Availability,"= list(range(dataset.count_cols())); >>> random.shuffle(indices); >>> dataset_reordered = dataset.choose_cols(indices). Take the first ten columns:. >>> dataset_result = dataset.choose_cols(list(range(10))). Parameters; ----------; indices : :obj:`list` of :obj:`int`; List of old column indices. Returns; -------; :class:`.MatrixTable`; """"""; n_cols = self.count_cols(); for i in indices:; if not 0 <= i < n_cols:; raise ValueError(f""'choose_cols': expect indices between 0 and {n_cols}, found {i}""); return MatrixTable(ir.MatrixChooseCols(self._mir, indices)). [docs] def n_partitions(self) -> int:; """"""Number of partitions. Notes; -----. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. Partitions are a; core concept of distributed computation in Spark, see `here; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. Returns; -------; int; Number of partitions.; """"""; return Env.backend().execute(ir.MatrixToValueApply(self._mir, {'name': 'NPartitionsMatrixTable'})). [docs] @typecheck_method(n_partitions=int, shuffle=bool); def repartition(self, n_partitions: int, shuffle: bool = True) -> 'MatrixTable':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:106664,resilien,resilient-distributed-datasets-rdds,106664,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['resilien'],['resilient-distributed-datasets-rdds']
Availability,"=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:155996,error,error,155996,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability,"=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change configuration.'). SparkContext._ensure_initialized(). self._gateway = SparkContext._gateway; self._jvm = SparkContext._jvm. # hail package; self._hail = getattr(self._jvm, 'is').hail. Env._jvm = self._jvm; Env._gateway = self._gateway. jsc = sc._jsc.sc() if sc else None. # we always pass 'quiet' to the JVM because stderr output needs; # to be routed through Python separately.; self._jhc = self._hail.HailContext.apply(; jsc, app_name, joption(master), local, log, True, append,; parquet_compression, min_block_size, branching_factor, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmethod; def get_running():; """"""Return the running Hail context in this Python session. **Example**. .. doctest::; :options: +SKIP. >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. :return: Current Hail context.; :rtype: :class:`.HailContext`; """""". return Env.hc(). @property; def version(self):; """"""Return the version of Hail associated with this HailContext. :rtype: str; """"""; return self._jhc.ver",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:3277,error,errors,3277,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,2,['error'],"['error', 'errors']"
Availability,"== {1, 2, 3}; True; >>> hl.eval(hl.flatten(a.b.inner)) == {1, 2, 3}; True. Parameters; ----------; item : :class:`str`; Field name. Returns; -------; :class:`.SetExpression`; A set formed by getting the given field for each struct in; this set; """""". return self.map(lambda x: x[item]). [docs]class DictExpression(Expression):; """"""Expression of type :class:`.tdict`. >>> d = hl.literal({'Alice': 43, 'Bob': 33, 'Charles': 44}); """""". @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(DictExpression, self).__init__(x, type, indices, aggregations); assert isinstance(type, tdict); self._kc = coercer_from_dtype(type.key_type); self._vc = coercer_from_dtype(type.value_type). [docs] @typecheck_method(item=expr_any); def __getitem__(self, item):; """"""Get the value associated with key `item`. Examples; --------. >>> hl.eval(d['Alice']); 43. Notes; -----; Raises an error if `item` is not a key of the dictionary. Use; :meth:`.DictExpression.get` to return missing instead of an error. Parameters; ----------; item : :class:`.Expression`; Key expression. Returns; -------; :class:`.Expression`; Value associated with key `item`.; """"""; if not self._kc.can_coerce(item.dtype):; raise TypeError(; ""dict encountered an invalid key type\n"" "" dict key type: '{}'\n"" "" type of 'item': '{}'"".format(; self.dtype.key_type, item.dtype; ); ); return self._index(self.dtype.value_type, self._kc.coerce(item)). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns whether a given key is present in the dictionary. Examples; --------. >>> hl.eval(d.contains('Alice')); True. >>> hl.eval(d.contains('Anne')); False. Parameters; ----------; item : :class:`.Expression`; Key to test for inclusion. Returns; -------; :class:`.BooleanExpression`; ``True`` if `item` is a key of the dictionary, ``False`` otherwise.; """"""; if not self._kc.can_coerce(item.dtype):; raise Ty",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:36943,error,error,36943,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['error'],['error']
Availability,"=Table); def from_rows_table(cls, table: Table) -> 'MatrixTable':; """"""Construct matrix table with no columns from a table. .. include:: _templates/experimental.rst. Examples; --------; Import a text table and construct a rows-only matrix table:. >>> table = hl.import_table('data/variant-lof.tsv'); >>> table = table.transmute(**hl.parse_variant(table['v'])).key_by('locus', 'alleles'); >>> sites_mt = hl.MatrixTable.from_rows_table(table). Notes; -----; All fields in the table become row-indexed fields in the; result. Parameters; ----------; table : :class:`.Table`; The table to be converted. Returns; -------; :class:`.MatrixTable`; """"""; col_values_uid = Env.get_uid(); entries_uid = Env.get_uid(); return (; table.annotate_globals(**{col_values_uid: hl.empty_array(hl.tstruct())}); .annotate(**{entries_uid: hl.empty_array(hl.tstruct())}); ._unlocalize_entries(entries_uid, col_values_uid, []); ). [docs] @typecheck_method(p=numeric, seed=nullable(int)); def sample_rows(self, p: float, seed=None) -> 'MatrixTable':; """"""Downsample the matrix table by keeping each row with probability ``p``. Examples; --------; Downsample the dataset to approximately 1% of its rows. >>> small_dataset = dataset.sample_rows(0.01). Notes; -----; Although the :class:`MatrixTable` returned by this method may be; small, it requires a full pass over the rows of the sampled object. Parameters; ----------; p : :obj:`float`; Probability of keeping each row.; seed : :obj:`int`; Random seed. Returns; -------; :class:`.MatrixTable`; Matrix table with approximately ``p * n_rows`` rows.; """""". if not 0 <= p <= 1:; raise ValueError(""Requires 'p' in [0,1]. Found p={}"".format(p)). return self.filter_rows(hl.rand_bool(p, seed)). [docs] @typecheck_method(p=numeric, seed=nullable(int)); def sample_cols(self, p: float, seed=None) -> 'MatrixTable':; """"""Downsample the matrix table by keeping each column with probability ``p``. Examples; --------; Downsample the dataset to approximately 1% of its columns. >>> small_dat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:127899,Down,Downsample,127899,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['Down'],['Downsample']
Availability,"={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {valid_clouds}.'; ). datasets = get_datasets_metadata(); names = set([dataset for dataset in datasets]); if name not in names:; raise ValueError(f'{name} is not a dataset available in the' f' repository.'). versions = set(dataset['version'] for dataset in datasets[name]['versions']); if version not in versions:; raise ValueError(; f'Version {version!r} not available for dataset' f' {name!r}.\n' f'Available versions: {versions}.'; ). reference_genomes = set(dataset['reference_genome'] for dataset in datasets[name]['versions']); if reference_genome not in reference_genomes:; raise ValueError(; f'Reference genome build {reference_genome!r} not'; f' available for dataset {name!r}.\n'; f'Available reference genome builds:'; f' {reference_genomes}.'; ). clouds = set(k for dataset in datasets[name]['versions'] for k in dataset['url'].keys()); if cloud not in clouds:; raise ValueError(f'Cloud platform {cloud!r} not available for dataset {name}.\nAvailable platforms: {clouds}.'). regions = set(k for dataset in datasets[name]['versions'] for k in dataset['url'][cloud].keys()); if region not in regions:; raise ValueError(; f'Region {region!r} not available for dataset'; f' {name!r} on cloud platform {cloud!r}.\n'; f'Available regions: {regions}.'; ). path = [; dataset['url'][cloud][region]; for dataset in datasets[name]['versions']; if all([dataset['version'] == version, dataset['reference_genome'] == reference_genome]); ]; assert len(path) == 1; path = path[0]; if path.startswith('s3://'):; try:; dataset = _read_dataset(path); except hl.utils.java.FatalError:; dataset = _read_dataset(path.replace('s3://', 's3a://')); else:; dataset = _read_dataset(path); return dataset. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:3734,avail,available,3734,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,5,"['Avail', 'avail']","['Available', 'available']"
Availability,"> 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). def call_hemi(kid_pp, parent, parent_pp, kid_ad_ratio):; p_data_given_dn = parent_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (parent_pp[1] + parent_pp[2]) * kid_pp[2] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (parent.DP) < min_dp_ratio) | (kid_ad_ratio < min_child_ab), failure); .when((hl.sum(parent.AD) == 0), failure); .when(parent.AD[1] / hl.sum(parent.AD) > max_parent_ab, failure); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.3, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confiden",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:32000,failure,failure,32000,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['failure'],['failure']
Availability,"> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. Returns:Current Hail context. Return type:HailContext. grep(regex, path, max_count=100)[source]¶; Grep big files, like, really fast.; Examples; Print all lines containing the string hello in file.txt:; >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in file1.txt and file2.txt:; >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). Background; grep() mimics the basic functionality of Unix grep in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regular expressions at RegExr. Parameters:; regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return. import_bgen(path, tolerance=0.2, sample_file=None, min_partitions=None)[source]¶; Import .bgen file(s) as variant dataset.; Examples; Importing a BGEN file as a VDS (assuming it has already been indexed).; >>> vds = hc.import_bgen(""data/example3.bgen"", sample_file=""data/example3.sample""). Notes; Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see here. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only unphased and diploid genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed.; Before importing, ensure that:. The sample file has the same number of samples as the BGEN file.; No duplicate sample IDs are present. To load multiple files at the same time, use Hadoop Glob Patterns.; Genotype probability (``gp``) representation:; The following modifications are made to genotype probabilities in BGEN v1.1 files:. Since genotype probabilities ar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:8454,toler,tolerance,8454,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['toler'],['tolerance']
Availability,"> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; if path.endswith('.ht'):; return hl.read_table(path); elif path.endswith('.mt'):; return hl.read_matrix_table(path); elif path.endswith('.bm'):; return hl.linalg.BlockMatrix.read(path); raise ValueError(f'Invalid path: {path}. Can only load datasets with .ht, .mt, or .bm extensions.'). [docs]def load_dataset(; name: str, version: Optional[str], reference_genome: Optional[str], region: str = 'us-central1', cloud: str = 'gcp'; ) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; """"""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """""". valid_regions = {'us', 'us-central1', 'europe-west1'}; if region not in valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:1738,avail,available,1738,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,2,['avail'],['available']
Availability,">> ds_ann = ds.annotate_rows(linreg =; ... hl.agg.linreg(ds.pheno.blood_pressure,; ... [1,; ... ds.GT.n_alt_alleles(),; ... ds.pheno.age,; ... ds.GT.n_alt_alleles() * ds.pheno.age])). Warning; As in the example, the intercept covariate 1 must be included; explicitly if desired. Notes; In relation to; lm.summary; in R, linreg(y, x = [1, mt.x1, mt.x2]) computes; summary(lm(y ~ x1 + x2)) and; linreg(y, x = [mt.x1, mt.x2], nested_dim=0) computes; summary(lm(y ~ x1 + x2 - 1)).; More generally, nested_dim defines the number of effects to fit in the; nested (null) model, with the effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; beta (tarray of tfloat64):; Estimated regression coefficient for each covariate.; standard_error (tarray of tfloat64):; Estimated standard error for each covariate.; t_stat (tarray of tfloat64):; t-statistic for each covariate.; p_value (tarray of tfloat64):; p-value for each covariate.; multiple_standard_error (tfloat64):; Estimated standard deviation of the random error.; multiple_r_squared (tfloat64):; Coefficient of determination for nested models.; adjusted_r_squared (tfloat64):; Adjusted multiple_r_squared taking into account degrees of; freedom.; f_stat (tfloat64):; F-statistic for nested models.; multiple_p_value (tfloat64):; p-value for the; F-test of; nested models.; n (tint64):; Number of samples included in the regression. A sample is included if and; only if y, all elements of x, and weight (if set) are non-missing. All but the last field are missing if n is less than or equal to the; number of covariates or if the covariates are linearly dependent.; If set, the weight parameter generalizes the model to weighted least; squares, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; If any weight is negative, the resulting statistics will be nan. Parameters:. y (Float64Expression) – Response (dependent variable).; x (Float64Expression or list of Float64Expression) – Covariates (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:28791,error,error,28791,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['error'],['error']
Availability,">>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using :py:meth:`~hail.KeyTable.filter`. >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). **Method**. The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with allele frequencies; :math:`p_s`, is given by:. .. math::. \\widehat{\phi_{ij}} := \\frac{1}{|S_{ij}|}\\sum_{s \in S_{ij}}\\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent. When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals. PC-Relate slightly modifies the usual estimator for relatedness:; occurences of population allele frequency are replaced with an; ""individual-specific allele frequency"". This modification allows the; method to correctly weight an allele according to an individual's unique; ancestry profile. The ""individual-specific allele frequency"" at a given genetic locus is; modeled by PC-Relate as a linear function of their first ``k`` principal; component coordinates. As such, the efficacy of this method rests on two; assumptions:. - an individual's first ``k`` principal component coordinates fully; describe their allele-frequency-relevant ancestry, and. - the relationship between ancestry (as de",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:170380,down,down,170380,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['down'],['down']
Availability,"AT R package which returns \(\tilde{Q}\):. \[\tilde{Q} = \frac{Q}{2}\]. Parameters:. group (Expression) – Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Float64Expression) – Column-indexed response (dependent variable) expression.; x (Float64Expression) – Entry-indexed expression for input (independent variable).; covariates (list of Float64Expression) – List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size (int) – Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance (float) – The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy (float) – The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations (int) – The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns:; Table – One row per-group. The key is group. The row fields are:. group : the group parameter.; size : tint64, the number of variants in this group.; q_stat : tfloat64, the \(Q\) statistic, see Notes for why this differs from the paper.; p_value : tfloat64, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes.; fault : tint32, the fault flag from pgenchisq(). The global fields are:. n_complete_samples : tint32, the number of samples with neither a missing; phenotype nor a missing covariate.; y_residual : tint32, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:76333,error,errors,76333,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability,"Added new; method BlockMatrix.to_ndarray.; (#10251) Added; suport for haploid GT calls to VCF combiner. Version 0.2.65; Released 2021-04-14. Default Spark Version Change. Starting from version 0.2.65, Hail uses Spark 3.1.1 by default. This; will also allow the use of all python versions >= 3.6. By building; hail from source, it is still possible to use older versions of; Spark. New features. (#10290) Added; hl.nd.solve.; (#10187) Added; NDArrayNumericExpression.sum. Performance improvements. (#10233) Loops; created with hl.experimental.loop will now clean up unneeded; memory between iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to be; associated with their original source file. Bug fixes. (#10182) Fixed; serious memory leak in certain uses of filter_intervals.; (#10133) Fix bug; where some pipelines incorrectly infer missingness, leading to a type; error.; (#10134) Teach; hl.king to treat filtered entries as missing values.; (#10158) Fixes hail; usage in latest versions of jupyter that rely on asyncio.; (#10174) Fixed bad; error message when incorrect return type specified with hl.loop. Version 0.2.63; Released 2021-03-01. (#10105) Hail will; now return frozenset and hail.utils.frozendict instead of; normal sets and dicts. Bug fixes. (#10035) Fix; mishandling of NaN values in hl.agg.hist, where they were; unintentionally included in the first bin.; (#10007) Improve; error message from hadoop_ls when file does not exist. Performance Improvements. (#10068) Make; certain array copies faster.; (#10061) Improve; code generation of hl.if_else and hl.coalesce. Version 0.2.62; Released 2021-02-03. New features. (#9936) Deprecated; hl.null in favor of hl.missing for naming consistency.; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:60991,error,error,60991,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports PGT fields from GVCFs.; (#13805) Fix; (#13767).; hailctl dataproc submit now expands ~ in the --files and; --pyfiles arguments.; (#13797) Fix; (#13756). Operations; that collect large results such as to_pandas may require up to 3x; less memory.; (#13826) Fix; (#13793). Ensure; hailctl describe -u overrides the gcs_requester_pays/project; config variable.; (#13814) Fix; (#13757). Pipelines; that are memory-bound by copious use of hl.literal, such as; hl.vds.filter_intervals, require substantially less memory.; (#13894) Fix; (#13837) in which; Hail could break a Spark installation if the Hail JAR appears on the; classpath before the Scala JARs.; (#13919) Fix; (#13915) which; prevented using a glob pattern in hl.import_vcf. Version 0.2.124; Released 2023-09-21. New Features. (#13608) Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) T",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:24102,avail,available,24102,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['avail'],['available']
Availability,"BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:37177,toler,tolerance,37177,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,6,['toler'],['tolerance']
Availability,"ClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Incre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:71228,error,error,71228,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:125916,error,error,125916,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,"E>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not have the appropriate permissions to check the default storage class of the bucket, the first object encountered in the bucket will have its storage class checked, and this will be assumed to be the default storage policy of the bucket. Shared between Query and Batch; Yes. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/configuration_reference.html:2154,error,erroring,2154,docs/0.2/configuration_reference.html,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html,1,['error'],['erroring']
Availability,"END field with non-reference genotype at '; ); + hl.str(mt.locus); + hl.str(' / '); + hl.str(mt.col_key[0]); ); ); rmt = rmt.select_entries(*(x for x in rmt.entry if x in used_ref_block_fields)); rmt = rmt.filter_rows(hl.agg.count() > 0). rmt = rmt.key_rows_by('locus').select_rows().select_cols(). if is_split:; rmt = rmt.distinct_by_row(). vmt = mt.filter_entries(hl.is_missing(mt.END)).drop('END')._key_rows_by_assert_sorted('locus', 'alleles'); vmt = vmt.filter_rows(hl.agg.count() > 0). return VariantDataset(rmt, vmt). def __init__(self, reference_data: MatrixTable, variant_data: MatrixTable):; self.reference_data: MatrixTable = reference_data; self.variant_data: MatrixTable = variant_data. self.validate(check_data=False). [docs] def write(self, path, **kwargs):; """"""Write to `path`.""""""; self.reference_data.write(VariantDataset._reference_path(path), **kwargs); self.variant_data.write(VariantDataset._variants_path(path), **kwargs). [docs] def checkpoint(self, path, **kwargs) -> 'VariantDataset':; """"""Write to `path` and then read from `path`.""""""; self.write(path, **kwargs); return read_vds(path). [docs] def n_samples(self) -> int:; """"""The number of samples present.""""""; return self.reference_data.count_cols(). @property; def reference_genome(self) -> ReferenceGenome:; """"""Dataset reference genome. Returns; -------; :class:`.ReferenceGenome`; """"""; return self.reference_data.locus.dtype.reference_genome. [docs] @typecheck_method(check_data=bool); def validate(self, *, check_data: bool = True):; """"""Eagerly checks necessary representational properties of the VDS."""""". rd = self.reference_data; vd = self.variant_data. def error(msg):; raise ValueError(f'VDS.validate: {msg}'). rd_row_key = rd.row_key.dtype; if (; not isinstance(rd_row_key, hl.tstruct); or len(rd_row_key) != 1; or not rd_row_key.fields[0] == 'locus'; or not isinstance(rd_row_key.types[0], hl.tlocus); ):; error(f""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_k",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:7726,checkpoint,checkpoint,7726,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['checkpoint'],['checkpoint']
Availability,"Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone. - s2 : :obj:`.tfloat64`, the variance of the residuals, :math:`\sigma^2` in the paper. """"""; mt = matrix_table_source('skat/x', x); k = len(covariates); if k == 0:; raise ValueError('_linear_skat: at least one covariate is required.'); _warn_if_no_intercept('_linear_skat', covariates); mt = mt._select_all(; row_exprs=dict(group=group, weight=weight), col_exprs=dict(y=y, covariates=covariates), entry_exprs=dict(x=x); ); mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])); yvec, covmat, n = mt.aggregate_cols(; (hl.agg.collect(hl.float(mt.y)), hl.agg.collect(mt.covariates",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:80415,fault,fault,80415,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability,"F[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:77818,error,errors,77818,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['error'],['errors']
Availability,"F[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statisti",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93540,error,errors,93540,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['error'],['errors']
Availability,"Files to import.; key : :class:`str` or :obj:`list` of :obj:`str`; Key fields(s).; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header : :obj:`bool`; If ``True```, assume the file has no header and name the N fields `f0`,; `f1`, ... `fN` (0-indexed).; impute : :obj:`bool`; If ``True``, Impute field types from the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; delimiter : :class:`str`; Field delimiter regex.; missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_fi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:59579,error,error,59579,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['error'],['error']
Availability,"GQ: int32; PL: array<int32>. Use MatrixTable.select_entries() to rearrange these fields if; necessary.; The following new fields are generated:. old_locus (locus) – The old locus, before filtering and computing; the minimal representation.; old_alleles (array<str>) – The old alleles, before filtering and; computing the minimal representation.; old_to_new (array<int32>) – An array that maps old allele index to; new allele index. Its length is the same as old_alleles. Alleles that; are filtered are missing.; new_to_old (array<int32>) – An array that maps new allele index to; the old allele index. Its length is the same as the modified alleles; field. Downcode algorithm; We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles.; GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; split_multi_hts().; The downcode algorithm would produce the following:; GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. GT: Downcode filtered alleles to reference.; AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms 25,5,10,20 to 40,20.; DP: No change.; PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded g",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:25546,down,downcode,25546,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['down'],['downcode']
Availability,"Hail can import data from many sources: TSV and CSV files, JSON files, FAM files, databases, Spark, etc. It can also read (and write) a native Hail format.; You can read a dataset with hl.read_table. It take a path and returns a Table. ht stands for Hail Table.; We’ve provided a method to download and import the MovieLens dataset of movie ratings in the Hail native format. Let’s read it!. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=https://dx.doi.org/10.1145/2827872. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_movie_lens('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 3:> (0 + 1) / 1]. [3]:. users = hl.read_table('data/users.ht'). Exploring Tables; The describe method prints the structure of a table: the fields and their types. [4]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. You can view the first few rows of the table using show.; 10 rows are di",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/03-tables.html:2049,avail,available,2049,docs/0.2/tutorials/03-tables.html,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html,1,['avail'],['available']
Availability,"Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks; Use Hail in a notebook; Initialize Hail; Display Bokeh plots. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Databricks. View page source. Databricks; The docker images described below are maintained by Databricks. Please direct questions about them; to Databricks.; Hail can be installed on a Databricks Spark cluster on Microsoft Azure, Amazon Web Services, or; Google Cloud Platform via an open source Docker container located in the Project Glow Dockerhub. Docker; files to build your own Hail container on Databricks can be found in the Glow Github repository.; Install Hail via Docker with Databricks Container Services.; Use the Docker Image URL, projectglow/databricks-hail:<hail_version>, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; When initializing Hail, pass in the pre-created SparkContext and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. note:. Enable skip_logging_configuration to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; Hail is not supported with Credential passthrough. code:; >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_logging_configuration=True) . Display Bokeh plots; Hail uses the Bokeh library to create plots. The show; function built into Bokeh does not work in Databricks. T",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/databricks.html:1200,avail,available,1200,docs/0.2/cloud/databricks.html,https://hail.is,https://hail.is/docs/0.2/cloud/databricks.html,1,['avail'],['available']
Availability,"ISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd.Z2 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z1), Z1=_ibd.Z1 / (_ibd.Z0 + _ibd.Z1), Z2=hl.float(0)),; ); .default(_ibd); ). result = result.annotate(ibd=hl.struct(Z0=result.Z0, Z1=result.Z1, Z2=result.Z2)); result = result.drop('Z0', 'Z1', 'Z2'); if bounded:; result = result.annotate(ibd=bound_result(result.ibd)); result = result.annotate(ibd=result.ibd.annotate",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:7907,checkpoint,checkpoint,7907,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,2,['checkpoint'],['checkpoint']
Availability,"IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd.Z2 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z1), Z1=_ibd.Z1 / (_ibd.Z0 + _ibd.Z1), Z2=hl.float(0)),; ); .default(_ibd); ). result = result.annotate(ibd=hl.struct(Z0=result.Z0, Z1=result.Z1, Z2=result.Z2)); result = result.drop('Z0', 'Z1', 'Z2'); if bounded:; result = result.annotate(ibd",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:7831,checkpoint,checkpoint,7831,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,2,['checkpoint'],['checkpoint']
Availability,"InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesn’t exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the rando",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:9739,checkpoint,checkpointing,9739,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpointing']
Availability,"K mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FI",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:125174,error,errors,125174,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,"KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:23830,redundant,redundant,23830,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,1,['redundant'],['redundant']
Availability,"Load reference genome from a JSON file. remove_liftover; Remove liftover to dest_reference_genome. remove_sequence; Remove the reference sequence. write; ""Write this reference genome to a file in JSON format. add_liftover(chain_file, dest_reference_genome)[source]; Register a chain file for liftover.; Examples; Access GRCh37 and GRCh38 using get_reference():; >>> rg37 = hl.get_reference('GRCh37') ; >>> rg38 = hl.get_reference('GRCh38') . Add a chain file from 37 to 38:; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) . Notes; This method can only be run once per reference genome. Use; has_liftover() to test whether a chain file has been registered.; The chain file format is described; here.; Chain files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37 to GRCh38; gs://hail-common/references/grch37_to_grch38.over.chain.gz; GRCh38 to GRCh37; gs://hail-common/references/grch38_to_grch37.over.chain.gz; Public download links are available; here. Parameters:. chain_file (str) – Path to chain file. Can be compressed (GZIP) or uncompressed.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to. add_sequence(fasta_file, index_file=None)[source]; Load the reference sequence from a FASTA file.; Examples; Access the GRCh37 reference genome using get_reference():; >>> rg = hl.get_reference('GRCh37') . Add a sequence file:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') . Add a sequence file with the default index location:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') . Notes; This method can only be run once per reference genome. Use; has_sequence() to test whether a sequence is loaded.; FASTA and index files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37. FASTA file: gs://hail-common/references/human_g1k_v37.fasta.gz; Index file: gs://hail",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:4614,down,download,4614,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html,2,"['avail', 'down']","['available', 'download']"
Availability,"New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:103162,error,error,103162,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. p_value (NumericExpression) – Row-indexed numeric expression of p-values.; approximate (bool) – If False, computes exact lambda GC (slower and uses more memory). Returns:; float – Genomic inflation factor (lambda genomic control). hail.methods.split_multi(ds, keep_star=False, left_aligned=False, *, permit_shuffle=False)[source]; Split multiallelic variants. Warning; In order to support a wide variety of data types, this function splits only; the variants on a MatrixTable, but not the genotypes. Use; split_multi_hts() if possible, or spl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:82537,fault,fault,82537,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['fault'],['fault']
Availability,"Parameters:; populations (int) – Number of populations.; samples (int) – Number of samples.; variants (int) – Number of variants.; num_partitions (int) – Number of partitions.; pop_dist (array of float or None) – Unnormalized population distribution; fst (array of float or None) – \(F_{ST}\) values; af_dist (UniformDist or BetaDist or TruncatedBetaDist) – Ancestral allele frequency distribution; seed (int) – Random seed. Returns:Variant dataset simulated using the Balding-Nichols model. Return type:VariantDataset. eval_expr(expr)[source]¶; Evaluate an expression. Parameters:expr (str) – Expression to evaluate. Return type:annotation. eval_expr_typed(expr)[source]¶; Evaluate an expression and return the result as well as its type. Parameters:expr (str) – Expression to evaluate. Return type:(annotation, Type). static get_running()[source]¶; Return the running Hail context in this Python session.; Example; >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. Returns:Current Hail context. Return type:HailContext. grep(regex, path, max_count=100)[source]¶; Grep big files, like, really fast.; Examples; Print all lines containing the string hello in file.txt:; >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in file1.txt and file2.txt:; >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). Background; grep() mimics the basic functionality of Unix grep in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regular expressions at RegExr. Parameters:; regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return. import_bgen(path, tolerance=0.2, sample_file=None, min_partitions=None)[source]¶; Import .bgen f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:7535,recover,recovery,7535,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['recover'],['recovery']
Availability,"Parameters:; regions (Optional[List[str]]) – The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters:; is_spot (bool) – If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the job’s storage size.; Examples; Set the job’s disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, jobs requesting one or more cores receive; 5 GiB of storage for the root file system /. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at /io. Batch automatically writes all ResourceFile to; /io.; The default storage size is 0 Gi. The minimum storage size is 0 Gi and the; maximum storage size is 64 Ti. If storage is set to a value between 0 Gi; and 10 Gi, the storage request is rounded up to 10 Gi. All values are; rounded up to the nearest Gi. Parameters:; storage (Union[str, int, None]) – Units are in bytes if storage is an int. If None, use the; default storage size for the Servi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:8211,echo,echo,8211,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['echo'],['echo']
Availability,"Python Version Compatibility Policy; Change Log. Batch. Python API; BashJob. View page source. BashJob. class hailtop.batch.job.BashJob(batch, token, *, name=None, attributes=None, shell=None); Bases: Job; Object representing a single bash job to execute.; Examples; Create a batch object:; >>> b = Batch(). Create a new bash job that prints hello to a temporary file t.ofile:; >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'). Write the temporary file t.ofile to a permanent location; >>> b.write_output(j.ofile, 'hello.txt'). Execute the DAG:; >>> b.run(). Notes; This class should never be created directly by the user. Use Batch.new_job(); or Batch.new_bash_job() instead.; Methods. command; Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the job’s command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; This method can be called more than once. It’s behavior is to append; commands to run to the set of previously defined commands r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:1284,echo,echo,1284,docs/batch/api/batch/hailtop.batch.job.BashJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html,2,['echo'],['echo']
Availability,"Query-on-Batch.; (#14122) Ensure that; stack traces are transmitted from workers to the driver to the; client.; (#14105) When a VCF; contains missing values in array fields, Hail now suggests using; array_elements_required=False. Deprecations. (#13987) Deprecate; default_reference parameter to hl.init, users should use; hl.default_reference with an argument to set new default; references usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:21022,reliab,reliably,21022,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['reliab'],['reliably']
Availability,"R),; ); else:; n_alt_alleles = hl.agg.sum(mt.GT.n_alt_alleles()); total_alleles = 2 * hl.agg.sum(hl.is_defined(mt.GT)); # subtract 1 from __alt_alleles to correct for the observed genotype; mt = mt.annotate_rows(; __prior=pop_frequency_prior,; __alt_alleles=n_alt_alleles,; __site_freq=hl.max((n_alt_alleles - 1) / total_alleles, pop_frequency_prior, MIN_POP_PRIOR),; ). mt = require_biallelic(mt, 'de_novo'). tm = trio_matrix(mt, pedigree, complete_trios=True). autosomal = tm.locus.in_autosome_or_par() | (tm.locus.in_x_nonpar() & tm.is_female); hemi_x = tm.locus.in_x_nonpar() & ~tm.is_female; hemi_y = tm.locus.in_y_nonpar() & ~tm.is_female; hemi_mt = tm.locus.in_mito() & tm.is_female. is_snp = hl.is_snp(tm.alleles[0], tm.alleles[1]); n_alt_alleles = tm.__alt_alleles; prior = tm.__site_freq; het_hom_hom = tm.proband_entry.GT.is_het() & tm.father_entry.GT.is_hom_ref() & tm.mother_entry.GT.is_hom_ref(); kid_ad_fail = tm.proband_entry.AD[1] / hl.sum(tm.proband_entry.AD) < min_child_ab. failure = hl.missing(hl.tstruct(p_de_novo=hl.tfloat64, confidence=hl.tstr)). kid = tm.proband_entry; dad = tm.father_entry; mom = tm.mother_entry. kid_linear_pl = 10 ** (-kid.PL / 10); kid_pp = hl.bind(lambda x: x / hl.sum(x), kid_linear_pl). dad_linear_pl = 10 ** (-dad.PL / 10); dad_pp = hl.bind(lambda x: x / hl.sum(x), dad_linear_pl). mom_linear_pl = 10 ** (-mom.PL / 10); mom_pp = hl.bind(lambda x: x / hl.sum(x), mom_linear_pl). kid_ad_ratio = kid.AD[1] / hl.sum(kid.AD); dp_ratio = kid.DP / (dad.DP + mom.DP). def call_auto(kid_pp, dad_pp, mom_pp, kid_ad_ratio):; p_data_given_dn = dad_pp[0] * mom_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (dad_pp[1] * mom_pp[0] + dad_pp[0] * mom_pp[1]) * kid_pp[1] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (dad.DP + mom.DP) < min_dp_ratio) | ~(kid_ad_ratio >=",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:29286,failure,failure,29286,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['failure'],['failure']
Availability,"R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of :math:`\chi^2(1)` distributions. +-------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +------+------+-----------------------------------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:105554,fault,fault,105554,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,"['error', 'fault']","['error', 'fault']"
Availability,"RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). results = []. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint_batching.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:15054,checkpoint,checkpoint,15054,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability,"Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; IntervalExpression. View page source. IntervalExpression. class hail.expr.IntervalExpression[source]; Expression of type tinterval.; >>> interval = hl.interval(3, 11); >>> locus_interval = hl.parse_locus_interval(""1:53242-90543""). Attributes. dtype; The data type of the expression. end; Returns the end point. includes_end; True if the interval includes the end point. includes_start; True if the interval includes the start point. start; Returns the start point. Methods. contains; Tests whether a value is contained in the interval. overlaps; True if the the supplied interval contains any value in common with this one. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of record",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html:1524,error,error,1524,docs/0.2/hail.expr.IntervalExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html,1,['error'],['error']
Availability,"Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_running; Return the running Hail context in this Python session. grep; Grep big files, like, really fast. import_bgen; Import .bgen file(s) as variant dataset. import_gen; Import .gen file(s) as variant dataset. import_plink; Import PLINK binary file (BED, BIM, FAM) as variant dataset. import_table; Import delimited text file (text table) as key table. import_vcf; Import VCF file(s) as variant dataset. index_bgen; Index .bgen files. read; Read .vds files as variant dataset. read_table; Read a KT file as key table. report; Print information and warnings about VCF + GEN import and deduplication. stop; Shut down the Hail context. write_partitioning; Write partitioning.json.gz file for legacy VDS file. balding_nichols_model(populations, samples, variants, num_partitions=None, pop_dist=None, fst=None, af_dist=<hail.stats.UniformDist instance>, seed=0)[source]¶; Simulate a variant dataset using the Balding-Nichols model.; Examples; To generate a VDS with 3 populations, 100 samples in total, and 1000 variants:; >>> vds = hc.balding_nichols_model(3, 100, 1000). To generate a VDS with 4 populations, 2000 samples, 5000 variants, 10 partitions, population distribution [0.1, 0.2, 0.3, 0.4], \(F_{ST}\) values [.02, .06, .04, .12], ancestral allele frequencies drawn from a truncated beta distribution with a = .01 and b = .05 over the interval [0.05, 1], and random seed 1:; >>> from hail.stats import TruncatedBetaDist; >>> vds = hc.balding_nichols_model(4, 40, 150, 10,; ... pop_dist=[0.1, 0.2, 0.3, 0.4],; ... fst=[.02, .06, .04, .12],; ... af_dist=TruncatedBetaDist(a=0.01, b=2.0, minVal=0.05, maxVal=1.0),",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:2610,down,down,2610,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['down'],['down']
Availability,"Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:20108,error,error,20108,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['error'],['error']
Availability,"T - 2)); + p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e22=1,; ). dataset = dataset.checkpoint(hl.utils.new_temp_file()). expectations = dataset.aggregate_rows(; hl.struct(; e00=hl.agg.sum(dataset._e00),; e10=hl.agg.sum(dataset._e10),; e20=hl.agg.sum(dataset._e20),; e11=hl.agg.sum(dataset._e11),; e21=hl.agg.sum(dataset._e21),; e22=hl.agg.sum(dataset._e22),; ); ). IS_HOM_REF = BlockMatrix.from_entry_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.flo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:7118,checkpoint,checkpoint,7118,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,2,['checkpoint'],['checkpoint']
Availability,"T / (T - 2)); + (q**3) * ((Y - 1) / Y) * ((Y - 2) / Y) * (T / (T - 1)) * (T / (T - 2)); + (p**2) * q * ((X - 1) / X) * (T / (T - 1)) * (T / (T - 2)); + p * (q**2) * ((Y - 1) / Y) * (T / (T - 1)) * (T / (T - 2)); ),; _e22=1,; ). dataset = dataset.checkpoint(hl.utils.new_temp_file()). expectations = dataset.aggregate_rows(; hl.struct(; e00=hl.agg.sum(dataset._e00),; e10=hl.agg.sum(dataset._e10),; e20=hl.agg.sum(dataset._e20),; e11=hl.agg.sum(dataset._e11),; e21=hl.agg.sum(dataset._e21),; e22=hl.agg.sum(dataset._e22),; ); ). IS_HOM_REF = BlockMatrix.from_entry_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:6977,checkpoint,checkpoint,6977,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,2,['checkpoint'],['checkpoint']
Availability,"T = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd.Z2 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z1), Z1=_ibd.Z1 / (_i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:7609,checkpoint,checkpoint,7609,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,2,['checkpoint'],['checkpoint']
Availability,"True, delete temporary directories with intermediate files.; backend_kwargs (Any) – See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str) – Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. Warning; To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; All JobResourceFile are temporary files and must be written; to a permanent location using write_output() if the output needs; to be saved. Parameters:. resource (Resource) – Resource to be written to a file.; dest (str) – Destination file path. For a single ResourceFile, this will; simply be dest. For a ResourceGroup, dest is the file; root and each resource file will be written to {root}.identifier; where identifier is the identifier of the file in the; ResourceGroup map. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:11791,echo,echo,11791,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['echo'],['echo']
Availability,"True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Get the value associated with key item.; Examples; >>> hl.eval(d['Alice']); 43. Notes; Raises an error if item is not a key of the dictionary. Use; DictExpression.get() to return missing instead of an error. Parameters:; item (Expression) – Key expression. Returns:; Expression – Value associated with key item. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns whether a given key is present in the dictionary.; Examples; >>> hl.eval(d.contains('Alice')); True. >>> hl.eval(d.contains('Anne')); False. Parameters:; item (Expression) – Key to test for inclusion. Returns:; BooleanExpression – True if item is a key of the dictionary, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.DictExpression.html:2576,error,error,2576,docs/0.2/hail.expr.DictExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html,1,['error'],['error']
Availability,"TruncatedBetaDist`. :param int seed: Random seed. :return: Variant dataset simulated using the Balding-Nichols model.; :rtype: :class:`.VariantDataset`; """""". if pop_dist is None:; jvm_pop_dist_opt = joption(pop_dist); else:; jvm_pop_dist_opt = joption(jarray(self._jvm.double, pop_dist)). if fst is None:; jvm_fst_opt = joption(fst); else:; jvm_fst_opt = joption(jarray(self._jvm.double, fst)). jvds = self._jhc.baldingNicholsModel(populations, samples, variants,; joption(num_partitions),; jvm_pop_dist_opt,; jvm_fst_opt,; af_dist._jrep(),; seed); return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(expr=strlike); def eval_expr_typed(self, expr):; """"""Evaluate an expression and return the result as well as its type. :param str expr: Expression to evaluate. :rtype: (annotation, :class:`.Type`). """""". x = self._jhc.eval(expr); t = Type._from_java(x._2()); v = t._convert_to_py(x._1()); return (v, t). [docs] @handle_py4j; @typecheck_method(expr=strlike); def eval_expr(self, expr):; """"""Evaluate an expression. :param str expr: Expression to evaluate. :rtype: annotation; """""". r, t = self.eval_expr_typed(expr); return r. [docs] def stop(self):; """""" Shut down the Hail context. It is not possible to have multiple Hail contexts running in a; single Python session, so use this if you need to reconfigure the Hail; context. Note that this also stops a running Spark context.; """""". self.sc.stop(); self.sc = None; Env._jvm = None; Env._gateway = None; Env._hc = None. [docs] @handle_py4j; @typecheck_method(path=strlike); def read_table(self, path):; """"""Read a KT file as key table. :param str path: KT file to read. :return: Key table read from disk.; :rtype: :class:`.KeyTable`; """""". jkt = self._jhc.readTable(path); return KeyTable(self, jkt). [docs] @handle_py4j; def report(self):; """"""Print information and warnings about VCF + GEN import and deduplication.""""""; self._jhc.report(). © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:33108,down,down,33108,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['down'],['down']
Availability,"Tutorials; GWAS Tutorial. View page source. GWAS Tutorial; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association test, and demonstrate the need to control for confounding caused by population stratification. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:1809,error,error,1809,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['error'],['error']
Availability,"Variant Datasets::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: ``use_exome_default_intervals=True`` and; ``use_genome_default_intervals=True``. The combiner serializes itself to `save_path` so that it can be restarted after failure. Parameters; ----------; save_path : :class:`str`; The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path : :class:`str`; The location to store the new VariantDataset.; temp_path : :class:`str`; The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome : :class:`.ReferenceGenome`; The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor : :class:`int`; The number of Variant Datasets to combine at once.; target_records : :class:`int`; The target number of variants per partition.; gvcf_batch_size : :class:`int`; The number of GVCFs to combine into a Variant Dataset at once.; contig_recoding : :class:`dict` mapping :class:`str` to :class:`str` or :obj:`None`; This mapping is applied to GVCF contigs before importing them into Hail. This is u",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:3899,failure,failure,3899,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['failure'],['failure']
Availability,"Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.; (#7608) hl.grep; now has a show argument that allows users to either print the; results (default) or return a dictionary of the results. hailctl dataproc. (#7717) Throw error; when mispelling arguments instead of silently quitting. Version 0.2.28; Released 2019-11-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailctl dataproc now supports --gcloud_configuration option. Documentation. (#7570) Hail has a; cheatsheet for Tables now. Version 0.2.27; Released 2019-11-15. New Features. (#7379) Add; delimiter argument to hl.import_matrix_table; (#7389) Add force; and force_bgz arguments to hl.experimental.import_gtf; (#7386)(#7394); Add {Table, MatrixTable}.tail.; (#7467) Added; hl.if_else as an alias for hl.cond; deprecated hl.cond.; (#7453) Add; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:80861,error,error,80861,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). results = []. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint_batching.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:14966,checkpoint,checkpoint,14966,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability,"]), True); return left.union(moved) if is_table else left.union_rows(moved, _check_cols=False). [docs]@typecheck(ds=oneof(Table, MatrixTable), keep_star=bool, left_aligned=bool, vep_root=str, permit_shuffle=bool); def split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False):; """"""Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema. .. code-block:: text. struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; :meth:`.MatrixTable.annotate_entries`. Examples; --------. >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi_hts`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; -----. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. :func:`.split_multi_hts` will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:117932,error,errors,117932,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['error'],['errors']
Availability,"],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table3.snp_errors[1],; }),; hl.struct(**{; ck_name: table3.proband[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[2],; 'snp_errors': table3.snp_errors[2],; }),; ]; ); table3 = table3.explode('xs'); table3 = table3.select(**table3.xs); table3 = (; table3.group_by(ck_name, 'fam_id'); .aggregate(errors=hl.agg.sum(table3.errors), snp_errors=hl.agg.sum(table3.snp_errors)); .key_by(ck_name); ). table4 = tm.select_rows(errors=hl.agg.count_where(hl.is_defined(tm.mendel_code))).rows(). return table1, table2, table3, table4. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree); def transmission_disequilibrium_test(dataset, pedigree) -> Table:; r""""""Performs the transmission disequilibrium test on trios. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------; Compute TDT association statistics and show the first two results:. >>> pedigree = hl.Pedigree.read('data/tdt_trios.fam'); >>> tdt_table = hl.transmission_disequilibrium_test(tdt_dataset, pedigree); >>> tdt_table.show(2) # doctest: +SKIP_OUTPUT_CHECK; +---------------+------------+-------+-------+----------+----------+; | locus | alleles | t | u | chi_sq | p_value |; +---------------+------------+-------+-------+----------+------",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:12540,error,errors,12540,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table3.snp_errors[1],; }),; hl.struct(**{; ck_name: table3.proband[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[2],; 'snp_errors': table3.snp_errors[2],; }),; ]; ); table3 = table3.explode('xs'); table3 = table3.select(**table3.xs); table3 = (; table3.group_by(ck_name, 'fam_id'); .aggregate(errors=hl.agg.sum(table3.errors), snp_errors=hl.agg.sum(table3.snp_errors)); .key_by(ck_name); ). table4 = tm.select_rows(errors=hl.agg.count_where(hl.is_defined(tm.mendel_code))).rows(). return table1, table2, table3, table4. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree); def transmission_disequilibrium_test(dataset, pedigree) -> Table:; r""""""Performs the transmission disequilibrium test on trios. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------; Compute TDT association statistics and show the first two results:. >>> pedigree = hl.Pedigree.read('data/tdt_trios.fam'); >>> tdt_table = hl.transmission_disequilibrium_test(tdt_dataset, pedigree); >>> tdt_table.show(2) # doctest: +SKIP_OUTPUT_CHECK; +---------------+------------+-------+-------+----------+----------+; | locus | alleles | t | u | chi_sq | p_value |; +---------------+------------+-------+-------+----------+----------+; | locus<GRCh37> | array<str> |",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:12565,error,errors,12565,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"_HEAP. Returns:; MatrixTable – Persisted dataset. rename(fields)[source]; Rename fields of a matrix table.; Examples; Rename column key s to SampleID, still keying by SampleID.; >>> dataset_result = dataset.rename({'s': 'SampleID'}). You can rename a field to a field name that already exists, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key s to info, and the row field info to vcf_info:; >>> dataset_result = dataset.rename({'s': 'info', 'info': 'vcf_info'}). Parameters:; fields (dict from str to str) – Mapping from old field names to new field names. Returns:; MatrixTable – Matrix table with renamed fields. repartition(n_partitions, shuffle=True)[source]; Change the number of partitions.; Examples; Repartition to 500 partitions:; >>> dataset_result = dataset.repartition(500). Notes; Check the current number of partitions with n_partitions().; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; \(M\) rows is first imported, each of the \(k\) partitions will; contain about \(M/k\) of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it’s recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see their documentation; for details.; When shuffle=True, Hail does a full shuffle of the data; and creates equal sized partitions. When shuffle=False,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the repartition and; coalesce commands in Spark, respectively. In particular,; when shuffle=False, n_partitions cannot exceed current; number of partitions. Parameters:. n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:51426,avail,available,51426,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['avail'],['available']
Availability,"_diploid_gt_index_call(4)); Call(alleles=[1, 2], phased=False). Parameters:; gt_index (int or Expression of type tint32) – Unphased, diploid genotype index. Returns:; CallExpression. hail.expr.functions.parse_call(s)[source]; Construct a call expression by parsing a string or string expression.; Examples; >>> hl.eval(hl.parse_call('0|2')); Call(alleles=[0, 2], phased=True). Notes; This method expects strings in the following format:. ploidy; Phased; Unphased. 0; |-; -. 1; |i; i. 2; i|j; i/j. 3; i|j|k; i/j/k. N; i|j|k|...|N; i/j/k/.../N. Parameters:; s (str or StringExpression) – String to parse. Returns:; CallExpression. hail.expr.functions.downcode(c, i)[source]; Create a new call by setting all alleles other than i to ref; Examples; Preserve the third allele and downcode all other alleles to reference.; >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters:. c (CallExpression) – A call.; i (Expression of type tint32) – The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns:; CallExpression. hail.expr.functions.triangle(n)[source]; Returns the triangle number of n.; Examples; >>> hl.eval(hl.triangle(3)); 6. Notes; The calculation is n * (n + 1) / 2. Parameters:; n (Expression of type tint32). Returns:; Expression of type tint32. hail.expr.functions.is_snp(ref, alt)[source]; Returns True if the alleles constitute a single nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_mnp(ref, alt)[source]; Returns True if the alleles constitute a multiple nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_mnp('AA', 'GT')); True. Para",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:11695,down,downcode,11695,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['down'],['downcode']
Availability,"_fields:; update_entries_expression['GT'] = hl.downcode(split.GT, split.a_index); if 'DP' in entry_fields:; update_entries_expression['DP'] = split.DP; if 'AD' in entry_fields:; update_entries_expression['AD'] = hl.or_missing(; hl.is_defined(split.AD), [hl.sum(split.AD) - split.AD[split.a_index], split.AD[split.a_index]]; ); if 'PL' in entry_fields:; pl = hl.or_missing(; hl.is_defined(split.PL),; (; hl.range(0, 3).map(; lambda i: hl.min(; (; hl.range(0, hl.triangle(split.old_alleles.length())); .filter(; lambda j: hl.downcode(; hl.unphased_diploid_gt_index_call(j), split.a_index; ).unphased_diploid_gt_index(); == i; ); .map(lambda j: split.PL[j]); ); ); ); ),; ); if 'GQ' in entry_fields:; update_entries_expression['PL'] = pl; update_entries_expression['GQ'] = hl.or_else(hl.gq_from_pl(pl), split.GQ); else:; update_entries_expression['PL'] = pl; elif 'GQ' in entry_fields:; update_entries_expression['GQ'] = split.GQ. if 'PGT' in entry_fields:; update_entries_expression['PGT'] = hl.downcode(split.PGT, split.a_index); if 'PID' in entry_fields:; update_entries_expression['PID'] = split.PID; return split.annotate_entries(**update_entries_expression).drop('old_locus', 'old_alleles'). [docs]@typecheck(call_expr=expr_call); def genetic_relatedness_matrix(call_expr) -> BlockMatrix:; r""""""Compute the genetic relatedness matrix (GRM). Examples; --------. >>> grm = hl.genetic_relatedness_matrix(dataset.GT). Notes; -----; The genetic relationship matrix (GRM) :math:`G` encodes genetic correlation; between each pair of samples. It is defined by :math:`G = MM^T` where; :math:`M` is a standardized version of the genotype matrix, computed as; follows. Let :math:`C` be the :math:`n \times m` matrix of raw genotypes; in the variant dataset, with rows indexed by :math:`n` samples and columns; indexed by :math:`m` bialellic autosomal variants; :math:`C_{ij}` is the; number of alternate alleles of variant :math:`j` carried by sample; :math:`i`, which can be 0, 1, 2, or missing. For each vari",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:125367,down,downcode,125367,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['down'],['downcode']
Availability,"_file='data/example.sample'); ... .write('output/gen_example2.vds')). Notes; For more information on the .gen file format, see here.; To ensure that the .gen file(s) and .sample file are correctly prepared for import:. If there are only 5 columns before the start of the genotype probability data (chromosome field is missing), you must specify the chromosome using the chromosome parameter; No duplicate sample IDs are allowed. The first column in the .sample file is used as the sample ID s.; Also, see section in import_bgen() linked here for information about Hail’s genotype probability representation.; Annotations; import_gen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .gen files to import.; sample_file (str) – The sample file.; tolerance (float) – If the sum of the genotype probabilities for a genotype differ from 1.0 by more than the tolerance, set the genotype to missing.; min_partitions (int or None) – Number of partitions.; chromosome (str or None) – Chromosome if not listed in the .gen file. Returns:Variant dataset imported from .gen and .sample files. Return type:VariantDataset. import_plink(bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quantpheno=False)[source]¶; Import PLINK binary file (BED, BIM, FAM) as variant dataset.; Examples; Import data from a PLINK binary file:; >>> vds = hc.import_plink(bed=""data/test.bed"",; ... bim=""data/test.bim"",; ... fam=""data/test.fam""). Notes; Only binary SNP-major mode files can be read into Hail. To convert your file from individual-major mode to SNP-major mode, use PLINK to read in your fileset and use the --make-bed option.; The centiMorgan position is not currently used in Hail (Column 3 in BIM file).; The ID (s) used by Hail is the individual ID (column 2 in FAM file). Warning; No d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:12015,toler,tolerance,12015,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['toler'],['tolerance']
Availability,"_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:61042,toler,tolerance,61042,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['toler'],['tolerance']
Availability,"_float64,; max=expr_float64,; max_iter=builtins.int,; epsilon=builtins.float,; tolerance=builtins.float,; ); def uniroot(f: Callable, min, max, *, max_iter=1000, epsilon=2.2204460492503131e-16, tolerance=1.220703e-4):; """"""Finds a root of the function `f` within the interval `[min, max]`. Examples; --------. >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; -----; `f(min)` and `f(max)` must not have the same sign. If no root can be found, the result of this call will be `NA` (missing). :func:`.uniroot` returns an estimate for a root with accuracy; `4 * epsilon * abs(x) + tolerance`. 4*EPSILON*abs(x) + tol. Parameters; ----------; f : function ( (arg) -> :class:`.Float64Expression`); Must return a :class:`.Float64Expression`.; min : :class:`.Float64Expression`; max : :class:`.Float64Expression`; max_iter : `int`; The maximum number of iterations before giving up.; epsilon : `float`; The scaling factor in the accuracy of the root found.; tolerance : `float`; The constant factor in approximate accuracy of the root found. Returns; -------; :class:`.Float64Expression`; The root of the function `f`.; """""". # Based on:; # https://github.com/wch/r-source/blob/e5b21d0397c607883ff25cca379687b86933d730/src/library/stats/src/zeroin.c. def error_if_missing(x):; res = f(x); return case().when(is_defined(res), res).or_error(format(""'uniroot': value of f(x) is missing for x = %.1e"", x)). wrapped_f = hl.experimental.define_function(error_if_missing, 'float'). def uniroot(recur, a, b, c, fa, fb, fc, prev, iterations_remaining):; tol = 2 * epsilon * abs(b) + tolerance / 2; cb = c - b; t1 = fb / fc; t2 = fb / fa; q1 = fa / fc # = t1 / t2; pq = if_else(; a == c,; (cb * t1) / (t1 - 1.0), # linear; -t2 * (cb * q1 * (q1 - t1) - (b - a) * (t1 - 1.0)) / ((q1 - 1.0) * (t1 - 1.0) * (t2 - 1.0)),; ) # quadratic. interpolated = if_else(; (sign(pq) == sign(cb)); & (0.75 * abs(cb) > abs(pq) + tol / 2) # b + pq within [b, c]; & (abs(pq) < abs(prev / 2)), # pq not too large; pq,; cb / 2,; ). ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:173271,toler,tolerance,173271,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['toler'],['tolerance']
Availability,"_hom_var = Env.get_uid(); is_defined = Env.get_uid(); mt = mt.unfilter_entries(); mt = mt.select_entries(**{; is_hom_ref: hl.float(hl.or_else(mt[call].is_hom_ref(), 0)),; is_het: hl.float(hl.or_else(mt[call].is_het(), 0)),; is_hom_var: hl.float(hl.or_else(mt[call].is_hom_var(), 0)),; is_defined: hl.float(hl.is_defined(mt[call])),; }); ref = hl.linalg.BlockMatrix.from_entry_expr(mt[is_hom_ref], block_size=block_size); het = hl.linalg.BlockMatrix.from_entry_expr(mt[is_het], block_size=block_size); var = hl.linalg.BlockMatrix.from_entry_expr(mt[is_hom_var], block_size=block_size); defined = hl.linalg.BlockMatrix.from_entry_expr(mt[is_defined], block_size=block_size); ref_var = (ref.T @ var).checkpoint(hl.utils.new_temp_file()); # We need the count of times the pair is AA,aa and aa,AA. ref_var is only; # AA,aa. Transposing ref_var gives var_ref, i.e. aa,AA.; #; # n.b. (REF.T @ VAR).T == (VAR.T @ REF) by laws of matrix multiply; N_AA_aa = ref_var + ref_var.T; N_Aa_Aa = (het.T @ het).checkpoint(hl.utils.new_temp_file()); # We count the times the row individual has a heterozygous genotype and the; # column individual has any defined genotype at all.; N_Aa_defined = (het.T @ defined).checkpoint(hl.utils.new_temp_file()). het_hom_balance = N_Aa_Aa - (2 * N_AA_aa); het_hom_balance = het_hom_balance.to_matrix_table_row_major(); n_hets_for_rows = N_Aa_defined.to_matrix_table_row_major(); n_hets_for_cols = N_Aa_defined.T.to_matrix_table_row_major(). kinship_between = het_hom_balance.rename({'element': 'het_hom_balance'}); kinship_between = kinship_between.annotate_entries(; n_hets_row=n_hets_for_rows[kinship_between.row_key, kinship_between.col_key].element,; n_hets_col=n_hets_for_cols[kinship_between.row_key, kinship_between.col_key].element,; ). col_index_field = Env.get_uid(); col_key = mt.col_key; cols = mt.add_col_index(col_index_field).key_cols_by(col_index_field).cols(). kinship_between = kinship_between.key_cols_by(**cols[kinship_between.col_idx].select(*col_key)). renam",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html:9144,checkpoint,checkpoint,9144,docs/0.2/_modules/hail/methods/relatedness/king.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html,2,['checkpoint'],['checkpoint']
Availability,"_uid: hl.empty_array(hl.tstruct())}); ._unlocalize_entries(entries_uid, col_values_uid, []); ). [docs] @typecheck_method(p=numeric, seed=nullable(int)); def sample_rows(self, p: float, seed=None) -> 'MatrixTable':; """"""Downsample the matrix table by keeping each row with probability ``p``. Examples; --------; Downsample the dataset to approximately 1% of its rows. >>> small_dataset = dataset.sample_rows(0.01). Notes; -----; Although the :class:`MatrixTable` returned by this method may be; small, it requires a full pass over the rows of the sampled object. Parameters; ----------; p : :obj:`float`; Probability of keeping each row.; seed : :obj:`int`; Random seed. Returns; -------; :class:`.MatrixTable`; Matrix table with approximately ``p * n_rows`` rows.; """""". if not 0 <= p <= 1:; raise ValueError(""Requires 'p' in [0,1]. Found p={}"".format(p)). return self.filter_rows(hl.rand_bool(p, seed)). [docs] @typecheck_method(p=numeric, seed=nullable(int)); def sample_cols(self, p: float, seed=None) -> 'MatrixTable':; """"""Downsample the matrix table by keeping each column with probability ``p``. Examples; --------; Downsample the dataset to approximately 1% of its columns. >>> small_dataset = dataset.sample_cols(0.01). Parameters; ----------; p : :obj:`float`; Probability of keeping each column.; seed : :obj:`int`; Random seed. Returns; -------; :class:`.MatrixTable`; Matrix table with approximately ``p * n_cols`` column.; """""". if not 0 <= p <= 1:; raise ValueError(""Requires 'p' in [0,1]. Found p={}"".format(p)). return self.filter_cols(hl.rand_bool(p, seed)). [docs] @typecheck_method(fields=dictof(str, str)); def rename(self, fields: Dict[str, str]) -> 'MatrixTable':; """"""Rename fields of a matrix table. Examples; --------. Rename column key `s` to `SampleID`, still keying by `SampleID`. >>> dataset_result = dataset.rename({'s': 'SampleID'}). You can rename a field to a field name that already exists, as long as; that field also gets renamed (no name collisions). Here, we rename t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:128706,Down,Downsample,128706,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['Down'],['Downsample']
Availability,"_versions.append(version); return available_versions. def __init__(self, url: Union[dict, str], version: Optional[str], reference_genome: Optional[str]):; self.url = url; self.version = version; self.reference_genome = reference_genome. def in_region(self, name: str, region: str) -> bool:; """"""Check if a :class:`.DatasetVersion` object is accessible in the; desired region. Parameters; ----------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset exists in the specified region.; """"""; current_version = self.version; available_regions = [k for k in self.url.keys()]; valid_region = region in available_regions; if not valid_region:; message = (; f'\nName: {name}\n'; f'Version: {current_version}\n'; f'This dataset exists but is not yet available in the'; f' {region} region bucket.\n'; f'Dataset is currently available in the'; f' {"", "".join(available_regions)} region bucket(s).\n'; f'Reach out to the Hail team at https://discuss.hail.is/'; f' to request this dataset in your region.'; ); warnings.warn(message, UserWarning, stacklevel=1); return valid_region. def maybe_index(self, indexer_key_expr: StructExpression, all_matches: bool) -> Optional[StructExpression]:; """"""Find the prefix of the given indexer expression that can index the; :class:`.DatasetVersion`, if it exists. Parameters; ----------; indexer_key_expr : :class:`StructExpression`; Row key struct from relational object to be annotated.; all_matches : :obj:`bool`; ``True`` if `indexer_key_expr` key is not unique, indicated in; :attr:`.Dataset.key_properties` for each dataset. If ``True``, value; of `indexer_key_expr` is array of all matches. If ``False``, there; will only be single value of expression. Returns; -------; :class:`StructExpression`, optional; Struct of compatible indexed values, if they exist.; """"""; return hl.read_ta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:4167,avail,available,4167,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['avail'],['available']
Availability,"`, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6423,error,errors,6423,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"`.MatrixTable.select_entries`, :meth:`.MatrixTable.transmute_entries`. The resulting dataset will be keyed by the split locus and alleles. :func:`.split_multi` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. - `old_locus` (*locus*) -- The original, unsplit locus. - `old_alleles` (*array<str>*) -- The original, unsplit alleles. All other fields are left unchanged. Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:112309,error,errors,112309,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['error'],['errors']
Availability,"`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded genotype, and shift so the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Expression Variables**. The following symbols are in scope for ``expr``:. - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; - ``aIndex`` (*Int*): the index of the allele being tested. The following symbols are in scope for ``annotation``:. - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; - ``aIndices`` (*Array[Int]*): the array of old indices (such that ``aIndices[newIndex] = oldIndex`` and ``aIndices[0] = 0``). :param str expr: Boolean filter expression involving v (variant), va (variant annotations), ; and aIndex (allele index). :param str annotation: Annotation modifying expression involving v (new variant), va (old variant annotations),; and aIndices (maps from new to old indices). :param bool subset: If true, subsets PL and AD, otherwise downcodes the PL and AD.; Genotype and GQ are set based on the resulting PLs. :param bool keep: If true, keep variants matching expr. :param bool filter_altered_genotypes: If true, genotypes that contain filtered-out alleles are set to missing. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :param bool keepStar: If true, keep variants where the only allele left is a ``*`` allele. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.filterAlleles(expr, annotation, filter_altered_genotypes, keep, subset, max_shift,; keep_star); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(expr=strlike,; keep=bool); def filter_genotypes(self, expr, keep=True):; """"""Filter genotypes based on expression. **Examples**. Filter genotypes by allele balance dependent on genot",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:65166,down,downcodes,65166,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['down'],['downcodes']
Availability,"`dict`; Coefficients to multiply each field. The coefficients are specified by; `coef_dict` value, the row (or col) field name is specified by `coef_dict` key.; """"""; assert str_expr is not None or ref_coef_dict is not None, ""str_expr and ref_coef_dict cannot both be None""; assert axis in {'rows', 'cols'}, ""axis must be 'rows' or 'cols'""; fields_to_search = tb.row if axis == 'rows' or isinstance(tb, Table) else tb.col; # when axis='rows' we're searching for annotations, axis='cols' searching for covariates; axis_field = 'annotation' if axis == 'rows' else 'covariate'; if str_expr is None:; # take all row (or col) fields in mt matching keys in coef_dict; coef_dict = {k: ref_coef_dict[k] for k in ref_coef_dict.keys() if k in fields_to_search}; # if intersect is empty: return error; assert len(coef_dict) > 0, f'None of the keys in ref_coef_dict match any {axis[:-1]} fields'; return coef_dict # return subset of ref_coef_dict; else:; # str_expr search in list of row (or col) fields; fields = [rf for rf in list(fields_to_search) if str_expr in rf]; assert len(fields) > 0, f'No {axis[:-1]} fields matched str_expr search: {str_expr}'; if ref_coef_dict is None:; print(f'Assuming coef = 1 for all {axis_field}s'); return {k: 1 for k in fields}; in_ref_coef_dict = set(fields).intersection(set(ref_coef_dict.keys())) # fields in ref_coef_dict; # if >0 fields returned by search are not in ref_coef_dict; if in_ref_coef_dict != set(fields):; # if none of the fields returned by search are in ref_coef_dict; assert len(in_ref_coef_dict) > 0, f'None of the {axis_field} fields in ref_coef_dict match search results'; fields_to_ignore = set(fields).difference(in_ref_coef_dict); print(f'Ignored fields from {axis_field} search: {fields_to_ignore}'); print('To include ignored fields, change str_expr to match desired fields'); fields = list(in_ref_coef_dict); return {k: ref_coef_dict[k] for k in fields}. [docs]@typecheck(mt=MatrixTable, y=expr_int32, P=oneof(int, float)); def ascertainment_bias(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html:30907,error,error,30907,docs/0.2/_modules/hail/experimental/ldscsim.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html,2,['error'],['error']
Availability,"`pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:36863,toler,tolerance,36863,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,"`shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.row_key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_matrix_table(tmp2).add_row_index(uid).key_rows_by(uid); ht.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions).drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions). return MatrixTable(; ir.MatrixRepartition(; self._mir, n_partitions, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'MatrixTable':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> dataset_result = dataset.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than or equal to `max_partitions`, do nothing. Returns;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:109052,checkpoint,checkpoint,109052,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['checkpoint'],['checkpoint']
Availability,"`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :class:`str`; Path to FASTA index file. Must be uncompressed.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end). Returns; -------; :class:`.ReferenceGenome`; """"""; par_strings = [""{}:{}-{}"".format(contig, start, end) for (contig, start, end) in par]; config = Env.backend().from_fasta_file(; name, fasta_file, index_file, x_contigs, y_contigs, mt_contigs, par_strings; ). rg = ReferenceGenome._from_config(config); rg.add_sequence(fasta_file, index_file); return rg. [docs] @typecheck_method(dest_reference_genome=reference_genome_type); def has_liftover(self, dest_reference_genome):; """"""``True`` if a liftover chain file is available from this reference; genome to the destination reference. Parameters; ----------; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`. Returns; -------; :obj:`bool`; """"""; return dest_reference_genome.name in self._liftovers. [docs] @typecheck_method(dest_reference_genome=reference_genome_type); def remove_liftover(self, dest_reference_genome):; """"""Remove liftover to `dest_reference_genome`. Parameters; ----------; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; """"""; if dest_reference_genome.name in self._liftovers:; del self._liftovers[dest_reference_genome.name]; Env.backend().remove_liftover(self.name, dest_reference_genome.name). [docs] @typecheck_method(chain_file=str, dest_reference_genome=reference_genome_type); def add_liftover(self, chain_file, dest_reference_genome):; """"""Register a chain file for liftover. Examples; --------; Access GRCh37 and GRCh38 using :func:`~hail.get_reference`:. >>>",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:12758,avail,available,12758,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,2,['avail'],['available']
Availability,"a PI_HAT value greater than or equal to 0.6.; ; >>> pruned_vds = vds.ibd_prune(0.6). Prune samples so that no two have a PI_HAT value greater than or equal to 0.5, with a tiebreaking expression that ; selects cases over controls:. >>> pruned_vds = vds.ibd_prune(; ... 0.5,; ... tiebreaking_expr=""if (sa1.isCase && !sa2.isCase) -1 else if (!sa1.isCase && sa2.isCase) 1 else 0""). **Notes**. The variant dataset returned may change in near future as a result of algorithmic improvements. The current algorithm is very efficient on datasets with many small; families, less so on datasets with large families. Currently, the algorithm works by deleting the person from each family who has the highest number of relatives,; and iterating until no two people have a PI_HAT value greater than that specified. If two people within a family have the same number of relatives, the tiebreaking_expr; given will be used to determine which sample gets deleted. ; ; The tiebreaking_expr namespace has the following variables available:; ; - ``s1``: The first sample id.; - ``sa1``: The annotations associated with s1.; - ``s2``: The second sample id. ; - ``sa2``: The annotations associated with s2. ; ; The tiebreaking_expr returns an integer expressing the preference for one sample over the other. Any negative integer expresses a preference for keeping ``s1``. Any positive integer expresses a preference for keeping ``s2``. A zero expresses no preference. This function must induce a `preorder <https://en.wikipedia.org/wiki/Preorder>`__ on the samples, in particular:. - ``tiebreaking_expr(sample1, sample2)`` must equal ``-1 * tie breaking_expr(sample2, sample1)``, which evokes the common sense understanding that if ``x < y`` then `y > x``.; - ``tiebreaking_expr(sample1, sample1)`` must equal 0, i.e. ``x = x``; - if sample1 is preferred to sample2 and sample2 is preferred to sample3, then sample1 must also be preferred to sample3. The last requirement is only important if you have three related sample",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:85472,avail,available,85472,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['avail'],['available']
Availability,"a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines.; >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... do_chores(b, user); >>> b.run(). Lastly, we provide an example of a more complicated batch that has an initial; job, then scatters jobs per user, then has a series of gather / sink jobs; to wait for the per user jobs to be done before completing. >>> def do_chores(b, head, user):; ... chores = []; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); ... j.depends_on(head); ... chores.append(j); ... sink = b.new_job(name=f'{user}-sink'); ... sink.depends_on(*chores); ... return sink. >>> b = hb.Batch(name='nested-scatter-3'); >>> head = b.new_job(name='head'); >>> user_sinks = []; >>> for user in ['Alice', 'Bob', 'Dan']:; ... user_sink = do_chores(b, head, user); ... user_sinks.append(user_sink); >>> final_sink = b.new_job(name='final-sink'); >>> final_sink.depends_on(*user_sinks); >>> b.run(). Input Files; Previously, we discussed that JobResourceFile are temporary files and; are created from Job objects. However, in order to read a file that; was not generated by executing jobs (input file), we use the method; Batch.read_input() to create an InputResourceFile. An input; resource file can be used exactly in the same way as a; JobResourceFile. We can refer to an input resource file in a command; using an f-string. In the example below, we add the file data/hello.txt as an; input resource file called inpu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:10250,echo,echo,10250,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability,"a will be stored in this cloud storage folder.; google_project (Optional[str]) – This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[str]) – The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions (Optional[List[str]]) – Cloud regions in which jobs may run. ServiceBackend.ANY_REGION indicates jobs may; run in any region. If unspecified or None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]]) – A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use “cold” storage. Attributes. ANY_REGION; A special value that indicates a job may run in any region. Methods. _async_run; Execute a batch. supported_regions; Get the supported cloud regions. ANY_REGION: ClassVar[List[str]] = ['any_region']; A special value that indicates a job may run in any region. async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait=True, open=False, disable_progress_bar=False, callback=None, token=None, **backend_kwargs); Execute a batch. Warning; This method should not be called directly. Instead, use batch.Batch.run(); and pass ServiceBackend specific arguments as key-word arguments. Parameters:. batch (Batch) – Batch to execute.; dry_run (bool) – If True, don’t execute code.; verbose (bool) – If True, print debugging output.; delete_scratch_on_exit",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:4470,avail,available,4470,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,2,['avail'],['available']
Availability,"a: variant annotations; aIndex (Int): the index of the allele being tested. The following symbols are in scope for annotation:. v (Variant): Variant; va: variant annotations; aIndices (Array[Int]): the array of old indices (such that aIndices[newIndex] = oldIndex and aIndices[0] = 0). Parameters:; expr (str) – Boolean filter expression involving v (variant), va (variant annotations), ; and aIndex (allele index); annotation (str) – Annotation modifying expression involving v (new variant), va (old variant annotations),; and aIndices (maps from new to old indices); subset (bool) – If true, subsets PL and AD, otherwise downcodes the PL and AD.; Genotype and GQ are set based on the resulting PLs.; keep (bool) – If true, keep variants matching expr; filter_altered_genotypes (bool) – If true, genotypes that contain filtered-out alleles are set to missing.; max_shift (int) – maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered.; keepStar (bool) – If true, keep variants where the only allele left is a * allele. Returns:Filtered variant dataset. Return type:VariantDataset. filter_genotypes(expr, keep=True)[source]¶; Filter genotypes based on expression.; Examples; Filter genotypes by allele balance dependent on genotype call:; >>> vds_result = vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ' +; ... '((g.isHomRef() && ab <= 0.1) || ' +; ... '(g.isHet() && ab >= 0.25 && ab <= 0.75) || ' +; ... '(g.isHomVar() && ab >= 0.9))'). Notes; expr is in genotype context so the following symbols are in scope:. s (Sample): sample; v (Variant): Variant; sa: sample annotations; va: variant annotations; global: global annotations. For more information, see the documentation on data representation, annotations, and; the expression language. Caution; When expr evaluates to missing, the genotype will be removed regardless of whether keep=True or keep=False. Parameters",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:51974,error,error,51974,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,"a_g^2)`, which in turn determines :math:`\\hat{\sigma}_e^2` and :math:`\\hat{h}^2`. We first compute the maximum log likelihood on a :math:`\delta`-grid that is uniform on the log scale, with :math:`\\mathrm{ln}(\delta)` running from -8 to 8 by 0.01, corresponding to :math:`h^2` decreasing from 0.9995 to 0.0005. If :math:`h^2` is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when :math:`\\hat{h}^2` is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_method>`__ to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on :math:`\\mathrm{ln}(\delta)` of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid. Note that :math:`h^2` is related to :math:`\\mathrm{ln}(\delta)` through the `sigmoid function <https://en.wikipedia.org/wiki/Sigmoid_function>`_. More precisely,. .. math::. h^2 = 1 - \mathrm{sigmoid}(\\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\\mathrm{ln}(\delta)). Hence one can change variables to extract a high-resolution discretization of the likelihood function of :math:`h^2` over :math:`[0,1]` at the corresponding REML estimators for :math:`\\beta` and :math:`\sigma_g^2`, as well as integrate over the normalized likelihood function using `change of variables <https://en.wikipedia.org/wiki/Integration_by_substitution>`_ and the `sigmoid differential equation <https://en.wikipedia.org/wiki/Sigmoid_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:131502,toler,tolerance,131502,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['toler'],['tolerance']
Availability,"a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; We will explain by example. Consider a hypothetical 3-allelic; variant:; A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi_hts() will create two biallelic variants (one for each; alternate allele) at the same position; A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT or PGT field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and 0/1.; The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the sum of the; other multiallelic entries.; The biallelic DP is the same as the multiallelic DP.; The biallelic PL entry for a genotype g is the minimum over PL entries; for multiallelic genotypes that downcode to g. For example, the PL for (A,; T) at 0/1 is the minimum of the PLs for 0/1 (50) and 1/2 (45), and thus 45.; Fixing an alternate allele and biallelic variant, downcoding gives a map; from multiallelic to biallelic alleles and genotypes. The biallelic AD entry; for an allele is just the sum of the multiallelic AD entries for alleles; that map to that allele. Similarly, the biallelic PL entry for a genotype is; the minimum over multiallelic PL entries for genotypes that map to that; genotype.; GQ is recomputed from PL if PL is provided and is not; missing. If not, it is copied from the original GQ.; Here is a second example for a het non-ref; A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as; A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. VCF Info Fields; Hail does not split fields in the info field. This means that if a; multiallelic site with info.AC value [10, 2] is split, each split; site will contain the same array [1",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:88920,down,downcode,88920,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['down'],['downcode']
Availability,"able.; There are only two operations on a grouped table, GroupedTable.partition_hint(); and GroupedTable.aggregate().; Attributes. Methods. aggregate; Aggregate by group, used after Table.group_by(). partition_hint; Set the target number of partitions for aggregation. aggregate(**named_exprs)[source]; Aggregate by group, used after Table.group_by().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters:; named_exprs (varargs of Expression) – Aggregation expressions. Returns:; Table – Aggregated table. partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a Table.group_by() / GroupedTable.aggregate(); pipeline:; >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedTable.aggregate() is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedTable – Same grouped table with a partition hint. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.GroupedTable.html:2486,down,downstream,2486,docs/0.2/hail.GroupedTable.html,https://hail.is,https://hail.is/docs/0.2/hail.GroupedTable.html,1,['down'],['downstream']
Availability,"able.locus)). sample_table_count = sample_table.count() - 2 # Skipping first 2 unneeded rows in sample file; gen_table = gen_table.annotate_globals(cols=hl.range(sample_table_count).map(lambda x: hl.struct(col_idx=x))); mt = gen_table._unlocalize_entries('entries', 'cols', ['col_idx']). sample_table = sample_table.tail(sample_table_count).add_index(); sample_table = sample_table.annotate(s=sample_table.text.split(' ')[0]); sample_table = sample_table.key_by(sample_table.idx); mt = mt.annotate_cols(s=sample_table[hl.int64(mt.col_idx)].s). mt = mt.annotate_entries(; GP=hl.rbind(; hl.sum(mt.GP),; lambda gp_sum: hl.if_else(; hl.abs(1.0 - gp_sum) > tolerance, hl.missing(hl.tarray(hl.tfloat64)), hl.abs((1 / gp_sum) * mt.GP); ),; ); ); mt = mt.annotate_entries(; GT=hl.rbind(; hl.argmax(mt.GP),; lambda max_idx: hl.if_else(; hl.len(mt.GP.filter(lambda y: y == mt.GP[max_idx])) == 1,; hl.switch(max_idx); .when(0, hl.call(0, 0)); .when(1, hl.call(0, 1)); .when(2, hl.call(1, 1)); .or_error(""error creating gt field.""),; hl.missing(hl.tcall),; ),; ); ); mt = mt.filter_entries(hl.is_defined(mt.GP)). mt = mt.key_cols_by('s').drop('col_idx', 'file', 'data'); mt = mt.key_rows_by('locus', 'alleles').select_entries('GT', 'GP'); return mt. [docs]@typecheck(; paths=oneof(str, sequenceof(str)),; key=table_key_type,; min_partitions=nullable(int),; impute=bool,; no_header=bool,; comment=oneof(str, sequenceof(str)),; delimiter=str,; missing=oneof(str, sequenceof(str)),; types=dictof(str, hail_type),; quote=nullable(char),; skip_blank_lines=bool,; force_bgz=bool,; filter=nullable(str),; find_replace=nullable(sized_tupleof(str, str)),; force=bool,; source_file_field=nullable(str),; ); def import_table(; paths,; key=None,; min_partitions=None,; impute=False,; no_header=False,; comment=(),; delimiter=""\t"",; missing=""NA"",; types={},; quote=None,; skip_blank_lines=False,; force_bgz=False,; filter=None,; find_replace=None,; force=False,; source_file_field=None,; ) -> Table:; """"""Import delimited text",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:53070,error,error,53070,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['error'],['error']
Availability,"able` whose rows and columns are keys are taken from; `call-expr`'s column keys. It has one entry field, `phi`.; """"""; mt = matrix_table_source('king/call_expr', call_expr); call = Env.get_uid(); mt = mt.annotate_entries(**{call: call_expr}). is_hom_ref = Env.get_uid(); is_het = Env.get_uid(); is_hom_var = Env.get_uid(); is_defined = Env.get_uid(); mt = mt.unfilter_entries(); mt = mt.select_entries(**{; is_hom_ref: hl.float(hl.or_else(mt[call].is_hom_ref(), 0)),; is_het: hl.float(hl.or_else(mt[call].is_het(), 0)),; is_hom_var: hl.float(hl.or_else(mt[call].is_hom_var(), 0)),; is_defined: hl.float(hl.is_defined(mt[call])),; }); ref = hl.linalg.BlockMatrix.from_entry_expr(mt[is_hom_ref], block_size=block_size); het = hl.linalg.BlockMatrix.from_entry_expr(mt[is_het], block_size=block_size); var = hl.linalg.BlockMatrix.from_entry_expr(mt[is_hom_var], block_size=block_size); defined = hl.linalg.BlockMatrix.from_entry_expr(mt[is_defined], block_size=block_size); ref_var = (ref.T @ var).checkpoint(hl.utils.new_temp_file()); # We need the count of times the pair is AA,aa and aa,AA. ref_var is only; # AA,aa. Transposing ref_var gives var_ref, i.e. aa,AA.; #; # n.b. (REF.T @ VAR).T == (VAR.T @ REF) by laws of matrix multiply; N_AA_aa = ref_var + ref_var.T; N_Aa_Aa = (het.T @ het).checkpoint(hl.utils.new_temp_file()); # We count the times the row individual has a heterozygous genotype and the; # column individual has any defined genotype at all.; N_Aa_defined = (het.T @ defined).checkpoint(hl.utils.new_temp_file()). het_hom_balance = N_Aa_Aa - (2 * N_AA_aa); het_hom_balance = het_hom_balance.to_matrix_table_row_major(); n_hets_for_rows = N_Aa_defined.to_matrix_table_row_major(); n_hets_for_cols = N_Aa_defined.T.to_matrix_table_row_major(). kinship_between = het_hom_balance.rename({'element': 'het_hom_balance'}); kinship_between = kinship_between.annotate_entries(; n_hets_row=n_hets_for_rows[kinship_between.row_key, kinship_between.col_key].element,; n_hets_col=n_hets_for_cols[ki",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html:8848,checkpoint,checkpoint,8848,docs/0.2/_modules/hail/methods/relatedness/king.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html,2,['checkpoint'],['checkpoint']
Availability,"adings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; compute_scores=bool,; transpose=bool,; ); def _blanczos_pca(; A,; k=10,; compute_loadings=False,; q_iterations=10,; oversampling_param=None,; block_size=128,; compute_scores=True,; transpose=False,; ):; r""""""Run randomized principal component analysis approximation (PCA); on numeric columns derived from a matrix table. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls. >>> eigenvalues, scores, _ = hl._blanczos_pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; -------; This method does **not** automatically mean-center or normalize each column.; If desired, such transformations should be incorporated in `entry_expr`. Hail will return an error if `entry_expr` evaluates to missing, nan, or; infinity on any entry. Notes; -----. PCA is run on the columns of the numeric matrix obtained by evaluating; `entry_expr` on each entry of the matrix table, or equivalently on the rows; of the **transposed** numeric matrix :math:`M` referenced below. PCA computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in; :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors; (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:18894,error,error,18894,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['error'],['error']
Availability,"agg.sum(ht.global_value * ht.a)); 30. Warning; Parallelizing very large local arrays will be slow. Parameters:. rows – List of row values, or expression of type array<struct{...}>.; schema (str or a hail type (see Types), optional) – Value type.; key (Union[str, List[str]]], optional) – Key field(s).; n_partitions (int, optional); partial_type (dict, optional) – A value type which may elide fields or have None in arbitrary places. The partial; type is used by hail where the type cannot be imputed.; globals (dict of str to any or StructExpression, optional) – A dict or struct{..} containing supplementary global data. Returns:; Table – A distributed Hail table created from the local collection of rows. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the table to both memory and disk:; >>> table = table.persist() . Notes; The Table.persist() and Table.cache() methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for Table.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; Table – Persisted table. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:54156,redundant,redundant,54156,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['redundant'],['redundant']
Availability,"aggregate(; fam_id=hl.agg.take(table2.fam_id, 1)[0],; children=hl.int32(hl.agg.count()),; errors=hl.agg.sum(table2.errors),; snp_errors=hl.agg.sum(table2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table3.snp_errors[1],; }),; hl.struct(**{; ck_name: table3.proband[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[2],; 'snp_errors': table3.snp_errors[2],; }),; ]; ); table3 = table3.explode('xs'); table3 = table3.select(**table3.xs); table3 = (; table3.group_by(ck_name, 'fam_id'); .aggregate(errors=hl.agg.sum(table3.errors), snp_errors=hl.agg.sum(table3.snp_errors)); .key_by(ck_name); ). table4 = tm.select_rows(errors=hl.agg.count_where(hl.is_defined(tm.mendel_code))).rows(). return table1, table2, table3, table4. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree); def transmission_disequilibrium_test(dataset, pedigree) -> Table:; r""""""Performs the transmission disequilibrium test on trios. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:12032,error,errors,12032,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"ail's initial version of :py:meth:`.lmmreg` scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used :py:meth:`.lmmreg` in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on `Google cloud <http://discuss.hail.is/t/using-hail-on-the-google-cloud-platform/80>`__. While :py:meth:`.lmmreg` computes the kinship matrix :math:`K` using distributed matrix multiplication (Step 2), the full `eigendecomposition <https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix>`__ (Step 3) is currently run on a single core of master using the `LAPACK routine DSYEVD <http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html>`__, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in :math:`n` are available `here <https://github.com/hail-is/hail/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:125300,avail,available,125300,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['avail'],['available']
Availability,"ail.VariantDataset.grm` is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. :param bool force_block: Force using Spark's BlockMatrix to compute kinship (advanced). :param bool force_gramian: Force using Spark's RowMatrix.computeGramian to compute kinship (advanced). :return: Realized Relationship Matrix for all samples.; :rtype: :py:class:`KinshipMatrix`; """"""; return KinshipMatrix(self._jvdf.rrm(force_block, force_gramian)). [docs] @handle_py4j; @typecheck_method(other=vds_type,; tolerance=numeric); def same(self, other, tolerance=1e-6):; """"""True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values. **Examples**. This will return True:. >>> vds.same(vds). **Notes**. The ``tolerance`` parameter sets the tolerance for equality when comparing floating-point fields. More precisely, :math:`x` and :math:`y` are equal if. .. math::. \abs{x - y} \leq tolerance * \max{\abs{x}, \abs{y}}. :param other: variant dataset to compare against; :type other: :class:`.VariantDataset`. :param float tolerance: floating-point tolerance for equality. :rtype: bool; """""". return self._jvds.same(other._jvds, tolerance). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(root=strlike,; keep_star=bool); def sample_qc(self, root='sa.qc', keep_star=False):; """"""Compute per-sample QC metrics. .. include:: requireTGenotype.rst. **Annotations**. :py:meth:`~hail.VariantDataset.sample_qc` computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; ``sa.qc.<identifier>`` (or ``<root>.<identifier>`` if a non-default root was passed):. +---------------------------+--------+----------------------------------------------------------+; | Name | Type | Description |; +===========================+========+==========================================================+; | ``callRate`` | Double | Fraction of ge",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:196594,toler,tolerance,196594,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['toler'],['tolerance']
Availability,ail.expr.DictExpression method). (hail.expr.Expression method). (hail.expr.Float32Expression method). (hail.expr.Float64Expression method). (hail.expr.Int32Expression method). (hail.expr.Int64Expression method). (hail.expr.IntervalExpression method). (hail.expr.LocusExpression method). (hail.expr.NDArrayExpression method). (hail.expr.NDArrayNumericExpression method). (hail.expr.NumericExpression method). (hail.expr.SetExpression method). (hail.expr.StringExpression method). (hail.expr.StructExpression method). (hail.expr.TupleExpression method). (hail.GroupedMatrixTable method). (hail.MatrixTable method). (hail.Table method). diagonal() (hail.linalg.BlockMatrix method). (in module hail.nd). dict() (in module hail.expr.functions). DictExpression (class in hail.expr). difference() (hail.expr.SetExpression method). distinct() (hail.Table method). distinct_by_col() (hail.MatrixTable method). distinct_by_row() (hail.MatrixTable method). dnorm() (in module hail.expr.functions). downcode() (in module hail.expr.functions). downsample() (in module hail.expr.aggregators). dpois() (in module hail.expr.functions). drop() (hail.expr.StructExpression method). (hail.MatrixTable method). (hail.Table method). dtype (hail.expr.ArrayExpression property). (hail.expr.ArrayNumericExpression property). (hail.expr.BooleanExpression property). (hail.expr.CallExpression property). (hail.expr.CollectionExpression property). (hail.expr.DictExpression property). (hail.expr.Expression property). (hail.expr.Float32Expression property). (hail.expr.Float64Expression property). (hail.expr.Int32Expression property). (hail.expr.Int64Expression property). (hail.expr.IntervalExpression property). (hail.expr.LocusExpression property). (hail.expr.NDArrayExpression property). (hail.expr.NDArrayNumericExpression property). (hail.expr.NumericExpression property). (hail.expr.SetExpression property). (hail.expr.StringExpression property). (hail.expr.StructExpression property). (hail.expr.TupleExpression proper,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genindex.html:18156,down,downcode,18156,docs/0.2/genindex.html,https://hail.is,https://hail.is/docs/0.2/genindex.html,1,['down'],['downcode']
Availability,"aining two arrays: `values` and `ranks`.; The `values` array contains an ordered sample of values seen. The `ranks`; array is one longer, and contains the approximate ranks for the; corresponding values. These represent a summary of the CDF of the distribution of values. In; particular, for any value `x = values(i)` in the summary, we estimate that; there are `ranks(i)` values strictly less than `x`, and that there are; `ranks(i+1)` values less than or equal to `x`. For any value `y` (not; necessarily in the summary), we estimate CDF(y) to be `ranks(i)`, where `i`; is such that `values(i-1) < y ≤ values(i)`. An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value `values(i)` occupying indices `ranks(i)` (inclusive) to; `ranks(i+1)` (exclusive). The returned struct also contains an array `_compaction_counts`, which is; used internally to support downstream error estimation. Warning; -------; This is an approximate and nondeterministic method. Parameters; ----------; expr : :class:`.Expression`; Expression to collect.; k : :obj:`int`; Parameter controlling the accuracy vs. memory usage tradeoff. Returns; -------; :class:`.StructExpression`; Struct containing `values` and `ranks` arrays.; """"""; raw_res = _agg_func(; 'ApproxCDF',; [hl.float64(expr)],; tstruct(levels=tarray(tint32), items=tarray(tfloat64), _compaction_counts=tarray(tint32)),; init_op_args=[k],; ); conv = {; tint32: lambda x: x.map(hl.int),; tint64: lambda x: x.map(hl.int64),; tfloat32: lambda x: x.map(hl.float32),; tfloat64: identity,; }; if _raw:; return raw_res; else:; raw_res = raw_res.annotate(items=conv[expr.dtype](raw_res['items'])); return _result_from_raw_cdf(raw_res). [docs]@typecheck(expr=expr_numeric, qs=expr_oneof(expr_numeric, expr_array(expr_numeric)), k=int); def approx_quantiles(expr, qs, k=100) -> Expression:; """,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:12871,down,downstream,12871,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,4,"['down', 'error']","['downstream', 'error']"
Availability,"aively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> dataset_result = dataset.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; MatrixTable – Matrix table with at most max_partitions partitions. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the dataset to both memory and disk:; >>> dataset = dataset.persist() . Notes; The MatrixTable.persist() and MatrixTable.cache(); methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for Table.write(),; which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; MatrixTable – Persisted dataset. rename(fields)[source]; Rename fields of a matrix table.; Examples; Rename column key s to SampleID, still keying by SampleID.; >>> dataset_result = dataset.rename({'s': 'SampleID'}). You can rename a field to a field name that already exists, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key s to info, and the row field info to vcf_info:; >>> dataset_result =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:49809,redundant,redundant,49809,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['redundant'],['redundant']
Availability,"al/ensembl_gene_annotations.txt',; 'HGDP_annotations': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_pop_and_sex_annotations.tsv',; 'HGDP_matrix_table': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_subset.vcf.bgz',; 'HGDP_ensembl_gene_annotations': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_gene_annotations.tsv',; 'movie_lens_100k': 'https://files.grouplens.org/datasets/movielens/ml-100k.zip',; }. tmp_dir: str = None. def init_temp_dir():; global tmp_dir; if tmp_dir is None:; tmp_dir = new_local_temp_dir(). def _dir_exists(fs, path):; return fs.exists(path) and fs.is_dir(path). def _file_exists(fs, path):; return fs.exists(path) and fs.is_file(path). def _copy_to_tmp(fs, src, extension=None):; dst = new_temp_file(extension=extension); fs.copy(src, dst); return dst. [docs]def get_1kg(output_dir, overwrite: bool = False):; """"""Download subset of the `1000 Genomes <http://www.internationalgenome.org/>`__; dataset and sample annotations. Notes; -----; The download is about 15M. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite any existing files/directories at `output_dir`.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). matrix_table_path = os.path.join(output_dir, '1kg.mt'); vcf_path = os.path.join(output_dir, '1kg.vcf.bgz'); sample_annotations_path = os.path.join(output_dir, '1kg_annotations.txt'); gene_annotations_path = os.path.join(output_dir, 'ensembl_gene_annotations.txt'). if (; overwrite; or not _dir_exists(fs, matrix_table_path); or not _file_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, '1kg.vcf.bgz'); source = resources['1kg_matrix_table']; info(f'downloading 1KG VCF ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, resources['1kg_matrix_table'], tmp_vcf); cluster_readable_vcf = _copy_to_tmp(fs, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:2036,down,download,2036,docs/0.2/_modules/hail/utils/tutorial.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html,2,['down'],['download']
Availability,"al; language documentation search. Bug fixes. (#8981) Fix; BlockMatrix OOM triggered by the MatrixWriteBlockMatrix; WriteBlocksRDD method. Version 0.2.45; Release 2020-06-15. Bug fixes. (#8948) Fix integer; overflow error when reading files >2G with hl.import_plink.; (#8903) Fix Python; type annotations for empty collection constructors and; hl.shuffle.; (#8942) Refactored; VCF combiner to support other GVCF schemas.; (#8941) Fixed; hl.import_plink with multiple data partitions. hailctl dataproc. (#8946) Fix bug when; a user specifies packages in hailctl dataproc start that are also; dependencies of the Hail package.; (#8939) Support; tuples in hailctl dataproc describe. Version 0.2.44; Release 2020-06-06. New Features. (#8914); hl.export_vcf can now export tables as sites-only VCFs.; (#8894) Added; hl.shuffle function to randomly permute arrays.; (#8854) Add; composable option to parallel text export for use with; gsutil compose. Bug fixes. (#8883) Fix an issue; related to failures in pipelines with force_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocurring when calling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:69916,failure,failures,69916,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['failure'],['failures']
Availability,"alable implementation of the score-based; variance-component test originally described in; Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test.; Row weights must be non-negative. Rows with missing weights are ignored. In; the R package skat—which assumes rows are variants—default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field AF, one can use the expression:; >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response y must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively.; The resulting Table provides the group’s key (id), thenumber of; rows in the group (size), the variance component score q_stat, the SKAT; p-value, and a fault flag. For the toy example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC; 3; 4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:80402,fault,fault,80402,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['fault'],['fault']
Availability,"alar expression'; ); ). source = source.select_entries(__GT=call); dataset = require_biallelic(source, 'mendel_errors'); tm = trio_matrix(dataset, pedigree, complete_trios=True); tm = tm.select_entries(; mendel_code=hl.mendel_error_code(; tm.locus, tm.is_female, tm.father_entry['__GT'], tm.mother_entry['__GT'], tm.proband_entry['__GT']; ); ); ck_name = next(iter(source.col_key)); tm = tm.filter_entries(hl.is_defined(tm.mendel_code)); tm = tm.rename({'id': ck_name}). entries = tm.entries(). table1 = entries.select('fam_id', 'mendel_code'). t2 = tm.annotate_cols(errors=hl.agg.count(), snp_errors=hl.agg.count_where(hl.is_snp(tm.alleles[0], tm.alleles[1]))); table2 = t2.key_cols_by().cols(); table2 = table2.select(; pat_id=table2.father[ck_name],; mat_id=table2.mother[ck_name],; fam_id=table2.fam_id,; errors=table2.errors,; snp_errors=table2.snp_errors,; ); table2 = table2.group_by('pat_id', 'mat_id').aggregate(; fam_id=hl.agg.take(table2.fam_id, 1)[0],; children=hl.int32(hl.agg.count()),; errors=hl.agg.sum(table2.errors),; snp_errors=hl.agg.sum(table2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:11125,error,errors,11125,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"alg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; VariantDataset. View page source. VariantDataset. class hail.vds.VariantDataset[source]; Class for representing cohort-level genomic data.; This class facilitates a sparse, split representation of genomic data in; which reference block data and variant data are contained in separate; MatrixTable objects. Parameters:. reference_data (MatrixTable) – MatrixTable containing only reference block data.; variant_data (MatrixTable) – MatrixTable containing only variant data. Attributes. ref_block_max_length_field; Name of global field that indicates max reference block length. reference_genome; Dataset reference genome. Methods. checkpoint; Write to path and then read from path. from_merged_representation; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples; The number of samples present. union_rows; Combine many VDSes with the same samples but disjoint variants. validate; Eagerly checks necessary representational properties of the VDS. write; Write to path. checkpoint(path, **kwargs)[source]; Write to path and then read from path. static from_merged_representation(mt, *, ref_block_fields=(), infer_ref_block_fields=True, is_split=False)[source]; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples()[source]; The number of samples present. ref_block_max_length_field = 'ref_block_max_length'; Name of global field that indicates max reference block length. property reference_genome; Dataset reference genome. Returns:; ReferenceGenome. union_rows()[source]; Combine many VDSes with the same samples but disjoint variants.; Examples; If a datas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html:1294,checkpoint,checkpoint,1294,docs/0.2/vds/hail.vds.VariantDataset.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html,1,['checkpoint'],['checkpoint']
Availability,"alid_intervals; ),; target=t['f3'],; ). else:; raise FatalError(""too few fields for BED file: expected 3 or more, but found {}"".format(len(t.row))). if skip_invalid_intervals and reference_genome:; t = t.filter(hl.is_defined(t.interval)). return t.key_by('interval'). [docs]@typecheck(path=str, quant_pheno=bool, delimiter=str, missing=str); def import_fam(path, quant_pheno=False, delimiter=r'\\s+', missing='NA') -> Table:; """"""Import a PLINK FAM file into a :class:`.Table`. Examples; --------. Import a tab-separated; `FAM file <https://www.cog-genomics.org/plink2/formats#fam>`__; with a case-control phenotype:. >>> fam_kt = hl.import_fam('data/case_control_study.fam'). Import a FAM file with a quantitative phenotype:. >>> fam_kt = hl.import_fam('data/quantitative_study.fam', quant_pheno=True). Notes; -----. In Hail, unlike PLINK, the user must *explicitly* distinguish between; case-control and quantitative phenotypes. Importing a quantitative; phenotype with ``quant_pheno=False`` will return an error; (unless all values happen to be `0`, `1`, `2`, or `-9`):. The resulting :class:`.Table` will have fields, types, and values that are interpreted as missing. - *fam_id* (:py:data:`.tstr`) -- Family ID (missing = ""0""); - *id* (:py:data:`.tstr`) -- Sample ID (key column); - *pat_id* (:py:data:`.tstr`) -- Paternal ID (missing = ""0""); - *mat_id* (:py:data:`.tstr`) -- Maternal ID (missing = ""0""); - *is_female* (:py:data:`.tstr`) -- Sex (missing = ""NA"", ""-9"", ""0""). One of:. - *is_case* (:py:data:`.tbool`) -- Case-control phenotype (missing = ""0"", ""-9"",; non-numeric or the ``missing`` argument, if given.; - *quant_pheno* (:py:data:`.tfloat64`) -- Quantitative phenotype (missing = ""NA"" or; the ``missing`` argument, if given. Warning; -------; Hail will interpret the value ""-9"" as a valid quantitative phenotype, which; differs from default PLINK behavior. Use ``missing='-9'`` to interpret this; value as missing. Parameters; ----------; path : :class:`str`; Path to FAM file.; quant_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:33133,error,error,33133,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['error'],['error']
Availability,"also contact the Hail team to discuss the; feasibility of running your own Hail Batch cluster. The Hail team is; accessible at both https://hail.zulipchat.com and; https://discuss.hail.is . Version 0.2.91; Release 2022-03-18. Bug fixes. (#11614) Update; hail.utils.tutorial.get_movie_lens to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix erro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:52230,error,error,52230,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"alue=raw_cdf['items'][i]); ); ); .aggregate(lambda x: hl.agg.group_by(x.value, hl.agg.sum(hl.bit_lshift(1, x.level)))); ); weights = item_weights.values(); ranks = weights.scan(lambda acc, weight: acc + weight, 0); values = item_weights.keys(); return hl.struct(values=values, ranks=ranks, _compaction_counts=raw_cdf._compaction_counts). @typecheck(k=expr_int32, left=expr_struct(), right=expr_struct()); def _cdf_combine(k, left, right):; t = tstruct(levels=tarray(tint32), items=tarray(tfloat64), _compaction_counts=tarray(tint32)); return _func('approxCDFCombine', t, k, left, right). @typecheck(cdf=expr_struct(), failure_prob=expr_oneof(expr_float32, expr_float64), all_quantiles=bool); def _error_from_cdf(cdf, failure_prob, all_quantiles=False):; """"""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :class:`.StructExpression`; Result of :func:`.approx_cdf` aggregator; failure_prob: :class:`.NumericExpression`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :class:`.NumericExpression`; Upper bound on error of quantile estimates.; """""". def compute_sum(cdf):; s = hl.sum(; hl.range(0, hl.len(cdf._compaction_counts)).map(lambda i: cdf._compaction_counts[i] * (2 ** (2 * i))); ); return s / (cdf.ranks[-1] ** 2). def update_grid_size(p, s):; return 4 * hl.sqrt(hl.log(2 * p / failure_prob) / (2 * s)). def compute_grid_size(s):; return hl.fold(lambda p, i: update_grid_size(p, s), 1 / failure_prob, hl.range(0, 5)). def compute_single_error(s, failure_prob=failure_prob):; return hl.sqrt(hl.log(2 / failure_prob) * s / 2). def compute_global_error(s):; return hl.rbind(compute_grid_size(s), lambda p: 1 / p + compute_single_error(s, failure_prob / p)). if all_quantiles:; return hl.rbind(cdf, lambda cdf: hl.rbind(compute_sum(cdf), compute_global_error)); ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:5403,error,error,5403,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,4,['error'],['error']
Availability,"alueError(; ""'gvcf_sample_names' and 'gvcf_paths' must have the same length ""; f'{len(gvcf_sample_names)} != {len(gvcf_paths)}'; ). if batch_size is None:; if gvcf_batch_size is None:; gvcf_batch_size = VariantDatasetCombiner._default_gvcf_batch_size; else:; pass; elif gvcf_batch_size is None:; warning(; 'The batch_size parameter is deprecated. '; 'The batch_size parameter will be removed in a future version of Hail. '; 'Please use gvcf_batch_size instead.'; ); gvcf_batch_size = batch_size; else:; raise ValueError(; 'Specify only one of batch_size and gvcf_batch_size. ' f'Received {batch_size} and {gvcf_batch_size}.'; ); del batch_size. def maybe_load_from_saved_path(save_path: str) -> Optional[VariantDatasetCombiner]:; if force:; return None; fs = hl.current_backend().fs; if fs.exists(save_path):; try:; combiner = load_combiner(save_path); warning(f'found existing combiner plan at {save_path}, using it'); # we overwrite these values as they are serialized, but not part of the; # hash for an autogenerated name and we want users to be able to overwrite; # these when resuming a combine (a common reason to need to resume a combine; # is a failure due to branch factor being too large); combiner._branch_factor = branch_factor; combiner._target_records = target_records; combiner._gvcf_batch_size = gvcf_batch_size; return combiner; except (ValueError, TypeError, OSError, KeyError) as e:; warning(; f'file exists at {save_path}, but it is not a valid combiner plan, overwriting\n'; f' caused by: {e}'; ); return None. # We do the first save_path check now after validating the arguments; if save_path is not None:; saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner. if len(gvcf_paths) > 0:; n_partition_args = (; int(intervals is not None); + int(import_interval_size is not None); + int(use_genome_default_intervals); + int(use_exome_default_intervals); ). if n_partition_args == 0:; raise ValueError(; ""'new_combiner': requ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:26193,failure,failure,26193,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['failure'],['failure']
Availability,"alues that are interpreted as missing. fam_id (tstr) – Family ID (missing = “0”); id (tstr) – Sample ID (key column); pat_id (tstr) – Paternal ID (missing = “0”); mat_id (tstr) – Maternal ID (missing = “0”); is_female (tstr) – Sex (missing = “NA”, “-9”, “0”). One of:. is_case (tbool) – Case-control phenotype (missing = “0”, “-9”,; non-numeric or the missing argument, if given.; quant_pheno (tfloat64) – Quantitative phenotype (missing = “NA” or; the missing argument, if given. Warning; Hail will interpret the value “-9” as a valid quantitative phenotype, which; differs from default PLINK behavior. Use missing='-9' to interpret this; value as missing. Parameters:. path (str) – Path to FAM file.; quant_pheno (bool) – If True, phenotype is interpreted as quantitative.; delimiter (str) – Field delimiter regex.; missing (str) – The string used to denote missing values. For case-control, 0, -9, and; non-numeric are also treated as missing. Returns:; Table. hail.methods.import_gen(path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None, reference_genome='default', contig_recoding=None, skip_invalid_loci=False)[source]; Import GEN file(s) as a MatrixTable.; Examples; >>> ds = hl.import_gen('data/example.gen',; ... sample_file='data/example.sample',; ... reference_genome='GRCh37'). Notes; For more information on the GEN file format, see here.; If the GEN file has only 5 columns before the start of the genotype; probability data (chromosome field is missing), you must specify the; chromosome using the chromosome parameter.; To load multiple files at the same time, use Hadoop Glob Patterns.; Column Fields. s (tstr) – Column key. This is the sample ID imported; from the first column of the sample file. Row Fields. locus (tlocus or tstruct) – Row key. The genomic; location consisting of the chromosome (1st column if present, otherwise; given by chromosome) and position (4th column if chromosome is not; defined). If reference_genome is defined, the type will be",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:16524,toler,tolerance,16524,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['toler'],['tolerance']
Availability,"alues to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the distribution of values. In; particular, for any value x = values(i) in the summary, we estimate that; there are ranks(i) values strictly less than x, and that there are; ranks(i+1) values less than or equal to x. For any value y (not; necessarily in the summary), we estimate CDF(y) to be ranks(i), where i; is such that values(i-1) < y ≤ values(i).; An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value values(i) occupying indices ranks(i) (inclusive) to; ranks(i+1) (exclusive).; The returned struct also contains an array _compaction_counts, which is; used internally to support downstream error estimation. Warning; This is an approximate and nondeterministic method. Parameters:. expr (Expression) – Expression to collect.; k (int) – Parameter controlling the accuracy vs. memory usage tradeoff. Returns:; StructExpression – Struct containing values and ranks arrays. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:34947,down,downstream,34947,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,2,"['down', 'error']","['downstream', 'error']"
Availability,"ambda_Approx filters the eigenvalues,; # presumably because a good estimate of the Generalized Chi-Sqaured CDF is not significantly; # affected by chi-squared components with very tiny weights.; threshold = 1e-5 * eigenvalues.sum() / eigenvalues.shape[0]; w = hl.array(eigenvalues).filter(lambda y: y >= threshold); genchisq_data = hl.pgenchisq(; ht.Q,; w=w,; k=hl.nd.ones(hl.len(w), dtype=hl.tint32),; lam=hl.nd.zeros(hl.len(w)),; mu=0,; sigma=0,; min_accuracy=accuracy,; max_iterations=iterations,; ); ht = ht.select(; 'size',; # for reasons unknown, the R implementation calls this expression the Q statistic (which is; # *not* what they write in the paper); q_stat=ht.Q / 2,; # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples', 'null_fit'). [docs]@typecheck(; key_expr=expr_any,; weight_expr=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; logistic=oneof(bool, sized_tupleof(nullable(int), nullable(float))),; max_size=int,; accuracy=numeric,; iterations=int,; ); def skat(; key_expr,; weight_expr,; y,; x,; covariates,; logistic: Union[bool, Tuple[int, float]] = False,; max_size: int = 46340,; accuracy: float = 1e-6,; iterations: int = 10000,; ) -> Table:; r""""""Test each keyed group of rows for association by linear or logistic; SKAT test. Examples; --------. Test each gene for association using the linear sequence kernel association; test:. >>> skat_table = hl.skat(key_expr=burden_ds.gene,; ... weight_expr=burden_ds.weight,; ... y=burden_ds.burden.pheno,; ... x=burden_ds.GT.n_alt_alleles(),; ... covariates=[1, burden_ds.burden.cov1, burden_ds.burden.cov2]). .. caution::. By default, th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:101146,fault,fault,101146,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability,"ample3.bgen"", sample_file=""data/example3.sample""). Notes; Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see here. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only unphased and diploid genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed.; Before importing, ensure that:. The sample file has the same number of samples as the BGEN file.; No duplicate sample IDs are present. To load multiple files at the same time, use Hadoop Glob Patterns.; Genotype probability (``gp``) representation:; The following modifications are made to genotype probabilities in BGEN v1.1 files:. Since genotype probabilities are understood to define a probability distribution, import_bgen() automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the tolerance parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains.; import_bgen() normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. Annotations; import_bgen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .bgen files to import.; tolerance (float) – If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1.; sample_file (str or None) – Sample file.; min_partitions (int or None) – Number of partitions. Returns:Variant dataset imported from .bgen file. Return type:VariantDataset. import_gen(path, sample_file=None, tolerance=0.2, min_part",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:9674,toler,tolerance,9674,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['toler'],['tolerance']
Availability,"amples; >>> hl.summarize_variants(dataset) ; ==============================; Number of variants: 346; ==============================; Alleles per variant; -------------------; 2 alleles: 346 variants; ==============================; Variants per contig; -------------------; 20: 346 variants; ==============================; Allele type distribution; ------------------------; SNP: 301 alleles; Deletion: 27 alleles; Insertion: 18 alleles; ==============================. Parameters:. mt (MatrixTable or Table) – Matrix table with a variant (locus / alleles) row key.; show (bool) – If True, print results instead of returning them.; handler. Notes; The result returned if show is False is a Struct with; five fields:. n_variants (int): Number of variants present in the matrix table.; allele_types (dict [str, int]): Number of alternate alleles in; each allele allele category.; contigs (dict [str, int]): Number of variants on each contig.; allele_counts (dict [int, int]): Number of variants broken down; by number of alleles (biallelic is 2, for example).; r_ti_tv (float): Ratio of transition alternate alleles to; transversion alternate alleles. Returns:; None or Struct – Returns None if show is True, or returns results as a struct. hail.methods.transmission_disequilibrium_test(dataset, pedigree)[source]; Performs the transmission disequilibrium test on trios. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Compute TDT association statistics and show the first two results:; >>> pedigree = hl.Pedigree.read('data/tdt_trios.fam'); >>> tdt_table = hl.transmission_disequilibrium_test(tdt_dataset, pedigree); >>> tdt_table.show(2) ; +---------------+------------+-------+-------+-",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:93307,down,down,93307,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['down'],['down']
Availability,"an also pass in a Hail field as a label argument, which determines how to color the data points. [7]:. mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD); filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))); mt = mt.filter_entries(filter_condition_ab); mt = hl.variant_qc(mt).cache(); common_mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01); gwas = hl.linear_regression_rows(y=common_mt.CaffeineConsumption, x=common_mt.GT.n_alt_alleles(), covariates=[1.0]); pca_eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(common_mt.GT). [Stage 16:> (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height=400)). 2-D histogram; For visualizing relationships between variables in large datasets (where scatter plots may be less informative since they highlight outliers), the histogram_2d() function will create a heatmap with the number of observations in each section of a 2-d grid based on two variables. [10]:. p = hl.plot.histogram2d(pca_scores.scores[0], pca_scores.scores[1]); show(p). Q-Q (Quantile-Quantile); The qq() function requires either a Python type or a Hail field containing p-values to be plotted. This function also allows for down",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/08-plotting.html:5750,down,downsample,5750,docs/0.2/tutorials/08-plotting.html,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html,1,['down'],['downsample']
Availability,"ance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors pe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:5500,error,errors,5500,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPUT_FILE` environment variable is not available for the single job that computes the consequence header when; ``csq=True``; """""". json_typ: hl.expr.HailType; data_bucket: str; data_mount: str; regions: List[str]; image: str; env: Dict[str, str]; data_bucket_is_requester_pays: bool; cloud: str; batch_run_command: List[str]; batch_run_csq_header_command: List[str]. @abc.abstractmethod; def command(; self, consequence: bool, tolerate_parse_error: bool, part_id: int, input_file: Optional[str], output_file: str; ) -> List[str]:; raise NotImplementedError. [docs]class VEPConfigGRCh37Version85(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh37 for VEP version 85. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:26247,avail,available,26247,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['avail'],['available']
Availability,"annotate_rows(mean_x=hl.agg.mean(mt.x)); mt = mt.annotate_rows(xvec=hl.nd.array(hl.agg.collect(hl.coalesce(mt.x, mt.mean_x)))); ht = mt.rows(). covmat = ht.covmat; null_fit = ht.null_fit; # FIXME: we should test a whole block of variants at a time not one-by-one; xvec = ht.xvec; yvec = ht.yvec. if test == 'score':; chi_sq, p = _poisson_score_test(null_fit, covmat, yvec, xvec); return ht.select(chi_sq_stat=chi_sq, p_value=p, **ht.pass_through).select_globals('null_fit'). X = hl.nd.hstack([covmat, xvec.T.reshape(-1, 1)]); b = hl.nd.hstack([null_fit.b, hl.nd.array([0.0])]); mu = sigmoid(X @ b); residual = yvec - mu; score = hl.nd.hstack([null_fit.score, hl.nd.array([xvec @ residual])]). fisher00 = null_fit.fisher; fisher01 = ((covmat.T * mu) @ xvec).reshape((-1, 1)); fisher10 = fisher01.T; fisher11 = hl.nd.array([[(mu * xvec.T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); b: NDArrayNumericExpression, # (K,); mu: NDArrayNumericExpression, # (N,); score: NDArrayNumericExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:67313,toler,tolerance,67313,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,"ant.; - `rsid` (:py:data:`.tstr`) -- rsID of the variant.; - `qual` (:py:data:`.tfloat64`) -- Floating-point number in the QUAL field.; - `info` (:class:`.tstruct`) -- All INFO fields defined in the VCF header; can be found in the struct `info`. Data types match the type specified; in the VCF header, and if the declared ``Number`` is not 1, the result; will be stored as an array. **Entry Fields**. :func:`.import_vcf` generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:101853,down,downstream,101853,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['down'],['downstream']
Availability,"ape[0], 1), (1 / S[0])); V = hl.nd.hstack((ones_normalized, V0)). # Compute matrix of regression coefficients for PCs (beta), shape (k, m); beta = BlockMatrix.from_ndarray(((1 / S) * V).T, block_size=block_size) @ g.T; beta = beta.checkpoint(new_temp_file('pc_relate_bm/beta', 'bm')). # Compute matrix of individual-specific AF estimates (mu), shape (m, n); mu = 0.5 * (BlockMatrix.from_ndarray(V * S, block_size=block_size) @ beta).T; # Replace entries in mu with NaN if invalid or if corresponding GT is missing (no contribution from that variant); mu = mu._apply_map2(; lambda _mu, _g: hl.if_else(_bad_mu(_mu, min_individual_maf) | hl.is_nan(_g), nan, _mu),; g,; sparsity_strategy='NeedsDense',; ); mu = mu.checkpoint(new_temp_file('pc_relate_bm/mu', 'bm')). # Compute kinship matrix (phi), shape (n, n); # Where mu is NaN (missing), set variance and centered AF to 0 (no contribution from that variant); variance = _replace_nan(mu * (1.0 - mu), 0.0).checkpoint(new_temp_file('pc_relate_bm/variance', 'bm')); centered_af = _replace_nan(g - (2.0 * mu), 0.0); phi = _gram(centered_af) / (4.0 * _gram(variance.sqrt())); phi = phi.checkpoint(new_temp_file('pc_relate_bm/phi', 'bm')); ht = phi.entries().rename({'entry': 'kin'}); ht = ht.annotate(k0=hl.missing(hl.tfloat64), k1=hl.missing(hl.tfloat64), k2=hl.missing(hl.tfloat64)). if statistics in ['kin2', 'kin20', 'all']:; # Compute inbreeding coefficient and dominance encoding of GT matrix; f_i = (2.0 * phi.diagonal()) - 1.0; gd = g._apply_map2(lambda _g, _mu: _dominance_encoding(_g, _mu), mu, sparsity_strategy='NeedsDense'); normalized_gd = gd - (variance * (1.0 + f_i)). # Compute IBD2 (k2) estimate; k2 = _gram(normalized_gd) / _gram(variance); ht = ht.annotate(k2=k2.entries()[ht.i, ht.j].entry). if statistics in ['kin20', 'all']:; # Get the numerator used in IBD0 (k0) computation (IBS0), compute indicator matrices for homozygotes; hom_alt = g._apply_map2(; lambda _g, _mu: hl.if_else((_g != 2.0) | hl.is_nan(_mu), 0.0, 1.0), mu, sparsit",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:20658,checkpoint,checkpoint,20658,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['checkpoint'],['checkpoint']
Availability,"are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete `separation <https://en.wikipedia.org/wiki/Separation_(statistics)>`__. A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of :math:`\\beta` under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where ``x`` is genotype, ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete se",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:144449,error,errors,144449,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,"are defined. Rows with missing; values in either field are removed prior to fitting the LD score; regression model. Parameters; ----------; weight_expr : :class:`.Float64Expression`; Row-indexed expression for the LD scores used to derive; variant weights in the model.; ld_score_expr : :class:`.Float64Expression`; Row-indexed expression for the LD scores used as covariates; in the model.; chi_sq_exprs : :class:`.Float64Expression` or :obj:`list` of; :class:`.Float64Expression`; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions for chi-squared; statistics resulting from genome-wide association; studies (GWAS).; n_samples_exprs: :class:`.NumericExpression` or :obj:`list` of; :class:`.NumericExpression`; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions indicating the number of; samples used in the studies that generated the test; statistics supplied to ``chi_sq_exprs``.; n_blocks : :obj:`int`; The number of blocks used in the jackknife approach to; estimating standard errors.; two_step_threshold : :obj:`int`; Variants with chi-squared statistics greater than this; value are excluded in the first step of the two-step; procedure used to fit the model.; n_reference_panel_variants : :obj:`int`, optional; Number of variants used to estimate the; SNP-heritability :math:`h_g^2`. Returns; -------; :class:`~.Table`; Table keyed by ``phenotype`` with intercept and heritability estimates; for each phenotype passed to the function."""""". chi_sq_exprs = wrap_to_list(chi_sq_exprs); n_samples_exprs = wrap_to_list(n_samples_exprs). assert (len(chi_sq_exprs) == len(n_samples_exprs)) or (len(n_samples_exprs) == 1); __k = 2 # number of covariates, including intercept. ds = chi_sq_exprs[0]._indices.source. analyze('ld_score_regression/weight_expr', weight_expr, ds._row_indices); analyze('ld_score_regression/ld_score_expr', ld_score_expr, ds._row_indices). # format input dataset; if isinstance(ds, hl.MatrixTable):; if len(chi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ld_score_regression.html:8220,error,errors,8220,docs/0.2/_modules/hail/experimental/ld_score_regression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ld_score_regression.html,2,['error'],['errors']
Availability,"ark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:1995,avail,available,1995,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['avail'],['available']
Availability,"ark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.row_key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_matrix_table(tmp2).add_row_index(uid).key_rows_by(uid); ht.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions).drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions). return MatrixTable(; ir.MatrixRepartition(; self._mir, n_partitions, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'MatrixTable':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> dataset_result = dataset.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:108901,checkpoint,checkpoint,108901,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['checkpoint'],['checkpoint']
Availability,"ary version 0.2.119 introduces a new file format version: 1.7.0. All; library versions before 0.2.119, for example 0.2.118, cannot read file; format version 1.7.0. All library versions after and including 0.2.119; can read file format version 1.7.0.; Each version of the Hail Python library can only write files using the; latest file format version it supports.; The hl.experimental package and other methods marked experimental in; the docs are exempt from this policy. Their functionality or even; existence may change without notice. Please contact us if you critically; depend on experimental functionality. Version 0.2.133; Released 2024-09-25. New Features. (#14619) Teach; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some da",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:10548,error,error,10548,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"ary when the sample size is below 10,000. Returns:; Table. hail.methods.mendel_errors(call, pedigree)[source]; Find Mendel errors; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:48631,error,errors,48631,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability,"as found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:125601,error,errors,125601,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,"as variant dataset.; Examples; Importing a BGEN file as a VDS (assuming it has already been indexed).; >>> vds = hc.import_bgen(""data/example3.bgen"", sample_file=""data/example3.sample""). Notes; Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see here. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only unphased and diploid genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed.; Before importing, ensure that:. The sample file has the same number of samples as the BGEN file.; No duplicate sample IDs are present. To load multiple files at the same time, use Hadoop Glob Patterns.; Genotype probability (``gp``) representation:; The following modifications are made to genotype probabilities in BGEN v1.1 files:. Since genotype probabilities are understood to define a probability distribution, import_bgen() automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the tolerance parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains.; import_bgen() normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. Annotations; import_bgen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .bgen files to import.; tolerance (float) – If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1.; sample_file (str or None) – Sample file.; min_partitions (int or None) – Number of partitions. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:9632,toler,tolerance,9632,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['toler'],['tolerance']
Availability,"ass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:7220,toler,tolerance,7220,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['toler'],['tolerance']
Availability,"association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)). Test if the phenotype is significantly associated with the genotype:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.76e+02 | 1.23e-05 | 0 |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:76290,fault,fault,76290,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability,"ata/LCR.interval_list') . Notes; If you are copying a file just to then load it into Python, you can use; open() instead. For example:; >>> with hfs.open('gs://my_bucket/results.csv', 'r') as f: ; ... df = pandas_df.read_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers) or local filesystem paths. Parameters:. src (str) – Source file URI.; dest (str) – Destination file URI. hailtop.fs.exists(path, *, requester_pays_config=None)[source]; Returns True if path exists. Parameters:; path (str). Returns:; bool. hailtop.fs.is_dir(path, *, requester_pays_config=None)[source]; Returns True if path both exists and is a directory. Parameters:; path (str). Returns:; bool. hailtop.fs.is_file(path, *, requester_pays_config=None)[source]; Returns True if path both exists and is a file. Parameters:; path (str). Returns:; bool. hailtop.fs.ls(path, *, requester_pays_config=None)[source]; Returns information about files at path.; Notes; Raises an error if path does not exist.; If path is a file, returns a list with one element. If path is a; directory, returns an element for each file contained in path (does not; search recursively).; Each dict element of the result list contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; list [dict]. hailtop.fs.mkdir(path, *, requester_pays_config=None)[source]; Ensure files can be created whose dirname is path. Warning; On file systems without a notion of directories, this function will do nothing. For example,; on Google Cloud Storage, this operation does nothing. hailtop.fs.open(path, mode='r', buffer_size=8192, *, requester_pays_config=None)[source]; Open a file from the local filesystem of from blob storage. Supported; blob storage providers are GCS, S3 and ABS.; Examp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/fs_api.html:3005,error,error,3005,docs/0.2/fs_api.html,https://hail.is,https://hail.is/docs/0.2/fs_api.html,1,['error'],['error']
Availability,"ata:`.tstr`) -- The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; - `rsid` (:py:data:`.tstr`) -- The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. **Entry Fields**. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the highest probability.; - `GP` (:class:`.tarray` of :py:data:`.tfloat64`) -- Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the probabilities is a distance greater than the `tolerance`; parameter from 1.0. Otherwise, the probabilities are normalized to sum to; 1.0. For example, the input ``[0.98, 0.0, 0.0]`` will be normalized to; ``[1.0, 0.0, 0.0]``. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; GEN files to import.; sample_file : :class:`str`; Sample file to import.; tolerance : :obj:`float`; If the sum of the genotype probabilities for a genotype differ from 1.0; by more than the tolerance, set the genotype to missing.; min_partitions : :obj:`int`, optional; Number of partitions.; chromosome : :class:`str`, optional; Chromosome if not included in the GEN file; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding : :obj:`dict` of :class:`str` to :obj:`str`, optional; Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by `reference_genome`.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`. Returns; -------; :class:`.MatrixTable`; """"""; gen_table = import_lines(path, min_partitions); sample_table = import_lines(sample_file); rg = reference_genome.name if reference_genome else None; if contig_recoding is None:; contig_recoding = hl.empty_dict(hl.tstr, hl.tstr); else:; contig_recoding = hl.dict(contig_recoding). gen_table = gen_table.transmute(data=gen_table.text.split(' ')). if chromosome is None:; last_rowf_i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:50049,toler,tolerance,50049,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['toler'],['tolerance']
Availability,"ataset, pedigree, complete_trios=True); tm = tm.select_entries(; mendel_code=hl.mendel_error_code(; tm.locus, tm.is_female, tm.father_entry['__GT'], tm.mother_entry['__GT'], tm.proband_entry['__GT']; ); ); ck_name = next(iter(source.col_key)); tm = tm.filter_entries(hl.is_defined(tm.mendel_code)); tm = tm.rename({'id': ck_name}). entries = tm.entries(). table1 = entries.select('fam_id', 'mendel_code'). t2 = tm.annotate_cols(errors=hl.agg.count(), snp_errors=hl.agg.count_where(hl.is_snp(tm.alleles[0], tm.alleles[1]))); table2 = t2.key_cols_by().cols(); table2 = table2.select(; pat_id=table2.father[ck_name],; mat_id=table2.mother[ck_name],; fam_id=table2.fam_id,; errors=table2.errors,; snp_errors=table2.snp_errors,; ); table2 = table2.group_by('pat_id', 'mat_id').aggregate(; fam_id=hl.agg.take(table2.fam_id, 1)[0],; children=hl.int32(hl.agg.count()),; errors=hl.agg.sum(table2.errors),; snp_errors=hl.agg.sum(table2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table3.snp_errors[1],; }),; hl.struc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:11258,error,errors,11258,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"ataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/index.html:5443,error,errors,5443,docs/0.2/methods/index.html,https://hail.is,https://hail.is/docs/0.2/methods/index.html,1,['error'],['errors']
Availability,"ath into the command for s.; We use another f-string in t’s command where we print the contents of s.ofile to stdout.; s.ofile is the same temporary file that was created in the command for t. Therefore,; Batch deduces that t must depend on s and thus creates an implicit dependency for t on s.; In both the LocalBackend and ServiceBackend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or “sink” that waits for all of the jobs in the scatter to be complete; before executing.; In the example below, we use a for loop to create a job for each one of; ‘Alice’, ‘Bob’, and ‘Dan’ that prints the name of the user programatically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.ne",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:6238,echo,echo,6238,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability,"aths : :class:`str` or :obj:`list` of :obj:`str`; Files to import.; key : :class:`str` or :obj:`list` of :obj:`str`; Key fields(s).; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header : :obj:`bool`; If ``True```, assume the file has no header and name the N fields `f0`,; `f1`, ... `fN` (0-indexed).; impute : :obj:`bool`; If ``True``, Impute field types from the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_fi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:117882,error,error,117882,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['error'],['error']
Availability,"ations.; samples (int) – Number of samples.; variants (int) – Number of variants.; num_partitions (int) – Number of partitions.; pop_dist (array of float or None) – Unnormalized population distribution; fst (array of float or None) – \(F_{ST}\) values; af_dist (UniformDist or BetaDist or TruncatedBetaDist) – Ancestral allele frequency distribution; seed (int) – Random seed. Returns:Variant dataset simulated using the Balding-Nichols model. Return type:VariantDataset. eval_expr(expr)[source]¶; Evaluate an expression. Parameters:expr (str) – Expression to evaluate. Return type:annotation. eval_expr_typed(expr)[source]¶; Evaluate an expression and return the result as well as its type. Parameters:expr (str) – Expression to evaluate. Return type:(annotation, Type). static get_running()[source]¶; Return the running Hail context in this Python session.; Example; >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. Returns:Current Hail context. Return type:HailContext. grep(regex, path, max_count=100)[source]¶; Grep big files, like, really fast.; Examples; Print all lines containing the string hello in file.txt:; >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in file1.txt and file2.txt:; >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). Background; grep() mimics the basic functionality of Unix grep in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regular expressions at RegExr. Parameters:; regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return. import_bgen(path, tolerance=0.2, sample_file=None, min_partitions=None)[source]¶; Import .bgen file(s) as variant dataset.; Examples; Importing ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:7555,recover,recover,7555,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['recover'],['recover']
Availability,"atrixTable, name=str); def variant_qc(mt, name='variant_qc') -> MatrixTable:; """"""Compute common variant statistics (quality control metrics). .. include:: ../_templates/req_tvariant.rst. Examples; --------. >>> dataset_result = hl.variant_qc(dataset). Notes; -----; This method computes variant statistics from the genotype data, returning; a new struct field `name` with the following metrics based on the fields; present in the entry schema. If `mt` contains an entry field `DP` of type :py:data:`.tint32`, then the; field `dp_stats` is computed. If `mt` contains an entry field `GQ` of type; :py:data:`.tint32`, then the field `gq_stats` is computed. Both `dp_stats`; and `gq_stats` are structs with with four fields:. - `mean` (``float64``) -- Mean value.; - `stdev` (``float64``) -- Standard deviation (zero degrees of freedom).; - `min` (``int32``) -- Minimum value.; - `max` (``int32``) -- Maximum value. If the dataset does not contain an entry field `GT` of type; :py:data:`.tcall`, then an error is raised. The following fields are always; computed from `GT`:. - `AF` (``array<float64>``) -- Calculated allele frequency, one element; per allele, including the reference. Sums to one. Equivalent to; `AC` / `AN`.; - `AC` (``array<int32>``) -- Calculated allele count, one element per; allele, including the reference. Sums to `AN`.; - `AN` (``int32``) -- Total number of called alleles.; - `homozygote_count` (``array<int32>``) -- Number of homozygotes per; allele. One element per allele, including the reference.; - `call_rate` (``float64``) -- Fraction of calls neither missing nor filtered.; Equivalent to `n_called` / :meth:`.count_cols`.; - `n_called` (``int64``) -- Number of samples with a defined `GT`.; - `n_not_called` (``int64``) -- Number of samples with a missing `GT`.; - `n_filtered` (``int64``) -- Number of filtered entries.; - `n_het` (``int64``) -- Number of heterozygous samples.; - `n_non_ref` (``int64``) -- Number of samples with at least one called; non-reference al",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:9097,error,error,9097,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['error'],['error']
Availability,"aulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It’s important to have easy ways to slice, dice, query, and summarize a dataset. Some of this function",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:2377,down,downloads,2377,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['down'],['downloads']
Availability,"ausing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with; missingness. Bug fixes. (#5088) Fixed name; separator in MatrixTable.make_table.; (#5104) Fixed; optimizer bug related to experimental functionality.; (#5122) Fixed error; constructing Table or MatrixTable objects with fields with; certain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:102024,error,error,102024,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"avily unbalanced dataset. An unbalanced dataset can be inefficient to operate on because the work is not evenly distributed across partitions.; ; :param int n: Desired number of partitions. :param bool shuffle: Whether to shuffle or naively coalesce.; ; :rtype: :class:`.KeyTable` ; """""". return KeyTable(self.hc, self._jkt.repartition(n, shuffle)). [docs] @staticmethod; @handle_py4j; @typecheck(path=strlike,; quantitative=bool,; delimiter=strlike,; missing=strlike); def import_fam(path, quantitative=False, delimiter='\\\\s+', missing='NA'):; """"""Import PLINK .fam file into a key table. **Examples**. Import case-control phenotype data from a tab-separated `PLINK .fam; <https://www.cog-genomics.org/plink2/formats#fam>`_ file into sample; annotations:. >>> fam_kt = KeyTable.import_fam('data/myStudy.fam'). In Hail, unlike PLINK, the user must *explicitly* distinguish between; case-control and quantitative phenotypes. Importing a quantitative; phenotype without ``quantitative=True`` will return an error; (unless all values happen to be ``0``, ``1``, ``2``, and ``-9``):. >>> fam_kt = KeyTable.import_fam('data/myStudy.fam', quantitative=True). **Columns**. The column, types, and missing values are shown below. - **ID** (*String*) -- Sample ID (key column); - **famID** (*String*) -- Family ID (missing = ""0""); - **patID** (*String*) -- Paternal ID (missing = ""0""); - **matID** (*String*) -- Maternal ID (missing = ""0""); - **isFemale** (*Boolean*) -- Sex (missing = ""NA"", ""-9"", ""0""); ; One of:; ; - **isCase** (*Boolean*) -- Case-control phenotype (missing = ""0"", ""-9"", non-numeric or the ``missing`` argument, if given.; - **qPheno** (*Double*) -- Quantitative phenotype (missing = ""NA"" or the ``missing`` argument, if given. :param str path: Path to .fam file. :param bool quantitative: If True, .fam phenotype is interpreted as quantitative. :param str delimiter: .fam file field delimiter regex. :param str missing: The string used to denote missing values.; For case-control, 0, -9, and ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:32218,error,error,32218,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,1,['error'],['error']
Availability,"ax(series_max, overall_max). color_mapping = continuous_nums_to_colors(overall_min, overall_max, plotly.colors.sequential.Viridis). def transform(df):; df[self.aesthetic_name] = df[self.aesthetic_name].map(lambda i: color_mapping(i)); return df. return transform. class ScaleColorHue(ScaleDiscrete):; def get_values(self, categories):; num_categories = len(categories); step = 1.0 / num_categories; interpolation_values = [step * i for i in range(num_categories)]; hsv_scale = px.colors.get_colorscale(""HSV""); return px.colors.sample_colorscale(hsv_scale, interpolation_values). class ScaleShapeAuto(ScaleDiscrete):; def get_values(self, categories):; return [; ""circle"",; ""square"",; ""diamond"",; ""cross"",; ""x"",; ""triangle-up"",; ""triangle-down"",; ""triangle-left"",; ""triangle-right"",; ""triangle-ne"",; ""triangle-se"",; ""triangle-sw"",; ""triangle-nw"",; ""pentagon"",; ""hexagon"",; ""hexagon2"",; ""octagon"",; ""star"",; ""hexagram"",; ""star-triangle-up"",; ""star-triangle-down"",; ""star-square"",; ""star-diamond"",; ""diamond-tall"",; ""diamond-wide"",; ""hourglass"",; ""bowtie"",; ""circle-cross"",; ""circle-x"",; ""square-cross"",; ""square-x"",; ""diamond-cross"",; ""diamond-x"",; ""cross-thin"",; ""x-thin"",; ""asterisk"",; ""hash"",; ""y-up"",; ""y-down"",; ""y-left"",; ""y-right"",; ""line-ew"",; ""line-ns"",; ""line-ne"",; ""line-nw"",; ""arrow-up"",; ""arrow-down"",; ""arrow-left"",; ""arrow-right"",; ""arrow-bar-up"",; ""arrow-bar-down"",; ""arrow-bar-left"",; ""arrow-bar-right"",; ]. class ScaleColorContinuousIdentity(ScaleContinuous):; def valid_dtype(self, dtype):; return dtype == tstr. [docs]def scale_x_log10(name=None):; """"""Transforms x axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, transformation=""log10""). [docs]def scale_y_log10(name=None):; """"""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:6813,down,down,6813,docs/0.2/_modules/hail/ggplot/scale.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html,10,['down'],['down']
Availability,"b to cloud computing. hailctl dataproc; As of version 0.2.15, pip installations of Hail come bundled with a command-line; tool, hailctl. This tool has a submodule called dataproc for working with; Google Dataproc clusters configured for Hail.; This tool requires the Google Cloud SDK.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rac",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:1813,down,down,1813,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['down'],['down']
Availability,"b.read_input('data/hello.txt'); >>> j = b.new_job(name='hello'); >>> j.command(f'cat {input}'); >>> b.run(). Why do we need to explicitly add input files to batches rather than referring; directly to the path in the command? You could refer directly to the path when using the; LocalBackend, but only if you are not specifying a docker image to use when running; the command with BashJob.image(). This is because Batch copies any input files to a special; temporary directory which gets mounted to the Docker container. When using the ServiceBackend,; input files would be files in Google Storage. Many commands do not know how to handle file; paths in Google Storage. Therefore, we suggest explicitly adding all input files as input resource; files to the batch so to make sure the same code can run in all scenarios. Files that are already; in a Docker image do not need to be read as inputs to the batch. Output Files; All files generated by Batch are temporary files! They are copied as appropriate between jobs; for downstream jobs’ use, but will be removed when the batch has completed. In order to save; files generated by a batch for future use, you need to explicitly call Batch.write_output().; The first argument to Batch.write_output() can be any type of ResourceFile which includes input resource; files and job resource files as well as resource groups as described below. The second argument to write_output; should be either a local file path or a google storage file path when using the LocalBackend.; For the ServiceBackend, the second argument must be a google storage file path.; >>> b = hb.Batch(name='hello-input'); >>> j = b.new_job(name='hello'); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Resource Groups; Many bioinformatics tools treat files as a group with a common file; path and specific file extensions. For example, PLINK; stores genetic data in three files: *.bed has the genotype data,; *.bim has the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:12401,down,downstream,12401,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['down'],['downstream']
Availability,"backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven’t already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we’ll append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesn’t exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:8895,checkpoint,checkpoint,8895,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,4,['checkpoint'],['checkpoint']
Availability,"bda i: hl.bind(; lambda t: hl.struct(; proband_entry=mt[entries_sym][t.id],; father_entry=mt[entries_sym][t.pat_id],; mother_entry=mt[entries_sym][t.mat_id],; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.drop(trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:4789,error,errors,4789,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,be a single; expression; compound keys are not accepted.; weight_expr and ld_score_expr must be row-indexed; fields.; chi_sq_exprs must be a single entry-indexed field; (not a list of fields).; n_samples_exprs must be a single entry-indexed field; (not a list of fields).; The phenotype field that keys the table returned by; ld_score_regression() will have values corresponding to the; column keys of the input matrix table. This function returns a Table with one row per set of summary; statistics passed to the chi_sq_exprs argument. The following; row-indexed fields are included in the table:. phenotype (tstr) – The name of the phenotype. The; returned table is keyed by this field. See the notes below for; details on the possible values of this field.; mean_chi_sq (tfloat64) – The mean chi-squared; test statistic for the given phenotype.; intercept (Struct) – Contains fields:. estimate (tfloat64) – A point estimate of the; intercept \(1 + Na\).; standard_error (tfloat64) – An estimate of; the standard error of this point estimate. snp_heritability (Struct) – Contains fields:. estimate (tfloat64) – A point estimate of the; SNP-heritability \(h_g^2\).; standard_error (tfloat64) – An estimate of; the standard error of this point estimate. Warning; ld_score_regression() considers only the rows for which both row; fields weight_expr and ld_score_expr are defined. Rows with missing; values in either field are removed prior to fitting the LD score; regression model. Parameters:. weight_expr (Float64Expression) – Row-indexed expression for the LD scores used to derive; variant weights in the model.; ld_score_expr (Float64Expression) – Row-indexed expression for the LD scores used as covariates; in the model.; chi_sq_exprs (Float64Expression or list of) – Float64Expression; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions for chi-squared; statistics resulting from genome-wide association; studies (GWAS).; n_samples_exprs (NumericExpression or ,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:13281,error,error,13281,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['error'],['error']
Availability,"be able to read tables or matrix tables written by this; version of Hail. Version 0.2.81; Release 2021-12-20. hailctl dataproc. (#11182) Updated; Dataproc image version to mitigate yet more Log4j vulnerabilities. Version 0.2.80; Release 2021-12-15. New features. (#11077); hl.experimental.write_matrix_tables now returns the paths of the; written matrix tables. hailctl dataproc. (#11157) Updated; Dataproc image version to mitigate the Log4j vulnerability.; (#10900) Added; --region parameter to hailctl dataproc submit.; (#11090) Teach; hailctl dataproc describe how to read URLs with the protocols; s3 (Amazon S3), hail-az (Azure Blob Storage), and file; (local file system) in addition to gs (Google Cloud Storage). Version 0.2.79; Release 2021-11-17. Bug fixes. (#11023) Fixed bug; in call decoding that was introduced in version 0.2.78. New features. (#10993) New; function p_value_excess_het. Version 0.2.78; Release 2021-10-19. Bug fixes. (#10766) Don’t throw; out of memory error when broadcasting more than 2^(31) - 1 bytes.; (#10910) Filters on; key field won’t be slowed down by uses of; MatrixTable.localize_entries or Table.rename.; (#10959) Don’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:56098,error,error,56098,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"be queried for filter membership with expressions ; like va.filters.contains(""VQSRTranche99.5...""). Variants that are flagged as “PASS” ; will have no filters applied; for these variants, va.filters.isEmpty() is true. Thus, ; filtering to PASS variants can be done with VariantDataset.filter_variants_expr(); as follows:; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). Annotations. va.filters (Set[String]) – Set containing all filters applied to a variant.; va.rsid (String) – rsID of the variant.; va.qual (Double) – Floating-point number in the QUAL field.; va.info (Struct) – All INFO fields defined in the VCF header; can be found in the struct va.info. Data types match the type; specified in the VCF header, and if the declared Number is not; 1, the result will be stored as an array. Parameters:; path (str or list of str) – VCF file(s) to read.; force (bool) – If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB.; force_bgz (bool) – If True, load .gz files as blocked gzip files (BGZF); header_file (str or None) – File to load VCF header from. If not specified, the first file in path is used.; min_partitions (int or None) – Number of partitions.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations or; genotypes.; store_gq (bool) – If True, store GQ FORMAT field instead of computing from PL. Only applies if generic=False.; pp_as_pl (bool) – If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if generic=False.; skip_bad_ad (bool) – If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if generic=False.; generic (bool) – If True, read the genotype with a generic schema.; call_fields (str or list of str) – FORMAT fields in VCF to treat as a TCall. Only applies if generic=True. Ret",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:23100,down,downstream,23100,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['down'],['downstream']
Availability,"be used with categorical or continuous expressions.; title (str, optional) – Title of the scatterplot.; xlabel (str, optional) – X-axis label.; ylabel (str, optional) – Y-axis label.; size (int) – Size of markers in screen space units.; legend (bool) – Whether or not to show the legend in the resulting figure.; hover_fields (Dict[str, Expression], optional) – Extra fields to be displayed when hovering over a point on the plot.; colors (bokeh.models.mappers.ColorMapper or Dict[str, bokeh.models.mappers.ColorMapper], optional) – If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using label.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width (int) – Plot width; height (int) – Plot height; collect_all (bool) – Deprecated. Use n_divisions instead.; n_divisions (int, optional) – Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use None to collect all points.; missing_label (str) – Label to use when a point is missing data for a categorical label. Returns:; bokeh.plotting.figure if no label or a single label was given, otherwise bokeh.models.layouts.Column. hail.plot.manhattan(pvals, locus=None, title=None, size=4, hover_fields=None, collect_all=None, n_divisions=500, significance_line=5e-08)[source]; Create a Manhattan plot. (https://en.wikipedia.org/wiki/Manhattan_plot). Parameters:. pvals (Float64Expression) – P-values to be plotted.; locus (LocusExpression, optional) – Locus values to be plotted.; title (str, optional) – Title of the plot.; size (int) – Size of markers in screen space units.; hover_fields (Dict[str, Expression], optional) – Dictionary of field names and values to be shown in the HoverTool of the plot.; collect_all (bool, optional) – Deprecated - use n_divisions instead.; n_divisions (int, optio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:12096,down,downsample,12096,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['down'],['downsample']
Availability,"ber of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. Return type:VariantDataset. naive_coalesce(max_partitions)[source]¶; Naively descrease the number of partitions. Warning; naive_coalesce() simply combines adjacent partitions to achieve the desired number. It does not attempt to rebalance, unlike repartition(), so it can produce a heavily unbalanced dataset. An unbalanced dataset can be inefficient to operate on because the work is not evenly distributed across partitions. Parameters:max_partitions (int) – Desired number of partitions. If the current number of partitions is less than max_partitions, do nothing. Returns:Variant dataset with the number of partitions equal to at most max_partitions. Return type:VariantDataset. num_partitions()[source]¶; Number of partitions.; Notes; The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. Partitions are a core concept of distributed computation in Spark, see here for details. Return type:int. num_samples¶; Number of samples. Return type:int. pc_relate(k, maf, block_size=512, min_kinship=-inf, statistics='all')[source]¶; Compute relatedness estimates between individuals using a variant of the; PC-Relate method. Danger; This method is experimental. We neither guarantee interface; stability nor that the results are viable for any particular use. Examples; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:; >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024.; >>> rel = vds.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:129219,avail,available,129219,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['avail'],['available']
Availability,"ble that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:156666,error,errors,156666,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,"ble. Legend entries for such plots can be clicked to show/hide; traces on the plot, but this does not work and is a known issue that; will only be addressed if hail.ggplot is migrated off of plotly.; (#12584) Fixed bug; which arose as an assertion error about type mismatches. This was; usually triggered when working with tuples.; (#12583) Fixed bug; which showed an empty table for ht.col_key.show().; (#12582) Fixed bug; where matrix tables with duplicate col keys do not show properly.; Also fixed bug where tables and matrix tables with HTML unsafe column; headers are rendered wrong in Jupyter.; (#12574) Fixed a; memory leak when processing tables. Could trigger unnecessarily high; memory use and out of memory errors when there are many rows per; partition or large key fields.; (#12565) Fixed a bug; that prevented exploding on a field of a Table whose value is a; random value. Version 0.2.107; Released 2022-12-14. Bug fixes. (#12543) Fixed; hl.vds.local_to_global error when LA array contains non-ascending; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12497) Added; support for scales, nrow, and ncol arguments, as well as; grouped legends, to hail.ggplot.facet_wrap.; (#12471) Added; hailctl batch submit command to run local scripts inside batch; jobs.; (#12525) Add support; for passing arguments to hailctl batch submit.; (#12465) Batch jobs’; status now contains the region the job ran in. The job itself can; access which region it is in through the HAIL_REGION environment; variable.; (#12464) When using; Query-on-Batch, all jobs for a single hail session are inserted into; the same batch instead of one batch per action.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; numbe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:41228,error,error,41228,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"ble1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:80110,redundant,redundant,80110,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['redundant'],['redundant']
Availability,"ble:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the tab",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6480,error,errors,6480,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"bool:; """"""Returns ``True`` if `path` exists. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return _fses[requester_pays_config].exists(path). [docs]def is_file(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> bool:; """"""Returns ``True`` if `path` both exists and is a file. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return _fses[requester_pays_config].is_file(path). [docs]def is_dir(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> bool:; """"""Returns ``True`` if `path` both exists and is a directory. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return _fses[requester_pays_config].is_dir(path). [docs]def stat(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> FileListEntry:; """"""Returns information about the file or directory at a given path. Notes; -----; Raises an error if `path` does not exist. The resulting dictionary contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`dict`; """"""; return _fses[requester_pays_config].stat(path). [docs]def ls(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> List[FileListEntry]:; """"""Returns information about files at `path`. Notes; -----; Raises an error if `path` does not exist. If `path` is a file, returns a list with one element. If `path` is a; directory, returns an element for each file contained in `path` (does not; search recursively). Each dict element of the result list contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:5432,error,error,5432,docs/0.2/_modules/hailtop/fs/fs_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html,2,['error'],['error']
Availability,"bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:; >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: ; ... f.write('result1: %s\n' % result1); ... f.write('result2: %s\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:; >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: ; ... print(unpack('<f', bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hailtop.fs.remove(path, *, requester_pays_config=None)[source]; Removes the file at path. If the file does not exist, this function does; nothing. path must be a URI (uniform resource identifier) or a path on the; local filesystem. Parameters:; path (str). hailtop.fs.rmtree(path, *, requester_pays_config=None)[source]; Recursively remove all files under the given path. On a local filesystem,; this removes the directory tree at path. On blob storage providers such as; GCS, S3 and ABS, this removes all files whose name starts with path. As such,; path must be a URI (uniform resource identifier) or a path on the local filesystem. Parameters:; path (str). hailtop.fs.stat(path, *, requester_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/fs_api.html:5679,error,error,5679,docs/0.2/fs_api.html,https://hail.is,https://hail.is/docs/0.2/fs_api.html,1,['error'],['error']
Availability,"butes=None, shell=None); Bases: Job; Object representing a single bash job to execute.; Examples; Create a batch object:; >>> b = Batch(). Create a new bash job that prints hello to a temporary file t.ofile:; >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'). Write the temporary file t.ofile to a permanent location; >>> b.write_output(j.ofile, 'hello.txt'). Execute the DAG:; >>> b.run(). Notes; This class should never be created directly by the user. Use Batch.new_job(); or Batch.new_bash_job() instead.; Methods. command; Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the job’s command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; This method can be called more than once. It’s behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command.; To declare a resource file of type JobResourceFile, use either; the get attribute syntax of job.{identifier} or the get item ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:1453,echo,echo,1453,docs/batch/api/batch/hailtop.batch.job.BashJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html,2,['echo'],['echo']
Availability,"by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:113518,error,errors,113518,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,"bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hailtop.fs.remove(path, *, requester_pays_config=None)[source]; Removes the file at path. If the file does not exist, this function does; nothing. path must be a URI (uniform resource identifier) or a path on the; local filesystem. Parameters:; path (str). hailtop.fs.rmtree(path, *, requester_pays_config=None)[source]; Recursively remove all files under the given path. On a local filesystem,; this removes the directory tree at path. On blob storage providers such as; GCS, S3 and ABS, this removes all files whose name starts with path. As such,; path must be a URI (uniform resource identifier) or a path on the local filesystem. Parameters:; path (str). hailtop.fs.stat(path, *, requester_pays_config=None)[source]; Returns information about the file or directory at a given path.; Notes; Raises an error if path does not exist.; The resulting dictionary contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; dict. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/fs_api.html:6806,error,error,6806,docs/0.2/fs_api.html,https://hail.is,https://hail.is/docs/0.2/fs_api.html,1,['error'],['error']
Availability,"c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. We use the image hailgenetics/genetics which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK.; We explicitly tell PLINK to only use 1Gi of memory because PLINK defaults to using half; of the machine’s memory. PLINK’s memory-available detection mechanism is unfortunately; unaware of the memory limit imposed by Batch. Not specifying resource requirements; correctly can cause performance degradations with PLINK.; PLINK creates a hard-coded file plink.clumped. We have to move that file to a temporary; Batch file {c.clumped} in order to use that file in downstream jobs. Merge Clumping Results; The third function concatenates all of the clumping results per chromosome into a single file; with one header line. The inputs are the Batch for which to create a new BashJob; and a list containing all of the individual clumping results files. We use the ubuntu:22.04; Docker image for this job. The return value is the new BashJob created.; def merge(batch, results):; """"""; Merge clumped results files together; """"""; merger = batch.new_job(name='merge-results'); merger.image('ubuntu:22.04'); if results:; merger.command(f'''; head -n 1 {results[0]} > {merger.ofile}; for result in {"" "".join(results)}; do; tail -n +2 ""$result"" >> {merger.ofile}; done; sed -i -e '/^$/d' {merger.ofile}; '''); return merger. Control Code; The last thing we want to do is use the functions we wrote above to create new jobs; on a Batch which can be executed with the ServiceBackend.; First, we import the Batch module as hb.; import hailtop.batch as hb. Next, we",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:9248,down,downstream,9248,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['down'],['downstream']
Availability,"cExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def fit(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = y @ hl.log(mu) - mu.sum(). next_b = b + delta_b; next_mu = hl.exp(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = (next_mu * X.T) @ X. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(; b=b,; score=score,; fisher=fisher,; mu=mu,; n_iterations=iteration,; log_lkhd=log_lkhd,; converged=True,; exploded=False,; ),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b, next_mu, next_score, next_fisher)); ). delta_b_struct = hl.nd.solve(fisher, score, no_crash=True). exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution; max_delta_b = nd_max(delta_b.map(lambda e: hl.abs(e))); return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.select(n_iterations=0, log_lkhd=0, converged=False, exploded=False); return hl.experimental.loop(fit, dtype, 1, b, mu, score, fisher). def _poisson_score_test(null_fit, covmat, y, xvec):; dof = 1. X = hl.nd.hstack([covmat, xvec.T.reshape(-1, 1)]); b = hl.nd.hstack([null_fit.b, hl.nd.array([0.0])]); mu = hl.exp(X @ b); score = hl.nd.hstack(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:68791,toler,tolerance,68791,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,"calize=True) -> int:; """"""Count the number of columns in the matrix. Examples; --------. Count the number of columns:. >>> n_cols = dataset.count_cols(). Returns; -------; :obj:`int`; Number of columns in the matrix.; """"""; count_ir = ir.TableCount(ir.MatrixColsTable(self._mir)); if _localize:; return Env.backend().execute(count_ir); else:; return construct_expr(ir.LiftMeOut(count_ir), hl.tint64). [docs] def count(self) -> Tuple[int, int]:; """"""Count the number of rows and columns in the matrix. Examples; --------. >>> dataset.count(). Returns; -------; :obj:`int`, :obj:`int`; Number of rows, number of cols.; """"""; count_ir = ir.MatrixCount(self._mir); return Env.backend().execute(count_ir). [docs] @typecheck_method(; output=str,; overwrite=bool,; stage_locally=bool,; _codec_spec=nullable(str),; _read_if_exists=bool,; _intervals=nullable(sequenceof(anytype)),; _filter_intervals=bool,; _drop_cols=bool,; _drop_rows=bool,; ); def checkpoint(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; _drop_cols=False,; _drop_rows=False,; ) -> 'MatrixTable':; """"""Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`MatrixTable`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_matrix_table`. It is; possible to read the file at this path later with; :func:`.read_matrix_table`. A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; :meth:`write`. Examples; --------; >>> dataset = dataset.checkpoint(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:80170,checkpoint,checkpoint,80170,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,3,"['Checkpoint', 'checkpoint']","['Checkpoint', 'checkpoint']"
Availability,"call].is_het(), 0)),; is_hom_var: hl.float(hl.or_else(mt[call].is_hom_var(), 0)),; is_defined: hl.float(hl.is_defined(mt[call])),; }); ref = hl.linalg.BlockMatrix.from_entry_expr(mt[is_hom_ref], block_size=block_size); het = hl.linalg.BlockMatrix.from_entry_expr(mt[is_het], block_size=block_size); var = hl.linalg.BlockMatrix.from_entry_expr(mt[is_hom_var], block_size=block_size); defined = hl.linalg.BlockMatrix.from_entry_expr(mt[is_defined], block_size=block_size); ref_var = (ref.T @ var).checkpoint(hl.utils.new_temp_file()); # We need the count of times the pair is AA,aa and aa,AA. ref_var is only; # AA,aa. Transposing ref_var gives var_ref, i.e. aa,AA.; #; # n.b. (REF.T @ VAR).T == (VAR.T @ REF) by laws of matrix multiply; N_AA_aa = ref_var + ref_var.T; N_Aa_Aa = (het.T @ het).checkpoint(hl.utils.new_temp_file()); # We count the times the row individual has a heterozygous genotype and the; # column individual has any defined genotype at all.; N_Aa_defined = (het.T @ defined).checkpoint(hl.utils.new_temp_file()). het_hom_balance = N_Aa_Aa - (2 * N_AA_aa); het_hom_balance = het_hom_balance.to_matrix_table_row_major(); n_hets_for_rows = N_Aa_defined.to_matrix_table_row_major(); n_hets_for_cols = N_Aa_defined.T.to_matrix_table_row_major(). kinship_between = het_hom_balance.rename({'element': 'het_hom_balance'}); kinship_between = kinship_between.annotate_entries(; n_hets_row=n_hets_for_rows[kinship_between.row_key, kinship_between.col_key].element,; n_hets_col=n_hets_for_cols[kinship_between.row_key, kinship_between.col_key].element,; ). col_index_field = Env.get_uid(); col_key = mt.col_key; cols = mt.add_col_index(col_index_field).key_cols_by(col_index_field).cols(). kinship_between = kinship_between.key_cols_by(**cols[kinship_between.col_idx].select(*col_key)). renaming, _ = deduplicate(list(col_key), already_used=set(col_key)); assert len(renaming) == len(col_key). kinship_between = kinship_between.key_rows_by(; **cols[kinship_between.row_idx].select(*col_key).ren",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html:9346,checkpoint,checkpoint,9346,docs/0.2/_modules/hail/methods/relatedness/king.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html,2,['checkpoint'],['checkpoint']
Availability,"call}. fam_exprs = {; 'fam_id': expr_or_else(fam_id, '0'),; 'ind_id': hl.or_else(ind_id, '0'),; 'pat_id': expr_or_else(pat_id, '0'),; 'mat_id': expr_or_else(mat_id, '0'),; 'is_female': expr_or_else(is_female, '0', lambda x: hl.if_else(x, '2', '1')),; 'pheno': expr_or_else(pheno, 'NA', lambda x: hl.if_else(x, '2', '1') if x.dtype == tbool else hl.str(x)),; }. locus = dataset.locus; a = dataset.alleles. bim_exprs = {; 'varid': expr_or_else(varid, hl.delimit([locus.contig, hl.str(locus.position), a[0], a[1]], ':')),; 'cm_position': expr_or_else(cm_position, 0.0),; }. for exprs, axis in [; (fam_exprs, dataset._col_indices),; (bim_exprs, dataset._row_indices),; (entry_exprs, dataset._entry_indices),; ]:; for name, expr in exprs.items():; analyze('export_plink/{}'.format(name), expr, axis). dataset = dataset._select_all(col_exprs=fam_exprs, col_key=[], row_exprs=bim_exprs, entry_exprs=entry_exprs). # check FAM ids for white space; t_cols = dataset.cols(); errors = []; for name in ['ind_id', 'fam_id', 'pat_id', 'mat_id']:; ids = t_cols.filter(t_cols[name].matches(r""\s+""))[name].collect(). if ids:; errors.append(f""""""expr '{name}' has spaces in the following values:\n""""""); for row in ids:; errors.append(f"""""" {row}\n""""""). if errors:; raise TypeError(""\n"".join(errors)). writer = ir.MatrixPLINKWriter(output); Env.backend().execute(ir.MatrixWrite(dataset._mir, writer)). [docs]@typecheck(; dataset=oneof(MatrixTable, Table),; output=str,; append_to_header=nullable(str),; parallel=nullable(ir.ExportType.checker),; metadata=nullable(dictof(str, dictof(str, dictof(str, str)))),; tabix=bool,; ); def export_vcf(dataset, output, append_to_header=None, parallel=None, metadata=None, *, tabix=False):; """"""Export a :class:`.MatrixTable` or :class:`.Table` as a VCF file. .. include:: ../_templates/req_tvariant.rst. Examples; --------; Export to VCF as a block-compressed file:. >>> hl.export_vcf(dataset, 'output/example.vcf.bgz'). Notes; -----; :func:`.export_vcf` writes the dataset to disk in ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:15392,error,errors,15392,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['error'],['errors']
Availability,"cceeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we’ll append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesn’t exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:9181,checkpoint,checkpoint,9181,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability,"ccess images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project’s private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. Functions. GWAS; To start, we will write a function that creates a new Job on an existing Batch that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the Job that is created in the function, which can be used later to; access the binary PLINK file output and association results in downstream jobs.; def gwas(batch, vcf, phenotypes):; """"""; QC data and get association test statistics; """"""; cores = 2; g = batch.new_job(name='run-gwas'); g.image('us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas:latest'); g.cpu(cores); g.declare_resource_group(ofile={; 'bed': '{root}.bed',; 'bim': '{root}.bim',; 'fam': '{root}.fam',; 'assoc': '{root}.assoc'; }); g.command(f'''; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores}; '''); return g. A couple of things to note about this function:. The image is the image created in the previous step. We copied the run_gwas.py; script into the root directory /. Therefore, to execute the run_gwas.py script, we; call /run_gwas.py.; The run_gwas.py script takes an output-file parameter and then creates files ending with; the extensions .bed, .bim, .fam, and .assoc. In order for Batch to know the script is; creating fil",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:6590,down,downstream,6590,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['down'],['downstream']
Availability,"ce genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""; if index_file is None:; index_file = re.sub(r'\.[^.]*$', '.fai', fasta_file); Env.backend().add_sequence(self.name, fasta_file, index_file); self._sequence_files = (fasta_file, index_file). [docs] def has_sequence(self):; """"""True if the reference sequence has been loaded. Returns; -------; :obj:`bool`; """"""; return self._sequence_files is not None. [docs] def remove_sequence(self):; """"""Remove the reference sequence.""""""; self._sequence_files = None; Env.backend().remove_sequence(self.name). [docs] @classmethod; @typecheck_method(; name=str,; fasta_file=str,; index_file=str,; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:10326,down,download,10326,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,4,"['avail', 'down']","['available', 'download']"
Availability,"ces of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; :func:`.split_multi_hts`. The downcode algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded genotype, and shift so; the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Subset algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The subset algorithm subsets the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:160407,Down,Downcode,160407,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['Down'],['Downcode']
Availability,"ch row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; path: :class:`str`; Path for output.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`.; """"""; hl.current_backend().validate_file(path). if not block_size:; block_size = BlockMatrix.default_block_size(). raise_unless_entry_indexed('BlockMatrix.write_from_entry_expr', entry_expr); mt = matrix_table_source('BlockMatrix.write_from_entry_expr', entry_expr). if not (mean_impute or center or normalize):; if entry_expr in mt._fields_inverse:; field = mt._fields_inverse[entry_expr]; mt.select_entries(field)._write_block_matrix(path, overwrite, field, block_size); else:; field = Env.get_uid(); mt.select_entries(**{field: entry_expr})._write_block_matr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:24076,error,error,24076,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['error'],['error']
Availability,"check_method(path=str, overwrite=bool, force_row_major=bool, stage_locally=bool); def write(self, path, overwrite=False, force_row_major=False, stage_locally=False):; """"""Writes the block matrix. .. include:: ../_templates/write_warning.rst. Parameters; ----------; path: :class:`str`; Path for output file.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; force_row_major: :obj:`bool`; If ``True``, transform blocks in column-major format; to row-major format before writing.; If ``False``, write blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path). writer = BlockMatrixNativeWriter(path, overwrite, force_row_major, stage_locally); Env.backend().execute(BlockMatrixWrite(self._bmir, writer)). [docs] @typecheck_method(path=str, overwrite=bool, force_row_major=bool, stage_locally=bool); def checkpoint(self, path, overwrite=False, force_row_major=False, stage_locally=False):; """"""Checkpoint the block matrix. .. include:: ../_templates/write_warning.rst. Parameters; ----------; path: :class:`str`; Path for output file.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; force_row_major: :obj:`bool`; If ``True``, transform blocks in column-major format; to row-major format before checkpointing.; If ``False``, checkpoint blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:20913,checkpoint,checkpoint,20913,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,3,"['Checkpoint', 'checkpoint']","['Checkpoint', 'checkpoint']"
Availability,"ckend(*args, billing_project=None, bucket=None, remote_tmpdir=None, google_project=None, token=None, regions=None, gcs_requester_pays_configuration=None, gcs_bucket_allow_list=None); Bases: Backend[Batch]; Backend that executes batches on Hail’s Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named “my-billing-account”; and stores temporary intermediate files in “gs://my-bucket/temporary-files”.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1478,echo,echo,1478,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,2,['echo'],['echo']
Availability,"ckpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:10088,checkpoint,checkpoint,10088,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability,"class hail.vds.VariantDataset[source]; Class for representing cohort-level genomic data.; This class facilitates a sparse, split representation of genomic data in; which reference block data and variant data are contained in separate; MatrixTable objects. Parameters:. reference_data (MatrixTable) – MatrixTable containing only reference block data.; variant_data (MatrixTable) – MatrixTable containing only variant data. Attributes. ref_block_max_length_field; Name of global field that indicates max reference block length. reference_genome; Dataset reference genome. Methods. checkpoint; Write to path and then read from path. from_merged_representation; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples; The number of samples present. union_rows; Combine many VDSes with the same samples but disjoint variants. validate; Eagerly checks necessary representational properties of the VDS. write; Write to path. checkpoint(path, **kwargs)[source]; Write to path and then read from path. static from_merged_representation(mt, *, ref_block_fields=(), infer_ref_block_fields=True, is_split=False)[source]; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples()[source]; The number of samples present. ref_block_max_length_field = 'ref_block_max_length'; Name of global field that indicates max reference block length. property reference_genome; Dataset reference genome. Returns:; ReferenceGenome. union_rows()[source]; Combine many VDSes with the same samples but disjoint variants.; Examples; If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:; >>> vds_paths = ['chr1.vds', 'chr2.vds'] ; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) ; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) . validate(*, check_data=True)[source]; Eagerly checks necessary representational properties of the VDS. write(path, **kwargs)[source]; W",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html:1677,checkpoint,checkpoint,1677,docs/0.2/vds/hail.vds.VariantDataset.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html,1,['checkpoint'],['checkpoint']
Availability,"clear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement in this set. Code; Dad; Mom; Kid; Copy State. 1; HomVar; HomVar; Het; Auto. 2; HomRef; HomRef; Het; Auto. 3; HomRef; ! HomRef; HomVar; Auto. 4; ! HomRef; HomRef; HomVar; Auto. 5; HomRef; HomRef; HomVar; Auto. 6; Ho",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:126117,error,error,126117,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,"code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with state “Success”; failure - Any job has completed with state “Failure” or “Error”; cancelled - Any job has been cancelled and no jobs have completed with state “Failure” or “Error”. Note; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; ‘failure’, other jobs that do not depend on the failed job will still run. In the case of cancelled,; it takes time to cancel a batch, especially for larger batches. Individual jobs cannot be cancelled or deleted. Instead, you can cancel the entire batch with the “Cancel”; button next to the row for that batch. You can also delete a batch with the “Delete” button. Warning; Deleting a batch only removes it from the UI. You will still be billed for a deleted batch. The UI has an advanced search mode with a custom query language to find batches and jobs.; Learn more on the Advanced Search Help page. Important Notes. Warning; To avoid expensive egress charges, input and output files should be located in buckets; that are multi-regional in the United States because Bat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:13156,failure,failure,13156,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['failure'],['failure']
Availability,"columns[fields[0]])):; for field in fields:; cur_val = columns[field][data_idx]. # Can't call isna on a collection or it will implicitly broadcast; if pandas.api.types.is_numeric_dtype(df[field].dtype) and pandas.isna(cur_val):; if isinstance(cur_val, float):; fixed_val = cur_val; elif isinstance(cur_val, np.floating):; fixed_val = cur_val.item(); else:; fixed_val = None; elif isinstance(df[field].dtype, pandas.StringDtype) and pandas.isna(cur_val): # No NaN to worry about; fixed_val = None; elif isinstance(cur_val, np.number):; fixed_val = cur_val.item(); else:; fixed_val = cur_val; data[data_idx][field] = fixed_val. for data_idx, field in enumerate(fields):; type_hint = dtypes_from_pandas(pd_dtypes[field]); if type_hint is not None:; hl_type_hints[field] = type_hint. new_table = hl.Table.parallelize(data, partial_type=hl_type_hints); return new_table if not key else new_table.key_by(*key). @typecheck_method(other=table_type, tolerance=nullable(numeric), absolute=bool, reorder_fields=bool); def _same(self, other, tolerance=1e-6, absolute=False, reorder_fields=False):; from hail.expr.functions import _values_similar. fd_f = set if reorder_fields else list. if fd_f(self.row) != fd_f(other.row):; print(f'Different row fields: \n {list(self.row)}\n {list(other.row)}'); return False; if fd_f(self.globals) != fd_f(other.globals):; print(f'Different globals fields: \n {list(self.globals)}\n {list(other.globals)}'); return False. if reorder_fields:; globals_order = list(self.globals); if list(other.globals) != globals_order:; other = other.select_globals(*globals_order). row_order = list(self.row); if list(other.row) != row_order:; other = other.select(*row_order). if self._type != other._type:; print(f'Table._same: types differ:\n {self._type}\n {other._type}'); return False. left = self; left = left.select_globals(left_globals=left.globals); left = left.group_by(key=left.key).aggregate(left_row=hl.agg.collect(left.row_value)). right = other; right = right.select_globals(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:127809,toler,tolerance,127809,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,4,['toler'],['tolerance']
Availability,"comparison. Returns; -------; :class:`.Table`; """""". rg = mt.interval.start.dtype.reference_genome. if len(rg.x_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple X contigs, this is not supported in 'impute_sex_chr_ploidy_from_interval_coverage'""; ); chr_x = rg.x_contigs[0]; if len(rg.y_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple Y contigs, this is not supported in 'impute_sex_chr_ploidy_from_interval_coverage'""; ); chr_y = rg.y_contigs[0]. mt = mt.annotate_rows(contig=mt.interval.start.contig); mt = mt.annotate_cols(__mean_dp=hl.agg.group_by(mt.contig, hl.agg.sum(mt.sum_dp) / hl.agg.sum(mt.interval_size))). mean_dp_dict = mt.__mean_dp; auto_dp = mean_dp_dict.get(normalization_contig, 0.0); x_dp = mean_dp_dict.get(chr_x, 0.0); y_dp = mean_dp_dict.get(chr_y, 0.0); per_sample = mt.transmute_cols(; autosomal_mean_dp=auto_dp,; x_mean_dp=x_dp,; x_ploidy=2 * x_dp / auto_dp,; y_mean_dp=y_dp,; y_ploidy=2 * y_dp / auto_dp,; ); info(""'impute_sex_chromosome_ploidy': computing and checkpointing coverage and karyotype metrics""); return per_sample.cols().checkpoint(new_temp_file('impute_sex_karyotype', extension='ht')). [docs]@typecheck(; vds=VariantDataset,; calling_intervals=oneof(Table, expr_array(expr_interval(expr_locus()))),; normalization_contig=str,; use_variant_dataset=bool,; ); def impute_sex_chromosome_ploidy(; vds: VariantDataset, calling_intervals, normalization_contig: str, use_variant_dataset: bool = False; ) -> Table:; """"""Impute sex chromosome ploidy from depth of reference or variant data within calling intervals. Returns a :class:`.Table` with sample ID keys, with the following fields:. - ``autosomal_mean_dp`` (*float64*): Mean depth on calling intervals on normalization contig.; - ``x_mean_dp`` (*float64*): Mean depth on calling intervals on X chromosome.; - ``x_ploidy`` (*float64*): Estimated ploidy on X chromosome. Equal to ``2 * x_mean_dp / autosomal_mean_dp``.; - ``y_mean_dp`` (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/methods.html:12182,checkpoint,checkpointing,12182,docs/0.2/_modules/hail/vds/methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html,2,['checkpoint'],['checkpointing']
Availability,"considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with sta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:12302,error,error,12302,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['error'],['error']
Availability,"correlation or realized relationship matrix (RRM) \(K\) as simply. \[K = MM^T\]; Note that the only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in grm() is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. Parameters:; force_block (bool) – Force using Spark’s BlockMatrix to compute kinship (advanced).; force_gramian (bool) – Force using Spark’s RowMatrix.computeGramian to compute kinship (advanced). Returns:Realized Relationship Matrix for all samples. Return type:KinshipMatrix. same(other, tolerance=1e-06)[source]¶; True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values.; Examples; This will return True:; >>> vds.same(vds). Notes; The tolerance parameter sets the tolerance for equality when comparing floating-point fields. More precisely, \(x\) and \(y\) are equal if. \[bs{x - y} \leq tolerance * \max{bs{x}, bs{y}}\]. Parameters:; other (VariantDataset) – variant dataset to compare against; tolerance (float) – floating-point tolerance for equality. Return type:bool. sample_annotations¶; Return a dict of sample annotations.; The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. Returns:dict. sample_ids¶; Return sampleIDs. Returns:List of sample IDs. Return type:list of str. sample_qc(root='sa.qc', keep_star=False)[source]¶; Compute per-sample QC metrics. Important; The genotype_schema() must be of type TGenotype in order to use this method. Annotations; sample_qc() computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; sa.qc.<identifier> (or <root>.<identifier> if a non-default root was passed):. Name; Type; Description. callRate; Double; Fraction of genotypes called. nHomRef; Int; Number of homozygous reference genotypes. nHet; Int; Number of heterozygous genoty",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:155480,toler,tolerance,155480,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['toler'],['tolerance']
Availability,"create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn’t be publically available. If you have an artifact registry associated with your project, then you can enable the service account to; view Do",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:2757,down,download,2757,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['down'],['download']
Availability,"cy. menu; Hail. Module code; hail.experimental.loop. Source code for hail.experimental.loop; from typing import Callable. from hail import ir; from hail.expr.expressions import construct_expr, construct_variable, expr_any, to_expr, unify_all; from hail.expr.types import hail_type; from hail.typecheck import anytype, typecheck; from hail.utils.java import Env. [docs]@typecheck(f=anytype, typ=hail_type, args=expr_any); def loop(f: Callable, typ, *args):; r""""""Define and call a tail-recursive function with given arguments. Notes; -----; The argument `f` must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. .. math::. f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}. we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)). Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called. This means that the result of any recursive call within the function must; also be the result of the entire function, without modification. Let's; consider two different recursive definitions for the triangle function; :math:`f(x) = 0 + 1 + \dots + x`:. >>> def triangle1(x):; ... if x == 1:; ... return x; ... return x + triangle1(x - 1). >>> def triangle2(x, total):; ... if x == 0:; ... return total; ... return triangle2(x - 1, total + x). The first function definition, `triangle1`, will call itself and then add x.; This is an example of a non-tail recursive function, since `triangle1(9)`; needs to modify the result of the inner recursive call to `triangle1(8)` by; adding 9 to the result. The second function is tail recursive: the result of `triangle2(9, 0)` is; the same as the result of the inner recursive call, `triangle2(8, 9)`. Example; -------; To find the sum of all the numbers from n=1...10:; >>> triangle_f = lambda f, x, total",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html:1435,error,error,1435,docs/0.2/_modules/hail/experimental/loop.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html,2,['error'],['error']
Availability,"d VEP annotations to your VDS, make sure to add the initialization action ; :code:`gs://hail-common/vep/vep/vep85-init.sh` when starting your cluster. :param annotations: List of annotations to import from the database.; :type annotations: str or list of str . :param gene_key: Existing variant annotation used to map variants to gene symbols if importing gene-level ; annotations. If not provided, the method will add VEP annotations and parse them as described in the ; database documentation to obtain one gene symbol per variant.; :type gene_key: str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". # import modules needed by this function; import sqlite3. # collect user-supplied annotations, converting str -> list if necessary and dropping duplicates; annotations = list(set(wrap_to_list(annotations))). # open connection to in-memory SQLite database; conn = sqlite3.connect(':memory:'). # load database with annotation metadata, print error if not on Google Cloud Platform; try:; f = hadoop_read('gs://annotationdb/ADMIN/annotationdb.sql'); except FatalError:; raise EnvironmentError('Cannot read from Google Storage. Must be running on Google Cloud Platform to use annotation database.'); else:; curs = conn.executescript(f.read()); f.close(). # parameter substitution string to put in SQL query; like = ' OR '.join('a.annotation LIKE ?' for i in xrange(2*len(annotations))). # query to extract path of all needed database files and their respective annotation exprs ; qry = """"""SELECT file_path, annotation, file_type, file_element, f.file_id; FROM files AS f INNER JOIN annotations AS a ON f.file_id = a.file_id; WHERE {}"""""".format(like). # run query and collect results in a file_path: expr dictionary; results = curs.execute(qry, [x + '.%' for x in annotations] + annotations).fetchall(). # all file_ids to be used; file_ids = list(set([x[4] for x in results])). # parameter substitution string; sub = ','.join('?' for x in file_ids). # query to fetch coun",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:35253,error,error,35253,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability,"d as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement in this set. Code; Dad; Mom; Kid; Copy State. 1; HomVar; HomVar; Het; Auto. 2; HomRef; HomRef; Het; Auto. 3; HomRef; ! HomRef; HomVar; Auto. 4; ! HomRef; HomRef; HomVar; Auto. 5; HomRef; HomRef; HomVar; Auto. 6; HomVar; ! HomVar; HomRef; Auto. 7; ! HomVar; HomVar; HomRef; Auto. 8; HomVar; HomVar; HomRef; Auto. 9; Any; HomVar; HomRef; HemiX. 10; Any; HomRef; HomVar; HemiX. 11; HomVar; Any; HomRef; HemiY. 12; HomRef; Any; HomVar; HemiY. This method only considers children with two parents and a defined sex.; PAR is currently defined with respect to reference; GRCh37:. X: 60001 - 2699520, 154931044 - 155260560; Y: 10001 - 2649520, 59034050 - 59363566. Parameters:pedigree (Pedigree) – Sample pedigree. Returns:Four tables with Mendel error statistics. Return type:(KeyTable, KeyTable, KeyTable, KeyTable). min_rep(max_shift=100)[source]¶; Gives minimal, left-aligned representation of alleles. Note that this can change the variant position.; Examples; 1. Simple trimming of a multi-allelic site, no change in variant position; 1:10000:TAA:TAA,AA => 1:10000:TA:T,A; 2. Trimming of a bi-allelic site leading to a change in position; 1:10000:AATAA,AAGAA => 1:10002:T:G. Parameters:max_shift (int) – maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. Return type:VariantDataset. naive_coalesce(max_partitions)[source]¶; Naively descrease the number of partitions. Warning; naive_coalesce() simply combines adjacent partitions to achieve the desired number. It does not attempt to rebalance, unlike repartition(), so it can produce a heavily unbalanced dataset. An unbalanced dataset can be inefficient to ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:127655,error,error,127655,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,"d on the PLs ignoring the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g., filtering alleles 1 and 2 transforms 25,5,10,20 to 25,20.; DP: No change.; PL: The filtered alleles’ columns are eliminated and the remaining columns shifted so the minimum value is 0.; GQ: The second-lowest PL (after shifting). Downcode algorithm; The downcode algorithm (subset=False) recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where downcodeing filtered alleles merges distinct genotypes, the minimum PL is used (since PL is on a log scale, this roughly corresponds to adding probabilities). The PLs; are then re-normalized (shifted) so that the most likely genotype has a PL of 0, and GT is set to this genotype.; If an allele is filtered, this algorithm acts similarly to split_multi().; The downcoding algorithm would produce the following:; GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. GT: Downcode filtered alleles to reference.; AD: The filtered alleles’ columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms 25,5,10,20 to 40,20.; DP: No change.; PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded genotype, and shift so the overall minimum PL is 0.; GQ: The second-lowest PL (after shifting). Expression Variables; The following symbols are in scope for expr:. v (Variant): Variant; va: variant annotations; aIndex (Int): the index of the allele being tested. The following symbols are in scope for annotation:. v (Variant): Variant; va: variant annotations; aIndices (Array[Int]): the array of old indices (such that aIndices[newIndex] = oldIndex and aIndices[0] = 0). Parameters:; expr (str) – Boolean filter expression involving v (variant), va (variant annotations), ; and aIndex (allele inde",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:50349,down,downcoding,50349,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['down'],['downcoding']
Availability,"d performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:87073,checkpoint,checkpoint,87073,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['checkpoint'],['checkpoint']
Availability,"d(). self._gateway = SparkContext._gateway; self._jvm = SparkContext._jvm. # hail package; self._hail = getattr(self._jvm, 'is').hail. Env._jvm = self._jvm; Env._gateway = self._gateway. jsc = sc._jsc.sc() if sc else None. # we always pass 'quiet' to the JVM because stderr output needs; # to be routed through Python separately.; self._jhc = self._hail.HailContext.apply(; jsc, app_name, joption(master), local, log, True, append,; parquet_compression, min_block_size, branching_factor, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmethod; def get_running():; """"""Return the running Hail context in this Python session. **Example**. .. doctest::; :options: +SKIP. >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. :return: Current Hail context.; :rtype: :class:`.HailContext`; """""". return Env.hc(). @property; def version(self):; """"""Return the version of Hail associated with this HailContext. :rtype: str; """"""; return self._jhc.version(). [docs] @handle_py4j; @typecheck_method(regex=strlike,; path=oneof(strlike, listof(strlike)),; max_count=integral); def grep(self, regex, path, max_count=100):; """"""Grep big files, like, really fast. **Exam",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:3500,avail,available,3500,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['avail'],['available']
Availability,"d(null_fit):; return (; hl.case(); .when(; ~null_fit.exploded,; (; hl.case(); .when(null_fit.converged, null_fit); .or_error(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:60066,toler,tolerance,60066,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['toler'],['tolerance']
Availability,"d). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Numbe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:49482,error,errors,49482,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability,"d:int,is_female:bool,fam_id:str}>'). trios_sym = Env.get_uid(); entries_sym = Env.get_uid(); cols_sym = Env.get_uid(). mt = mt.annotate_globals(**{trios_sym: hl.literal(trios, trios_type)}); mt = mt._localize_entries(entries_sym, cols_sym); mt = mt.annotate_globals(**{; cols_sym: hl.map(; lambda i: hl.bind(; lambda t: hl.struct(; id=mt[cols_sym][t.id][k],; proband=mt[cols_sym][t.id],; father=mt[cols_sym][t.pat_id],; mother=mt[cols_sym][t.mat_id],; is_female=t.is_female,; fam_id=t.fam_id,; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.annotate(**{; entries_sym: hl.map(; lambda i: hl.bind(; lambda t: hl.struct(; proband_entry=mt[entries_sym][t.id],; father_entry=mt[entries_sym][t.pat_id],; mother_entry=mt[entries_sym][t.mat_id],; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.drop(trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four ta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:4236,error,errors,4236,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"dTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; if n <= 0:; raise ValueError(n); self._buffer_size = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate(self, **named_exprs) -> 'Table':; """"""Aggregate by group, used after :meth:`.Table.group_by`. Examples; --------; Compute the mean value of `X` and the sum of `Z` per unique `ID`:. >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_res",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:5682,down,downstream,5682,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['down'],['downstream']
Availability,"d_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) Fixed a combinatorial explosion in cancellation calculation in the LocalBackend; (#12917) ABS blob URIs in the form of https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH> are now supported when running in Azure. The hail-az scheme for referencing ABS blobs is now deprecated and will be removed in a future release. Version 0.2.114. (#12780) PythonJobs now handle argume",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:1849,error,error,1849,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['error'],['error']
Availability,"dataset. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False, _drop_cols=False, _drop_rows=False)[source]; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; MatrixTable. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_matrix_table(). It is; possible to read the file at this path later with; read_matrix_table(). A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; write().; Examples; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'). choose_cols(indices)[source]; Choose a new set of columns from a list of old column indices.; Examples; Randomly shuffle column order:; >>> import random; >>> indices = list(range(dataset.count_cols())); >>> random.shuffle(indices); >>> dataset_reordered = dataset.choose_cols(indices). Take the first ten columns:; >>> dataset_result = dataset.choose_cols(list(range(10))). Parameters:; indices (list of int) – List of old column indices. Returns:; MatrixTable. property col; Returns a struct expression of all column-indexed fields, including keys.; Examples; Get all column field names:; >>> list(dataset.col) ; ['s', 'sample_qc', 'is_case', 'pheno', 'cov', 'cov1', 'cov2', 'cohorts', 'pop']. Returns:; StructExpression – Struct of all column fields. property col_key; Column key struct.; Examples; Get the column key field names:; >>> list(dataset.col_key); ['s']. Returns:; StructExpression. property col_value; Returns a struct expression including all non-key colu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:19136,checkpoint,checkpoint,19136,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['checkpoint'],['checkpoint']
Availability,"dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:5941,error,errors,5941,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"datasets:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2). Given a list of datasets, take the union of all rows:; >>> all_datasets = [dataset_to_union_1, dataset_to_union_2]. The following three syntaxes are equivalent:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2); >>> dataset_result = all_datasets[0].union_rows(*all_datasets[1:]); >>> dataset_result = hl.MatrixTable.union_rows(*all_datasets). Notes; In order to combine two datasets, three requirements must be met:. The column keys must be identical, both in type, value, and ordering.; The row key schemas and row schemas must match.; The entry schemas must match. The column fields in the resulting dataset are the column fields from; the first dataset; the column schemas do not need to match.; This method does not deduplicate; if a row exists identically in two; datasets, then it will be duplicated in the result. Warning; This method can trigger a shuffle, if partitions from two datasets; overlap. Parameters:; datasets (varargs of MatrixTable) – Datasets to combine. Returns:; MatrixTable – Dataset with rows from each member of datasets. unpersist()[source]; Unpersists this dataset from memory/disk.; Notes; This function will have no effect on a dataset that was not previously; persisted. Returns:; MatrixTable – Unpersisted dataset. write(output, overwrite=False, stage_locally=False, _codec_spec=None, _partitions=None)[source]; Write to disk.; Examples; >>> dataset.write('output/dataset.mt'). Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. See also; read_matrix_table(). Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:68744,checkpoint,checkpoint,68744,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['checkpoint'],['checkpoint']
Availability,"delian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:; >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped). Export all mendel errors to a text file:; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table clo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:124315,error,error,124315,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,"delta\) to find the REML estimate \((\hat{\delta}, \hat{\beta}, \hat{\sigma}_g^2)\) of the triple \((\delta, \beta, \sigma_g^2)\), which in turn determines \(\hat{\sigma}_e^2\) and \(\hat{h}^2\).; We first compute the maximum log likelihood on a \(\delta\)-grid that is uniform on the log scale, with \(\mathrm{ln}(\delta)\) running from -8 to 8 by 0.01, corresponding to \(h^2\) decreasing from 0.9995 to 0.0005. If \(h^2\) is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when \(\hat{h}^2\) is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on \(\mathrm{ln}(\delta)\) of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid.; Note that \(h^2\) is related to \(\mathrm{ln}(\delta)\) through the sigmoid function. More precisely,. \[h^2 = 1 - \mathrm{sigmoid}(\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\mathrm{ln}(\delta))\]; Hence one can change variables to extract a high-resolution discretization of the likelihood function of \(h^2\) over \([0,1]\) at the corresponding REML estimators for \(\beta\) and \(\sigma_g^2\), as well as integrate over the normalized likelihood function using change of variables and the sigmoid differential equation.; For convenience, global.lmmreg.fit.normLkhdH2 records the the likelihood function of \(h^2\) normalized over the discrete grid 0.01, 0.02, ..., 0.98, 0.99",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:102401,toler,tolerance,102401,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['toler'],['tolerance']
Availability,"distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.row_key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_matrix_table(tmp2).add_row_index(uid).key_rows_by(uid); ht.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions).drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions). return MatrixTable(; ir.MatrixRepartition(; self._mir, n_partitions, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'MatrixTable':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> dataset_result = dataset.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:108999,checkpoint,checkpoint,108999,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['checkpoint'],['checkpoint']
Availability,"ductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :py:meth:`~hail.VariantDataset.pc_relate` differs from the reference; implementation in a couple key ways:. - the principal components analysis does not use an unrelated set of; individuals. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). **Notes**. The ``block_size`` controls memory usage and parallelism. If it is large; enough to hold an entire sample-by-sample matrix of 64-bit doubles in; memory, then only one Spark worker node can be used to compute matrix; operations. If it is too small, communication overhead will begin to; dominate the computation's time. The author has found that on Google; Dataproc (where each core has about 3.75GB of memory), setting; ``block_size`` larger than 512 tends to cause memory exhaustion errors. The minimum allele frequency filter is applied per-pair: if either of; the two individual's individual-specific minor allele frequency is below; the threshold, then the variant's contribution to relatedness estimates; is zero. Under the PC-Relate model, kinship, \[ \phi_{ij} \], ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, \[ k^{(2)}_{ij} \],; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs. - Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation. - ""Third degree relatives"" are those pairs sharing; \[ 2^{",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:175129,error,errors,175129,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,"dvanced). :param bool force_gramian: Force using Spark's RowMatrix.computeGramian to compute kinship (advanced). :return: Realized Relationship Matrix for all samples.; :rtype: :py:class:`KinshipMatrix`; """"""; return KinshipMatrix(self._jvdf.rrm(force_block, force_gramian)). [docs] @handle_py4j; @typecheck_method(other=vds_type,; tolerance=numeric); def same(self, other, tolerance=1e-6):; """"""True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values. **Examples**. This will return True:. >>> vds.same(vds). **Notes**. The ``tolerance`` parameter sets the tolerance for equality when comparing floating-point fields. More precisely, :math:`x` and :math:`y` are equal if. .. math::. \abs{x - y} \leq tolerance * \max{\abs{x}, \abs{y}}. :param other: variant dataset to compare against; :type other: :class:`.VariantDataset`. :param float tolerance: floating-point tolerance for equality. :rtype: bool; """""". return self._jvds.same(other._jvds, tolerance). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(root=strlike,; keep_star=bool); def sample_qc(self, root='sa.qc', keep_star=False):; """"""Compute per-sample QC metrics. .. include:: requireTGenotype.rst. **Annotations**. :py:meth:`~hail.VariantDataset.sample_qc` computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; ``sa.qc.<identifier>`` (or ``<root>.<identifier>`` if a non-default root was passed):. +---------------------------+--------+----------------------------------------------------------+; | Name | Type | Description |; +===========================+========+==========================================================+; | ``callRate`` | Double | Fraction of genotypes called |; +---------------------------+--------+----------------------------------------------------------+; | ``nHomRef`` | Int | Number of homozygous reference genotypes |; +---------------------------+--------+---------------",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:196837,toler,tolerance,196837,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['toler'],['tolerance']
Availability,"e 2: Left distinct join: ht[ht2.key] or ht[ht2.field1, ht2.field2]""; ) from e. @property; def key(self) -> StructExpression:; """"""Row key struct. Examples; --------. List of key field names:. >>> list(table1.key); ['ID']. Number of key fields:. >>> len(table1.key); 1. Returns; -------; :class:`.StructExpression`; """"""; return self._key. @property; def _value(self) -> 'StructExpression':; return self.row.drop(*self.key). [docs] def n_partitions(self):; """"""Returns the number of partitions in the table. Examples; --------. Range tables can be constructed with an explicit number of partitions:. >>> ht = hl.utils.range_table(100, n_partitions=10); >>> ht.n_partitions(); 10. Small files are often imported with one partition:. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True); >>> ht2.n_partitions(); 1. The `min_partitions` argument to :func:`.import_table` forces more partitions, but it can; produce empty partitions. Empty partitions do not affect correctness but introduce; unnecessary extra bookkeeping that slows down the pipeline. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True, min_partitions=10); >>> ht2.n_partitions(); 10. Returns; -------; :obj:`int`; Number of partitions. """"""; return Env.backend().execute(ir.TableToValueApply(self._tir, {'name': 'NPartitionsTable'})). [docs] def count(self):; """"""Count the number of rows in the table. Examples; --------. Count the number of rows in a table loaded from 'data/kt_example1.tsv'. Each line of the TSV; becomes one row in the Hail Table. >>> ht = hl.import_table('data/kt_example1.tsv', impute=True); >>> ht.count(); 4. Returns; -------; :obj:`int`; The number of rows in the table. """"""; return Env.backend().execute(ir.TableCount(self._tir)). async def _async_count(self):; return await Env.backend()._async_execute(ir.TableCount(self._tir)). def _force_count(self):; return Env.backend().execute(ir.TableToValueApply(self._tir, {'name': 'ForceCountTable'})). async def _async_force_count(self)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:12746,down,down,12746,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['down'],['down']
Availability,"e Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each job j uses an f-string; to create a temporary output file j.ofile where the output to echo is redirected.; We then use all of the output files in the sink command by creating a string; with the temporary output file names for each job. A JobResourceFile; is a Batch-specific object that inherits from str. Therefore, you can use; JobResourceFile as if they were strings, which we do with the join; command for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; We can also create a nested scatter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new Batch object b. Then for each user in ‘Alice’, ‘Bob’, and ‘Dan’; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:7752,echo,echo,7752,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability,"e TypeError(; f""'import_bgen' requires all elements in 'variants' are a non-empty prefix of the BGEN key type: {expected_vtype!r}""; ). vir = variants._tir; if (; isinstance(vir, ir.TableRead); and isinstance(vir.reader, ir.TableNativeReader); and vir.reader.intervals is None; and variants.count() == variants.distinct().count(); ):; variants_path = vir.reader.path; else:; variants_path = new_temp_file(prefix='bgen_included_vars', extension='ht'); variants.distinct().write(variants_path); else:; variants_path = None. reader = ir.MatrixBGENReader(path, sample_file, index_file_map, n_partitions, block_size, variants_path). mt = MatrixTable(ir.MatrixRead(reader)).drop(; *[fd for fd in ['GT', 'GP', 'dosage'] if fd not in entry_set],; *[fd for fd in ['rsid', 'varid', 'offset', 'file_idx'] if fd not in row_set],; ). return mt. [docs]@typecheck(; path=oneof(str, sequenceof(str)),; sample_file=nullable(str),; tolerance=numeric,; min_partitions=nullable(int),; chromosome=nullable(str),; reference_genome=nullable(reference_genome_type),; contig_recoding=nullable(dictof(str, str)),; skip_invalid_loci=bool,; ); def import_gen(; path,; sample_file=None,; tolerance=0.2,; min_partitions=None,; chromosome=None,; reference_genome='default',; contig_recoding=None,; skip_invalid_loci=False,; ) -> MatrixTable:; """"""; Import GEN file(s) as a :class:`.MatrixTable`. Examples; --------. >>> ds = hl.import_gen('data/example.gen',; ... sample_file='data/example.sample',; ... reference_genome='GRCh37'). Notes; -----. For more information on the GEN file format, see `here; <http://www.stats.ox.ac.uk/%7Emarchini/software/gwas/file_format.html#mozTocId40300>`__. If the GEN file has only 5 columns before the start of the genotype; probability data (chromosome field is missing), you must specify the; chromosome using the `chromosome` parameter. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns; <sec-hadoop-glob>`. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:47079,toler,tolerance,47079,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,4,['toler'],['tolerance']
Availability,"e `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6286,error,errors,6286,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"e advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.row_key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_matrix_table(tmp2).add_row_index(uid).key_rows_by(uid); ht.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions).drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions). return MatrixTable(; ir.MatrixRepartition(; self._mir, n_partitions, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'MatrixTable':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> dataset_result = dataset.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:108811,checkpoint,checkpoint,108811,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['checkpoint'],['checkpoint']
Availability,"e an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:62615,toler,tolerance,62615,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,6,['toler'],['tolerance']
Availability,"e call includes two different alleles. is_het_non_ref; Evaluate whether the call includes two different alleles, neither of which is reference. is_het_ref; Evaluate whether the call includes two different alleles, one of which is reference. is_hom_ref; Evaluate whether the call includes two reference alleles. is_hom_var; Evaluate whether the call includes two identical alternate alleles. is_non_ref; Evaluate whether the call includes one or more non-reference alleles. n_alt_alleles; Returns the number of non-reference alleles. one_hot_alleles; Returns an array containing the summed one-hot encoding of the alleles. unphase; Returns an unphased version of this call. unphased_diploid_gt_index; Return the genotype index for unphased, diploid calls. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Get the i*th* allele.; Examples; Index with a single integer:; >>> hl.eval(call[0]); 0. >>> hl.eval(call[1]); 1. Parameters:; item (int or Expression of type tint32) – Allele index. Returns:; Expression of type tint32. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two express",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.CallExpression.html:2180,error,error,2180,docs/0.2/hail.expr.CallExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.CallExpression.html,1,['error'],['error']
Availability,"e code.; verbose (bool) – If True, print debugging output.; delete_scratch_on_exit (bool) – If True, delete temporary directories with intermediate files.; backend_kwargs (Any) – See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str) – Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. Warning; To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; All JobResourceFile are temporary files and must be written; to a permanent location using write_output() if the output needs; to be saved. Parameters:. resource (Resource) – Resource to be written to a file.; dest (str) – Destination file path. For a single ResourceFile, this will; simply be dest. For a ResourceGroup, dest is the file; root and each resource file will be written to {root}.identifier; where identifier is the identifier of the file in the; ResourceGroup map. Previous; Next ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:11586,echo,echo,11586,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['echo'],['echo']
Availability,"e extant Variant Datasets:; gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: use_exome_default_intervals=True and; use_genome_default_intervals=True.; The combiner serializes itself to save_path so that it can be restarted after failure. Parameters:. save_path (str) – The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path (str) – The location to store the new VariantDataset.; temp_path (str) – The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome (ReferenceGenome) – The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor (int) – The number of Variant Datasets to combine at once.; target_records (int) – The target number of variants per partition.; gvcf_batch_size (int) – The number of GVCFs to combine into a Variant Dataset at once.; contig_recoding (dict mapping str to str or None) – This mapping is applied to GVCF contigs before importing them into Hail. This is used to; handle GVCFs containing invalid contig names. For example, GRCh38 GVCFs which contain the; co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:2654,failure,failure,2654,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,1,['failure'],['failure']
Availability,"e for any row, otherwise False. cache()[source]; Persist this table in memory.; Examples; Persist the table in memory:; >>> table = table.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; Table – Cached table. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False)[source]; Checkpoint the table to disk by writing and reading. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; Table. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_table(). It is; possible to read the file at this path later with read_table().; Examples; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). collect(_localize=True, *, _timed=False)[source]; Collect the rows of the table into a local list.; Examples; Collect a list of all X records:; >>> all_xs = [row['X'] for row in table1.select(table1.X).collect()]. Notes; This method returns a list whose elements are of type Struct. Fields; of these structs can be accessed similarly to fields on a table, using dot; methods (struct.foo) or string indexing (struct['foo']). Warning; Using this method can cause out of memory errors. Only collect small tables. Returns:; list of Struct – List of rows. collect_by_key(name='values')[source]; Collect values for each unique key into an array. Note; Requires a keyed table. Examples; >>> t1 = hl.Table.parallelize([; ... {'t': 'foo', 'x': 4, 'y': 'A'},; ... {'t': 'bar', 'x': 2, 'y': 'B'},; ... {'t': 'bar', 'x': -3, 'y': 'C'},; ... {'t': 'quam', 'x': 0, 'y': 'D'}],; ... hl.tstruct(t=hl.tstr, x=hl.tint32, y=hl.tstr),; ... key='t'). >>> t1.show(); +--------+---",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:16014,checkpoint,checkpoint,16014,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['checkpoint'],['checkpoint']
Availability,"e list of results we’ll append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesn’t exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:9617,checkpoint,checkpointing,9617,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpointing']
Availability,"e only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in grm() is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. Parameters:; force_block (bool) – Force using Spark’s BlockMatrix to compute kinship (advanced).; force_gramian (bool) – Force using Spark’s RowMatrix.computeGramian to compute kinship (advanced). Returns:Realized Relationship Matrix for all samples. Return type:KinshipMatrix. same(other, tolerance=1e-06)[source]¶; True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values.; Examples; This will return True:; >>> vds.same(vds). Notes; The tolerance parameter sets the tolerance for equality when comparing floating-point fields. More precisely, \(x\) and \(y\) are equal if. \[bs{x - y} \leq tolerance * \max{bs{x}, bs{y}}\]. Parameters:; other (VariantDataset) – variant dataset to compare against; tolerance (float) – floating-point tolerance for equality. Return type:bool. sample_annotations¶; Return a dict of sample annotations.; The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. Returns:dict. sample_ids¶; Return sampleIDs. Returns:List of sample IDs. Return type:list of str. sample_qc(root='sa.qc', keep_star=False)[source]¶; Compute per-sample QC metrics. Important; The genotype_schema() must be of type TGenotype in order to use this method. Annotations; sample_qc() computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; sa.qc.<identifier> (or <root>.<identifier> if a non-default root was passed):. Name; Type; Description. callRate; Double; Fraction of genotypes called. nHomRef; Int; Number of homozygous reference genotypes. nHet; Int; Number of heterozygous genotypes. nHomVar; Int; Number of homozygous alternate genotypes. nCalled; Int; Sum of nHomRef + nH",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:155590,toler,tolerance,155590,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['toler'],['tolerance']
Availability,"e parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains.; import_bgen() normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. Annotations; import_bgen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .bgen files to import.; tolerance (float) – If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1.; sample_file (str or None) – Sample file.; min_partitions (int or None) – Number of partitions. Returns:Variant dataset imported from .bgen file. Return type:VariantDataset. import_gen(path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None)[source]¶; Import .gen file(s) as variant dataset.; Examples; Read a .gen file and a .sample file and write to a .vds file:; >>> (hc.import_gen('data/example.gen', sample_file='data/example.sample'); ... .write('output/gen_example1.vds')). Load multiple files at the same time with Hadoop glob patterns:; >>> (hc.import_gen('data/example.chr*.gen', sample_file='data/example.sample'); ... .write('output/gen_example2.vds')). Notes; For more information on the .gen file format, see here.; To ensure that the .gen file(s) and .sample file are correctly prepared for import:. If there are only 5 columns before the start of the genotype probability data (chromosome field is missing), you must specify the chromosome using the chromosome parameter; No duplicate sample IDs are allowed. The first column in the .sample file is used as the sample ID s.; Also, see section in import_bgen() linked here for information about Hail’s genotype p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:10652,toler,tolerance,10652,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['toler'],['tolerance']
Availability,"e result. The second function is tail recursive: the result of `triangle2(9, 0)` is; the same as the result of the inner recursive call, `triangle2(8, 9)`. Example; -------; To find the sum of all the numbers from n=1...10:; >>> triangle_f = lambda f, x, total: hl.if_else(x == 0, total, f(x - 1, total + x)); >>> x = hl.experimental.loop(triangle_f, hl.tint32, 10, 0); >>> hl.eval(x); 55. Let's say we want to find the root of a polynomial equation:; >>> def polynomial(x):; ... return 5 * x**3 - 2 * x - 1. We'll use `Newton's method<https://en.wikipedia.org/wiki/Newton%27s_method>`; to find it, so we'll also define the derivative:. >>> def derivative(x):; ... return 15 * x**2 - 2. and starting at :math:`x_0 = 0`, we'll compute the next step :math:`x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}`; until the difference between :math:`x_{i}` and :math:`x_{i+1}` falls below; our convergence threshold:. >>> threshold = 0.005; >>> def find_root(f, guess, error):; ... converged = hl.is_defined(error) & (error < threshold); ... new_guess = guess - (polynomial(guess) / derivative(guess)); ... new_error = hl.abs(new_guess - guess); ... return hl.if_else(converged, guess, f(new_guess, new_error)); >>> x = hl.experimental.loop(find_root, hl.tfloat, 0.0, hl.missing(hl.tfloat)); >>> hl.eval(x); 0.8052291984599675. Warning; -------; Using arguments of a type other than numeric types and booleans can cause; memory issues if if you expect the recursive call to happen many times. Parameters; ----------; f : function ( (marker, \*args) -> :class:`.Expression`; Function of one callable marker, denoting where the recursive call (or calls) is located,; and many `args`, the loop variables.; typ : :class:`str` or :class:`.HailType`; Type the loop returns.; args : variable-length args of :class:`.Expression`; Expressions to initialize the loop values.; Returns; -------; :class:`.Expression`; Result of the loop with `args` as initial loop values.; """""". loop_name = Env.get_uid(). def contains_recursive_c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html:3132,error,error,3132,docs/0.2/_modules/hail/experimental/loop.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html,4,['error'],['error']
Availability,"e resulting genotype schema is not TGenotype,; subsequent function calls on the annotated variant dataset may not work such as; pca() and linreg().; Hail performance may be significantly slower if the annotated variant dataset does not have a; genotype schema equal to TGenotype.; Genotypes are immutable. For example, if g is initially of type Genotype, the expression; g.gt = g.gt + 1 will return a Struct with one field gt of type Int and NOT a Genotype; with the gt incremented by 1. Parameters:expr (str or list of str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_global(path, annotation, annotation_type)[source]¶; Add global annotations from Python objects.; Examples; Add populations as a global annotation:; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). Notes; This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given annotation; parameter. Parameters:; path (str) – annotation path starting in ‘global’; annotation – annotation to add to global; annotation_type (Type) – Hail type of annotation. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_global_expr(expr)[source]¶; Annotate global with expression.; Example; Annotate global with an array of populations:; >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'). Create, then overwrite, then drop a global annotation:; >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS""]'); >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'); >>> vds = vds.annotate_global_expr('global.pops = drop(global, pops)'). The expression namespace contains only one variable:. global: global annotations. Parameters:expr (str or list of str) – Annotation expression. Returns:Annota",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:12159,down,downstream,12159,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['down'],['downstream']
Availability,"e resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs.; In the example below, we have defined a Batch b with the name ‘hello’.; We use the method Batch.new_job() to create a job object which we call j and then; use the method BashJob.command() to tell Batch that we want to execute echo “hello world”.; However, at this point, Batch hasn’t actually run the job to print “hello world”. All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:2374,echo,echo,2374,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability,"e run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:4192,down,downstream,4192,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['down'],['downstream']
Availability,"e samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:124639,error,error,124639,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,"e “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, Ho",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:125879,error,errors,125879,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,"e', 'Bob', 'Charlie']). See also; CollectionExpression. Attributes. dtype; The data type of the expression. Methods. aggregate; Uses the aggregator library to compute a summary from an array. append; Append an element to the array and return the result. contains; Returns a boolean indicating whether item is found in the array. extend; Concatenate two arrays and return the result. first; Returns the first element of the array, or missing if empty. grouped; Partition an array into fixed size subarrays. head; Deprecated in favor of first(). index; Returns the first index of x, or missing. last; Returns the last element of the array, or missing if empty. scan; Map each element of the array to cumulative value of function f, with initial value zero. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Index into or slice the array.; Examples; Index with a single integer:; >>> hl.eval(names[1]); 'Bob'. >>> hl.eval(names[-1]); 'Charlie'. Slicing is also supported:; >>> hl.eval(names[1:]); ['Bob', 'Charlie']. Parameters:; item (slice or Expression of type tint32) – Index or slice. Returns:; Expression – Element or array slice. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; ot",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:1802,error,error,1802,docs/0.2/hail.expr.ArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html,1,['error'],['error']
Availability,"e('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False, computes exact lambda GC (slower and uses more memory). Returns; -------; :obj:`float`; Genomic inflation factor (lambda genomic control).; """"""; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109269,toler,tolerance,109269,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,"e, Gene, nHet). Parameters:; key_exprs (str or list of str) – Named expression(s) for which fields are keys.; agg_exprs (str or list of str) – Named aggregation expression(s). Return type:KeyTable. annotate_alleles_expr(expr, propagate_gq=False)[source]¶; Annotate alleles with expression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; To create a variant annotation va.nNonRefSamples: Array[Int] where the ith entry of; the array is the number of samples carrying the ith alternate allele:; >>> vds_result = vds.annotate_alleles_expr('va.nNonRefSamples = gs.filter(g => g.isCalledNonRef()).count()'). Notes; This method is similar to annotate_variants_expr(). annotate_alleles_expr() dynamically splits multi-allelic sites,; evaluates each expression on each split allele separately, and for each expression annotates with an array with one element per alternate allele. In the splitting, genotypes are downcoded and each alternate allele is represented; using its minimal representation (see split_multi() for more details). Parameters:; expr (str or list of str) – Annotation expression.; propagate_gq (bool) – Propagate GQ instead of computing from (split) PL. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_genotypes_expr(expr)[source]¶; Annotate genotypes with expression.; Examples; Convert the genotype schema to a TStruct with two fields GT and CASE_HET:; >>> vds_result = vds.annotate_genotypes_expr('g = {GT: g.gt, CASE_HET: sa.pheno.isCase && g.isHet()}'). Assume a VCF is imported with generic=True and the resulting genotype schema; is a Struct and the field GTA is a Call type. Use the .toGenotype() method in the; expression language to convert a Call to a Genotype. vds_gta will have a genotype schema equal to; TGenotype; >>> vds_gta = (hc.import_vcf('data/example3.vcf.bgz', generic=True, call_fields=['GTA']); ... .annotate_genotypes_expr('g = g.GTA.toGenotype()')). Notes; annotate_genotypes_expr() eval",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:9470,down,downcoded,9470,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['down'],['downcoded']
Availability,"e, max_shift=100, keep_star=False):; """"""Filter a user-defined set of alternate alleles for each variant.; If all alternate alleles of a variant are filtered, the; variant itself is filtered. The expr expression is; evaluated for each alternate allele, but not for; the reference allele (i.e. ``aIndex`` will never be zero). .. include:: requireTGenotype.rst. **Examples**. To remove alternate alleles with zero allele count and; update the alternate allele count annotation with the new; indices:. >>> vds_result = vds.filter_alleles('va.info.AC[aIndex - 1] == 0',; ... annotation='va.info.AC = aIndices[1:].map(i => va.info.AC[i - 1])',; ... keep=False). Note that we skip the first element of ``aIndices`` because; we are mapping between the old and new *allele* indices, not; the *alternate allele* indices. **Notes**. If ``filter_altered_genotypes`` is true, genotypes that contain filtered-out alleles are set to missing. :py:meth:`~hail.VariantDataset.filter_alleles` implements two algorithms for filtering alleles: subset and downcode. We will illustrate their; behavior on the example genotype below when filtering the first alternate allele (allele 1) at a site with 1 reference; allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. **Subset algorithm**. The subset algorithm (the default, ``subset=True``) subsets the; AD and PL arrays (i.e. removes entries corresponding to filtered alleles); and then sets GT to the genotype with the minimum PL. Note; that if the genotype changes (as in the example), the PLs; are re-normalized (shifted) so that the most likely genotype has a PL of; 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard any; probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:61773,down,downcode,61773,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['down'],['downcode']
Availability,"e, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:124735,error,errors,124735,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,"e.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().unpersist(self). @overload; def collect(self) -> List[hl.Struct]: ... @overload; def collect(self, _localize=False) -> ArrayExpression: ... [docs] @typecheck_method(_localize=bool, _timed=bool); def collect(self, _localize=True, *, _timed=False):; """"""Collect the rows of the table into a local list. Examples; --------; Collect a list of all `X` records:. >>> all_xs = [row['X'] for row in table1.select(table1.X).collect()]. Notes; -----; This method returns a list whose elements are of type :class:`.Struct`. Fields; of these structs can be accessed similarly to fields on a table, using dot; methods (``struct.foo``) or string indexing (``struct['foo']``). Warning; -------; Using this method can cause out of memory errors. Only collect small tables. Returns; -------; :obj:`list` of :class:`.Struct`; List of rows.; """"""; if len(self.key) > 0:; t = self.order_by(*self.key); else:; t = self; rows_ir = ir.GetField(ir.TableCollect(t._tir), 'rows'); e = construct_expr(rows_ir, hl.tarray(t.row.dtype)); if _localize:; return Env.backend().execute(e._ir, timed=_timed); else:; return e. [docs] def describe(self, handler=print, *, widget=False):; """"""Print information about the fields in the table. Note; ----; The `widget` argument is **experimental**. Parameters; ----------; handler : Callable[[str], None]; Handler function for returned string.; widget : bool; Create an interactive IPython widget.; """"""; if widget:; from hail.experimental.interact import interact. return interact(self). def format_type(typ):; return typ.pretty(indent=4).lstrip(). if len(self.globals) == 0:; global_fields = '\n None'; else:; global_fields = ''.join(; ""\n '{name}': {type} "".format(name=f, type=format_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:81825,error,errors,81825,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['error'],['errors']
Availability,"e.select_entries(__GT=call); dataset = require_biallelic(source, 'mendel_errors'); tm = trio_matrix(dataset, pedigree, complete_trios=True); tm = tm.select_entries(; mendel_code=hl.mendel_error_code(; tm.locus, tm.is_female, tm.father_entry['__GT'], tm.mother_entry['__GT'], tm.proband_entry['__GT']; ); ); ck_name = next(iter(source.col_key)); tm = tm.filter_entries(hl.is_defined(tm.mendel_code)); tm = tm.rename({'id': ck_name}). entries = tm.entries(). table1 = entries.select('fam_id', 'mendel_code'). t2 = tm.annotate_cols(errors=hl.agg.count(), snp_errors=hl.agg.count_where(hl.is_snp(tm.alleles[0], tm.alleles[1]))); table2 = t2.key_cols_by().cols(); table2 = table2.select(; pat_id=table2.father[ck_name],; mat_id=table2.mother[ck_name],; fam_id=table2.fam_id,; errors=table2.errors,; snp_errors=table2.snp_errors,; ); table2 = table2.group_by('pat_id', 'mat_id').aggregate(; fam_id=hl.agg.take(table2.fam_id, 1)[0],; children=hl.int32(hl.agg.count()),; errors=hl.agg.sum(table2.errors),; snp_errors=hl.agg.sum(table2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:11150,error,errors,11150,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"e; AD and PL arrays (i.e. removes entries corresponding to filtered alleles); and then sets GT to the genotype with the minimum PL. Note; that if the genotype changes (as in the example), the PLs; are re-normalized (shifted) so that the most likely genotype has a PL of; 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard any; probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g., filtering alleles 1 and 2 transforms 25,5,10,20 to 25,20.; DP: No change.; PL: The filtered alleles’ columns are eliminated and the remaining columns shifted so the minimum value is 0.; GQ: The second-lowest PL (after shifting). Downcode algorithm; The downcode algorithm (subset=False) recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where downcodeing filtered alleles merges distinct genotypes, the minimum PL is used (since PL is on a log scale, this roughly corresponds to adding probabilities). The PLs; are then re-normalized (shifted) so that the most likely genotype has a PL of 0, and GT is set to this genotype.; If an allele is filtered, this algorithm acts similarly to split_multi().; The downcoding algorithm would produce the following:; GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. GT: Downcode filtered alleles to reference.; AD: The filtered alleles’ columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms 25,5,10,20 to 40,20.; DP: No change.; PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:49741,down,downcode,49741,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['down'],['downcode']
Availability,"e; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. p_value (NumericExpression) – Row-indexed numeric expression of p-values.; approximate (bool) – If False, computes exact lambda GC (slower and uses more memory). Returns:; float – Genomic inflation factor (lambda genomic control). hail.methods.split_multi(ds, keep_star=False, left_aligned=False, *, permit_shuffle=False)[source]; Split multiallelic variants. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:82381,toler,tolerance,82381,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['toler'],['tolerance']
Availability,"e; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we’ve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the genome. The; complete code is provided here for your reference. run_rf_simple.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_te",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:11185,checkpoint,checkpoint,11185,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability,"eIds()); return self._sample_ids. @property; @handle_py4j; def sample_annotations(self):; """"""Return a dict of sample annotations. The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. :return: dict; """""". if self._sample_annotations is None:; zipped_annotations = Env.jutils().iterableToArrayList(; self._jvds.sampleIdsAndAnnotations(); ); r = {}; for element in zipped_annotations:; r[element._1()] = self.sample_schema._convert_to_py(element._2()); self._sample_annotations = r; return self._sample_annotations. [docs] @handle_py4j; def num_partitions(self):; """"""Number of partitions. **Notes**. The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. Partitions are a core concept of distributed computation in Spark, see `here <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__ for details. :rtype: int; """""". return self._jvds.nPartitions(). @property; @handle_py4j; def num_samples(self):; """"""Number of samples. :rtype: int; """""". if self._num_samples is None:; self._num_samples = self._jvds.nSamples(); return self._num_samples. [docs] @handle_py4j; def count_variants(self):; """"""Count number of variants in variant dataset. :rtype: long; """""". return self._jvds.countVariants(). [docs] @handle_py4j; def was_split(self):; """"""True if multiallelic variants have been split into multiple biallelic variants. Result is True if :py:meth:`~hail.VariantDataset.split_multi` or :py:meth:`~hail.VariantDataset.filter_multi` has been called on this variant dataset,; or if the variant dataset was imported with :py:meth:`~hail.HailContext.import_plink`, :py:meth:`~hail.HailContext.import_gen`,; or :py:meth:`~hail.HailContext.import_bgen`, or if the variant dataset was simulated with :py:meth:`~hail.HailContext.balding_nichols_model`. :rtype: bool; """""". return self._",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:5012,resilien,resilient-distributed-datasets-rdds,5012,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['resilien'],['resilient-distributed-datasets-rdds']
Availability,"e_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table3.snp_errors[1],; }),; hl.struct(**{; ck_name: table3.proband[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[2],; 'snp_errors': table3.snp_errors[2],; }),; ]; ); table3 = table3.explode('xs'); table3 = table3.select(**table3.xs); table3 = (; table3.group_by(ck_name, 'fam_id'); .aggregate(errors=hl.agg.sum(table3.errors), snp_errors=hl.agg.sum(table3.snp_errors)); .key_by(ck_name); ). table4 = tm.select_rows(errors=hl.agg.count_where(hl.is_defined(tm.mendel_code))).rows(). return table1, table2, table3, table4. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree); def transmission_disequilibrium_test(dataset, pedigree) -> Table:; r""""""Performs the transmission disequilibrium test on trios. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------; Compute TDT association statistics and show the first two results:. >>> pedigree = hl.Pedigree.read('data/tdt_trios.fam'); >>> tdt_table = hl.transmission_disequilibrium_test(tdt_dataset, pedigree); >>> tdt_table.show(2) # doctest: +SKIP_OUTPUT_CHECK; +---------------+------------+-------+-------+----------+----------+; | locus | alleles | t | u | chi_sq | p_value |; +---------------+------------+-------+-------+----------+----------+; | locus<GRCh37> | array<str> | int64 | int64 | float64 | float64 |; +---------------+------------+-------+-------+",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:12662,error,errors,12662,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"e_cols; aggregations, and hl.sample_qc. Version 0.2.48; Released 2020-07-07. Bug fixes. (#9029) Fix crash; when using hl.agg.linreg with no aggregated data records.; (#9028) Fixed memory; leak affecting Table.annotate with scans,; hl.experimental.densify, and Table.group_by / aggregate.; (#8978) Fixed; aggregation behavior of; MatrixTable.{group_rows_by, group_cols_by} to skip filtered; entries. Version 0.2.47; Released 2020-06-23. Bug fixes. (#9009) Fix memory; leak when counting per-partition. This caused excessive memory use in; BlockMatrix.write_from_entry_expr, and likely in many other; places.; (#9006) Fix memory; leak in hl.export_bgen.; (#9001) Fix double; close error that showed up on Azure Cloud. Version 0.2.46; Released 2020-06-17. Site. (#8955) Natural; language documentation search. Bug fixes. (#8981) Fix; BlockMatrix OOM triggered by the MatrixWriteBlockMatrix; WriteBlocksRDD method. Version 0.2.45; Release 2020-06-15. Bug fixes. (#8948) Fix integer; overflow error when reading files >2G with hl.import_plink.; (#8903) Fix Python; type annotations for empty collection constructors and; hl.shuffle.; (#8942) Refactored; VCF combiner to support other GVCF schemas.; (#8941) Fixed; hl.import_plink with multiple data partitions. hailctl dataproc. (#8946) Fix bug when; a user specifies packages in hailctl dataproc start that are also; dependencies of the Hail package.; (#8939) Support; tuples in hailctl dataproc describe. Version 0.2.44; Release 2020-06-06. New Features. (#8914); hl.export_vcf can now export tables as sites-only VCFs.; (#8894) Added; hl.shuffle function to randomly permute arrays.; (#8854) Add; composable option to parallel text export for use with; gsutil compose. Bug fixes. (#8883) Fix an issue; related to failures in pipelines with force_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocur",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:69139,error,error,69139,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"e_excess_het: float64; }; }; --------------------------------------------------------; Source:; <hail.matrixtable.MatrixTable object at 0x7f0460fbcb50>; Index:; ['row']; --------------------------------------------------------. These statistics actually look pretty good: we don’t need to filter this dataset. Most datasets require thoughtful quality control, though. The filter_rows method can help!. Let’s do a GWAS!; First, we need to restrict to variants that are :. common (we’ll use a cutoff of 1%); not so far from Hardy-Weinberg equilibrium as to suggest sequencing error. [35]:. mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01). [36]:. mt = mt.filter_rows(mt.variant_qc.p_value_hwe > 1e-6). [37]:. print('Samples: %d Variants: %d' % (mt.count_cols(), mt.count_rows())). [Stage 37:> (0 + 1) / 1]. Samples: 250 Variants: 7774. These filters removed about 15% of sites (we started with a bit over 10,000). This is NOT representative of most sequencing datasets! We have already downsampled the full thousand genomes dataset to include more common variants than we’d expect by chance.; In Hail, the association tests accept column fields for the sample phenotype and covariates. Since we’ve already got our phenotype of interest (caffeine consumption) in the dataset, we are good to go:. [38]:. gwas = hl.linear_regression_rows(y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.row.describe(). [Stage 41:> (0 + 1) / 1]. --------------------------------------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; n: int32,; sum_x: float64,; y_transpose_x: float64,; beta: float64,; standard_error: float64,; t_stat: float64,; p_value: float64; }; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f0460f91d00>; Index:; ['row']; --------------------------------------------------------. Looking at the bottom of the above printout, you can see the linear regression adds new row fields fo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:18822,down,downsampled,18822,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['down'],['downsampled']
Availability,"e_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, 'HGDP.vcf.bgz'); source = resources['HGDP_matrix_table']; info(f'downloading HGDP VCF ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, resources['HGDP_matrix_table'], tmp_vcf); cluster_readable_vcf = _copy_to_tmp(fs, local_path_uri(tmp_vcf), extension='vcf.bgz'); info('importing VCF and writing to matrix table...'); hl.import_vcf(cluster_readable_vcf, min_partitions=16, reference_genome='GRCh38').write(; matrix_table_path, overwrite=True; ). tmp_sample_annot = os.path.join(tmp_dir, 'HGDP_annotations.txt'); source = resources['HGDP_annotations']; info(f'downloading HGDP annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['HGDP_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene_annot), gene_annotations_path); hl.hadoop_copy(local_path_uri(tmp_vcf), vcf_path); info('Done!'); else:; info('HGDP files found'). [docs]def get_movie_lens(output_dir, overwrite: bool = False):; """"""Download public Movie Lens dataset. Notes; -----; The download is about 6M. See the; `MovieLens website <https://grouplens.org/datasets/movielens/100k/>`__; for more information about this dataset. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite existing files/directories at those locations.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). paths = [os.path.join(output_dir, x) for x in ['movies.ht', 'ratings.ht', 'users.ht']]; if overwrite or any(not",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:5833,down,downloading,5833,docs/0.2/_modules/hail/utils/tutorial.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html,2,['down'],['downloading']
Availability,"e_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'); >>> vds = vds.annotate_global_expr('global.pops = drop(global, pops)'). The expression namespace contains only one variable:. - ``global``: global annotations. :param expr: Annotation expression; :type expr: str or list of str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = ','.join(expr). jvds = self._jvds.annotateGlobalExpr(expr); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(path=strlike,; annotation=anytype,; annotation_type=Type); def annotate_global(self, path, annotation, annotation_type):; """"""Add global annotations from Python objects. **Examples**. Add populations as a global annotation:; ; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). **Notes**. This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given ``annotation``; parameter. :param str path: annotation path starting in 'global'. :param annotation: annotation to add to global. :param annotation_type: Hail type of annotation; :type annotation_type: :py:class:`.Type`. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". annotation_type._typecheck(annotation). annotated = self._jvds.annotateGlobal(annotation_type._convert_to_j(annotation), annotation_type._jtype, path); assert annotated.globalSignature().typeCheck(annotated.globalAnnotation()), 'error in java type checking'; return VariantDataset(self.hc, annotated). [docs] @handle_py4j; @typecheck_method(expr=oneof(strlike, listof(strlike))); def annotate_samples_expr(self, expr):; """"""Annotate samples with expression. **Examples**. Compute per-sample GQ statistics for hets:. >>> vds_result = (vds.annotate_samples_expr('sa.gqHetStats = gs.filter(g => g.i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:12866,down,downstream,12866,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['down'],['downstream']
Availability,"e_table(100); >>> ht = ht.annotate(arr = hl.range(0, 5).map(lambda _: hl.rand_bool(0.5))). Aggregate to compute the fraction True per element:; >>> ht.aggregate(hl.agg.array_agg(lambda element: hl.agg.fraction(element), ht.arr)) ; [0.54, 0.55, 0.46, 0.52, 0.48]. Notes; This function requires that all values of array have the same length. If; two values have different lengths, then an exception will be thrown.; The f argument should be a function taking one argument, an expression of; the element type of array, and returning an expression including; aggregation(s). The type of the aggregated expression returned by; array_agg() is an array of elements of the return type of f. Parameters:. f – Aggregation function to apply to each element of the exploded array.; array (ArrayExpression) – Array to aggregate. Returns:; ArrayExpression. hail.expr.aggregators.downsample(x, y, label=None, n_divisions=500)[source]; Downsample (x, y) coordinate datapoints. Parameters:. x (NumericExpression) – X-values to be downsampled.; y (NumericExpression) – Y-values to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the distribution of values. In; particular, for any value x = values(i) in the summary, we ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:33282,down,downsampled,33282,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['down'],['downsampled']
Availability,"e`, or :class:`.BlockMatrix`; """""". valid_regions = {'us', 'us-central1', 'europe-west1'}; if region not in valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {valid_clouds}.'; ). datasets = get_datasets_metadata(); names = set([dataset for dataset in datasets]); if name not in names:; raise ValueError(f'{name} is not a dataset available in the' f' repository.'). versions = set(dataset['version'] for dataset in datasets[name]['versions']); if version not in versions:; raise ValueError(; f'Version {version!r} not available for dataset' f' {name!r}.\n' f'Available versions: {versions}.'; ). reference_genomes = set(dataset['reference_genome'] for dataset in datasets[name]['versions']); if reference_genome not in reference_genomes:; raise ValueError(; f'Reference genome build {reference_genome!r} not'; f' available for dataset {name!r}.\n'; f'Available reference genome builds:'; f' {reference_genomes}.'; ). clouds = set(k for dataset in datasets[name]['versions'] for k in dataset['url'].keys()); if cloud not in clouds:; raise ValueError(f'Cloud platform {cloud!r} not available for dataset {name}.\nAvailable platforms: {clouds}.'). regions = set(k for dataset in datasets[name]['versions'] for k in dataset['url'][cloud].keys()); if region not in regions:; raise ValueError(; f'Region {region!r} not available for dataset'; f' {name!r} on cloud platform {cloud!r}.\n'; f'Available regions: {regions}.'; ). path = [; dataset['url'][cloud][region]; for dataset in datasets[name]['versions']; if all([dataset['version'] == version, dataset['reference_genome'] == reference_genome]); ]; assert len(path) == 1; path = path[0]; if path.startswith('s3://'):; try:; dataset = _read_dataset(path); except hl.utils.ja",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:3467,avail,available,3467,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,2,['avail'],['available']
Availability,"eads aren’t where they should be: if we find a genotype called homozygous reference with >10% alternate reads, a genotype called homozygous alternate with >10% reference reads, or a genotype called heterozygote without a ref / alt balance near 1:1, it is likely to be an error.; In a low-depth dataset like 1KG, it is hard to detect bad genotypes using this metric, since a read ratio of 1 alt to 10 reference can easily be explained by binomial sampling. However, in a high-depth dataset, a read ratio of 10:100 is a sure cause for concern!. [32]:. ab = mt.AD[1] / hl.sum(mt.AD). filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))). fraction_filtered = mt.aggregate_entries(hl.agg.fraction(~filter_condition_ab)); print(f'Filtering {fraction_filtered * 100:.2f}% entries out of downstream analysis.'); mt = mt.filter_entries(filter_condition_ab). [Stage 34:> (0 + 1) / 1]. Filtering 3.60% entries out of downstream analysis. [ ]:. Variant QC is a bit more of the same: we can use the variant_qc function to produce a variety of useful statistics, plot them, and filter. [33]:. mt = hl.variant_qc(mt). [34]:. mt.row.describe(). --------------------------------------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; rsid: str,; qual: float64,; filters: set<str>,; info: struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: array<int32>,; MLEAF: array<float64>,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; QD: float64,; ReadPosRankSum: float64,; set: str; },; variant_qc: struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozyg",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:16643,down,downstream,16643,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['down'],['downstream']
Availability,"ect is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn’t be publically available. If you have",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:2625,down,downstream,2625,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['down'],['downstream']
Availability,"ed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:78074,error,error,78074,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"ed bug; in call decoding that was introduced in version 0.2.78. New features. (#10993) New; function p_value_excess_het. Version 0.2.78; Release 2021-10-19. Bug fixes. (#10766) Don’t throw; out of memory error when broadcasting more than 2^(31) - 1 bytes.; (#10910) Filters on; key field won’t be slowed down by uses of; MatrixTable.localize_entries or Table.rename.; (#10959) Don’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile if intervals were; created conditionally.; (#10746) Various; compiler improvements, decrease likelihood of ClassTooLarge; errors.; (#10829) Fix a bug; where hl.missing and CaseBuilder.or_error failed if their; type was a struct containing a field starting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the tabl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:56853,error,errors,56853,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"ed per-pair: if either of; the two individual's individual-specific minor allele frequency is below; the threshold, then the variant's contribution to relatedness estimates; is zero. Under the PC-Relate model, kinship, \[ \phi_{ij} \], ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, \[ k^{(2)}_{ij} \],; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs. - Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation. - ""Third degree relatives"" are those pairs sharing; \[ 2^{-3} = 12.5 % \] of their genetic material, the results of; PCRelate are often too noisy to reliably distinguish these pairs from; higher-degree-relative-pairs or unrelated pairs. The resulting :py:class:`.KeyTable` entries have the type: *{ i: String,; j: String, kin: Double, k2: Double, k1: Double, k0: Double }*. The key; list is: *i: String, j: String*. :param int k: The number of principal components to use to distinguish; ancestries. :param float maf: The minimum individual-specific allele frequency for; an allele used to measure relatedness. :param int block_size: the side length of the blocks of the block-; distributed matrices; this should be set such; that at least three of these matrices fit in; memory (in addition to all other objects; necessary for Spark and Hail). :param float min_kinship: Pairs of samples with kinship lower than; ``min_kinship`` are excluded from the results. :param str statistics: the set of statistics to compute, 'phi' will only; compute the kinship statistic, 'phik2' will; compute the kinship and identity-by-descent two; statistics, 'phik2k0' wi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:176179,reliab,reliably,176179,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['reliab'],['reliably']
Availability,"ed=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project’s private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. Functions. GWAS; To start, we will write a function that creates a new Job on an existing Batch that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the Job that is created in the function, which can be used later to; access the binary PLINK file output and association results in downstream jobs.; def gwas(batch, v",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:5582,avail,available,5582,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['avail'],['available']
Availability,"ed_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) . Notes; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with estimated allele; frequencies \(\widehat{p}_{s}\) at SNP \(s\), is given by:. \[\widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left(1-\widehat{p}_{s}\right)}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals.; PC-Relate slightly modifies the usual estimator for relatedness:; occurrences of population allele frequency are replaced with an; “individual-specific allele frequency”. This modification allows the; method to correctly weight an allele according to an individual’s unique; ancestry profile.; The “individual-specific allele frequency” at a given genetic locus is; modeled by PC-Relate as a linear function of a sample’s first k; principal component coordinates. As such, the efficacy of this method; rests on two assumptions:. an individual’s first k principal component coordinates fully; describe their allele-frequency-relevant ancestry, and; the relationship between ancestry (as descri",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:14015,down,down,14015,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['down'],['down']
Availability,"ed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... cova",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26093,toler,tolerance,26093,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['toler'],['tolerance']
Availability,"edness; identity_by_descent(); king(); pc_relate(); simulate_random_mating(). Miscellaneous; grep(); maximal_independent_set(); rename_duplicates(); segment_intervals(). Import / Export. export_elasticsearch(t, host, port, index, ...); Export a Table to Elasticsearch. export_gen(dataset, output[, precision, gp, ...]); Export a MatrixTable as GEN and SAMPLE files. export_bgen(mt, output[, gp, varid, rsid, ...]); Export MatrixTable as MatrixTable as BGEN 1.2 file with 8 bits of per probability. export_plink(dataset, output[, call, ...]); Export a MatrixTable as PLINK2 BED, BIM and FAM files. export_vcf(dataset, output[, ...]); Export a MatrixTable or Table as a VCF file. get_vcf_metadata(path); Extract metadata from VCF header. import_bed(path[, reference_genome, ...]); Import a UCSC BED file as a Table. import_bgen(path, entry_fields[, ...]); Import BGEN file(s) as a MatrixTable. import_fam(path[, quant_pheno, delimiter, ...]); Import a PLINK FAM file into a Table. import_gen(path[, sample_file, tolerance, ...]); Import GEN file(s) as a MatrixTable. import_locus_intervals(path[, ...]); Import a locus interval list as a Table. import_matrix_table(paths[, row_fields, ...]); Import tab-delimited file(s) as a MatrixTable. import_plink(bed, bim, fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/index.html:2475,toler,tolerance,2475,docs/0.2/methods/index.html,https://hail.is,https://hail.is/docs/0.2/methods/index.html,1,['toler'],['tolerance']
Availability,"een iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to be; associated with their original source file. Bug fixes. (#10182) Fixed; serious memory leak in certain uses of filter_intervals.; (#10133) Fix bug; where some pipelines incorrectly infer missingness, leading to a type; error.; (#10134) Teach; hl.king to treat filtered entries as missing values.; (#10158) Fixes hail; usage in latest versions of jupyter that rely on asyncio.; (#10174) Fixed bad; error message when incorrect return type specified with hl.loop. Version 0.2.63; Released 2021-03-01. (#10105) Hail will; now return frozenset and hail.utils.frozendict instead of; normal sets and dicts. Bug fixes. (#10035) Fix; mishandling of NaN values in hl.agg.hist, where they were; unintentionally included in the first bin.; (#10007) Improve; error message from hadoop_ls when file does not exist. Performance Improvements. (#10068) Make; certain array copies faster.; (#10061) Improve; code generation of hl.if_else and hl.coalesce. Version 0.2.62; Released 2021-02-03. New features. (#9936) Deprecated; hl.null in favor of hl.missing for naming consistency.; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now; eagerly deletes temporary files produced by some BlockMatrix; operations.; (#9835) hl.any; and hl.all now also support a single collection argument and a; varargs of Boolean expressions.; (#9816); hl.pc_relate now includes values on the diagonal of kinship,; IBD-0, IBD-1, and IBD-2; (#9736) Let; NDArrayExpression.reshape take varargs instead of mandating a tuple.; (#9766); hl.export_vcf now warns if INFO field names are invalid according; to the VCF 4.3 spec. Bug fixes. (#9976) Fixed; show() representation of Hail dictionaries. Performance improvements. (#9909) Improved; performa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:61521,error,error,61521,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn’t be publically available. If you have an artifact registry associated with your project, then you can enable the service account to; view Docker images with the command below where SERVICE_ACCOUNT_NAME is your full service account; name, and <REPO> is the name of your repository you want to grant access to and has a path that; has the following prefix us-docker.pkg.dev/<MY_PROJECT>:; gcloud artifacts repositories add-iam-policy-binding <REPO> \; --member=<SERVICE_ACCOUNT_NAME> --role=roles/artifactregistry.repoAdmin. Billing; The cost for executing a job depends on the underlying machine type, the region in which the VM is running in,; and how much CPU and memory is being requested. Currently, Batch runs most jobs on 16 core, spot, n1; machines with 10 GB of persistent SSD boot disk and 375 GB of local SSD. The costs are as follows:. Compute cost. Caution; The prices shown below are approximate prices based on us-central1. Actual prices are; based on the curren",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:3589,avail,available,3589,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['avail'],['available']
Availability,"el_errors'); tm = trio_matrix(dataset, pedigree, complete_trios=True); tm = tm.select_entries(; mendel_code=hl.mendel_error_code(; tm.locus, tm.is_female, tm.father_entry['__GT'], tm.mother_entry['__GT'], tm.proband_entry['__GT']; ); ); ck_name = next(iter(source.col_key)); tm = tm.filter_entries(hl.is_defined(tm.mendel_code)); tm = tm.rename({'id': ck_name}). entries = tm.entries(). table1 = entries.select('fam_id', 'mendel_code'). t2 = tm.annotate_cols(errors=hl.agg.count(), snp_errors=hl.agg.count_where(hl.is_snp(tm.alleles[0], tm.alleles[1]))); table2 = t2.key_cols_by().cols(); table2 = table2.select(; pat_id=table2.father[ck_name],; mat_id=table2.mother[ck_name],; fam_id=table2.fam_id,; errors=table2.errors,; snp_errors=table2.snp_errors,; ); table2 = table2.group_by('pat_id', 'mat_id').aggregate(; fam_id=hl.agg.take(table2.fam_id, 1)[0],; children=hl.int32(hl.agg.count()),; errors=hl.agg.sum(table2.errors),; snp_errors=hl.agg.sum(table2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:11233,error,errors,11233,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"element is used as the hover label. If no label or a single label is provided, then returns :class:`bokeh.plotting.figure`; Otherwise returns a :class:`bokeh.models.layouts.Column` containing:; - a :class:`bokeh.models.widgets.inputs.Select` dropdown selection widget for labels; - a :class:`bokeh.plotting.figure` containing the interactive scatter plot. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space unit",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:31076,down,down,31076,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['down'],['down']
Availability,"eles make it possible to keep the data small to; match its inherent information content. Component tables; The VariantDataset is made up of two component matrix tables – the; reference_data and the variant_data.; The reference_data matrix table is a sparse matrix of reference blocks. The; reference_data matrix table has row key locus, but; does not have an alleles key or field. The column key is the sample ID. The; entries indicate regions of reference calls with similar sequencing metadata; (depth, quality, etc), starting from vds.reference_data.locus.position and; ending at vds.reference_data.END (inclusive!). There is no GT call field; because all calls in the reference data are implicitly homozygous reference (in; the future, a table of ploidy by interval may be included to allow for proper; representation of structural variation, but there is no standard representation; for this at current). A record from a component GVCF is included in the; reference_data if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner).; The variant_data matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the variant_data if; it does not define the END INFO field. This means that some records of the; variant_data can be no-call (./.) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the VariantDataset; Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. Analyses that use prebuilt methods. Some analyses can be supported by using; only the utility functions defined in the hl.vds module, like; vds.sample_qc().; Analyses that use variant data and/or reference data separately. Some; pipelines need to interrog",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/index.html:7008,error,error,7008,docs/0.2/vds/index.html,https://hail.is,https://hail.is/docs/0.2/vds/index.html,1,['error'],['error']
Availability,"eles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; -----. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. :func:`.split_multi_hts` will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic `GT` or `PGT` field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and 0/1. The biallelic alt `AD` entry is just the multiallelic `AD` entry; corresponding to the alternate allele. The ref AD entry is the sum of the; other multiallelic entries. The biallelic `DP` is the same as the multiallelic `DP`. The biallelic `PL` entry for a genotype g is the minimum over `PL` entries; for multiallelic genotypes that downcode to g. For example, the `PL` for (A,; T) at 0/1 is the minimum of the PLs for 0/1 (50) and 1/2 (45), and thus 45. Fixing an alternate allele and biallelic variant, downcoding gives a map; from multiallelic to biallelic alleles and genotypes. The biallelic `AD` entry; for an allele is just the sum of the multiallelic `AD` entries for alleles; that map to that allele. Similarly, the biallelic `PL` entry for a genotype is; the minimum over multiallelic `PL` entries for genotypes that map to that; genotype. `GQ` is recomputed from `PL` if `PL` is provided and is not; missing. If not, it is copied from the original GQ. Here is a second example for a het non-ref. .. code-block:: text. A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as. .. code-block:: text. A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. **VCF Info Fields**. Hail does not split fields in the info field. This means that if a; multiallelic site with `info.AC` value `",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:119593,down,downcode,119593,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['down'],['downcode']
Availability,"ement is used as the hover label. This function returns a :class:`bokeh.models.layouts.Column` containing two :class:`figure.Row`:; - The first row contains the X-axis marginal density and a selection widget if multiple entries are specified in the ``label``; - The second row contains the scatter plot and the y-axis marginal density. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label in the scatter plot.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points in the scatter plot displays their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:37951,down,down,37951,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['down'],['down']
Availability,"en (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement in this set. Code; Dad; Mom; Kid; Copy State. 1; HomVar; HomVar; Het; Auto. 2; HomRef; HomRef; Het; Auto. 3;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:126002,error,error,126002,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,"ence_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An array; containing the alleles of the variant. The reference allele (4th column if; `chromosome` is not defined) is the first element of the array and the; alternate allele (5th column if `chromosome` is not defined) is the second; element.; - `varid` (:py:data:`.tstr`) -- The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; - `rsid` (:py:data:`.tstr`) -- The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. **Entry Fields**. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the highest probability.; - `GP` (:class:`.tarray` of :py:data:`.tfloat64`) -- Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the probabilities is a distance greater than the `tolerance`; parameter from 1.0. Otherwise, the probabilities are normalized to sum to; 1.0. For example, the input ``[0.98, 0.0, 0.0]`` will be normalized to; ``[1.0, 0.0, 0.0]``. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; GEN files to import.; sample_file : :class:`str`; Sample file to import.; tolerance : :obj:`float`; If the sum of the genotype probabilities for a genotype differ from 1.0; by more than the tolerance, set the genotype to missing.; min_partitions : :obj:`int`, optional; Number of partitions.; chromosome : :class:`str`, optional; Chromosome if not included in the GEN file; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding : :obj:`dict` of :class:`str` to :obj:`str`, optional; Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by `reference_genome`.; skip_invalid_loci : :obj:`bool`; If ``True``",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:49605,toler,tolerance,49605,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['toler'],['tolerance']
Availability,"ences usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Miss",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:21406,error,error,21406,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"endel_errors': expected 'call' to be an expression of 'MatrixTable', found {}"".format(; ""expression of '{}'"".format(source.__class__) if source is not None else 'scalar expression'; ); ). source = source.select_entries(__GT=call); dataset = require_biallelic(source, 'mendel_errors'); tm = trio_matrix(dataset, pedigree, complete_trios=True); tm = tm.select_entries(; mendel_code=hl.mendel_error_code(; tm.locus, tm.is_female, tm.father_entry['__GT'], tm.mother_entry['__GT'], tm.proband_entry['__GT']; ); ); ck_name = next(iter(source.col_key)); tm = tm.filter_entries(hl.is_defined(tm.mendel_code)); tm = tm.rename({'id': ck_name}). entries = tm.entries(). table1 = entries.select('fam_id', 'mendel_code'). t2 = tm.annotate_cols(errors=hl.agg.count(), snp_errors=hl.agg.count_where(hl.is_snp(tm.alleles[0], tm.alleles[1]))); table2 = t2.key_cols_by().cols(); table2 = table2.select(; pat_id=table2.father[ck_name],; mat_id=table2.mother[ck_name],; fam_id=table2.fam_id,; errors=table2.errors,; snp_errors=table2.snp_errors,; ); table2 = table2.group_by('pat_id', 'mat_id').aggregate(; fam_id=hl.agg.take(table2.fam_id, 1)[0],; children=hl.int32(hl.agg.count()),; errors=hl.agg.sum(table2.errors),; snp_errors=hl.agg.sum(table2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.st",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:10947,error,errors,10947,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"enome`.; """""". if not 0.0 <= min_match <= 1.0:; raise TypeError(""'liftover' requires 'min_match' is in the range [0, 1]. Got {}"".format(min_match)). if isinstance(x.dtype, tlocus):; rg = x.dtype.reference_genome; method_name = ""liftoverLocus""; rtype = tstruct(result=tlocus(dest_reference_genome), is_negative_strand=tbool); else:; rg = x.dtype.point_type.reference_genome; method_name = ""liftoverLocusInterval""; rtype = tstruct(result=tinterval(tlocus(dest_reference_genome)), is_negative_strand=tbool). if not rg.has_liftover(dest_reference_genome.name):; raise TypeError(; """"""Reference genome '{}' does not have liftover to '{}'.; Use 'add_liftover' to load a liftover chain file."""""".format(rg.name, dest_reference_genome.name); ). expr = _func(method_name, rtype, x, to_expr(min_match, tfloat64)); if not include_strand:; expr = expr.result; return expr. [docs]@typecheck(; f=func_spec(1, expr_float64),; min=expr_float64,; max=expr_float64,; max_iter=builtins.int,; epsilon=builtins.float,; tolerance=builtins.float,; ); def uniroot(f: Callable, min, max, *, max_iter=1000, epsilon=2.2204460492503131e-16, tolerance=1.220703e-4):; """"""Finds a root of the function `f` within the interval `[min, max]`. Examples; --------. >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; -----; `f(min)` and `f(max)` must not have the same sign. If no root can be found, the result of this call will be `NA` (missing). :func:`.uniroot` returns an estimate for a root with accuracy; `4 * epsilon * abs(x) + tolerance`. 4*EPSILON*abs(x) + tol. Parameters; ----------; f : function ( (arg) -> :class:`.Float64Expression`); Must return a :class:`.Float64Expression`.; min : :class:`.Float64Expression`; max : :class:`.Float64Expression`; max_iter : `int`; The maximum number of iterations before giving up.; epsilon : `float`; The scaling factor in the accuracy of the root found.; tolerance : `float`; The constant factor in approximate accuracy of the root found. Returns; -------; :class:`.Float64Express",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:172390,toler,tolerance,172390,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['toler'],['tolerance']
Availability,"enotype(self):; return self._jvds.isGenericGenotype(). @property; @handle_py4j; def sample_ids(self):; """"""Return sampleIDs. :return: List of sample IDs.; :rtype: list of str; """""". if self._sample_ids is None:; self._sample_ids = jiterable_to_list(self._jvds.sampleIds()); return self._sample_ids. @property; @handle_py4j; def sample_annotations(self):; """"""Return a dict of sample annotations. The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. :return: dict; """""". if self._sample_annotations is None:; zipped_annotations = Env.jutils().iterableToArrayList(; self._jvds.sampleIdsAndAnnotations(); ); r = {}; for element in zipped_annotations:; r[element._1()] = self.sample_schema._convert_to_py(element._2()); self._sample_annotations = r; return self._sample_annotations. [docs] @handle_py4j; def num_partitions(self):; """"""Number of partitions. **Notes**. The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. Partitions are a core concept of distributed computation in Spark, see `here <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__ for details. :rtype: int; """""". return self._jvds.nPartitions(). @property; @handle_py4j; def num_samples(self):; """"""Number of samples. :rtype: int; """""". if self._num_samples is None:; self._num_samples = self._jvds.nSamples(); return self._num_samples. [docs] @handle_py4j; def count_variants(self):; """"""Count number of variants in variant dataset. :rtype: long; """""". return self._jvds.countVariants(). [docs] @handle_py4j; def was_split(self):; """"""True if multiallelic variants have been split into multiple biallelic variants. Result is True if :py:meth:`~hail.VariantDataset.split_multi` or :py:meth:`~hail.VariantDataset.filter_multi` has been called on this variant dataset,; or if the variant dataset was imported w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:4858,avail,available,4858,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['avail'],['available']
Availability,"enotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. \[f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}\]; we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)); Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called.; This means that the result of any recursive call within the function must; also be the result of the entire function, without modification. Let’s; consider two different recursive definitions for the triangle function; \(f(x) = 0 + 1 + \dots + x\):; >>> def triangle1(x):; ... if x == 1:; ... return x; ... return x + triangle1(x - 1). >>> def triangle2(x, total):; ... if x == 0:; ... return total; ... return triangle2(x - 1, total + x). The first function definition, triangle1, will call itself and then add x.; This is an example of a non-tail recursive function, since triangle1(9); needs to modify the result of the inner recursive call to triangle1(8) by; adding 9 to the result.; The second function is tail recursive: the result of triangle2(9, 0) is; the same as the result of the inner recursive call, triangle2(8, 9).; Example; To find the sum of all the numbers from n=1…10:; >>> triangle_f = lambda f, x, total: ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:40922,error,error,40922,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['error'],['error']
Availability,"ens of relational object to be annotated. Returns; -------; :class:`tuple`; """"""; gene_field = Env.get_uid(); gencode = self.__by_name['gencode'].index_compatible_version(rel.key); return gene_field, rel.annotate(**{gene_field: gencode.gene_name}). def _check_availability(self, names: Iterable) -> None:; """"""Check if datasets given in `names` are available in the annotation; database instance. Parameters; ----------; names : :obj:`iterable`; Names to check.; """"""; unavailable = [x for x in names if x not in self.__by_name.keys()]; if unavailable:; raise ValueError(f'datasets: {unavailable} not available' f' in the {self.region} region.'). [docs] @typecheck_method(rel=oneof(table_type, matrix_table_type), names=str); def annotate_rows_db(self, rel: Union[Table, MatrixTable], *names: str) -> Union[Table, MatrixTable]:; """"""Add annotations from datasets specified by name to a relational; object. List datasets with :attr:`~.available_datasets`. An interactive query builder is available in the; `Hail Annotation Database documentation; </docs/0.2/annotation_database_ui.html>`_. Examples; --------; Annotate a :class:`.MatrixTable` with ``gnomad_lof_metrics``:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') # doctest: +SKIP. Annotate a :class:`.Table` with ``clinvar_gene_summary``, ``CADD``,; and ``DANN``:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> ht = db.annotate_rows_db(ht, 'clinvar_gene_summary', 'CADD', 'DANN') # doctest: +SKIP. Notes; -----. If a dataset is gene-keyed, the annotation will be a dictionary mapping; from gene name to the annotation value. There will be one entry for each; gene overlapping the given locus. If a dataset does not have unique rows for each key (consider the; ``gencode`` genes, which may overlap; and ``clinvar_variant_summary``,; which contains many overlapping multiple nucleotide variants), then the; result will be an array of annotation values, one ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:14657,avail,available,14657,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['avail'],['available']
Availability,"entries of the mt by column as separate text files. pc_project(call_expr, loadings_expr, af_expr); Projects genotypes onto pre-computed PCs. dplyr-inspired Methods. gather(ht, key, value, *fields); Collapse fields into key-value pairs. separate(ht, field, into, delim); Separate a field into multiple fields by splitting on a delimiter character or position. spread(ht, field, value[, key]); Spread a key-value pair of fields across multiple fields. Functions. hail.experimental.load_dataset(name, version, reference_genome, region='us-central1', cloud='gcp')[source]; Load a genetic dataset from Hail’s repository.; Example; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters:. name (str) – Name of the dataset to load.; version (str, optional) – Version of the named dataset to load (see available versions in; documentation). Possibly None for some datasets.; reference_genome (str, optional) – Reference genome build, 'GRCh37' or 'GRCh38'. Possibly None; for some datasets.; region (str) – Specify region for bucket, 'us', 'us-central1', or 'europe-west1', (default is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'bin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:4345,avail,available,4345,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['avail'],['available']
Availability,"eparator in MatrixTable.make_table.; (#5104) Fixed; optimizer bug related to experimental functionality.; (#5122) Fixed error; constructing Table or MatrixTable objects with fields with; certain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); F",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:102907,error,error,102907,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"epend on s and thus creates an implicit dependency for t on s.; In both the LocalBackend and ServiceBackend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or “sink” that waits for all of the jobs in the scatter to be complete; before executing.; In the example below, we use a for loop to create a job for each one of; ‘Alice’, ‘Bob’, and ‘Dan’ that prints the name of the user programatically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:6386,echo,echo,6386,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability,"er generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Release 2022-10-18. Bug Fixes. (#12305): Fixed a; rare crash reading tables/matrixtables with _intervals. Version 0.2.102; Released 2022-10-06. New Features. (#12218) Missing; values are now supported in primitive columns in Table.to_pandas.; (#12254); Cross-product-style legends for data groups have been replaced with; factored ones (consistent with ggplot2’s implementation) for; hail.ggplot.geom_point, and support has been added for custom; legend group labels.; (#12268); VariantDataset now implements union_rows for combining; datasets with the same samples but disjoint variants. Bug Fixes. (#12278) Fixed bug; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:43649,error,error,43649,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"er in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:3518,echo,echo,3518,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability,"er is applied per-pair: if either of; the two individual’s individual-specific minor allele frequency is below; the threshold, then the variant’s contribution to relatedness estimates; is zero.; Under the PC-Relate model, kinship, [ phi_{ij} ], ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection.; Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, [ k^{(2)}_{ij} ],; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs.; Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation; “Third degree relatives” are those pairs sharing; [ 2^{-3} = 12.5 % ] of their genetic material, the results of; PCRelate are often too noisy to reliably distinguish these pairs from; higher-degree-relative-pairs or unrelated pairs. The resulting KeyTable entries have the type: { i: String,; j: String, kin: Double, k2: Double, k1: Double, k0: Double }. The key; list is: i: String, j: String. Parameters:; k (int) – The number of principal components to use to distinguish; ancestries.; maf (float) – The minimum individual-specific allele frequency for; an allele used to measure relatedness.; block_size (int) – the side length of the blocks of the block-; distributed matrices; this should be set such; that at least three of these matrices fit in; memory (in addition to all other objects; necessary for Spark and Hail).; min_kinship (float) – Pairs of samples with kinship lower than; min_kinship are excluded from the results.; statistics (str) – the set of statistics to compute, ‘phi’ will only; compute the kinship statistic, ‘phik2’ will; compute the kinship and identity-by-descent two; statistics, ‘phik2k0’ will compute the kinship; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:136648,reliab,reliably,136648,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['reliab'],['reliably']
Availability,"er than 50 MB). Examples; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:; >>> with hadoop_open('gs://my-bucket/df.csv', 'w') as f: ; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:; >>> with hadoop_open('gs://my-bucket/notes.txt') as f: ; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:; >>> with hadoop_open('gs://my-bucket/notes.txt', 'w') as f: ; ... f.write('result1: %s\n' % result1); ... f.write('result2: %s\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:; >>> from struct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: ; ... print(unpack('<f', bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). Caution; These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use hadoop_copy(); to move your file to a distributed file system. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hail.utils.hadoop_copy(src, dest)[source]; Copy a file through the Hadoop filesystem API.; Supports distributed file systems like hdfs, gs, and s3.; Examples; Copy a file from Google Cloud Storage to a local file:; >>> hadoop_copy('gs://hail-common",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:5639,error,error,5639,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,1,['error'],['error']
Availability,"er=A,Type=String,Description=""Allele count for high quality genotypes (DP >= 10, GQ >= 20)""; ##FILTER=<ID=HardFilter,Description=""This site fails GATK suggested hard filters."">. Parameters:; ann_path (str) – Path to variant annotation beginning with va.; attributes (dict) – A str-str dict containing the attributes to set. Returns:Annotated dataset with the attribute added to the variant annotation. Return type:VariantDataset. split_multi(propagate_gq=False, keep_star_alleles=False, max_shift=100)[source]¶; Split multiallelic variants. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; >>> vds.split_multi().write('output/split.vds'). Notes; We will explain by example. Consider a hypothetical 3-allelic; variant:; A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi will create two biallelic variants (one for each; alternate allele) at the same position; A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT field is downcoded once for each; alternate allele. A call for an alternate allele maps to 1 in; the biallelic variant corresponding to itself and 0; otherwise. For example, in the example above, 0/2 maps to 0/0; and 0/1. The genotype 1/2 maps to 0/1 and 0/1.; The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the; sum of the other multiallelic entries.; The biallelic DP is the same as the multiallelic DP.; The biallelic PL entry for for a genotype g is the minimum; over PL entries for multiallelic genotypes that downcode to; g. For example, the PL for (A, T) at 0/1 is the minimum of the; PLs for 0/1 (50) and 1/2 (45), and thus 45.; Fixing an alternate allele and biallelic variant, downcoding; gives a map from multiallelic to biallelic alleles and; genotypes. The biallelic AD entry for an allele is just the; sum of the multiallelic AD entries for alleles that map to; that allele. Similarly, the biallelic PL entry for a genotype; is",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:162659,down,downcoded,162659,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['down'],['downcoded']
Availability,"er_entry (tstruct) - Mother entry fields. Parameters:; pedigree (Pedigree). Returns:; MatrixTable. hail.methods.variant_qc(mt, name='variant_qc')[source]; Compute common variant statistics (quality control metrics). Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Examples; >>> dataset_result = hl.variant_qc(dataset). Notes; This method computes variant statistics from the genotype data, returning; a new struct field name with the following metrics based on the fields; present in the entry schema.; If mt contains an entry field DP of type tint32, then the; field dp_stats is computed. If mt contains an entry field GQ of type; tint32, then the field gq_stats is computed. Both dp_stats; and gq_stats are structs with with four fields:. mean (float64) – Mean value.; stdev (float64) – Standard deviation (zero degrees of freedom).; min (int32) – Minimum value.; max (int32) – Maximum value. If the dataset does not contain an entry field GT of type; tcall, then an error is raised. The following fields are always; computed from GT:. AF (array<float64>) – Calculated allele frequency, one element; per allele, including the reference. Sums to one. Equivalent to; AC / AN.; AC (array<int32>) – Calculated allele count, one element per; allele, including the reference. Sums to AN.; AN (int32) – Total number of called alleles.; homozygote_count (array<int32>) – Number of homozygotes per; allele. One element per allele, including the reference.; call_rate (float64) – Fraction of calls neither missing nor filtered.; Equivalent to n_called / count_cols().; n_called (int64) – Number of samples with a defined GT.; n_not_called (int64) – Number of samples with a missing GT.; n_filtered (int64) – Number of filtered entries.; n_het (int64) – Number of heterozygous samples.; n_non_ref (int64) – Number of samples with at least one called; non-reference allele.; het_freq_hwe (float64) – Expected frequency of heterozygous; samples under ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:99331,error,error,99331,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['error']
Availability,"er_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Vari",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:49445,error,error,49445,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['error']
Availability,"erated.; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table. hail.methods.split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False)[source]; Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema.; struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; MatrixTable.annotate_entries().; Examples; >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; This method assumes ds contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi_hts; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; We will explain by example. Consider a hypothetical 3-allelic; variant:; A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi_hts() will create two biallelic variants (one for each; alternate allele) at the same position; A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT or PGT ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:87331,error,errors,87331,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability,"erimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below.; In addition, a search bar is provided if looking for a specific annotation; within our curated collection.; Use the “Copy to Clipboard” button to copy the generated Hail code, and paste; the command into your own Hail script. Search. Database Query; . Copy to Clipboard; . Hail generated code:. db = hl.experimental.DB(region='us-central1', cloud='gcp'); mt = db.annotate_rows_db(mt); . name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/annotation_database_ui.html:1818,avail,available,1818,docs/0.2/annotation_database_ui.html,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html,1,['avail'],['available']
Availability,"ernal tools (such as bcftools and GATK) unless they are explicitly inserted using the append_to_header option.; Hail only exports the contents of va.info to the INFO field. No other annotations besides va.info are exported.; The genotype schema must have the type TGenotype or TStruct. If the type is; TGenotype, then the FORMAT fields will be GT, AD, DP, GQ, and PL (or PP if export_pp is True).; If the type is TStruct, then the exported FORMAT fields will be the names of each field of the Struct.; Each field must have a type of String, Char, Int, Double, or Call. Arrays and Sets are also allowed as long as they are not nested.; For example, a field with type Array[Int] can be exported but not a field with type Array[Array[Int]].; Nested Structs are also not allowed. Caution; If samples or genotypes are filtered after import, the value stored in va.info.AC value may no longer reflect the number of called alternate alleles in the filtered VDS. If the filtered VDS is then exported to VCF, downstream tools may produce erroneous results. The solution is to create new annotations in va.info or overwrite existing annotations. For example, in order to produce an accurate AC field, one can run variant_qc() and copy the va.qc.AC field to va.info.AC:; >>> (vds.filter_genotypes('g.gq >= 20'); ... .variant_qc(); ... .annotate_variants_expr('va.info.AC = va.qc.AC'); ... .export_vcf('output/example.vcf.bgz')). Parameters:; output (str) – Path of .vcf file to write.; append_to_header (str or None) – Path of file to append to VCF header.; export_pp (bool) – If true, export linear-scaled probabilities (Hail’s pp field on genotype) as the VCF PP FORMAT field.; parallel (bool) – If true, return a set of VCF files (one per partition) rather than serially concatenating these files. file_version()[source]¶; File version of variant dataset. Return type:int. filter_alleles(expr, annotation='va = va', subset=True, keep=True, filter_altered_genotypes=False, max_shift=100, keep_star=False)[sourc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:46425,down,downstream,46425,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['down'],['downstream']
Availability,"errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:156025,error,error,156025,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['error'],['error']
Availability,"erval boundaries. Results in a smaller result, but this filter mode; is more computationally expensive to evaluate.; keep : :obj:`bool`; Whether to keep, or filter out (default) rows that fall within any; interval in `intervals`. Returns; -------; :class:`.VariantDataset`; """"""; if split_reference_blocks and not keep:; raise ValueError(""'filter_intervals': cannot use 'split_reference_blocks' with keep=False""); return _parameterized_filter_intervals(; vds, intervals, keep=keep, mode='split_at_boundaries' if split_reference_blocks else 'variants_only'; ). [docs]@typecheck(vds=VariantDataset, filter_changed_loci=bool); def split_multi(vds: 'VariantDataset', *, filter_changed_loci: bool = False) -> 'VariantDataset':; """"""Split the multiallelic variants in a :class:`.VariantDataset`. Parameters; ----------; vds : :class:`.VariantDataset`; Dataset in VariantDataset representation.; filter_changed_loci : :obj:`bool`; If any REF/ALT pair changes locus under :func:`.min_rep`, filter that; variant instead of throwing an error. Returns; -------; :class:`.VariantDataset`; """"""; variant_data = hl.experimental.sparse_split_multi(vds.variant_data, filter_changed_loci=filter_changed_loci); reference_data = vds.reference_data. if 'LGT' in reference_data.entry:; if 'GT' in reference_data.entry:; reference_data = reference_data.drop('LGT'); else:; reference_data = reference_data.transmute_entries(GT=reference_data.LGT). return VariantDataset(reference_data=reference_data, variant_data=variant_data). @typecheck(ref=MatrixTable, intervals=Table); def segment_reference_blocks(ref: 'MatrixTable', intervals: 'Table') -> 'MatrixTable':; """"""Returns a matrix table of reference blocks segmented according to intervals. Loci outside the given intervals are discarded. Reference blocks that start before; but span an interval will appear at the interval start locus. Note; ----; Assumes disjoint intervals which do not span contigs. Requires start-inclusive intervals. Parameters; ----------; ref : :clas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/methods.html:23796,error,error,23796,docs/0.2/_modules/hail/vds/methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html,2,['error'],['error']
Availability,"erview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our discussion forum. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/query_on_batch.html:1808,avail,available,1808,docs/0.2/cloud/query_on_batch.html,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html,1,['avail'],['available']
Availability,"es the block matrix. write_from_entry_expr; Writes a block matrix from a matrix table entry expression. property T; Matrix transpose. Returns:; BlockMatrix. abs()[source]; Element-wise absolute value. Returns:; BlockMatrix. property block_size; Block size. Returns:; int. cache()[source]; Persist this block matrix in memory.; Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; BlockMatrix – Cached block matrix. ceil()[source]; Element-wise ceiling. Returns:; BlockMatrix. checkpoint(path, overwrite=False, force_row_major=False, stage_locally=False)[source]; Checkpoint the block matrix. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before checkpointing.; If False, checkpoint blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static default_block_size()[source]; Default block side length. densify()[source]; Restore all dropped blocks as explicit blocks of zeros. Returns:; BlockMatrix. diagonal()[source]; Extracts diagonal elements as a row vector. Returns:; BlockMatrix. property element_type; The type of the elements. entries(keyed=True)[source]; Returns a table with the indices and value of each block matrix entry.; Examples; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[5, 7], [2, 8]]), 2); >>> entries_table = block_matrix.entries(); >>> entries_table.show(); +-------+-------+----------+; | i | j | entry |; +-------+-------+----------+; | int64 | int64 | float64 |; +-------+-------+----------+; | 0 | 0 | 5.00e+00 |; | 0 | 1 | 7.00e+00 |; | 1 | 0 | 2.00e+00 |; | 1 | 1 | 8.00e+00 |; +-------+-------+----------+. Notes; The re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:11219,checkpoint,checkpoint,11219,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['checkpoint'],['checkpoint']
Availability,"es), 0),; 'top': slopes,; }; plot.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, confidence=(1, 10, 0.01)). return fig, mk_interact; else:; return fig. def _max_entropy_cdf(min_x, max_x, x, y, e):; def compare(x1, y1, x2, y2):; return x1 * y2 - x2 * y1. new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). fx = min_x # fixed x; fy = 0 # fixed y; li = 0 # index of lower slope; ui = 0 # index of upper slope; ldx = x[li] - fx; udx = x[ui] - fx; ldy = y[li + 1] - e - fy; udy = y[ui] + e - fy; j = 1; while ui < len(x) and li < len(x):; if j == len(x):; ub = 1; lb = 1; xj = max_x; else:; ub = y[j] + e; lb = y[j + 1] - e; xj = x[j]; dx = xj - fx; judy = ub - fy; jldy = lb - fy; if compare(ldx, ldy, dx, judy) < 0:; # line must bend down at j; fx = x[li]; fy = y[li + 1] - e; new_y[li] = fy; keep[li] = True; j = li + 1; if j >= len(x):; break; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; j += 1; continue; elif compare(udx, udy, dx, jldy) > 0:; # line must bend up at j; fx = x[ui]; fy = y[ui] + e; new_y[ui] = fy; keep[ui] = True; j = ui + 1; if j >= len(x):; break; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; j += 1; continue; if j >= len(x):; break; if compare(udx, udy, dx, judy) < 0:; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; if compare(ldx, ldy, dx, jldy) > 0:; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; j += 1; return new_y, keep. [docs]def smoothed_pdf(; data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter.; smoothing : float; Degree of smoothing.; legend : str; Label of data on the x-axis.; tit",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:6698,down,down,6698,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['down'],['down']
Availability,"es; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """""". _valid_key_properties: ClassVar = {'gene', 'unique'}; _valid_regions: ClassVar = {'us', 'us-central1', 'europe-west1'}; _valid_clouds: ClassVar = {'gcp', 'aws'}; _valid_combinations: ClassVar = {('us', 'aws'), ('us-central1', 'gcp'), ('europe-west1', 'gcp')}. def __init__(; self,; *,; region: str = 'us-central1',; cloud: str = 'gcp',; url: Optional[str] = None,; config: Optional[dict] = None,; ):; if region not in DB._valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid regions are {DB._valid_regions}.'; ); if cloud not in DB._valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {DB._valid_clouds}.'; ); if (region, cloud) not in DB._valid_combinations:; raise ValueError(; f'The {region!r} region is not available for'; f' the {cloud!r} cloud platform. '; f'Valid region, cloud combinations are'; f' {DB._valid_combinations}.'; ); if config is not None and url is not None:; raise ValueError(; f'Only specify one of the parameters url and' f' config, received: url={url} and config={config}'; ); if config is None:; if url is None:; config = get_datasets_metadata(); else:; session = external_requests_client_session(); response = retry_response_returning_functions(session.get, url); config = response.json(); assert isinstance(config, dict); elif not isinstance(config, dict):; raise ValueError(f'expected a dict mapping dataset names to ' f'configurations, but found {config}'); config = {k: v for k, v in config.items() if 'annotation_db' in v}; self.region = region; self.cloud = cloud; self.url = url; self.config = config; self.__by_name = {; k: Dataset.from_name_and_json(k, v, region, cloud); for k, v in config.items(); if Dataset.from_name_and_json(k, v, region, cloud) is not None",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:11325,avail,available,11325,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['avail'],['available']
Availability,"ession scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#9161) Fix bugs; calculating row_id in hl.import_matrix_table. Version 0.2.50; Released 2020-07-23. Bug fixes. (#9114) CHANGELOG:; Fixed crash when using repeated calls to hl.filter_intervals. New features. (#9101) Add; hl.nd.{concat, hstack, vstack} to concatenate ndarrays.; (#9105) Add; hl.nd.{eye, identity} to create identity matrix ndar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:66846,error,error,66846,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"ession. View page source. DictExpression. class hail.expr.DictExpression[source]; Expression of type tdict.; >>> d = hl.literal({'Alice': 43, 'Bob': 33, 'Charles': 44}). Attributes. dtype; The data type of the expression. Methods. contains; Returns whether a given key is present in the dictionary. get; Returns the value associated with key k or a default value if that key is not present. items; Returns an array of tuples containing key/value pairs in the dictionary. key_set; Returns the set of keys in the dictionary. keys; Returns an array with all keys in the dictionary. map_values; Transform values of the dictionary according to a function. size; Returns the size of the dictionary. values; Returns an array with all values in the dictionary. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Get the value associated with key item.; Examples; >>> hl.eval(d['Alice']); 43. Notes; Raises an error if item is not a key of the dictionary. Use; DictExpression.get() to return missing instead of an error. Parameters:; item (Expression) – Key expression. Returns:; Expression – Value associated with key item. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.DictExpression.html:1657,error,error,1657,docs/0.2/hail.expr.DictExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html,1,['error'],['error']
Availability,"est; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). results = []. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint_batching.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:15018,checkpoint,checkpoint,15018,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability,"eta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:46982,toler,tolerance,46982,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['toler'],['tolerance']
Availability,"ethods work. See also; Table.transmute(), MatrixTable.select_globals(), MatrixTable.annotate_globals(). Parameters:; named_exprs (keyword args of Expression) – Annotation expressions. Returns:; MatrixTable. transmute_rows(**named_exprs)[source]; Similar to MatrixTable.annotate_rows(), but drops referenced fields.; Notes; This method adds new row fields according to named_exprs, and drops; all row fields referenced in those expressions. See; Table.transmute() for full documentation on how transmute; methods work. Note; transmute_rows() will not drop key fields. Note; This method supports aggregation over columns. See also; Table.transmute(), MatrixTable.select_rows(), MatrixTable.annotate_rows(). Parameters:; named_exprs (keyword args of Expression) – Annotation expressions. Returns:; MatrixTable. unfilter_entries()[source]; Unfilters filtered entries, populating fields with missing values. Returns:; MatrixTable. Notes; This method is used in the case that a pipeline downstream of filter_entries(); requires a fully dense (no filtered entries) matrix table.; Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See also; filter_entries(), compute_entry_filter_stats(). union_cols(other, row_join_type='inner', drop_right_row_fields=True)[source]; Take the union of dataset columns. Warning; This method does not preserve the global fields from the other matrix table. Examples; Union the columns of two datasets:; >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; In order to combine two datasets, three requirements must be met:. The row keys must match.; The column key schemas and column schemas must match.; The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match.; This method creates a MatrixTable which contains all columns; from both input datasets. The set of row",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:64784,down,downstream,64784,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['down'],['downstream']
Availability,"eturn (; hl.case(); .when(; nbins > 0,; hl.bind(; lambda bs: hl.case(); .when((bs > 0) & hl.is_finite(bs), result(s, nbins, bs, freq_dict)); .or_error(; ""'hist': start=""; + hl.str(s); + "" end=""; + hl.str(e); + "" bins=""; + hl.str(nbins); + "" requires positive bin size.""; ),; hl.float64(e - s) / nbins,; ),; ); .or_error(hl.literal(""'hist' requires positive 'bins', but bins="") + hl.str(nbins)); ). if _result_from_hist_agg_f is None:; _result_from_hist_agg_f = hl.experimental.define_function(; wrap_errors, hl.tfloat64, hl.tfloat64, hl.tint32, hl.tdict(hl.tint32, hl.tint64); ). return _result_from_hist_agg_f(start, end, bins, freq_dict). [docs]@typecheck(x=expr_float64, y=expr_float64, label=nullable(oneof(expr_str, expr_array(expr_str))), n_divisions=int); def downsample(x, y, label=None, n_divisions=500) -> ArrayExpression:; """"""Downsample (x, y) coordinate datapoints. Parameters; ----------; x : :class:`.NumericExpression`; X-values to be downsampled.; y : :class:`.NumericExpression`; Y-values to be downsampled.; label : :class:`.StringExpression` or :class:`.ArrayExpression`; Additional data for each (x, y) coordinate. Can pass in multiple fields in an :class:`.ArrayExpression`.; n_divisions : :obj:`int`; Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns; -------; :class:`.ArrayExpression`; Expression for downsampled coordinate points (x, y). The element type of the array is; :class:`.ttuple` of :py:data:`.tfloat64`, :py:data:`.tfloat64`, and :class:`.tarray` of :py:data:`.tstr`; """"""; if label is None:; label = hl.missing(hl.tarray(hl.tstr)); elif isinstance(label, StringExpression):; label = hl.array([label]); return _agg_func(; 'Downsample', [x, y, label], tarray(ttuple(tfloat64, tfloat64, tarray(tstr))), init_op_args=[n_divisions]; ). @typecheck(expr=expr_any, n=expr_int32); def _reservoir_sample(expr, n):; return _agg_func('ReservoirSample', [expr], tarray(expr.dtype), [n]). [docs]@typecheck(gp=expr_array",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:44318,down,downsampled,44318,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['down'],['downsampled']
Availability,"evels.find(lambda w: y >= w)); ).aggregate(c=hail.agg.count()); data = grouped_ht.filter(; hail.is_defined(grouped_ht.x); & (grouped_ht.x != str(x_range[1])); & hail.is_defined(grouped_ht.y); & (grouped_ht.y != str(y_range[1])); ); return data. def _collect_scatter_plot_data(; x: Tuple[str, NumericExpression],; y: Tuple[str, NumericExpression],; fields: Optional[Dict[str, Expression]] = None,; n_divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinstance(v, StringExpression) else v for k, v in expressions.items()}. agg_f = x[1]._aggregation_method(); res = agg_f(; hail.agg.downsample(; x[1], y[1], label=list(expressions.values()) if expressions else None, n_divisions=n_divisions; ); ); source_pd = pd.DataFrame([; dict(; **{x[0]: point[0], y[0]: point[1]},; **(dict(zip(expressions, point[2])) if point[2] is not None else {}),; ); for point in res; ]); source_pd = source_pd.astype(numeric_expr, co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:23267,down,downsample,23267,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['down'],['downsample']
Availability,"ew_bash_job() instead.; Methods. command; Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the job’s command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; This method can be called more than once. It’s behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command.; To declare a resource file of type JobResourceFile, use either; the get attribute syntax of job.{identifier} or the get item syntax of; job[‘identifier’]. If an object for that identifier doesn’t exist,; then one will be created automatically (only allowed in the; command() method). The identifier name can be any valid Python; identifier such as ofile5000.; All JobResourceFile are temporary files and must be written to; a permanent location using Batch.write_output() if the output; needs to be saved.; Only resources can be referred to in commands. Referencing a; batch.Batch or Job will result in an error. Parameters:; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:1957,echo,echo,1957,docs/batch/api/batch/hailtop.batch.job.BashJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html,2,['echo'],['echo']
Availability,"ew_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: ``use_exome_default_intervals=True`` and; ``use_genome_default_intervals=True``. The combiner serializes itself to `save_path` so that it can be restarted after failure. Parameters; ----------; save_path : :class:`str`; The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path : :class:`str`; The location to store the new VariantDataset.; temp_path : :class:`str`; The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome : :class:`.ReferenceGenome`; The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor : :class:`int`; The number of Variant Datasets to combine at once.; target_records : :class:`int`; The target number of var",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:3597,avail,available,3597,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['avail'],['available']
Availability,"example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC; 3; 4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:81322,fault,fault,81322,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['fault'],['fault']
Availability,"expr (str) – Sample aggregation expression (per key).; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of linear regression key table and sample aggregation key table. Return type:(KeyTable, KeyTable). linreg_multi_pheno(ys, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0)[source]¶; Test each variant for association with multiple phenotypes using linear regression.; This method runs linear regression for multiple phenotypes more efficiently; than looping over linreg(). Warning; linreg_multi_pheno() uses the same set of samples for each phenotype,; namely the set of samples for which all phenotypes and covariates are defined. Annotations; With the default root, the following four variant annotations are added.; The indexing of these annotations corresponds to that of y. va.linreg.beta (Array[Double]) – array of fit genotype coefficients, \(\hat\beta_1\); va.linreg.se (Array[Double]) – array of estimated standard errors, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Array[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Array[Double]) – array of \(p\)-values. Parameters:; ys – list of one or more response expressions.; covariates (list of str) – list of covariate expressions.; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosage genotypes rather than hard call genotypes.; min_ac (int) – Minimum alternate allele count.; min_af (float) – Minimum alternate allele frequency. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. lmmreg(kinshipMatrix, y, covariates=[], global_root='global.lmmreg', va_root='va.lmmreg', run_assoc=True, use_ml=False, delta=None, sparsity_threshold=1.0, use_dosages=False, n_eigs=None, dropped_variance_fraction=None)[source]¶; Use a kinship-based linear mixed model to estimate the genetic component of phenotypi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:90673,error,errors,90673,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,"expr_any); def default(self, then):; """"""Finish the switch statement by adding a default case. Notes; -----; If no value from a :meth:`~.SwitchBuilder.when` call is matched, then; `then` is returned. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0 and self._when_missing_case is None:; return then; self._unify_type(then.dtype); return self._finish(then). [docs] def or_missing(self):; """"""Finish the switch statement by returning missing. Notes; -----; If no value from a :meth:`~.SwitchBuilder.when` call is matched, then; the result is missing. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_missing' cannot be called without at least one 'when' call""); from hail.expr.functions import missing. return self._finish(missing(self._ret_type)). [docs] @typecheck_method(message=expr_str); def or_error(self, message):; """"""Finish the switch statement by throwing an error with the given message. Notes; -----; If no value from a :meth:`.SwitchBuilder.when` call is matched, then an; error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_error' cannot be called without at least one 'when' call""); error_expr = construct_expr(ir.Die(message._ir, self._ret_type), self._ret_type); return self._finish(error_expr). [docs]class CaseBuilder(ConditionalBuilder):; """"""Class for chaining multiple if-else statements. Examples; --------. >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; -----; All expressions appearing as the `then` parameters to; :meth:`~hail.expr.builders.CaseBuilder.when` or; :meth:`~hail.expr.builders.CaseBui",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/builders.html:4849,error,error,4849,docs/0.2/_modules/hail/expr/builders.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/builders.html,2,['error'],['error']
Availability,"expr_or_else(pat_id, '0'),; 'mat_id': expr_or_else(mat_id, '0'),; 'is_female': expr_or_else(is_female, '0', lambda x: hl.if_else(x, '2', '1')),; 'pheno': expr_or_else(pheno, 'NA', lambda x: hl.if_else(x, '2', '1') if x.dtype == tbool else hl.str(x)),; }. locus = dataset.locus; a = dataset.alleles. bim_exprs = {; 'varid': expr_or_else(varid, hl.delimit([locus.contig, hl.str(locus.position), a[0], a[1]], ':')),; 'cm_position': expr_or_else(cm_position, 0.0),; }. for exprs, axis in [; (fam_exprs, dataset._col_indices),; (bim_exprs, dataset._row_indices),; (entry_exprs, dataset._entry_indices),; ]:; for name, expr in exprs.items():; analyze('export_plink/{}'.format(name), expr, axis). dataset = dataset._select_all(col_exprs=fam_exprs, col_key=[], row_exprs=bim_exprs, entry_exprs=entry_exprs). # check FAM ids for white space; t_cols = dataset.cols(); errors = []; for name in ['ind_id', 'fam_id', 'pat_id', 'mat_id']:; ids = t_cols.filter(t_cols[name].matches(r""\s+""))[name].collect(). if ids:; errors.append(f""""""expr '{name}' has spaces in the following values:\n""""""); for row in ids:; errors.append(f"""""" {row}\n""""""). if errors:; raise TypeError(""\n"".join(errors)). writer = ir.MatrixPLINKWriter(output); Env.backend().execute(ir.MatrixWrite(dataset._mir, writer)). [docs]@typecheck(; dataset=oneof(MatrixTable, Table),; output=str,; append_to_header=nullable(str),; parallel=nullable(ir.ExportType.checker),; metadata=nullable(dictof(str, dictof(str, dictof(str, str)))),; tabix=bool,; ); def export_vcf(dataset, output, append_to_header=None, parallel=None, metadata=None, *, tabix=False):; """"""Export a :class:`.MatrixTable` or :class:`.Table` as a VCF file. .. include:: ../_templates/req_tvariant.rst. Examples; --------; Export to VCF as a block-compressed file:. >>> hl.export_vcf(dataset, 'output/example.vcf.bgz'). Notes; -----; :func:`.export_vcf` writes the dataset to disk in VCF format as described in the; `VCF 4.2 spec <https://samtools.github.io/hts-specs/VCFv4.2.pdf>`__. Use t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:15536,error,errors,15536,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['error'],['errors']
Availability,"expressions. Return type:(annotation or list of annotation, Type or list of Type). rename_samples(mapping)[source]¶; Rename samples.; Examples; >>> vds_result = vds.rename_samples({'ID1': 'id1', 'ID2': 'id2'}). Use a file with an “old_id” and “new_id” column to rename samples:; >>> mapping_table = hc.import_table('data/sample_mapping.txt'); >>> mapping_dict = {row.old_id: row.new_id for row in mapping_table.collect()}; >>> vds_result = vds.rename_samples(mapping_dict). Parameters:mapping (dict) – Mapping from old to new sample IDs. Returns:Dataset with remapped sample IDs. Return type:VariantDataset. repartition(num_partitions, shuffle=True)[source]¶; Increase or decrease the number of variant dataset partitions.; Examples; Repartition the variant dataset to have 500 partitions:; >>> vds_result = vds.repartition(500). Notes; Check the current number of partitions with num_partitions().; The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. When a variant dataset with \(M\) variants is first imported, each of the \(k\) partition will contain about \(M/k\) of the variants. Since each partition has some computational overhead, decreasing the number of partitions can improve performance after significant filtering. Since it’s recommended to have at least 2 - 4 partitions per core, increasing the number of partitions can allow one to take advantage of more cores.; Partitions are a core concept of distributed computation in Spark, see here for details. With shuffle=True, Hail does a full shuffle of the data and creates equal sized partitions. With shuffle=False, Hail combines existing partitions to avoid a full shuffle. These algorithms correspond to the repartition and coalesce commands in Spark, respectively. In particular, when shuffle=False, num_partitions cannot exceed current number of partitions. Parameters:; num_parti",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:151939,avail,available,151939,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['avail'],['available']
Availability,"f k and scores_expr is not None:; raise ValueError(""pc_relate_bm: exactly one of 'k' and 'scores_expr' "" ""must be set, found both""); else:; raise ValueError(""pc_relate_bm: exactly one of 'k' and 'scores_expr' "" ""must be set, found neither""). n_missing = scores_table.aggregate(agg.count_where(hl.is_missing(scores_table.__scores))); if n_missing > 0:; raise ValueError(f'Found {n_missing} columns with missing scores array.'); pc_scores = hl.nd.array(scores_table.collect(_localize=False).map(lambda x: x.__scores)). # Define NaN for missing values, otherwise cannot convert expr to block matrix; nan = hl.float64(float('NaN')). # Create genotype matrix, set missing GT entries to NaN; mt = mt.select_entries(__gt=call_expr.n_alt_alleles()).unfilter_entries(); gt_with_nan_expr = hl.or_else(hl.float64(mt.__gt), nan); if not block_size:; block_size = BlockMatrix.default_block_size(); g = BlockMatrix.from_entry_expr(gt_with_nan_expr, block_size=block_size); g = g.checkpoint(new_temp_file('pc_relate_bm/g', 'bm')); sqrt_n_samples = hl.nd.array([hl.sqrt(g.shape[1])]). # Recover singular values, S0, as vector of column norms of pc_scores if necessary; if compute_S0:; S0 = (pc_scores ** hl.int32(2)).sum(0).map(lambda x: hl.sqrt(x)); else:; S0 = hl.nd.array(eigens).map(lambda x: hl.sqrt(x)); # Set first entry of S to sqrt(n), for intercept term in beta; S = hl.nd.hstack((sqrt_n_samples, S0))._persist(); # Recover V from pc_scores with inv(S0); V0 = (pc_scores * (1 / S0))._persist(); # Set all entries in first column of V to 1/sqrt(n), for intercept term in beta; ones_normalized = hl.nd.full((V0.shape[0], 1), (1 / S[0])); V = hl.nd.hstack((ones_normalized, V0)). # Compute matrix of regression coefficients for PCs (beta), shape (k, m); beta = BlockMatrix.from_ndarray(((1 / S) * V).T, block_size=block_size) @ g.T; beta = beta.checkpoint(new_temp_file('pc_relate_bm/beta', 'bm')). # Compute matrix of individual-specific AF estimates (mu), shape (m, n); mu = 0.5 * (BlockMatrix.from_ndarray(V",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:19064,checkpoint,checkpoint,19064,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['checkpoint'],['checkpoint']
Availability,"f variants in a different order than PLINK. Be sure to provide enough disk space per worker because :py:meth:`.ld_prune` `persists <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ up to 3 copies of the data to both memory and disk.; The amount of disk space required will depend on the size and minor allele frequency of the input data and the prune parameters ``r2`` and ``window``. The number of bytes stored in memory per variant is about ``nSamples / 4 + 50``. .. warning::. The variants in the pruned set are not guaranteed to be identical each time :py:meth:`.ld_prune` is run. We recommend running :py:meth:`.ld_prune` once and exporting the list of LD pruned variants using; :py:meth:`.export_variants` for future use. :param float r2: Maximum :math:`R^2` threshold between two variants in the pruned set within a given window. :param int window: Width of window in base-pairs for computing pair-wise :math:`R^2` values. :param int memory_per_core: Total amount of memory available for each core in MB. If unsure, use the default value. :param int num_cores: The number of cores available. Equivalent to the total number of workers times the number of cores per worker. :return: Variant dataset filtered to those variants which remain after LD pruning.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.ldPrune(r2, window, num_cores, memory_per_core); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(force_local=bool); def ld_matrix(self, force_local=False):; """"""Computes the linkage disequilibrium (correlation) matrix for the variants in this VDS. **Examples**. >>> ld_mat = vds.ld_matrix(). **Notes**. Each entry (i, j) in the LD matrix gives the :math:`r` value between variants i and j, defined as; `Pearson's correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; :math:`\\rho_{x_i,x_j}` between the two genotype vectors :math:`x_i` and :math:`x_j`. .. math::",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:95310,avail,available,95310,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['avail'],['available']
Availability,"f; that the filtered alleles are not real so we should discard any; probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g., filtering alleles 1 and 2 transforms ``25,5,10,20`` to ``25,20``.; - DP: No change.; - PL: The filtered alleles' columns are eliminated and the remaining columns shifted so the minimum value is 0.; - GQ: The second-lowest PL (after shifting). **Downcode algorithm**. The downcode algorithm (``subset=False``) recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where downcodeing filtered alleles merges distinct genotypes, the minimum PL is used (since PL is on a log scale, this roughly corresponds to adding probabilities). The PLs; are then re-normalized (shifted) so that the most likely genotype has a PL of 0, and GT is set to this genotype.; If an allele is filtered, this algorithm acts similarly to :py:meth:`~hail.VariantDataset.split_multi`. The downcoding algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: The filtered alleles' columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded genotype, and shift so the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Expression Variables**. The following symbols are in scope for ``expr``:. - ``v`` (*Variant*): :ref:`variant`; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:63411,down,downcodeing,63411,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['down'],['downcodeing']
Availability,"failure = hl.missing(hl.tstruct(p_de_novo=hl.tfloat64, confidence=hl.tstr)). kid = tm.proband_entry; dad = tm.father_entry; mom = tm.mother_entry. kid_linear_pl = 10 ** (-kid.PL / 10); kid_pp = hl.bind(lambda x: x / hl.sum(x), kid_linear_pl). dad_linear_pl = 10 ** (-dad.PL / 10); dad_pp = hl.bind(lambda x: x / hl.sum(x), dad_linear_pl). mom_linear_pl = 10 ** (-mom.PL / 10); mom_pp = hl.bind(lambda x: x / hl.sum(x), mom_linear_pl). kid_ad_ratio = kid.AD[1] / hl.sum(kid.AD); dp_ratio = kid.DP / (dad.DP + mom.DP). def call_auto(kid_pp, dad_pp, mom_pp, kid_ad_ratio):; p_data_given_dn = dad_pp[0] * mom_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (dad_pp[1] * mom_pp[0] + dad_pp[0] * mom_pp[1]) * kid_pp[1] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (dad.DP + mom.DP) < min_dp_ratio) | ~(kid_ad_ratio >= min_child_ab), failure); .when((hl.sum(mom.AD) == 0) | (hl.sum(dad.AD) == 0), failure); .when(; (mom.AD[1] / hl.sum(mom.AD) > max_parent_ab) | (dad.AD[1] / hl.sum(dad.AD) > max_parent_ab), failure; ); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:30308,failure,failure,30308,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['failure'],['failure']
Availability,"failure_prob) / (2 * s)). def compute_grid_size(s):; return hl.fold(lambda p, i: update_grid_size(p, s), 1 / failure_prob, hl.range(0, 5)). def compute_single_error(s, failure_prob=failure_prob):; return hl.sqrt(hl.log(2 / failure_prob) * s / 2). def compute_global_error(s):; return hl.rbind(compute_grid_size(s), lambda p: 1 / p + compute_single_error(s, failure_prob / p)). if all_quantiles:; return hl.rbind(cdf, lambda cdf: hl.rbind(compute_sum(cdf), compute_global_error)); else:; return hl.rbind(cdf, lambda cdf: hl.rbind(compute_sum(cdf), compute_single_error)). def _error_from_cdf_python(cdf, failure_prob, all_quantiles=False):; """"""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :obj:`dict`; Result of :func:`.approx_cdf` aggregator, evaluated to a python dict; failure_prob: :obj:`float`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :obj:`float`; Upper bound on error of quantile estimates.; """"""; import math. s = 0; for i in builtins.range(builtins.len(cdf._compaction_counts)):; s += cdf._compaction_counts[i] << (2 * i); s = s / (cdf.ranks[-1] ** 2). def update_grid_size(p):; return 4 * math.sqrt(math.log(2 * p / failure_prob) / (2 * s)). def compute_grid_size(s):; p = 1 / failure_prob; for _ in builtins.range(5):; p = update_grid_size(p); return p. def compute_single_error(s, failure_prob=failure_prob):; return math.sqrt(math.log(2 / failure_prob) * s / 2). if s == 0:; # no compactions ergo no error; return 0; elif all_quantiles:; p = compute_grid_size(s); return 1 / p + compute_single_error(s, failure_prob / p); else:; return compute_single_error(s, failure_prob). [docs]@typecheck(t=hail_type); def missing(t: Union[HailType, str]):; """"""Creates an expression representing a missing value of a specified type. Examples; --------. >",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:6928,error,error,6928,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['error'],['error']
Availability,"fig is not None and url is not None:; raise ValueError(; f'Only specify one of the parameters url and' f' config, received: url={url} and config={config}'; ); if config is None:; if url is None:; config = get_datasets_metadata(); else:; session = external_requests_client_session(); response = retry_response_returning_functions(session.get, url); config = response.json(); assert isinstance(config, dict); elif not isinstance(config, dict):; raise ValueError(f'expected a dict mapping dataset names to ' f'configurations, but found {config}'); config = {k: v for k, v in config.items() if 'annotation_db' in v}; self.region = region; self.cloud = cloud; self.url = url; self.config = config; self.__by_name = {; k: Dataset.from_name_and_json(k, v, region, cloud); for k, v in config.items(); if Dataset.from_name_and_json(k, v, region, cloud) is not None; }. @property; def available_datasets(self) -> List[str]:; """"""List of names of available annotation datasets. Returns; -------; :obj:`list`; List of available annotation datasets.; """"""; return sorted(self.__by_name.keys()). @staticmethod; def _row_lens(rel: Union[Table, MatrixTable]) -> Union[TableRows, MatrixRows]:; """"""Get row lens from relational object. Parameters; ----------; rel : :class:`Table` or :class:`MatrixTable`. Returns; -------; :class:`TableRows` or :class:`MatrixRows`; """"""; if isinstance(rel, MatrixTable):; return MatrixRows(rel); elif isinstance(rel, Table):; return TableRows(rel); else:; raise ValueError('annotation database can only annotate Hail' ' MatrixTable or Table'). def _dataset_by_name(self, name: str) -> Dataset:; """"""Retrieve :class:`Dataset` object by name. Parameters; ----------; name : :obj:`str`; Name of dataset. Returns; -------; :class:`Dataset`; """"""; if name not in self.__by_name:; raise ValueError(; f'{name} not found in annotation database,'; f' you may list all known dataset names'; f' with available_datasets'; ); return self.__by_name[name]. def _annotate_gene_name(self, rel: Union[TableR",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:12463,avail,available,12463,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['avail'],['available']
Availability,"files:. - Since genotype probabilities are understood to define a probability distribution, :py:meth:`~hail.HailContext.import_bgen` automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the ``tolerance`` parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains. - :py:meth:`~hail.HailContext.import_bgen` normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. **Annotations**. :py:meth:`~hail.HailContext.import_bgen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .bgen files to import.; :type path: str or list of str. :param float tolerance: If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1. :param sample_file: Sample file.; :type sample_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :return: Variant dataset imported from .bgen file.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importBgens(jindexed_seq_args(path), joption(sample_file),; tolerance, joption(min_partitions)); return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; sample_file=nullable(strlike),; tolerance=numeric,; min_partitions=nullable(integral),; chromosome=nullable(strlike)); def import_gen(self, path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None):; """"""Import .gen file(s) as variant dataset. **Examples**. Read a .gen file and a .sample file and write to a .vds file:. >>> (hc.import_gen('data/example.gen', sample_fil",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:7576,toler,tolerance,7576,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['toler'],['tolerance']
Availability,"filter_rows() to remove; them. Examples; Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sam",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:49138,error,errors,49138,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability,"first_mismatch]:; first_mismatch += 1; error(; f""mismatch in columns keys: ref={ref_cols[first_mismatch]}, var={var_cols[first_mismatch]} at position {first_mismatch}""; ). # check locus distinctness; n_rd_rows = rd.count_rows(); n_distinct = rd.distinct_by_row().count_rows(). if n_distinct != n_rd_rows:; error(f'reference data loci are not distinct: found {n_rd_rows} rows, but {n_distinct} distinct loci'). # check END field; end_exprs = dict(; missing_end=hl.agg.filter(hl.is_missing(rd.END), hl.agg.take((rd.row_key, rd.col_key), 5)),; end_before_position=hl.agg.filter(rd.END < rd.locus.position, hl.agg.take((rd.row_key, rd.col_key), 5)),; ); if VariantDataset.ref_block_max_length_field in rd.globals:; rbml = rd[VariantDataset.ref_block_max_length_field]; end_exprs['blocks_too_long'] = hl.agg.filter(; rd.END - rd.locus.position + 1 > rbml, hl.agg.take((rd.row_key, rd.col_key), 5); ). res = rd.aggregate_entries(hl.struct(**end_exprs)). if res.missing_end:; error(; 'found records in reference data with missing END field\n '; + '\n '.join(str(x) for x in res.missing_end); ); if res.end_before_position:; error(; 'found records in reference data with END before locus position\n '; + '\n '.join(str(x) for x in res.end_before_position); ); blocks_too_long = res.get('blocks_too_long', []); if blocks_too_long:; error(; 'found records in reference data with blocks larger than `ref_block_max_length`\n '; + '\n '.join(str(x) for x in blocks_too_long); ). def _same(self, other: 'VariantDataset'):; return self.reference_data._same(other.reference_data) and self.variant_data._same(other.variant_data). [docs] def union_rows(*vdses):; """"""Combine many VDSes with the same samples but disjoint variants. **Examples**. If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:. >>> vds_paths = ['chr1.vds', 'chr2.vds'] # doctest: +SKIP; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) # doctest: +SKIP; ... hl.vds.VariantDataset.unio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:11021,error,error,11021,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['error'],['error']
Availability,"fix means; the value is in bytes.; For the ServiceBackend, the values ‘lowmem’, ‘standard’,; and ‘highmem’ are also valid arguments. ‘lowmem’ corresponds to; approximately 1 Gi/core, ‘standard’ corresponds to approximately; 4 Gi/core, and ‘highmem’ corresponds to approximately 7 Gi/core.; The default value is ‘standard’. Parameters:; memory (Union[str, int, None]) – Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (‘standard’). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in ‘us-central1’:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]]) – The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters:; is_spot (bool) – If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:7030,echo,echo,7030,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['echo'],['echo']
Availability,"fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with an actual file path into the command for s.; We use another f-string in t’s command where we print the contents of s.ofile to stdout.; s.ofile is the same temporary file that was created in the command for t. Therefore,; Batch deduces that t must depend on s and thus creates an implicit dependency for t on s.; In both the LocalBackend and ServiceBackend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or “sink” that waits for all of the jobs in the scatter to be complete; before executing.; In the example below, we use a for loop to create a job for each one of; ‘Alice’, ‘Bob’, and ‘Dan’ that prints the name of the user programatically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:5618,echo,echo,5618,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability,"following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; https://docs.docker.com/engine/reference/builder/. Building Images; To create a Docker image, use; docker build -t us-docker.pkg.dev/<my-project>/<my-image>:<tag> -f Dockerfile . * `<dir>` is the context directory, `.` means the current w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/docker_resources.html:1893,down,download,1893,docs/batch/docker_resources.html,https://hail.is,https://hail.is/docs/batch/docker_resources.html,1,['down'],['download']
Availability,"foo and {root}.bar where {root} is a random; identifier.; Notes; The identifier is used to refer to a specific resource file. For example,; given the resource group rg, you can use the attribute notation; rg.identifier or the get item notation rg[identifier].; The file extensions for each file are derived from the identifier. This; is equivalent to “{root}.identifier” from; BashJob.declare_resource_group(). We are planning on adding; flexibility to incorporate more complicated extensions in the future; such as .vcf.bgz. For now, use JobResourceFile.add_extension(); to add an extension to a resource file. Parameters:; kwargs (Union[str, PathLike]) – Key word arguments where the name/key is the identifier and the value; is the file path. Return type:; ResourceGroup. run(dry_run=False, verbose=False, delete_scratch_on_exit=True, **backend_kwargs); Execute a batch.; Examples; Create a simple batch with one job and execute it:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command('echo ""hello world""'); >>> b.run(). Parameters:. dry_run (bool) – If True, don’t execute code.; verbose (bool) – If True, print debugging output.; delete_scratch_on_exit (bool) – If True, delete temporary directories with intermediate files.; backend_kwargs (Any) – See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str) – Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:10504,echo,echo,10504,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['echo'],['echo']
Availability,"four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. E",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:5548,error,error,5548,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['error']
Availability,"from hail.typecheck import setof, typecheck. from ...ir import MakeTuple; from ..expressions import Expression, ExpressionException, expr_any; from .indices import Aggregation, Indices. @typecheck(caller=str, expr=Expression, expected_indices=Indices, aggregation_axes=setof(str), broadcast=bool); def analyze(caller: str, expr: Expression, expected_indices: Indices, aggregation_axes: Set = set(), broadcast=True):; from hail.utils import error, warning. indices = expr._indices; source = indices.source; axes = indices.axes; aggregations = expr._aggregations. warnings = []; errors = []. expected_source = expected_indices.source; expected_axes = expected_indices.axes. if source is not None and source is not expected_source:; bad_refs = []; for name, inds in get_refs(expr).items():; if inds.source is not expected_source:; bad_refs.append(name); errors.append(; ExpressionException(; ""'{caller}': source mismatch\n""; "" Expected an expression from source {expected}\n""; "" Found expression derived from source {actual}\n""; "" Problematic field(s): {bad_refs}\n\n""; "" This error is commonly caused by chaining methods together:\n""; "" >>> ht.distinct().select(ht.x)\n\n""; "" Correct usage:\n""; "" >>> ht = ht.distinct()\n""; "" >>> ht = ht.select(ht.x)"".format(; caller=caller, expected=expected_source, actual=source, bad_refs=list(bad_refs); ); ); ). # check for stray indices by subtracting expected axes from observed; if broadcast:; unexpected_axes = axes - expected_axes; strictness = ''; else:; unexpected_axes = axes if axes != expected_axes else set(); strictness = 'strictly '. if unexpected_axes:; # one or more out-of-scope fields; refs = get_refs(expr); bad_refs = []; for name, inds in refs.items():; if broadcast:; bad_axes = inds.axes.intersection(unexpected_axes); if bad_axes:; bad_refs.append((name, inds)); elif inds.axes != expected_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0; errors.append(; ExpressionException(; ""scope violation: '{caller}' expects an expressi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:1647,error,error,1647,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html,2,['error'],['error']
Availability,"from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:52933,error,error,52933,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"g the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; :func:`.split_multi_hts`. The downcode algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded genotype, and shift so; the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Subset algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The subset algorithm subsets the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most l",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:160166,Down,Downcode,160166,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['Down'],['Downcode']
Availability,"g values are mean-imputed within variant.; The method produces a symmetric block-sparse matrix supported in a; neighborhood of the diagonal. If variants \(i\) and \(j\) are on the; same contig and within radius base pairs (inclusive) then the; \((i, j)\) element is their; Pearson correlation coefficient.; Otherwise, the \((i, j)\) element is 0.0.; Rows with a constant value (i.e., zero variance) will result in nan; correlation values. To avoid this, first check that all variants vary or; filter out constant variants (for example, with the help of; aggregators.stats()).; If the global_position() on locus_expr is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that’s; been ordered by locus_expr.; Set coord_expr to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-nan, on the; same source as locus_expr, and ascending with respect to locus; position for each contig; otherwise the method will raise an error. Warning; See the warnings in row_correlation(). In particular, for large; matrices it may be preferable to run its stages separately.; entry_expr and locus_expr are implicitly aligned by row-index, though; they need not be on the same source. If their sources differ in the number; of rows, an error will be raised; otherwise, unintended misalignment may; silently produce unexpected results. Parameters:. entry_expr (Float64Expression) – Entry-indexed numeric expression on matrix table.; locus_expr (LocusExpression) – Row-indexed locus expression on a table or matrix table that is; row-aligned with the matrix table of entry_expr.; radius (int or float) – Radius of window for row values.; coord_expr (Float64Expression, optional) – Row-indexed numeric expression for the row value on the same table or; matrix table as locus_expr.; By default, the row value is given by the locus position.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:41620,error,error,41620,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['error']
Availability,"g we find 4 or 5 iterations; nearly always suffice. Convergence may also fail due to explosion,; which refers to low-level numerical linear algebra exceptions caused by; manipulating ill-conditioned matrices. Explosion may result from (nearly); linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-com",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:52891,error,errors,52891,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['error'],['errors']
Availability,"g we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation_. .. _separation: https://en.wikipedia.org/wiki/Separation_(statistics). A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of :math:`\beta` under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where ``x`` is genotype,; ``y`` is phenotype, and ``logistf`` is from the logistf package:. .. code-block:: R. x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-com",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:32435,error,errors,32435,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['error'],['errors']
Availability,"g; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop* methods now properly support creation and; modification time.; (#12566) Improve; error message when combining incompatibly indexed fields in certain; operations including array indexing. Version 0.2.108; Released 2023-1-12. New Features. (#12576); hl.import_bgen and hl.export_bgen now support compression; with Zstd. Bug fixes. (#12585); hail.ggplots that have more than one legend group or facet are; now interactive. If such a plot has enough legend entries that the; legend would be taller than the plot, the legend will now be; scrollable. Legend entries for such plots can be clicked to show/hide; traces on the plot, but this does not work and is a known issue that; will only be addressed if hail.ggplot is migrated off of plotly.; (#12584) Fixed bug; which arose as an assertion error about type mismatches. This was; usually triggered when working with tuples.; (#12583) Fixed bug; which showed an empty table for ht.col_key.show().; (#12582) Fixed bug; where matrix tables with duplicate col keys do not show properly.; Also fixed bug where tables and matrix tables with HTML unsafe column; headers are rendere",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:39787,error,error,39787,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"gate over rows into a local value. Examples; --------; Aggregate over rows:. >>> table1.aggregate(hl.struct(fraction_male=hl.agg.fraction(table1.SEX == 'M'),; ... mean_x=hl.agg.mean(table1.X))); Struct(fraction_male=0.5, mean_x=6.5). Note; ----; This method supports (and expects!) aggregation over rows. Parameters; ----------; expr : :class:`.Expression`; Aggregation expression. Returns; -------; any; Aggregated value dependent on `expr`.; """"""; expr = to_expr(expr); base, _ = self._process_joins(expr); analyze('Table.aggregate', expr, self._global_indices, {self._row_axis}). agg_ir = ir.TableAggregate(base._tir, expr._ir). if _localize:; return Env.backend().execute(hl.ir.MakeTuple([agg_ir]))[0]. return construct_expr(ir.LiftMeOut(agg_ir), expr.dtype). [docs] @typecheck_method(; output=str,; overwrite=bool,; stage_locally=bool,; _codec_spec=nullable(str),; _read_if_exists=bool,; _intervals=nullable(sequenceof(anytype)),; _filter_intervals=bool,; ); def checkpoint(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; ) -> 'Table':; """"""Checkpoint the table to disk by writing and reading. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`Table`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_table`. It is; possible to read the file at this path later with :func:`.read_table`. Examples; --------; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). """"""; hl.current_backend().validate_file(output). if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; self.write(output=output, overwrite=overwrite, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:58731,checkpoint,checkpoint,58731,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,3,"['Checkpoint', 'checkpoint']","['Checkpoint', 'checkpoint']"
Availability,"gation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:1051,down,download,1051,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['down'],['download']
Availability,"gators. Users may now; import these functions and use them directly.; (#14405); VariantDataset.validate now checks that all ref blocks are no; longer than the ref_block_max_length field, if it exists. Bug Fixes. (#14420) Fixes a; serious, but likely rare, bug in the Table/MatrixTable reader, which; has been present since Sep 2020. It manifests as many (around half or; more) of the rows being dropped. This could only happen when 1); reading a (matrix)table whose partitioning metadata allows rows with; the same key to be split across neighboring partitions, and 2); reading it with a different partitioning than it was written. 1); would likely only happen by reading data keyed by locus and alleles,; and rekeying it to only locus before writing. 2) would likely only; happen by using the _intervals or _n_partitions arguments to; read_(matrix)_table, or possibly repartition. Please reach; out to us if you’re concerned you may have been affected by this.; (#14330) Fixes; erroneous error in export_vcf with unphased haploid Calls.; (#14303) Fix; missingness error when sampling entries from a MatrixTable.; (#14288) Contigs may; now be compared for inquality while filtering rows. Deprecations. (#14386); MatrixTable.make_table is deprecated. Use .localize_entries; instead. Version 0.2.128; Released 2024-02-16; In GCP, the Hail Annotation DB and Datasets API have moved from; multi-regional US and EU buckets to regional US-CENTRAL1 and; EUROPE-WEST1 buckets. These buckets are requester pays which means; unless your cluster is in the US-CENTRAL1 or EUROPE-WEST1 region, you; will pay a per-gigabyte rate to read from the Annotation DB or Datasets; API. We must make this change because reading from a multi-regional; bucket into a regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:14687,error,error,14687,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"ge account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP \; --vep GRCh37 \; --vep-loftee-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/loftee-GRCh37 \; --vep-homo-sapiens-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/homo-sapiens-GRCh37. Those two URIs must point at directories containing the VEP data files. You can populate them by; downloading the two tar files using gcloud storage cp,; gs://hail-us-central1-vep/loftee-beta/GRCh37.tar and gs://hail-us-central1-vep/homo-sapiens/85_GRCh37.tar,; extracting them into a local folder, and uploading that folder to your storage account using az; storage copy. The hail-us-central1-vep Google Cloud Storage bucket is a requester pays bucket which means; you must pay the cost of transferring them out of Google Cloud. We do not provide these files in; Azure because Azure Blob Storage lacks an equivalent cost control mechanism.; Hail also supports VEP for GRCh38 variants. The required tar files are located at; gs://hail-REGION-vep/loftee-beta/GRCh38.tar and; gs://hail-REGION-vep/homo-sapiens/95_GRCh38.tar.; A cluster started without the --vep argument is unable to run VEP and cannot be modified to run; VEP. You must start a new cluster using --vep. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/azure.html:2710,down,downloading,2710,docs/0.2/cloud/azure.html,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html,1,['down'],['downloading']
Availability,"ge(1, len(spvals) + 1, 1)]; obs = [-log(p, 10) for p in spvals]; plt.clf(); plt.scatter(exp, obs); plt.plot(np.arange(0, max(xMax, yMax)), c=""red""); plt.xlabel(""Expected p-value (-log10 scale)""); plt.ylabel(""Observed p-value (-log10 scale)""); plt.xlim(0, xMax); plt.ylim(0, yMax); plt.show(). Python makes it easy to make a Q-Q (quantile-quantile); plot. In [47]:. qqplot(gwas.query_variants('variants.map(v => va.linreg.pval).collect()'),; 5, 6). Confounded!¶; The observed p-values drift away from the expectation immediately.; Either every SNP in our dataset is causally linked to caffeine; consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate; this phenotype. This leads to a; stratified; distribution of the phenotype. The solution is to include ancestry as a; covariate in our regression.; The; linreg; method can also take sample annotations to use as covariates. We already; annotated our samples with reported ancestry, but it is good to be; skeptical of these labels due to human error. Genomes don’t have that; problem! Instead of using reported ancestry, we will use genetic; ancestry by including computed principal components in our model.; The; pca; method produces sample PCs in sample annotations, and can also produce; variant loadings and global eigenvalues when asked. In [48]:. pca = common_vds.pca('sa.pca', k=5, eigenvalues='global.eigen'). 2018-10-18 01:26:55 Hail: INFO: Running PCA with 5 components... In [49]:. pprint(pca.globals). {u'eigen': {u'PC1': 56.34707905481798,; u'PC2': 37.8109003010398,; u'PC3': 16.91974301822238,; u'PC4': 2.707349935634387,; u'PC5': 2.0851252187821174}}. In [50]:. pprint(pca.sample_schema). Struct{; Population: String,; SuperPopulation: String,; isFemale: Boolean,; PurpleHair: Boolean,; CaffeineConsumption: Int,; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:24183,error,error,24183,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['error'],['error']
Availability,"gene CHD8 (assumes the variant annotation va.gene exists):; >>> vds_result = vds.filter_variants_expr('va.gene == ""CHD8""'). Remove all variants on chromosome 1:; >>> vds_result = vds.filter_variants_expr('v.contig == ""1""', keep=False). Caution; The double quotes on ""1"" are necessary because v.contig is of type String. Notes; The following symbols are in scope for expr:. v (Variant): Variant; va: variant annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for variant v. For more information, see the Overview and the Expression Language. Caution; When expr evaluates to missing, the variant will be removed regardless of whether keep=True or keep=False. Parameters:; expr (str) – Boolean filter expression.; keep (bool) – Keep variants where expr evaluates to true. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_list(variants, keep=True)[source]¶; Filter variants with a list of variants.; Examples; Filter VDS down to a list of variants:; >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True). Notes; This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap with any supplied variant will not be loaded at all. This property; enables filter_variants_list to be used for reasonably low-latency queries of one; or more variants, even on large datasets. Parameters:; variants (list of Variant) – List of variants to keep or remove.; keep (bool) – If true, keep variants in variants, otherwise remove them. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_table(table, keep=True)[source]¶; Filter variants with a Variant keyed key table.; Example; Filter variants of a VDS to those appearing in a text file:; >>> kt = hc.import_table('data/sample_variants.txt', key='Variant', impute=True); >>> filtered_vds = vds.filter_variants_table(kt, keep=True). Keep all variants whose ch",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:59509,down,down,59509,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['down'],['down']
Availability,"genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel err",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6335,error,errors,6335,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"genotype_schema() must be of type TGenotype in order to use this method. Examples; Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:; >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped). Export all mendel errors to a text file:; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors pe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:124268,error,errors,124268,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,"get(nbins, 0),; ). def wrap_errors(s, e, nbins, freq_dict):; return (; hl.case(); .when(; nbins > 0,; hl.bind(; lambda bs: hl.case(); .when((bs > 0) & hl.is_finite(bs), result(s, nbins, bs, freq_dict)); .or_error(; ""'hist': start=""; + hl.str(s); + "" end=""; + hl.str(e); + "" bins=""; + hl.str(nbins); + "" requires positive bin size.""; ),; hl.float64(e - s) / nbins,; ),; ); .or_error(hl.literal(""'hist' requires positive 'bins', but bins="") + hl.str(nbins)); ). if _result_from_hist_agg_f is None:; _result_from_hist_agg_f = hl.experimental.define_function(; wrap_errors, hl.tfloat64, hl.tfloat64, hl.tint32, hl.tdict(hl.tint32, hl.tint64); ). return _result_from_hist_agg_f(start, end, bins, freq_dict). [docs]@typecheck(x=expr_float64, y=expr_float64, label=nullable(oneof(expr_str, expr_array(expr_str))), n_divisions=int); def downsample(x, y, label=None, n_divisions=500) -> ArrayExpression:; """"""Downsample (x, y) coordinate datapoints. Parameters; ----------; x : :class:`.NumericExpression`; X-values to be downsampled.; y : :class:`.NumericExpression`; Y-values to be downsampled.; label : :class:`.StringExpression` or :class:`.ArrayExpression`; Additional data for each (x, y) coordinate. Can pass in multiple fields in an :class:`.ArrayExpression`.; n_divisions : :obj:`int`; Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns; -------; :class:`.ArrayExpression`; Expression for downsampled coordinate points (x, y). The element type of the array is; :class:`.ttuple` of :py:data:`.tfloat64`, :py:data:`.tfloat64`, and :class:`.tarray` of :py:data:`.tstr`; """"""; if label is None:; label = hl.missing(hl.tarray(hl.tstr)); elif isinstance(label, StringExpression):; label = hl.array([label]); return _agg_func(; 'Downsample', [x, y, label], tarray(ttuple(tfloat64, tfloat64, tarray(tstr))), init_op_args=[n_divisions]; ). @typecheck(expr=expr_any, n=expr_int32); def _reservoir_sample(expr, n):; return _agg_func('ReservoirSample', [e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:44256,down,downsampled,44256,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['down'],['downsampled']
Availability,"gh parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, opt",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:15373,toler,tolerance,15373,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['toler'],['tolerance']
Availability,"gh the same; effect can be achieved for * by using @. Warning; For binary operations, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for + and *, place the; block matrix operand first; for -, /, and @, first convert; the ndarray to a block matrix using from_numpy(). Warning; Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product.; The \((i, j)\)-block in the product a @ b is computed by summing; the products of corresponding blocks in block row \(i\) of a and; block column \(j\) of b. So overall, in addition to this; multiplication and addition, the evaluation of a @ b realizes each; block of a as many times as the number of block columns of b; and realizes each block of b as many times as the number of; block rows of a.; This becomes a performance and resilience issue whenever a or b; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating a @ (c @ d) will; effectively evaluate c @ d as many times as the number of block rows; in a.; To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:; >>> c = BlockMatrix.read('c.bm') ; >>> d = BlockMatrix.read('d.bm') ; >>> (c @ d).write('cd.bm') ; >>> a = BlockMatrix.read('a.bm') ; >>> e = a @ BlockMatrix.read('cd.bm') . Indexing and slicing; Block matrices also support NumPy-style 2-dimensional; indexing and slicing,; with two differences.; First, slices start:stop:step must be non-empty with positive step.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional.; For example, for a block matrix bm with 10 rows and 10 columns:. bm[0, 0] is the element in row 0 and column 0 of bm.; bm[0:1, 0] is a block matrix with 1 row, 1 column,; and element bm[0, 0].; bm[2, :] is a block matri",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:3539,resilien,resilience,3539,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['resilien'],['resilience']
Availability,"ghborhood of the diagonal. If variants :math:`i` and :math:`j` are on the; same contig and within `radius` base pairs (inclusive) then the; :math:`(i, j)` element is their; `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__.; Otherwise, the :math:`(i, j)` element is ``0.0``. Rows with a constant value (i.e., zero variance) will result in ``nan``; correlation values. To avoid this, first check that all variants vary or; filter out constant variants (for example, with the help of; :func:`.aggregators.stats`). If the :meth:`.global_position` on `locus_expr` is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that's; been ordered by `locus_expr`. Set `coord_expr` to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-``nan``, on the; same source as `locus_expr`, and ascending with respect to locus; position for each contig; otherwise the method will raise an error. Warning; -------; See the warnings in :meth:`row_correlation`. In particular, for large; matrices it may be preferable to run its stages separately. `entry_expr` and `locus_expr` are implicitly aligned by row-index, though; they need not be on the same source. If their sources differ in the number; of rows, an error will be raised; otherwise, unintended misalignment may; silently produce unexpected results. Parameters; ----------; entry_expr : :class:`.Float64Expression`; Entry-indexed numeric expression on matrix table.; locus_expr : :class:`.LocusExpression`; Row-indexed locus expression on a table or matrix table that is; row-aligned with the matrix table of `entry_expr`.; radius: :obj:`int` or :obj:`float`; Radius of window for row values.; coord_expr: :class:`.Float64Expression`, optional; Row-indexed numeric expression for the row value on the same table or; matrix table as ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:140007,error,error,140007,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['error'],['error']
Availability,"ght start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, your; Java installation must be an arm64 installation. Using x86_64 Java; with Hail on Apple M1 or M2 will cause SIGILL errors. If you have an; Apple M1 or Apple M2 and /usr/libexec/java_home -V does not; include (arm64), you must switch to an arm64 version of the JVM.; (#14022) Fix; (#13937) caused by; faulty library code in the Google Cloud Storage API Java client; library.; (#13812) Permit; hailctl batch submit to accept relative paths. Fix; (#13785).; (#13885) Hail; Query-on-Batch previously used Class A Operations for all interaction; with blobs. This change ensures that QoB only uses Class A Operations; when necessary.; (#14127); hailctl dataproc start ... --dry-run now uses shell escapes such; that, after copied and pasted into a shell, the gcloud command; works as expected.; (#14062) Fix; (#14052) which; caused incorrect results for identity by descent in Query-on-Batch.; (#14122) Ensure that; stack traces are transmitted from workers to the driver to the; client.; (#14105) When a VCF; contains missing values in array fields, Hail now suggests using; array_elements_r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:19254,error,errors,19254,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used lmmreg() in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.; While lmmreg() computes the kinship matrix \(K\) using distributed matrix multiplication (Step 2), the full eigendecomposition (Step 3) is currently run on a single core of master using the LAPACK routine DSYEVD, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:96908,avail,available,96908,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['avail'],['available']
Availability,"gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the \(n \times n\) sample correlation or realized relationship matrix (RRM) \(K\) as simply. \[K = MM^T\]; Note that the only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in grm() is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. Parameters:; force_block (bool) – Force using Spark’s BlockMatrix to compute kinship (advanced).; force_gramian (bool) – Force using Spark’s RowMatrix.computeGramian to compute kinship (advanced). Returns:Realized Relationship Matrix for all samples. Return type:KinshipMatrix. same(other, tolerance=1e-06)[source]¶; True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values.; Examples; This will return True:; >>> vds.same(vds). Notes; The tolerance parameter sets the tolerance for equality when comparing floating-point fields. More precisely, \(x\) and \(y\) are equal if. \[bs{x - y} \leq tolerance * \max{bs{x}, bs{y}}\]. Parameters:; other (VariantDataset) – variant dataset to compare against; tolerance (float) – floating-point tolerance for equality. Return type:bool. sample_annotations¶; Return a dict of sample annotations.; The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. Returns:dict. sample_ids¶; Return sampleIDs. Returns:List of sample IDs. Return type:list of str. sample_qc(root='sa.qc', keep_star=False)[source]¶; Compute per-sample QC metrics. Important; The genotype_schema() must be of type TGenotype in order to use this method. Annotations; sample_qc() computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; sa.qc.<identifier> (or <root>.<identifier> if a non-default root was passed):. Name; Type; Description. callRate; Double; Fra",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:155326,toler,tolerance,155326,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['toler'],['tolerance']
Availability,"gle field element. to_ndarray; Collects a BlockMatrix into a local hail ndarray expression on driver. to_numpy; Collects the block matrix into a NumPy ndarray. to_table_row_major; Returns a table where each row represents a row in the block matrix. tofile; Collects and writes data to a binary file. tree_matmul; Matrix multiplication in situations with large inner dimension. unpersist; Unpersists this block matrix from memory/disk. write; Writes the block matrix. write_from_entry_expr; Writes a block matrix from a matrix table entry expression. property T; Matrix transpose. Returns:; BlockMatrix. abs()[source]; Element-wise absolute value. Returns:; BlockMatrix. property block_size; Block size. Returns:; int. cache()[source]; Persist this block matrix in memory.; Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; BlockMatrix – Cached block matrix. ceil()[source]; Element-wise ceiling. Returns:; BlockMatrix. checkpoint(path, overwrite=False, force_row_major=False, stage_locally=False)[source]; Checkpoint the block matrix. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before checkpointing.; If False, checkpoint blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static default_block_size()[source]; Default block side length. densify()[source]; Restore all dropped blocks as explicit blocks of zeros. Returns:; BlockMatrix. diagonal()[source]; Extracts diagonal elements as a row vector. Returns:; BlockMatrix. property element_type; The type of the elements. entries(keyed=True)[source]; Returns a table with the indices and value of each block matrix entry.; Ex",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:10731,checkpoint,checkpoint,10731,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['checkpoint'],['checkpoint']
Availability,"gth is the same as the modified `alleles`; field. **Downcode algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; :func:`.split_multi_hts`. The downcode algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded genotype, and shift so; the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Subset algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The subset algorithm subsets the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:160011,down,downcode,160011,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['down'],['downcode']
Availability,"guage. View page source. Introduction to the Expression Language¶; This notebook starts with the basics of the Hail expression language,; and builds up practical experience with the type system, syntax, and; functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice,; dice, filter, and query genetic data. These are covered in the next; notebook!; The best part about a Jupyter Notebook is that you don’t just have to; run what we’ve written - you can and should change the code and see; what happens!. Setup¶; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc.; As always, visit the documentation; on the Hail website for full reference. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. Hail Expression Language¶; The Hail expression language is used everywhere in Hail: filtering; conditions, describing covariates and phenotypes, storing summary; statistics about variants and samples, generating synthetic data,; plotting, exporting, and more. The Hail expression language takes the; form of Python strings passed into various Hail methods like; filter_variants_expr; and linear; regression.; The expression language is a programming language just like Python or R; or Scala. While the syntax is different, programming experience will; certainly translate. We have built the expression language with the hope; that even people new to programming are able to use it to explore; genetic data, even if this means copying motifs and expressions found on; places like Hail discussion forum.; For learning purposes, HailContext contains the method; eval_expr_ty",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:1706,avail,available,1706,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,1,['avail'],['available']
Availability,"h a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware tha",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:3832,echo,echo,3832,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability,"h of each array; is identical, and will apply the operation positionally (a1 * a2 will; multiply the first element of a1 by the first element of a2, the; second element of a1 by the second element of a2, and so on).; Arithmetic with a scalar will apply the operation to each element of the; array.; >>> a1 = hl.literal([0, 1, 2, 3, 4, 5]). >>> a2 = hl.literal([1, -1, 1, -1, 1, -1]). Attributes. dtype; The data type of the expression. Methods. __add__(other)[source]; Positionally add an array or a scalar.; Examples; >>> hl.eval(a1 + 5); [5, 6, 7, 8, 9, 10]. >>> hl.eval(a1 + a2); [1, 0, 3, 2, 5, 4]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to add. Returns:; ArrayNumericExpression – Array of positional sums. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other)[source]; Positionally divide by an array or a scalar using floor division.; Examples; >>> hl.eval(a1 // 2); [0, 0, 1, 1, 2, 2]. Parameters:; other (NumericExpression or ArrayNumericExpression). Returns:; ArrayNumericExpression. __ge__(other); Return self>=value. __getitem__(item); Index into or slice the array.; Examples; Index with a single integer:; >>> hl.eval(names[1]); 'Bob'. >>> hl.eval(names[-1]); 'Charlie'. Slicing is also supported:; >>> hl.eval(names[1:]); ['Bob', 'Charlie']. Parameters:; item (slice or Expression of type tint32) – Index or slice. Returns:; Expression – Element or array slice. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __mod__(other)[source]; Positionally compute the left modulo the right.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:1959,error,error,1959,docs/0.2/hail.expr.ArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html,1,['error'],['error']
Availability,"h starting with 'va', period-delimited. :param str attribute: The attribute to remove (key). :return: Annotated dataset with the updated variant annotation without the attribute.; :rtype: :class:`.VariantDataset`. """""". return VariantDataset(self.hc, self._jvds.deleteVaAttribute(ann_path, attribute)). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(propagate_gq=bool,; keep_star_alleles=bool,; max_shift=integral); def split_multi(self, propagate_gq=False, keep_star_alleles=False, max_shift=100):; """"""Split multiallelic variants. .. include:: requireTGenotype.rst. **Examples**. >>> vds.split_multi().write('output/split.vds'). **Notes**. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT field is downcoded once for each; alternate allele. A call for an alternate allele maps to 1 in; the biallelic variant corresponding to itself and 0; otherwise. For example, in the example above, 0/2 maps to 0/0; and 0/1. The genotype 1/2 maps to 0/1 and 0/1. The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the; sum of the other multiallelic entries. The biallelic DP is the same as the multiallelic DP. The biallelic PL entry for for a genotype g is the minimum; over PL entries for multiallelic genotypes that downcode to; g. For example, the PL for (A, T) at 0/1 is the minimum of the; PLs for 0/1 (50) and 1/2 (45), and thus 45. Fixing an alternate allele and biallelic variant, downcoding; gives a map from multiallelic to biallelic alleles and; genotypes. The biallelic AD entry for an allele is just the; sum of the multiallelic AD entries for alleles that map to; that allele. Similarly, the biallelic PL entry for a genotype; is the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:208836,down,downcoded,208836,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['down'],['downcoded']
Availability,"h the Sequence Kernel Association Test; <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/>`__. Row weights must be non-negative. Rows with missing weights are ignored. In; the R package ``skat``---which assumes rows are variants---default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field `AF`, one can use the expression:. >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response `y` must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively. The resulting :class:`.Table` provides the group's key (`id`), thenumber of; rows in the group (`size`), the variance component score `q_stat`, the SKAT; `p-value`, and a `fault` flag. For the toy example above, the table has the; form:. +-------+------+--------+---------+-------+; | id | size | q_stat | p_value | fault |; +=======+======+========+=========+=======+; | geneA | 2 | 4.136 | 0.205 | 0 |; +-------+------+--------+---------+-------+; | geneB | 1 | 5.659 | 0.195 | 0 |; +-------+------+--------+---------+-------+; | geneC | 3 | 4.122 | 0.192 | 0 |; +-------+------+--------+---------+-------+. Groups larger than `max_size` appear with missing `q_stat`, `p_value`, and; `fault`. The hard limit on the number of rows in a group is 46340. Note that the variance component score `q_stat` agrees with ``Q`` in the R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:104331,fault,fault,104331,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability,"h(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; SwitchBuilder – Mutates and returns self. when_missing(then)[source]; Add a test for missingness. If the base expression is missing,; returns then. Parameters:; then (Expression). Returns:; SwitchBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:2013,error,error,2013,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,1,['error'],['error']
Availability,"hBuilder. return SwitchBuilder(expr). [docs]@typecheck(f=anytype, exprs=expr_any, _ctx=nullable(str)); def bind(f: Callable, *exprs, _ctx=None):; """"""Bind a temporary variable and use it in a function. Examples; --------. >>> hl.eval(hl.bind(lambda x: x + 1, 1)); 2. :func:`.bind` also can take multiple arguments:. >>> hl.eval(hl.bind(lambda x, y: x / y, x, x)); 1.0. Parameters; ----------; f : function ( (args) -> :class:`.Expression`); Function of `exprs`.; exprs : variable-length args of :class:`.Expression`; Expressions to bind. Returns; -------; :class:`.Expression`; Result of evaluating `f` with `exprs` as arguments.; """"""; args = []; uids = []; irs = []. for expr in exprs:; uid = Env.get_uid(base=_ctx); args.append(construct_variable(uid, expr._type, expr._indices, expr._aggregations)); uids.append(uid); irs.append(expr._ir). lambda_result = to_expr(f(*args)); if _ctx:; indices, aggregations = unify_all(lambda_result) # FIXME: hacky. May drop field refs from errors?; else:; indices, aggregations = unify_all(*exprs, lambda_result). res_ir = lambda_result._ir; for uid, value_ir in builtins.zip(uids, irs):; if _ctx == 'agg':; res_ir = ir.AggLet(uid, value_ir, res_ir, is_scan=False); elif _ctx == 'scan':; res_ir = ir.AggLet(uid, value_ir, res_ir, is_scan=True); else:; res_ir = ir.Let(uid, value_ir, res_ir). return construct_expr(res_ir, lambda_result.dtype, indices, aggregations). [docs]def rbind(*exprs, _ctx=None):; """"""Bind a temporary variable and use it in a function. This is :func:`.bind` with flipped argument order. Examples; --------. >>> hl.eval(hl.rbind(1, lambda x: x + 1)); 2. :func:`.rbind` also can take multiple arguments:. >>> hl.eval(hl.rbind(4.0, 2.0, lambda x, y: x / y)); 2.0. Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Expressions to bind.; f : function ( (args) -> :class:`.Expression`); Function of `exprs`. Returns; -------; :class:`.Expression`; Result of evaluating `f` with `exprs` as arguments.; """""". *args, f = ex",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:17873,error,errors,17873,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['error'],['errors']
Availability,"hail.expr.NDArrayExpression[source]; Expression of type tndarray.; >>> nd = hl.nd.array([[1, 2], [3, 4]]). Attributes. T; Reverse the dimensions of this ndarray. dtype; The data type of the expression. ndim; The number of dimensions of this ndarray. shape; The shape of this ndarray. Methods. map; Applies an element-wise operation on an NDArray. map2; Applies an element-wise binary operation on two NDArrays. reshape; Reshape this ndarray to a new shape. transpose; Permute the dimensions of this ndarray according to the ordering of axes. property T; Reverse the dimensions of this ndarray. For an n-dimensional array a,; a[i_0, …, i_n-1, i_n] = a.T[i_n, i_n-1, …, i_0].; Same as self.transpose().; See also transpose(). Returns:; NDArrayExpression. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of record",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html:1715,error,error,1715,docs/0.2/hail.expr.NDArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html,1,['error'],['error']
Availability,"hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinstance(v, StringExpression) else v for k, v in expressions.items()}. agg_f = x[1]._aggregation_method(); res = agg_f(; hail.agg.downsample(; x[1], y[1], label=list(expressions.values()) if expressions else None, n_divisions=n_divisions; ); ); source_pd = pd.DataFrame([; dict(; **{x[0]: point[0], y[0]: point[1]},; **(dict(zip(expressions, point[2])) if point[2] is not None else {}),; ); for point in res; ]); source_pd = source_pd.astype(numeric_expr, copy=False). return source_pd. def _get_categorical_palette(factors: List[str]) -> ColorMapper:; n = max(3, len(factors)); _palette: Sequence[str]; if n < len(palette):; _palette = palette; elif n < 21:; from bokeh.palettes import Category20. _palette = Category20[n]; else:; from bokeh.palettes import viridis. _palette = viridis(n). return CategoricalColorMapper(factors=factors, palette=_palette). def _get_scatter_plot_elements(; sp: Plot,; source_pd: pd.DataFrame,; x_col: str,; y_col: str,; label_cols: List[str],; colors: Optional[Dict[str, ColorMapper]] = None,; size: int = 4,; hover_cols: Optional[Set[str]] = None,; ) -> Union[; Tuple[Plot, Dict[str, List[LegendItem]], Legend, ColorBar,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:23974,down,downsample,23974,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['down'],['downsample']
Availability,"he UI; Important Notes. Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:1241,avail,available,1241,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['avail'],['available']
Availability,"he first element of the array and the; alternate allele (5th column if chromosome is not defined) is the second; element.; varid (tstr) – The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; rsid (tstr) – The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. Entry Fields. GT (tcall) – The hard call corresponding to the genotype with; the highest probability.; GP (tarray of tfloat64) – Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the probabilities is a distance greater than the tolerance; parameter from 1.0. Otherwise, the probabilities are normalized to sum to; 1.0. For example, the input [0.98, 0.0, 0.0] will be normalized to; [1.0, 0.0, 0.0]. Parameters:. path (str or list of str) – GEN files to import.; sample_file (str) – Sample file to import.; tolerance (float) – If the sum of the genotype probabilities for a genotype differ from 1.0; by more than the tolerance, set the genotype to missing.; min_partitions (int, optional) – Number of partitions.; chromosome (str, optional) – Chromosome if not included in the GEN file; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of str to str, optional) – Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by reference_genome.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome. Returns:; MatrixTable. hail.methods.import_locus_intervals(path, reference_genome='default', skip_invalid_intervals=False, contig_recoding=None, **kwargs)[source]; Import a locus interval list as a Table.; Examples; Add the row field capture_region indicating inclusion in; at least one locus interval from capture_intervals.txt:; >>> intervals = hl.import_locus_intervals('data/capture_intervals.txt', reference_genome='GRCh37'); >>> result = dataset.annotate_rows(capture_region = hl.is_def",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:18812,toler,tolerance,18812,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['toler'],['tolerance']
Availability,"he locus ``'1:45323'``:. >>> hl.eval(hl.get_sequence('1', 45323, reference_genome='GRCh37')) # doctest: +SKIP; ""T"". Notes; -----; This function requires `reference genome` has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Returns ``None`` if `contig` and `position` are not valid coordinates in; `reference_genome`. Parameters; ----------; contig : :class:`.Expression` of type :py:data:`.tstr`; Locus contig.; position : :class:`.Expression` of type :py:data:`.tint32`; Locus position.; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus of interest. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus of interest. Truncates at; contig boundary.; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Must have a reference sequence available. Returns; -------; :class:`.StringExpression`; """""". if not reference_genome.has_sequence():; raise TypeError(; ""Reference genome '{}' does not have a sequence loaded. Use 'add_sequence' to load the sequence from a FASTA file."".format(; reference_genome.name; ); ). return _func(""getReferenceSequence"", tstr, contig, position, before, after, type_args=(tlocus(reference_genome),)). [docs]@typecheck(contig=expr_str, reference_genome=reference_genome_type); def is_valid_contig(contig, reference_genome='default') -> BooleanExpression:; """"""Returns ``True`` if `contig` is a valid contig name in `reference_genome`. Examples; --------. >>> hl.eval(hl.is_valid_contig('1', reference_genome='GRCh37')); True. >>> hl.eval(hl.is_valid_contig('chr1', reference_genome='GRCh37')); False. Parameters; ----------; contig : :class:`.Expression` of type :py:data:`.tstr`; reference_genome : :class:`str` or :class:`.ReferenceGenome`. Returns; -------; :class:`.BooleanExpression`; """"""; return",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:161407,avail,available,161407,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['avail'],['available']
Availability,"his method runs linear regression for multiple phenotypes; more efficiently than looping over :py:meth:`.linreg`. This; method is more efficient than :py:meth:`.linreg_multi_pheno`; but doesn't implicitly filter on allele count or allele; frequency. .. warning::. :py:meth:`.linreg3` uses the same set of samples for each phenotype,; namely the set of samples for which **all** phenotypes and covariates are defined. **Annotations**. With the default root, the following four variant annotations are added.; The indexing of the array annotations corresponds to that of ``y``. - **va.linreg.nCompleteSamples** (*Int*) -- number of samples used; - **va.linreg.AC** (*Double*) -- sum of the genotype values ``x``; - **va.linreg.ytx** (*Array[Double]*) -- array of dot products of each phenotype vector ``y`` with the genotype vector ``x``; - **va.linreg.beta** (*Array[Double]*) -- array of fit genotype coefficients, :math:`\hat\beta_1`; - **va.linreg.se** (*Array[Double]*) -- array of estimated standard errors, :math:`\widehat{\mathrm{se}}`; - **va.linreg.tstat** (*Array[Double]*) -- array of :math:`t`-statistics, equal to :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; - **va.linreg.pval** (*Array[Double]*) -- array of :math:`p`-values. :param ys: list of one or more response expressions.; :type covariates: list of str. :param covariates: list of covariate expressions.; :type covariates: list of str. :param str root: Variant annotation path to store result of linear regression. :param bool use_dosages: If true, use dosage genotypes rather than hard call genotypes. :param int variant_block_size: Number of variant regressions to perform simultaneously. Larger block size requires more memmory. :return: Variant dataset with linear regression variant annotations.; :rtype: :py:class:`.VariantDataset`. """""". jvds = self._jvdf.linreg3(jarray(Env.jvm().java.lang.String, ys),; jarray(Env.jvm().java.lang.String, covariates), root, use_dosages, variant_block_size); return VariantDataset(self.hc,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:113368,error,errors,113368,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,"hl.import_vcf(cluster_readable_vcf, min_partitions=16, reference_genome='GRCh38').write(; matrix_table_path, overwrite=True; ). tmp_sample_annot = os.path.join(tmp_dir, 'HGDP_annotations.txt'); source = resources['HGDP_annotations']; info(f'downloading HGDP annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['HGDP_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene_annot), gene_annotations_path); hl.hadoop_copy(local_path_uri(tmp_vcf), vcf_path); info('Done!'); else:; info('HGDP files found'). [docs]def get_movie_lens(output_dir, overwrite: bool = False):; """"""Download public Movie Lens dataset. Notes; -----; The download is about 6M. See the; `MovieLens website <https://grouplens.org/datasets/movielens/100k/>`__; for more information about this dataset. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite existing files/directories at those locations.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). paths = [os.path.join(output_dir, x) for x in ['movies.ht', 'ratings.ht', 'users.ht']]; if overwrite or any(not _dir_exists(fs, f) for f in paths):; init_temp_dir(); source = resources['movie_lens_100k']; tmp_path = os.path.join(tmp_dir, 'ml-100k.zip'); info(f'downloading MovieLens-100k data ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_path); with zipfile.ZipFile(tmp_path, 'r') as z:; z.extractall(tmp_dir). user_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.user'); movie_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.item'); ratings_table_path = os.path.join(tmp_dir, 'ml-100k",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:6332,down,download,6332,docs/0.2/_modules/hail/utils/tutorial.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html,2,['down'],['download']
Availability,"hon_job() instead.; Methods. always_copy_output; Set the job to always copy output to cloud storage, even if the job failed. always_run; Set the job to always run, even if dependencies fail. cloudfuse; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. cpu; Set the job's CPU requirements. depends_on; Explicitly set dependencies on other jobs. env. gcsfuse; Add a bucket to mount with gcsfuse. memory; Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool) – If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool) – If True, set job to always run. Return type:; Self. Returns:; Same job object set to always run. cloudfuse(bucket, mount_point, *, read_only=True); Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. Warning; There are performance and cost implications of using gcsfuse; or blobfuse. Examples; Google Clou",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:1647,echo,echo,1647,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['echo'],['echo']
Availability,"hrm{ln}(\delta)). Hence one can change variables to extract a high-resolution discretization of the likelihood function of :math:`h^2` over :math:`[0,1]` at the corresponding REML estimators for :math:`\\beta` and :math:`\sigma_g^2`, as well as integrate over the normalized likelihood function using `change of variables <https://en.wikipedia.org/wiki/Integration_by_substitution>`_ and the `sigmoid differential equation <https://en.wikipedia.org/wiki/Sigmoid_function#Properties>`_. For convenience, ``global.lmmreg.fit.normLkhdH2`` records the the likelihood function of :math:`h^2` normalized over the discrete grid ``0.01, 0.02, ..., 0.98, 0.99``. The length of the array is 101 so that index ``i`` contains the likelihood at percentage ``i``. The values at indices 0 and 100 are left undefined. By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of :math:`h^2` as follows. Let :math:`x_2` be the maximum likelihood estimate of :math:`h^2` and let :math:`x_ 1` and :math:`x_3` be just to the left and right of :math:`x_2`. Let :math:`y_1`, :math:`y_2`, and :math:`y_3` be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. .. math::. \\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\\frac{1}{2 \sigma^2}. The standard error :math:`\\hat{\sigma}` is then estimated by solving for :math:`\sigma`. Note that the mean and standard deviation of the (discretized or continuous) distribution held in ``global.lmmreg.fit.normLkhdH2`` will not coincide with :math:`\\hat{h}^2` and :math:`\\hat{\sigma}`, since this distribution only becomes normal in the infinite sample limi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:132979,error,error,132979,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability,"ht.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP \; --vep GRCh37 \; --vep-loftee-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/loftee-GRCh37 \; --vep-homo-sapiens-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/homo-sapiens-GRCh37. Those two URIs must point at directories containing the VEP data files. You can populate them by; downloading the two tar files using gcloud storage cp,; gs://hail-us-central1-vep/loftee-beta/GRCh37.tar and gs://hail-us-central1-vep/homo-sapiens/85_GRCh37.tar,; extracting them into a local folder, and uploading that folder to your storage account using az; storage copy. The hail-us-central1-vep Google Cloud Storage bucket is a requester pays bucket which means; you must pay the cost of tr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/azure.html:2062,down,down,2062,docs/0.2/cloud/azure.html,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html,1,['down'],['down']
Availability,"ht:; >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the Table.show() method to see the first few rows; of a table.; Table.take() will collect the first n rows of a table into a local; Python list:; >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a Struct whose elements can be; accessed using Python’s get attribute or get item notation:; >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The Table.head() method is helpful for testing pipelines. It subsets a; table to the first n rows, causing downstream operations to run much more; quickly.; Table.describe() is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ht.ID.dtype), and the full row and global types can be accessed with; ht.row.dtype and ht.globals.dtype. The row fields that are part of the; key can be accessed with Table.key. The Table.count() method; returns the number of rows. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/table.html:8903,down,downstream,8903,docs/0.2/overview/table.html,https://hail.is,https://hail.is/docs/0.2/overview/table.html,1,['down'],['downstream']
Availability,"https://en.wikipedia.org/wiki/Q-Q_plot). If no label or a single label is provided, then returns :class:`bokeh.plotting.figure`; Otherwise returns a :class:`bokeh.models.layouts.Column` containing:; - a :class:`bokeh.models.widgets.inputs.Select` dropdown selection widget for labels; - a :class:`bokeh.plotting.figure` containing the interactive qq plot. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; pvals : :class:`.NumericExpression`; List of x-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]]; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:47482,down,down,47482,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['down'],['down']
Availability,"https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Release 2022-10-18. Bug Fixes. (#12305): Fixed a; rare crash reading tables/matrixtables with _intervals. Version 0.2.102; Released 2022-10-06. New Features. (#12218) Missing; values are now supported in primitive columns in Table.to_pandas.; (#12254); Cross-product-style legends for data groups have been replaced with; factored ones (consistent with ggplot2’s implementation) for; hail.ggplot.geom_point, and support has been added for custom; legend group labels.; (#12268); VariantDataset now implements union_rows for combining; datasets with the same samples but disjoint variants. Bug Fixes. (#12278) Fixed bug; made more likely by 0.2.101 in which Hail errors when interacting; with a NumPy integ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:43767,error,error,43767,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"iantDataset; from .combine import (; calculate_even_genome_partitioning,; calculate_new_intervals,; combine,; combine_r,; combine_variant_datasets,; defined_entry_fields,; make_reference_stream,; make_variant_stream,; transform_gvcf,; ). [docs]class VDSMetadata(NamedTuple):; """"""The path to a Variant Dataset and the number of samples within. Parameters; ----------; path : :class:`str`; Path to the variant dataset.; n_samples : :class:`int`; Number of samples contained within the Variant Dataset at `path`. """""". path: str; n_samples: int. class CombinerOutType(NamedTuple):; """"""A container for the types of a VDS"""""". reference_type: tmatrix; variant_type: tmatrix. FAST_CODEC_SPEC = """"""{; ""name"": ""LEB128BufferSpec"",; ""child"": {; ""name"": ""BlockingBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""ZstdBlockBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""StreamBlockBufferSpec""; }; }; }; }"""""". [docs]class VariantDatasetCombiner: # pylint: disable=too-many-instance-attributes; """"""A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. Examples; --------. A Variant Dataset comprises one or more sequences. A new Variant Dataset is constructed from; GVCF files and/or extant Variant Datasets. For example, the following produces a new Variant; Dataset from four GVCF files containing whole genome sequences ::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:2036,failure,failure-tolerant,2036,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['failure'],['failure-tolerant']
Availability,"iant_expr (str or list of str) – Variant annotation expressions.; genotype_expr (str or list of str) – Genotype annotation expressions.; key (str or list of str) – List of key columns.; separator (str) – Separator to use between sample IDs and genotype expression left-hand side identifiers. Return type:KeyTable. mendel_errors(pedigree)[source]¶; Find Mendel errors; count per variant, individual and nuclear; family. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:; >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped). Export all mendel errors to a text file:; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclea",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:123806,error,errors,123806,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,"ic_consequences',; 'motif_feature_consequences',; 'regulatory_feature_consequences',; 'transcript_consequences',; ); }). if isinstance(ds, Table):; return split.annotate(**update_rows_expression).drop('old_locus', 'old_alleles'). split = split.annotate_rows(**update_rows_expression); entry_fields = ds.entry. expected_field_types = {; 'GT': hl.tcall,; 'AD': hl.tarray(hl.tint),; 'DP': hl.tint,; 'GQ': hl.tint,; 'PL': hl.tarray(hl.tint),; 'PGT': hl.tcall,; 'PID': hl.tstr,; }. bad_fields = []; for field in entry_fields:; if field in expected_field_types and entry_fields[field].dtype != expected_field_types[field]:; bad_fields.append((field, entry_fields[field].dtype, expected_field_types[field])). if bad_fields:; msg = '\n '.join([f""'{x[0]}'\tfound: {x[1]}\texpected: {x[2]}"" for x in bad_fields]); raise TypeError(""'split_multi_hts': Found invalid types for the following fields:\n "" + msg). update_entries_expression = {}; if 'GT' in entry_fields:; update_entries_expression['GT'] = hl.downcode(split.GT, split.a_index); if 'DP' in entry_fields:; update_entries_expression['DP'] = split.DP; if 'AD' in entry_fields:; update_entries_expression['AD'] = hl.or_missing(; hl.is_defined(split.AD), [hl.sum(split.AD) - split.AD[split.a_index], split.AD[split.a_index]]; ); if 'PL' in entry_fields:; pl = hl.or_missing(; hl.is_defined(split.PL),; (; hl.range(0, 3).map(; lambda i: hl.min(; (; hl.range(0, hl.triangle(split.old_alleles.length())); .filter(; lambda j: hl.downcode(; hl.unphased_diploid_gt_index_call(j), split.a_index; ).unphased_diploid_gt_index(); == i; ); .map(lambda j: split.PL[j]); ); ); ); ),; ); if 'GQ' in entry_fields:; update_entries_expression['PL'] = pl; update_entries_expression['GQ'] = hl.or_else(hl.gq_from_pl(pl), split.GQ); else:; update_entries_expression['PL'] = pl; elif 'GQ' in entry_fields:; update_entries_expression['GQ'] = split.GQ. if 'PGT' in entry_fields:; update_entries_expression['PGT'] = hl.downcode(split.PGT, split.a_index); if 'PID' in entry_fields:;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:124421,down,downcode,124421,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['down'],['downcode']
Availability,"ical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use `None` to collect all points.; missing_label: str; Label to use when a point is missing data for a categorical label. Returns; -------; :class:`bokeh.plotting.figure` if no label or a single label was given, otherwise :class:`bokeh.models.layouts.Column`; """"""; hover_fields = {} if hover_fields is None else hover_fields; label_by_col: Dict[str, Expression]; if label is None:; label_by_col = {}; elif isinstance(label, Expression):; label_by_col = {'label': label}; else:; assert isinstance(label, dict); label_by_col = label. source = pvals._indices.source; if isinstance(source, Table):; ht = source.select(p_value=pvals, **hover_fields, **label_by_col); else:; assert isinstance(source, MatrixTable); ht = source.select_rows(p_value=pvals, **hover_fields, **label_by_col).rows(); ht = ht.key_by().select('p_value', *hover_fields, *label_by_col).key_by('p_value'); n = ht.aggregate(aggregators.count",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:49077,down,downsample,49077,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['down'],['downsample']
Availability,"icate these weights in Hail using alternate allele; frequencies stored in a row-indexed field `AF`, one can use the expression:. >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response `y` must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively. The resulting :class:`.Table` provides the group's key (`id`), thenumber of; rows in the group (`size`), the variance component score `q_stat`, the SKAT; `p-value`, and a `fault` flag. For the toy example above, the table has the; form:. +-------+------+--------+---------+-------+; | id | size | q_stat | p_value | fault |; +=======+======+========+=========+=======+; | geneA | 2 | 4.136 | 0.205 | 0 |; +-------+------+--------+---------+-------+; | geneB | 1 | 5.659 | 0.195 | 0 |; +-------+------+--------+---------+-------+; | geneC | 3 | 4.122 | 0.192 | 0 |; +-------+------+--------+---------+-------+. Groups larger than `max_size` appear with missing `q_stat`, `p_value`, and; `fault`. The hard limit on the number of rows in a group is 46340. Note that the variance component score `q_stat` agrees with ``Q`` in the R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of :math:`\chi^2(1)` distributions. +-------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +-----",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:104702,fault,fault,104702,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability,"icitly** if desired. Notes; -----. This method provides a scalable implementation of the score-based; variance-component test originally described in; `Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test; <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/>`__. Row weights must be non-negative. Rows with missing weights are ignored. In; the R package ``skat``---which assumes rows are variants---default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field `AF`, one can use the expression:. >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response `y` must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively. The resulting :class:`.Table` provides the group's key (`id`), thenumber of; rows in the group (`size`), the variance component score `q_stat`, the SKAT; `p-value`, and a `fault` flag. For the toy example above, the table has the; form:. +-------+------+--------+---------+-------+; | id | size | q_stat | p_value | fault |; +=======+======+========+=========+=======+; | geneA | 2 | 4.136 | 0.205 | 0 |; +-------+------+--------+---------+-------+; | geneB | 1 | 5.659 | 0.195 | 0 |; +-------+------+--------+---------+-------+; | geneC | 3 | 4.122 | 0.192 | 0 |; +-------+------+--------+---------+-------+. Groups larger than `max_size` appear with missing `q_stat`, `p_value`, and; `fault`. The hard limit on the number of rows in a group is 46340. Note that the variance component score `q_stat` agrees with ``Q`` in the R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:104187,fault,fault,104187,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability,"id, '0'),; 'is_female': expr_or_else(is_female, '0', lambda x: hl.if_else(x, '2', '1')),; 'pheno': expr_or_else(pheno, 'NA', lambda x: hl.if_else(x, '2', '1') if x.dtype == tbool else hl.str(x)),; }. locus = dataset.locus; a = dataset.alleles. bim_exprs = {; 'varid': expr_or_else(varid, hl.delimit([locus.contig, hl.str(locus.position), a[0], a[1]], ':')),; 'cm_position': expr_or_else(cm_position, 0.0),; }. for exprs, axis in [; (fam_exprs, dataset._col_indices),; (bim_exprs, dataset._row_indices),; (entry_exprs, dataset._entry_indices),; ]:; for name, expr in exprs.items():; analyze('export_plink/{}'.format(name), expr, axis). dataset = dataset._select_all(col_exprs=fam_exprs, col_key=[], row_exprs=bim_exprs, entry_exprs=entry_exprs). # check FAM ids for white space; t_cols = dataset.cols(); errors = []; for name in ['ind_id', 'fam_id', 'pat_id', 'mat_id']:; ids = t_cols.filter(t_cols[name].matches(r""\s+""))[name].collect(). if ids:; errors.append(f""""""expr '{name}' has spaces in the following values:\n""""""); for row in ids:; errors.append(f"""""" {row}\n""""""). if errors:; raise TypeError(""\n"".join(errors)). writer = ir.MatrixPLINKWriter(output); Env.backend().execute(ir.MatrixWrite(dataset._mir, writer)). [docs]@typecheck(; dataset=oneof(MatrixTable, Table),; output=str,; append_to_header=nullable(str),; parallel=nullable(ir.ExportType.checker),; metadata=nullable(dictof(str, dictof(str, dictof(str, str)))),; tabix=bool,; ); def export_vcf(dataset, output, append_to_header=None, parallel=None, metadata=None, *, tabix=False):; """"""Export a :class:`.MatrixTable` or :class:`.Table` as a VCF file. .. include:: ../_templates/req_tvariant.rst. Examples; --------; Export to VCF as a block-compressed file:. >>> hl.export_vcf(dataset, 'output/example.vcf.bgz'). Notes; -----; :func:`.export_vcf` writes the dataset to disk in VCF format as described in the; `VCF 4.2 spec <https://samtools.github.io/hts-specs/VCFv4.2.pdf>`__. Use the ``.vcf.bgz`` extension rather than ``.vcf`` in the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:15628,error,errors,15628,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['error'],['errors']
Availability,"idehat{\sigma_{js}}}\]; The estimator for identity-by-descent two is given by:. \[\widehat{k^{(2)}_{ij}} := \frac{\sum_{s \in S_{ij}}X_{is} X_{js}}{\sum_{s \in S_{ij}}\widehat{\sigma^2_{is}} \widehat{\sigma^2_{js}}}\]; The estimator for identity-by-descent zero is given by:. \[\begin{split}\widehat{k^{(0)}_{ij}} :=; \begin{cases}; \frac{\text{IBS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}} \widehat{\mu_{is}}^2(1 - \widehat{\mu_{js}})^2 + (1 - \widehat{\mu_{is}})^2\widehat{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}\end{split}\]; The estimator for identity-by-descent one is given by:. \[\widehat{k^{(1)}_{ij}} := 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}\]; Details; The PC-Relate method is described in “Model-free Estimation of Recent; Genetic Relatedness”. Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the GENESIS Bioconductor package .; pc_relate() differs from the reference; implementation in a couple key ways:. the principal components analysis does not use an unrelated set of; individuals; the estimators do not perform small sample correction; the algorithm does not provide an option to use population-wide; allele frequency estimates; the algorithm does not provide an option to not use “overall; standardization” (see R pcrelate documentation). Notes; The block_size controls memory usage and parallelism. If it is large; enough to hold an entire sample-by-sample matrix of 64-bit doubles in; memory, then only one Spark worker node can be used to compute matrix; operations. If it is too small, communication overhead will begin to; dominate the computation’s time. The author has found that on Google; Dataproc (where each core has about 3.75GB of memory), setting; block_size larger than 512 tends to cause memory exhaustion errors.; The minimum allele frequency filter is applied p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:134659,avail,available,134659,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['avail'],['available']
Availability,"identity-by-descent two; statistics, 'phik2k0' will compute the kinship; statistics and both identity-by-descent two and; zero, 'all' computes the kinship statistic and; all three identity-by-descent statistics. :return: A :py:class:`.KeyTable` mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two.; :rtype: :py:class:`.KeyTable`. """""". intstatistics = { ""phi"" : 0, ""phik2"" : 1, ""phik2k0"" : 2, ""all"" : 3 }[statistics]. return KeyTable(self.hc, self._jvdf.pcRelate(k, maf, block_size, min_kinship, intstatistics)). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this variant dataset to memory and/or disk. **Examples**. Persist the variant dataset to both memory and disk:. >>> vds_result = vds.persist(). **Notes**. The :py:meth:`~hail.VariantDataset.persist` and :py:meth:`~hail.VariantDataset.cache` methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.VariantDataset.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data.; ; .. warning ::; ; Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DI",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:178141,redundant,redundant,178141,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['redundant'],['redundant']
Availability,"ield_names:; dd[f.lower()].append(f). item_lower = item.lower(). field_matches = difflib.get_close_matches(item_lower, dd, n=5). s = [""{} instance has no field '{}'"".format(class_name, item)]; if field_matches:; s.append('\n Did you mean:'); for f in field_matches:; for orig_f in dd[f]:; s.append(""\n {}"".format(handler(orig_f))); if has_describe:; s.append(""\n Hint: use 'describe()' to show the names of all data fields.""); return ''.join(s). def check_collisions(caller, names, indices, override_protected_indices=None):; from hail.expr.expressions import ExpressionException. fields = indices.source._fields. if override_protected_indices is not None:. def invalid(e):; return e._indices in override_protected_indices. else:. def invalid(e):; return e._indices != indices. # check collisions with fields on other axes; for name in names:; if name in fields and invalid(fields[name]):; msg = f""{caller!r}: name collision with field indexed by {list(fields[name]._indices.axes)}: {name!r}""; error('Analysis exception: {}'.format(msg)); raise ExpressionException(msg). # check duplicate fields; for k, v in Counter(names).items():; if v > 1:; from hail.expr.expressions import ExpressionException. raise ExpressionException(f""{caller!r}: selection would produce duplicate field {k!r}""). def get_key_by_exprs(caller, exprs, named_exprs, indices, override_protected_indices=None):; from hail.expr.expressions import ExpressionException, analyze, to_expr. exprs = [indices.source[e] if isinstance(e, str) else e for e in exprs]; named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}. bindings = []. def is_top_level_field(e):; return e in indices.source._fields_inverse. existing_key_fields = []; final_key = []; for e in exprs:; analyze(caller, e, indices, broadcast=False); if not e._ir.is_nested_field:; raise ExpressionException(; f""{caller!r} expects keyword arguments for complex expressions\n""; f"" Correct: ht = ht.key_by('x')\n""; f"" Correct: ht = ht.key_by(ht.x)\n""; f"" Correct: ht = h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:10856,error,error,10856,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,2,['error'],['error']
Availability,"iers.; (#5294) Fix crash in; hl.trio_matrix when sample IDs are missing.; (#5295) Fix crash in; Table.index related to key field incompatibilities. Version 0.2.9; Released 2019-01-30. New features. (#5149) Added bitwise; transformation functions:; hl.bit_{and, or, xor, not, lshift, rshift}.; (#5154) Added; hl.rbind function, which is similar to hl.bind but expects a; function as the last argument instead of the first. Performance improvements. (#5107) Hail’s Python; interface generates tighter intermediate code, which should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with; missingness. Bug fixes. (#5088) Fixed name; separator in MatrixTable.make_table.; (#5104) Fixed; optimizer bug related to experimental functionality.; (#5122) Fixed error; constructing Table or MatrixTable objects with fields with; certain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockM",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:101211,error,error,101211,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"ies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and somet",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:84745,down,downstream,84745,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['down'],['downstream']
Availability,"ify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """""". valid_regions = {'us', 'us-central1', 'europe-west1'}; if region not in valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {valid_clouds}.'; ). datasets = get_datasets_metadata(); names = set([dataset for dataset in datasets]); if name not in names:; raise ValueError(f'{name} is not a dataset available in the' f' repository.'). versions = set(dataset['version'] for dataset in datasets[name]['versions']); if version not in versions:; raise ValueError(; f'Version {version!r} not available for dataset' f' {name!r}.\n' f'Available versions: {versions}.'; ). reference_genomes = set(dataset['reference_genome'] for dataset in datasets[name]['versions']); if reference_genome not in reference_genomes:; raise ValueError(; f'Reference genome build {reference_genome!r} not'; f' available for dataset {name!r}.\n'; f'Available reference genome builds:'; f' {reference_genomes}.'; ). clouds = set(k for dataset in datasets[name]['versions'] for k in dataset['url'].keys()); if cloud not in clouds:; raise ValueError(f'Cloud platform {cloud!r} not available for dataset {name}.\nAvailable platforms: {clouds}.'). regions = set(k for dataset in datasets[name]['versions'] for k in dataset['url'][cloud].keys()); if region not in regions:; raise ValueError(; f'Region {region!r} not available for dataset'; f' {name!r} on cloud platform {cloud!r}.\n'; f'Available regions: {regions}.'; ). path = [; dataset['url'][cloud][region];",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:3172,avail,available,3172,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,2,['avail'],['available']
Availability,"igenvalues). [18.084111467840707,; 9.984076405601847,; 3.540687229805949,; 2.655598108390125,; 1.596852701724399,; 1.5405241027955296,; 1.507713504116216,; 1.4744976712480349,; 1.467690539034742,; 1.4461994473306554]. [43]:. pcs.show(5, width=100). sscoresstrarray<float64>; ""HG00096""[1.22e-01,2.81e-01,-1.10e-01,-1.27e-01,6.68e-02,3.29e-03,-2.26e-02,4.26e-02,-9.30e-02,1.83e-01]; ""HG00099""[1.14e-01,2.89e-01,-1.06e-01,-6.78e-02,4.72e-02,2.87e-02,5.28e-03,-1.57e-02,1.75e-02,-1.98e-02]; ""HG00105""[1.09e-01,2.79e-01,-9.95e-02,-1.06e-01,8.79e-02,1.44e-02,2.80e-02,-3.38e-02,-1.08e-03,2.25e-02]; ""HG00118""[1.26e-01,2.95e-01,-7.58e-02,-1.08e-01,1.76e-02,7.91e-03,-5.25e-02,3.05e-02,2.00e-02,-7.78e-02]; ""HG00129""[1.06e-01,2.86e-01,-9.69e-02,-1.15e-01,1.03e-02,2.65e-02,-8.51e-02,2.49e-02,5.67e-02,-8.31e-03]; showing top 5 rows. Now that we’ve got principal components per sample, we may as well plot them! Human history exerts a strong effect in genetic datasets. Even with a 50MB sequencing dataset, we can recover the major human populations. [44]:. mt = mt.annotate_cols(scores = pcs[mt.s].scores). [45]:. p = hl.plot.scatter(mt.scores[0],; mt.scores[1],; label=mt.pheno.SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2'); show(p). [Stage 161:> (0 + 1) / 1]. Now we can rerun our linear regression, controlling for sample sex and the first few principal components. We’ll do this with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field. [46]:. gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 166:> (0 + 1) / 1]. We’ll first make a Q-Q plot to assess inflation…. [47]:. p = hl.plot.qq(gwas.p_value); show(p). That’s more like it! This shape is indicative of a well-controlled (but not especially well-powered) study. And now for the Manhattan plot:. [48]:. p = hl.plot.manhattan",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:22289,recover,recover,22289,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['recover'],['recover']
Availability,"ighly customizable plots. The Grammar of Graphics; The key idea here is that there’s not one magic function to make the plot you want. Plots are built up from a set of core primitives that allow for extensive customization. Let’s start with an example. We are going to plot y = x^2 for x from 0 to 10. First we make a hail table representing that data:. [2]:. ht = hl.utils.range_table(10); ht = ht.annotate(squared = ht.idx**2). Every plot starts with a call to ggplot, and then requires adding a geom to specify what kind of plot you’d like to create. [3]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_line(); fig.show(). Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2013-0.2.133-4c60fddb171a.log; SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. aes creates an “aesthetic mapping”, which maps hail expressions to aspects of the plot. There is a predefined list of aesthetics supported by every geom. Most take an x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or points:. [5]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_point(); fig.show(). There are optional aesthetics too. If we want, we could color the points based on whether they’re even or odd:. [6]:. fig = gg",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/09-ggplot.html:2025,avail,available,2025,docs/0.2/tutorials/09-ggplot.html,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html,1,['avail'],['available']
Availability,"ij}-2p_j)^2}},\]; with \(M_{ij} = 0\) for \(C_{ij}\) missing (i.e. mean genotype imputation). This scaling normalizes each variant column to have empirical variance \(1/m\), which gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the \(n \times n\) sample correlation or realized relationship matrix (RRM) \(K\) as simply. \[K = MM^T\]; Note that the only difference between the Realized Relationship Matrix and the Genetic Relationship Matrix (GRM) used in grm() is the variant (column) normalization: where RRM uses empirical variance, GRM uses expected variance under Hardy-Weinberg Equilibrium. Parameters:; force_block (bool) – Force using Spark’s BlockMatrix to compute kinship (advanced).; force_gramian (bool) – Force using Spark’s RowMatrix.computeGramian to compute kinship (advanced). Returns:Realized Relationship Matrix for all samples. Return type:KinshipMatrix. same(other, tolerance=1e-06)[source]¶; True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values.; Examples; This will return True:; >>> vds.same(vds). Notes; The tolerance parameter sets the tolerance for equality when comparing floating-point fields. More precisely, \(x\) and \(y\) are equal if. \[bs{x - y} \leq tolerance * \max{bs{x}, bs{y}}\]. Parameters:; other (VariantDataset) – variant dataset to compare against; tolerance (float) – floating-point tolerance for equality. Return type:bool. sample_annotations¶; Return a dict of sample annotations.; The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. Returns:dict. sample_ids¶; Return sampleIDs. Returns:List of sample IDs. Return type:list of str. sample_qc(root='sa.qc', keep_star=False)[source]¶; Compute per-sample QC metrics. Important; The genotype_schema() must be of type TGenotype in order to use this method. Annotations; sample_qc() computes 20 sample statistics from the ; genotype data and stores the r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:155121,toler,tolerance,155121,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['toler'],['tolerance']
Availability,"il = tm.proband_entry.AD[1] / hl.sum(tm.proband_entry.AD) < min_child_ab. failure = hl.missing(hl.tstruct(p_de_novo=hl.tfloat64, confidence=hl.tstr)). kid = tm.proband_entry; dad = tm.father_entry; mom = tm.mother_entry. kid_linear_pl = 10 ** (-kid.PL / 10); kid_pp = hl.bind(lambda x: x / hl.sum(x), kid_linear_pl). dad_linear_pl = 10 ** (-dad.PL / 10); dad_pp = hl.bind(lambda x: x / hl.sum(x), dad_linear_pl). mom_linear_pl = 10 ** (-mom.PL / 10); mom_pp = hl.bind(lambda x: x / hl.sum(x), mom_linear_pl). kid_ad_ratio = kid.AD[1] / hl.sum(kid.AD); dp_ratio = kid.DP / (dad.DP + mom.DP). def call_auto(kid_pp, dad_pp, mom_pp, kid_ad_ratio):; p_data_given_dn = dad_pp[0] * mom_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (dad_pp[1] * mom_pp[0] + dad_pp[0] * mom_pp[1]) * kid_pp[1] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (dad.DP + mom.DP) < min_dp_ratio) | ~(kid_ad_ratio >= min_child_ab), failure); .when((hl.sum(mom.AD) == 0) | (hl.sum(dad.AD) == 0), failure); .when(; (mom.AD[1] / hl.sum(mom.AD) > max_parent_ab) | (dad.AD[1] / hl.sum(dad.AD) > max_parent_ab), failure; ); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:30213,failure,failure,30213,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['failure'],['failure']
Availability,"il.expr.expressions.expression_utils. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.expressions.expression_utils. Source code for hail.expr.expressions.expression_utils; from typing import Dict, Set. from hail.typecheck import setof, typecheck. from ...ir import MakeTuple; from ..expressions import Expression, ExpressionException, expr_any; from .indices import Aggregation, Indices. @typecheck(caller=str, expr=Expression, expected_indices=Indices, aggregation_axes=setof(str), broadcast=bool); def analyze(caller: str, expr: Expression, expected_indices: Indices, aggregation_axes: Set = set(), broadcast=True):; from hail.utils import error, warning. indices = expr._indices; source = indices.source; axes = indices.axes; aggregations = expr._aggregations. warnings = []; errors = []. expected_source = expected_indices.source; expected_axes = expected_indices.axes. if source is not None and source is not expected_source:; bad_refs = []; for name, inds in get_refs(expr).items():; if inds.source is not expected_source:; bad_refs.append(name); errors.append(; ExpressionException(; ""'{caller}': source mismatch\n""; "" Expected an expression from source {expected}\n""; "" Found expression derived from source {actual}\n""; "" Problematic field(s): {bad_refs}\n\n""; "" This error is commonly caused by chaining methods together:\n""; "" >>> ht.distinct().select(ht.x)\n\n""; "" Correct usage:\n""; "" >>> ht = ht.distinct()\n""; "" >>> ht = ht.select(ht.x)"".format(; caller=caller, expected=expected_source, actual=source, bad_refs=list(bad_refs); ); ); ). # check for stray indices by subtracting expected axes from observed; if broadcast:; unexpec",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:1013,error,error,1013,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html,2,['error'],['error']
Availability,"ile at the destination.; force_row_major: :obj:`bool`; If ``True``, transform blocks in column-major format; to row-major format before writing.; If ``False``, write blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path). writer = BlockMatrixNativeWriter(path, overwrite, force_row_major, stage_locally); Env.backend().execute(BlockMatrixWrite(self._bmir, writer)). [docs] @typecheck_method(path=str, overwrite=bool, force_row_major=bool, stage_locally=bool); def checkpoint(self, path, overwrite=False, force_row_major=False, stage_locally=False):; """"""Checkpoint the block matrix. .. include:: ../_templates/write_warning.rst. Parameters; ----------; path: :class:`str`; Path for output file.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; force_row_major: :obj:`bool`; If ``True``, transform blocks in column-major format; to row-major format before checkpointing.; If ``False``, checkpoint blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:21341,checkpoint,checkpointing,21341,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['checkpoint'],['checkpointing']
Availability,"ile(path). [docs]def hadoop_is_dir(path: str) -> bool:; """"""Returns ``True`` if `path` both exists and is a directory. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().is_dir(path). [docs]def hadoop_stat(path: str) -> Dict[str, Any]:; """"""Returns information about the file or directory at a given path. Notes; -----; Raises an error if `path` does not exist. The resulting dictionary contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`dict`; """"""; return Env.fs().stat(path).to_legacy_dict(). [docs]def hadoop_ls(path: str) -> List[Dict[str, Any]]:; """"""Returns information about files at `path`. Notes; -----; Raises an error if `path` does not exist. If `path` is a file, returns a list with one element. If `path` is a; directory, returns an element for each file contained in `path` (does not; search recursively). Each dict element of the result list contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:5814,error,error,5814,docs/0.2/_modules/hail/utils/hadoop_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html,2,['error'],['error']
Availability,"ile); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class”.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:1575,error,error,1575,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,4,['error'],['error']
Availability,"ile, True)); + hl.str("" on line ""); + hl.str(row.row_id - get_file_start(row) + 1); + hl.str("" at value '""); + hl.str(row.split_array[idx]); + hl.str(""':\n""); + hl.str(msg); ). def parse_type_or_error(hail_type, row, idx, not_entries=True):; value = row.split_array[idx]; if hail_type == hl.tint32:; parsed_type = hl.parse_int32(value); elif hail_type == hl.tint64:; parsed_type = hl.parse_int64(value); elif hail_type == hl.tfloat32:; parsed_type = hl.parse_float32(value); elif hail_type == hl.tfloat64:; parsed_type = hl.parse_float64(value); else:; parsed_type = value. if not_entries:; error_clarify_msg = hl.str("" at row field '"") + hl.str(hl_row_fields[idx]) + hl.str(""'""); else:; error_clarify_msg = (; hl.str("" at column id '""); + hl.str(hl_columns[idx - num_of_row_fields]); + hl.str(""' for entry field 'x' ""); ). return hl.if_else(; hl.is_missing(value),; hl.missing(hail_type),; hl.case(); .when(~hl.is_missing(parsed_type), parsed_type); .or_error(error_msg(row, idx, f""error parsing value into {hail_type!s}"" + error_clarify_msg)),; ). num_of_row_fields = len(row_fields.keys()); add_row_id = False; if len(row_key) == 0:; add_row_id = True; row_key = ['row_id']. if sep is not None:; if delimiter is not None:; raise ValueError(f'expecting either sep or delimiter but received both: ' f'{sep}, {delimiter}'); delimiter = sep; del sep. if delimiter is None:; delimiter = '\t'; if len(delimiter) != 1:; raise FatalError('delimiter or sep must be a single character'). if add_row_id:; if 'row_id' in row_fields:; raise FatalError(; ""import_matrix_table reserves the field name 'row_id' for"" 'its own use, please use a different name'; ). for k, v in row_fields.items():; if v not in {tint32, tint64, tfloat32, tfloat64, tstr}:; raise FatalError(; f'import_matrix_table expects field types to be one of:'; f""'int32', 'int64', 'float32', 'float64', 'str': field {k!r} had type '{v}'""; ). if entry_type not in {tint32, tint64, tfloat32, tfloat64, tstr}:; raise FatalError(; """"""import_matrix_t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:76807,error,error,76807,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['error'],['error']
Availability,"illustrate. The movie dataset comes in multiple parts. Here are a few questions we might naturally ask about the dataset:. What is the mean rating per genre?; What is the favorite movie for each occupation?; What genres are most preferred by women vs men?. We’ll use joins to combine datasets in order to answer these questions.; Let’s initialize Hail, fetch the tutorial data, and load three tables: users, movies, and ratings. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'). users = hl.read_table('data/users.ht'); movies = hl.read_table('data/movies.ht'); ratings = hl.read_table('data/ratings.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2010-0.2.133-4c60fddb171a.log; 2024-10-04 20:10:22.038 Hail: INFO: Movie Lens files found!. The Key to Understanding Joins; To understand joins in Hail, we need to revisit one of the crucial properties of tables: the key.; A table has an ordered list of fields known as the key. Our users table has one key, the id field. We can see all the fields, as well as the keys, of a table by calling describe(). [2]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. key is a struct expression of all of the key fields, so we can refer to the key of a table without explicitly specifying",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/06-joins.html:1789,avail,available,1789,docs/0.2/tutorials/06-joins.html,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html,1,['avail'],['available']
Availability,"ines where; seqname is not consistent with the reference genome.; min_partitions (int or None) – Minimum number of partitions (passed to import_table).; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns:; Table. hail.experimental.get_gene_intervals(gene_symbols=None, gene_ids=None, transcript_ids=None, verbose=True, reference_genome=None, gtf_file=None)[source]; Get intervals of genes or transcripts.; Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable.; On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz; Example; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) . Parameters:. gene_symbols (list of str, optional) – Gene symbols (e.g. PCSK9).; gene_ids (list of str, optional) – Gene IDs (e.g. ENSG00000223972).; transcript_ids (list of str, optional) – Transcript IDs (e.g. ENSG00000223972).; verbose (bool) – If True, print which genes and transcripts were matched in the GTF file.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use (passed along to import_gtf).; gtf_file (str) – GTF file to load. If none is provided, but reference_genome is one of; GRCh37 or GRCh38, a default will be used (on Google Cloud Platform). Returns:; list of Interval. hail.experimental.export_entries_by_col(mt, path, batch_size=256, bgzip=True, header_jso",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:27672,avail,available,27672,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['avail'],['available']
Availability,"ing columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:125226,error,errors,125226,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,"inner dimension. unpersist; Unpersists this block matrix from memory/disk. write; Writes the block matrix. write_from_entry_expr; Writes a block matrix from a matrix table entry expression. property T; Matrix transpose. Returns:; BlockMatrix. abs()[source]; Element-wise absolute value. Returns:; BlockMatrix. property block_size; Block size. Returns:; int. cache()[source]; Persist this block matrix in memory.; Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; BlockMatrix – Cached block matrix. ceil()[source]; Element-wise ceiling. Returns:; BlockMatrix. checkpoint(path, overwrite=False, force_row_major=False, stage_locally=False)[source]; Checkpoint the block matrix. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before checkpointing.; If False, checkpoint blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static default_block_size()[source]; Default block side length. densify()[source]; Restore all dropped blocks as explicit blocks of zeros. Returns:; BlockMatrix. diagonal()[source]; Extracts diagonal elements as a row vector. Returns:; BlockMatrix. property element_type; The type of the elements. entries(keyed=True)[source]; Returns a table with the indices and value of each block matrix entry.; Examples; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[5, 7], [2, 8]]), 2); >>> entries_table = block_matrix.entries(); >>> entries_table.show(); +-------+-------+----------+; | i | j | entry |; +-------+-------+----------+; | int64 | int64 | float64 |; +-------+-------+----------+; | 0 | 0 | 5.00e+00 |; | 0 | 1 | 7.00e+00 |; |",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:11193,checkpoint,checkpointing,11193,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['checkpoint'],['checkpointing']
Availability,"ins one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`~.LocusExpression.in_autosome`. - Auto -- in autosome or in PAR or female child; - HemiX -- in non-PAR of X and male child; - HemiY -- in non-PAR of Y and male child. `Any` refers to the set \{ HomRef, Het, HomVar, NoCall \} and `~`; de",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6984,error,errors,6984,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"ins)); ). if _result_from_hist_agg_f is None:; _result_from_hist_agg_f = hl.experimental.define_function(; wrap_errors, hl.tfloat64, hl.tfloat64, hl.tint32, hl.tdict(hl.tint32, hl.tint64); ). return _result_from_hist_agg_f(start, end, bins, freq_dict). [docs]@typecheck(x=expr_float64, y=expr_float64, label=nullable(oneof(expr_str, expr_array(expr_str))), n_divisions=int); def downsample(x, y, label=None, n_divisions=500) -> ArrayExpression:; """"""Downsample (x, y) coordinate datapoints. Parameters; ----------; x : :class:`.NumericExpression`; X-values to be downsampled.; y : :class:`.NumericExpression`; Y-values to be downsampled.; label : :class:`.StringExpression` or :class:`.ArrayExpression`; Additional data for each (x, y) coordinate. Can pass in multiple fields in an :class:`.ArrayExpression`.; n_divisions : :obj:`int`; Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns; -------; :class:`.ArrayExpression`; Expression for downsampled coordinate points (x, y). The element type of the array is; :class:`.ttuple` of :py:data:`.tfloat64`, :py:data:`.tfloat64`, and :class:`.tarray` of :py:data:`.tstr`; """"""; if label is None:; label = hl.missing(hl.tarray(hl.tstr)); elif isinstance(label, StringExpression):; label = hl.array([label]); return _agg_func(; 'Downsample', [x, y, label], tarray(ttuple(tfloat64, tfloat64, tarray(tstr))), init_op_args=[n_divisions]; ). @typecheck(expr=expr_any, n=expr_int32); def _reservoir_sample(expr, n):; return _agg_func('ReservoirSample', [expr], tarray(expr.dtype), [n]). [docs]@typecheck(gp=expr_array(expr_float64)); def info_score(gp) -> StructExpression:; r""""""Compute the IMPUTE information score. Examples; --------; Calculate the info score per variant:. >>> gen_mt = hl.import_gen('data/example.gen', sample_file='data/example.sample'); >>> gen_mt = gen_mt.annotate_rows(info_score = hl.agg.info_score(gen_mt.GP)). Calculate group-specific info scores per variant:. >>> gen_mt = hl.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:44692,down,downsampled,44692,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['down'],['downsampled']
Availability,"ins=10, range=((0, 1), None)). Parameters:. x (NumericExpression) – Expression for x-axis (from a Hail table).; y (NumericExpression) – Expression for y-axis (from the same Hail table as x).; bins (int or [int, int]) – The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range (None or ((float, float), (float, float))) – The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be considered outliers; and not tallied in the histogram. If this value is None, or either of the inner lists is None,; the range will be computed from the data.; width (int) – Plot width (default 600px).; height (int) – Plot height (default 600px).; title (str) – Title of the plot.; colors (Sequence[str]) – List of colors (hex codes, or strings as described; here). Compatible with one of the many; built-in palettes available here.; log (bool) – Plot the log10 of the bin counts. Returns:; bokeh.plotting.figure. hail.plot.scatter(x, y, label=None, title=None, xlabel=None, ylabel=None, size=4, legend=True, hover_fields=None, colors=None, width=800, height=800, collect_all=None, n_divisions=500, missing_label='NA')[source]; Create an interactive scatter plot.; x and y must both be either:; - a NumericExpression from the same Table.; - a tuple (str, NumericExpression) from the same Table. If passed as a tuple the first element is used as the hover label.; If no label or a single label is provided, then returns bokeh.plotting.figure; Otherwise returns a bokeh.models.layouts.Column containing:; - a bokeh.models.widgets.inputs.Select dropdown selection widget for labels; - a bokeh.plotting.figure containing the interactive scatter plot; Points will be colored by one of the labels defined in the label using the color scheme defined in; the corresponding entry of colors if provided (othe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:5899,avail,available,5899,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['avail'],['available']
Availability,"ion Database; Database Query. Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Annotation Database. View page source. Annotation Database. Warning; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below.; In addition, a search bar is provided if looking for a specific annotation; within our curated collection.; Use the “Copy to Clipboard” button to copy the generated Hail code, and paste; the command into your own Hail script. Search. Database Query; . Copy to Clipboard; . Hail generated code:.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/annotation_database_ui.html:1220,avail,available,1220,docs/0.2/annotation_database_ui.html,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html,1,['avail'],['available']
Availability,"ion of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probability p = Prob(\(Z < x\)) with \(Z\); a normal random variable. Defaults to a standard normal random variable. Parameters:. x (float or Expression of type tfloat64); mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – S",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:20913,fault,fault,20913,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['fault'],['fault']
Availability,"ion of type tint32) – Index or slice. Returns:; Expression – Element or array slice. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __mod__(other)[source]; Positionally compute the left modulo the right.; Examples; >>> hl.eval(a1 % 2); [0, 1, 0, 1, 0, 1]. Parameters:; other (NumericExpression or ArrayNumericExpression). Returns:; ArrayNumericExpression. __mul__(other)[source]; Positionally multiply by an array or a scalar.; Examples; >>> hl.eval(a2 * 5); [5, -5, 5, -5, 5, -5]. >>> hl.eval(a1 * a2); [0, -1, 2, -3, 4, -5]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to multiply by. Returns:; ArrayNumericExpression – Array of positional products. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__()[source]; Negate elements of the array.; Examples; >>> hl.eval(-a1); [0, -1, -2, -3, -4, -5]. Returns:; ArrayNumericExpression – Array expression of the same type. __pow__(other)[source]; Positionally raise to the power of an array or a scalar.; Examples; >>> hl.eval(a1 ** 2); [0.0, 1.0, 4.0, 9.0, 16.0, 25.0]. >>> hl.eval(a1 ** a2); [0.0, 1.0, 2.0, 0.3333333333333333, 4.0, 0.2]. Parameters:; other (NumericExpression or ArrayNumericExpression). Returns:; ArrayNumericExpression. __sub__(other)[source]; Positionally subtract an array or a scalar.; Examples; >>> hl.eval(a2 - 1); [0, -2, 0, -2, 0, -2]. >>> hl.eval(a1 - a2); [-1, 2, 1, 4, 3, 6]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to subtract. Returns:; ArrayNumericExpression – Array of positio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:3696,error,error,3696,docs/0.2/hail.expr.ArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html,1,['error'],['error']
Availability,"ion() considers only the rows for which both row; fields weight_expr and ld_score_expr are defined. Rows with missing; values in either field are removed prior to fitting the LD score; regression model. Parameters:. weight_expr (Float64Expression) – Row-indexed expression for the LD scores used to derive; variant weights in the model.; ld_score_expr (Float64Expression) – Row-indexed expression for the LD scores used as covariates; in the model.; chi_sq_exprs (Float64Expression or list of) – Float64Expression; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions for chi-squared; statistics resulting from genome-wide association; studies (GWAS).; n_samples_exprs (NumericExpression or list of) – NumericExpression; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions indicating the number of; samples used in the studies that generated the test; statistics supplied to chi_sq_exprs.; n_blocks (int) – The number of blocks used in the jackknife approach to; estimating standard errors.; two_step_threshold (int) – Variants with chi-squared statistics greater than this; value are excluded in the first step of the two-step; procedure used to fit the model.; n_reference_panel_variants (int, optional) – Number of variants used to estimate the; SNP-heritability \(h_g^2\). Returns:; Table – Table keyed by phenotype with intercept and heritability estimates; for each phenotype passed to the function. hail.experimental.write_expression(expr, path, overwrite=False)[source]; Write an Expression.; In the same vein as Python’s pickle, write out an expression; that does not have a source (such as one that comes from; Table.aggregate with _localize=False).; Example; >>> ht = hl.utils.range_table(100).annotate(x=hl.rand_norm()); >>> mean_norm = ht.aggregate(hl.agg.mean(ht.x), _localize=False); >>> mean_norm; >>> hl.eval(mean_norm); >>> hl.experimental.write_expression(mean_norm, 'output/expression.he'). Parameters:. expr (Expressi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:14589,error,errors,14589,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['error'],['errors']
Availability,"ion. Note; It is possible to use gnomAD reference allele frequencies with the following:; >>> gnomad_sites = hl.experimental.load_dataset('gnomad_genome_sites', version='3.1.2') ; >>> charr_result = hl.compute_charr(mt, ref_af=(1 - gnomad_sites[mt.row_key].freq[0].AF)) . If the dataset is loaded from a gvcf and has NON_REF alleles, drop the last allele with the following or load it with the hail vcf combiner:; >>> mt = mt.key_rows_by(locus=mt.locus, alleles=mt.alleles[:-1]). Parameters:. ds (MatrixTable or VariantDataset) – Dataset.; min_af – Minimum reference allele frequency to filter variants.; max_af – Maximum reference allele frequency to filter variants.; min_dp – Minimum sequencing depth to filter variants.; max_dp – Maximum sequencing depth to filter variants.; min_gq – Minimum genotype quality to filter variants; ref_AF – Reference AF expression. Necessary when the sample size is below 10,000. Returns:; Table. hail.methods.mendel_errors(call, pedigree)[source]; Find Mendel errors; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:47753,error,errors,47753,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability,"ion_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:103414,error,errors,103414,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"ion` that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a :class:`.CaseBuilder`. Parameters; ----------; condition: :class:`.BooleanExpression`; then : :class:`.Expression`. Returns; -------; :class:`.CaseBuilder`; Mutates and returns `self`.; """"""; self._unify_type(then.dtype); self._cases.append((condition, then)); return self. [docs] @typecheck_method(then=expr_any); def default(self, then):; """"""Finish the case statement by adding a default case. Notes; -----; If no condition from a :meth:`~.CaseBuilder.when` call is ``True``,; then `then` is returned. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; return then; self._unify_type(then.dtype); return self._finish(then). [docs] def or_missing(self):; """"""Finish the case statement by returning missing. Notes; -----; If no condition from a :meth:`.CaseBuilder.when` call is ``True``, then; the result is missing. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_missing' cannot be called without at least one 'when' call""); from hail.expr.functions import missing. return self._finish(missing(self._ret_type)). [docs] @typecheck_method(message=expr_str); def or_error(self, message):; """"""Finish the case statement by throwing an error with the given message. Notes; -----; If no condition from a :meth:`.CaseBuilder.when` call is ``True``, then; an error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_error' cannot be called without at least one 'when' call""); error_expr = construct_expr(ir.Die(message._ir, self._ret_type), self._ret_type); return self._finish(error_expr). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/builders.html:8126,error,error,8126,docs/0.2/_modules/hail/expr/builders.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/builders.html,4,['error'],['error']
Availability,"ional US and EU buckets to regional US-CENTRAL1 and; EUROPE-WEST1 buckets. These buckets are requester pays which means; unless your cluster is in the US-CENTRAL1 or EUROPE-WEST1 region, you; will pay a per-gigabyte rate to read from the Annotation DB or Datasets; API. We must make this change because reading from a multi-regional; bucket into a regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, Table.annotate,; and Table.filter.; (#14242) Add; examples to Table.sample, Table.head, and; Table.semi_join. New Features. (#14206) Introduce; hailctl config set http/timeout_in_seconds which Batch and QoB; users can use to increase the timeout on their laptops. Laptops tend; to have flaky internet connections and a timeout of 300 seconds; produces a more robust experience.; (#14178) Reduce VDS; Combiner runtime slightly by computing the maximum ref block length; without executing the combination pipeline twice.; (#14207) VDS; Combiner now verifies that every GVCF path and sample name is unique. Bug Fixes. (#14300) Require; orjson<3.9.12 to avoid a segfault introduced in orjson 3.9.12; (#14071) Use indexed; VEP cache files for GRCh38 on both dataproc and QoB.; (#14232) Allow use; of large numbers of fields on a table without triggering; ClassTooLargeException: Class too large:.; (#14246)(#14245); Fix a bug, introduced in 0.2.114, in which; Table.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output partitions.; (#14202) Support; coercing {} (the empty dictionary) into any Struct type (with all; missing fields).; (#14239) Remo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:16133,robust,robust,16133,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['robust'],['robust']
Availability,"ions:; assert isinstance(agg, Aggregation); refs = get_refs(*agg.exprs); agg_axes = agg.agg_axes(). # check for stray indices; unexpected_agg_axes = agg_axes - expected_agg_axes; if unexpected_agg_axes:; # one or more out-of-scope fields; bad_refs = []; for name, inds in refs.items():; bad_axes = inds.axes.intersection(unexpected_agg_axes); if bad_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0. errors.append(; ExpressionException(; ""scope violation: '{caller}' supports aggregation over indices {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}"".format(; caller=caller,; expected=list(aggregation_axes),; axes=list(agg_axes),; stray=list(unexpected_agg_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)); for name, inds in bad_refs; ),; ); ); ); else:; errors.append(ExpressionException(""'{}' does not support aggregation"".format(caller))). for w in warnings:; warning('{}'.format(w.msg)); if errors:; for e in errors:; error('{}'.format(e.msg)); raise errors[0]. @typecheck(expression=expr_any); def eval_timed(expression):; """"""Evaluate a Hail expression, returning the result and the times taken for; each stage in the evaluation process. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; (Any, dict); Result of evaluating `expression` and a dictionary of the timings; """""". from hail.utils.java import Env. analyze('eval', expression, Indices(expression._indices.source)); if expression._indices.source is None:; ir_type = expression._ir.typ; expression_type = expression.dtype; if ir_type != expression.dtype:; raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); ir = expression._ir; else:; uid = Env.get_uid(); ir = expression._indices.source.select_globals(**{uid: expression}).index_globals()[uid]._ir. return Env.backend().execute(M",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:4346,error,errors,4346,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html,6,['error'],"['error', 'errors']"
Availability,"iplications use a matrix-block-size of 1024 by 1024.; >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using filter().; >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). Method; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with allele frequencies; \(p_s\), is given by:. \[\widehat{\phi_{ij}} := \frac{1}{|S_{ij}|}\sum_{s \in S_{ij}}\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals.; PC-Relate slightly modifies the usual estimator for relatedness:; occurences of population allele frequency are replaced with an; “individual-specific allele frequency”. This modification allows the; method to correctly weight an allele according to an individual’s unique; ancestry profile.; The “individual-specific allele frequency” at a given genetic locus is; modeled by PC-Relate as a linear function of their first k principal; component coordinates. As such, the efficacy of this method rests on two; assumptions:. an individual’s first k principal component coordinates fully; describe their allele-frequency-relevant ancestry, and; the relationship between ancestry (as described by",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:131097,down,down,131097,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['down'],['down']
Availability,"irectory. In [4]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr.write('Done!\n'). All files are present and accounted for!. Loading data from disk¶; Hail has its own internal data representation, called a Variant Dataset; (VDS). This is both an on-disk file format and a Python; object. See the; overview for a complete story.; Here, we read a VDS from disk.; This dataset was created by downsampling a public 1000 genomes VCF to; about 50 MB. In [5]:. vds = hc.read('data/1kg.vds'). Getting to know our data¶; It’s important to have easy ways to slice, dice, query, and summarize a; dataset. Some of these methods are demonstrated below.; The; summarize; method is useful for providing a broad overview of the data contained in; a dataset. In [6]:. vds.summarize().report(). Samples: 1000; Variants: 10961; Call Rate: 0.983163; Contigs: ['X', '12', '8', '19', '4', '15', '11', '9', '22', '13', '16', '5', '10', '21', '6', '1', '17', '14', '20', '2', '18', '7', '3']; Multiallelics: 0; SNPs: 10961; MNPs: 0; Insertions: 0; Deletions: 0; Complex Alleles: 0; Star Alleles: 0; Max Alleles: 2. The; query_variants; method is the first time we’ll see the Hail expression; language. The expression; language allows for a variety of incredibly expressive queries and; computations, but is probably the most complex part of Hail. See the; pair of tutorials on the expression language to learn more!; Here, we ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:3007,down,downsampling,3007,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['down'],['downsampling']
Availability,"is VDS. ld_prune; Prune variants in linkage disequilibrium (LD). linreg; Test each variant for association using linear regression. linreg3; Test each variant for association with multiple phenotypes using linear regression. linreg_burden; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the linear regression model. linreg_multi_pheno; Test each variant for association with multiple phenotypes using linear regression. lmmreg; Use a kinship-based linear mixed model to estimate the genetic component of phenotypic variance (narrow-sense heritability) and optionally test each variant for association. logreg; Test each variant for association using logistic regression. logreg_burden; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the logistic regression model. make_table; Produce a key with one row per variant and one or more columns per sample. mendel_errors; Find Mendel errors; count per variant, individual and nuclear family. min_rep; Gives minimal, left-aligned representation of alleles. naive_coalesce; Naively descrease the number of partitions. num_partitions; Number of partitions. pc_relate; Compute relatedness estimates between individuals using a variant of the PC-Relate method. pca; Run Principal Component Analysis (PCA) on the matrix of genotypes. persist; Persist this variant dataset to memory and/or disk. query_genotypes; Performs aggregation queries over genotypes, and returns Python object(s). query_genotypes_typed; Performs aggregation queries over genotypes, and returns Python object(s) and type(s). query_samples; Performs aggregation queries over samples and sample annotations, and returns Python object(s). query_samples_typed; Performs aggregation queries over samples and sample annotations, and returns Python object(s) and type(s). query_variants; Performs aggregation queries over variants and variant annotations, and returns Python object(s). quer",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:5657,error,errors,5657,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,"is equal to 5 any row in any row of the table:; >>> if table1.any(table1.C1 == 5):; ... print(""At least one row has C1 equal 5.""). Parameters:; expr (BooleanExpression) – Boolean expression. Returns:; bool – True if the predicate evaluated for True for any row, otherwise False. cache()[source]; Persist this table in memory.; Examples; Persist the table in memory:; >>> table = table.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; Table – Cached table. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False)[source]; Checkpoint the table to disk by writing and reading. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; Table. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_table(). It is; possible to read the file at this path later with read_table().; Examples; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). collect(_localize=True, *, _timed=False)[source]; Collect the rows of the table into a local list.; Examples; Collect a list of all X records:; >>> all_xs = [row['X'] for row in table1.select(table1.X).collect()]. Notes; This method returns a list whose elements are of type Struct. Fields; of these structs can be accessed similarly to fields on a table, using dot; methods (struct.foo) or string indexing (struct['foo']). Warning; Using this method can cause out of memory errors. Only collect small tables. Returns:; list of Struct – List of rows. collect_by_key(name='values')[source]; Collect values for each unique key into an array. Note; Requires a keyed table. Examples; >>> t1 = hl.Table.parallelize([",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:15764,checkpoint,checkpoint,15764,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['checkpoint'],['checkpoint']
Availability,"is; Eplilogue. Introduction to the expression language; Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Overview. View page source. Overview¶; This notebook is designed to provide a broad overview of Hail’s; functionality, with emphasis on the functionality to manipulate and; query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [4]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:1395,error,error,1395,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['error'],['error']
Availability,"is; will also allow the use of all python versions >= 3.6. By building; hail from source, it is still possible to use older versions of; Spark. New features. (#10290) Added; hl.nd.solve.; (#10187) Added; NDArrayNumericExpression.sum. Performance improvements. (#10233) Loops; created with hl.experimental.loop will now clean up unneeded; memory between iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to be; associated with their original source file. Bug fixes. (#10182) Fixed; serious memory leak in certain uses of filter_intervals.; (#10133) Fix bug; where some pipelines incorrectly infer missingness, leading to a type; error.; (#10134) Teach; hl.king to treat filtered entries as missing values.; (#10158) Fixes hail; usage in latest versions of jupyter that rely on asyncio.; (#10174) Fixed bad; error message when incorrect return type specified with hl.loop. Version 0.2.63; Released 2021-03-01. (#10105) Hail will; now return frozenset and hail.utils.frozendict instead of; normal sets and dicts. Bug fixes. (#10035) Fix; mishandling of NaN values in hl.agg.hist, where they were; unintentionally included in the first bin.; (#10007) Improve; error message from hadoop_ls when file does not exist. Performance Improvements. (#10068) Make; certain array copies faster.; (#10061) Improve; code generation of hl.if_else and hl.coalesce. Version 0.2.62; Released 2021-02-03. New features. (#9936) Deprecated; hl.null in favor of hl.missing for naming consistency.; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now; eagerly deletes temporary files produced by some BlockMatrix; operations.; (#9835) hl.any; and hl.all now also support a single collection argument and a; varargs of Boolean expressions.; (#9816); hl.pc_relate now includes values on the d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:61169,error,error,61169,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"ist_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Release 2022-10-18. Bug Fixes. (#12305): Fixed a; rare crash reading tables/matrixtables with _intervals. Version 0.2.102; Released 2022-10-06. New Features. (#12218) Missing; values are now supported in primitive columns in Table.to_pandas.; (#12254); Cross-product-style legends for data groups have been replaced with; factored ones (consistent with ggplot2’s implementation) for; hail.ggplot.geom_point, and support has been added for custom; legend group labels.; (#12268); VariantDataset now implements union_rows for combining; datasets with the same samples but disjoint variants. Bug Fixes. (#12278) Fixed bug; made more likely by 0.2.101 in which Hail errors when interacting; with a NumPy integer or floating point type.; (#12277) Fixed bug; in reading tables/matrixtables with partition intervals that led to; error or segfault. Version 0.2.101; Released 2022-10-04. New Features. (#12218) Support; missing values in primitive columns in Table.to_pandas.; (#12195) Add a; impute_sex_chr_ploidy_from_interval_coverage to impute sex ploidy; directly from a coverage MT.; (#12222); Query-on-Batch pipelines now add worker jobs to the same batch as the; driver job instead of producing a new batch per stage.; (#12244) Added; support for custom labels for per-group legends to; hail.ggplot.geom_point via the legend_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Dep",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:44718,error,errors,44718,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"it.select('n_iterations', 'converged', 'exploded'),; ). def logistic_score_test(X, y, null_fit):; m = X.shape[1]; m0 = null_fit.b.shape[0]; b = hl.nd.hstack([null_fit.b, hl.nd.zeros((hl.int32(m - m0)))]). X0 = X[:, 0:m0]; X1 = X[:, m0:]. mu = hl.expit(X @ b). score_0 = null_fit.score; score_1 = X1.T @ (y - mu); score = hl.nd.hstack([score_0, score_1]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). solve_attempt = hl.nd.solve(fisher, score, no_crash=True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterati",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:43413,toler,tolerance,43413,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,"iteral(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Slice or index into the string.; Examples; >>> hl.eval(s[:15]); 'The quick brown'. >>> hl.eval(s[0]); 'T'. Parameters:; item (slice or Expression of type tint32) – Slice or character index. Returns:; StringExpression – Substring or character at index item. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(substr)[source]; Returns whether substr is contained in the string.; Examples; >>> hl.eval(s.contains('fox')); True. >>> hl.eval(s.contains('dog')); False. Note; This method is case-sensitive. Parameters:; substr (StringExpression). Returns:; BooleanExpression. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. endswith(substr)[source]; Returns whether substr is a suffix of the string.; Examples; >>> hl.eval",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.StringExpression.html:3445,error,error,3445,docs/0.2/hail.expr.StringExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.StringExpression.html,1,['error'],['error']
Availability,"itions (passed to import_table).; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns:; Table. hail.experimental.get_gene_intervals(gene_symbols=None, gene_ids=None, transcript_ids=None, verbose=True, reference_genome=None, gtf_file=None)[source]; Get intervals of genes or transcripts.; Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable.; On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz; Example; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) . Parameters:. gene_symbols (list of str, optional) – Gene symbols (e.g. PCSK9).; gene_ids (list of str, optional) – Gene IDs (e.g. ENSG00000223972).; transcript_ids (list of str, optional) – Transcript IDs (e.g. ENSG00000223972).; verbose (bool) – If True, print which genes and transcripts were matched in the GTF file.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use (passed along to import_gtf).; gtf_file (str) – GTF file to load. If none is provided, but reference_genome is one of; GRCh37 or GRCh38, a default will be used (on Google Cloud Platform). Returns:; list of Interval. hail.experimental.export_entries_by_col(mt, path, batch_size=256, bgzip=True, header_json_in_file=True, use_string_key_as_file_name=False)[source]; Export entries of the mt by column as separate text files.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:27779,avail,available,27779,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['avail'],['available']
Availability,"iven_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (parent.DP) < min_dp_ratio) | (kid_ad_ratio < min_child_ab), failure); .when((hl.sum(parent.AD) == 0), failure); .when(parent.AD[1] / hl.sum(parent.AD) > max_parent_ab, failure); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.3, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). de_novo_call = (; hl.case(); .when(~het_hom_hom | kid_ad_fail, failure); .when(autosomal, hl.bind(call_auto, kid_pp, dad_pp, mom_pp, kid_ad_ratio)); .when(hemi_x | hemi_mt, hl.bind(call_hemi, kid_pp, mom, mom_pp, kid_ad_ratio)); .when(hemi_y, hl.bind(call_hemi, kid_pp, dad, dad_pp, kid_ad_ratio)); .or_missing(); ). tm = tm.annotate_entries(__call=de_novo_call); tm = tm.filter_entries(hl.is_defined(tm.__call)); entries = tm.entries(); return entries.select(; '__site_freq',; 'proband',; 'father',; 'mother',; 'proband_entry',; 'father_entry',; 'mother_entry',; 'is_female',; **entries.__call,; ).rename({'__site_freq': 'prior'}). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:33130,failure,failure,33130,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['failure'],['failure']
Availability,"ix with 1 row, 1 column,; and element bm[0, 0].; bm[2, :] is a block matrix with 1 row, 10 columns,; and elements from row 2 of bm.; bm[:3, -1] is a block matrix with 3 rows, 1 column,; and the first 3 elements of the last column of bm.; bm[::2, ::2] is a block matrix with 5 rows, 5 columns,; and all evenly-indexed elements of bm. Use filter(), filter_rows(), and filter_cols() to; subset to non-slice subsets of rows and columns, e.g. to rows [0, 2, 5].; Block-sparse representation; By default, block matrices compute and store all blocks explicitly.; However, some applications involve block matrices in which:. some blocks consist entirely of zeroes.; some blocks are not of interest. For example, statistical geneticists often want to compute and manipulate a; banded correlation matrix capturing “linkage disequilibrium” between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra.; To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a block-sparse matrix. Otherwise, we say the matrix; is block-dense. The property is_sparse() encodes this state.; Dropped blocks are not stored in memory or on write(). In fact,; blocks that are dropped prior to an action like export() or; to_numpy() are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes.; Block-sparse matrices may be created with; sparsify_band(),; sparsify_rectangles(),; sparsify_row_intervals(),; and sparsify_triangl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:5612,down,downstream,5612,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['down'],['downstream']
Availability,"ix, use write_from_entry_expr() directly to; avoid writing the result twice. See write_from_entry_expr() for; further documentation. Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use write_from_entry_expr() to write to external storage.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). classmethod from_ndarray(ndarray_expression, block_size=4096)[source]; Create a BlockMatrix from an ndarray. classmethod from_numpy(ndarray, block_size=None)[source]; Distributes a NumPy ndarray; as a block matrix.; Examples; >>> import numpy as np; >>> a = np.random.rand(10, 20); >>> bm = BlockMatrix.from_numpy(a). Notes; The ndarray must have two dimensions, each of non-zero size.; The number of entries must be less than \(2^{31}\). Parameters:. ndarray (numpy.ndarray) – ndarray with two dimensions, each of non-zero size.; block_size (int, optional) – Block size. Default given by default",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:22610,error,error,22610,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['error'],['error']
Availability,"ix. Parameters; ----------; expr : bool or :class:`.BooleanExpression`; Filter expression.; keep : bool; Keep entries where `expr` is true. Returns; -------; :class:`.MatrixTable`; Filtered matrix table. Examples; --------. Keep entries where the sum of `AD` is greater than 10 and `GQ` is greater than 20:. >>> dataset_result = dataset.filter_entries((hl.sum(dataset.AD) > 10) & (dataset.GQ > 20)). Warning; -------; When `expr` evaluates to missing, the entry will be removed regardless of; `keep`. Note; ----; This method does not support aggregation. Notes; -----; The expression `expr` will be evaluated for every entry of the table.; If `keep` is ``True``, then entries where `expr` evaluates to ``True``; will be kept (the filter removes the entries where the predicate; evaluates to ``False``). If `keep` is ``False``, then entries where; `expr` evaluates to ``True`` will be removed (the filter keeps the; entries where the predicate evaluates to ``False``). Filtered entries are removed entirely from downstream operations. This; means that the resulting matrix table has sparsity -- that is, that the; number of entries is **smaller** than the product of :meth:`count_rows`; and :meth:`count_cols`. To re-densify a filtered matrix table, use the; :meth:`unfilter_entries` method to restore filtered entries, populated; all fields with missing values. Below are some properties of an; entry-filtered matrix table. 1. Filtered entries are not included in the :meth:`entries` table. >>> mt_range = hl.utils.range_matrix_table(10, 10); >>> mt_range = mt_range.annotate_entries(x = mt_range.row_idx + mt_range.col_idx); >>> mt_range.count(); (10, 10). >>> mt_range.entries().count(); 100. >>> mt_filt = mt_range.filter_entries(mt_range.x % 2 == 0); >>> mt_filt.count(); (10, 10). >>> mt_filt.count_rows() * mt_filt.count_cols(); 100. >>> mt_filt.entries().count(); 50. 2. Filtered entries are not included in aggregation. >>> mt_filt.aggregate_entries(hl.agg.count()); 50. >>> mt_filt = mt_filt",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:56793,down,downstream,56793,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['down'],['downstream']
Availability,"ixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Filtering and Annotation Tutorial. View page source. Filtering and Annotation Tutorial. Filter; You can filter the rows of a table with Table.filter. This returns a table of those rows for which the expression evaluates to True. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2009-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:44.088 Hail: INFO: Movie Lens files found!. [2]:. users.filter(users.occupation == 'programmer').count(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 66. We can also express this query in multiple ways using aggregations:. [3]:. users.aggregate(hl.agg.filter(users.occupation == 'programmer', hl.agg.count())). [3]:. 66. [4]:. users.aggregate(hl.agg.counter(users.occupation == 'programmer'))[True]. [4]:. 66. Annotate; You can add new fields to a table with annotate. As an example, let’s create a new column called cleaned_occupation that replaces missing entries in the occupation field labeled as ‘other’ with ‘non",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html:1367,avail,available,1367,docs/0.2/tutorials/05-filter-annotate.html,https://hail.is,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html,1,['avail'],['available']
Availability,"ize of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance (float) – The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy (float) – The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations (int) – The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns:; Table – One row per-group. The key is group. The row fields are:. group : the group parameter.; size : tint64, the number of variants in this group.; q_stat : tfloat64, the \(Q\) statistic, see Notes for why this differs from the paper.; p_value : tfloat64, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes.; fault : tint32, the fault flag from pgenchisq(). The global fields are:. n_complete_samples : tint32, the number of samples with neither a missing; phenotype nor a missing covariate.; y_residual : tint32, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone.; s2 : tfloat64, the variance of the residuals, \(\sigma^2\) in the paper.; null_fit:. b : tndarray vector of coefficients.; score : tndarray vector of score statistics.; fisher : tndarray matrix of fisher statistics.; mu : tndarray the expected value under the null model.; n_iterations : tint32 the number of iterations before termination.; log_lkhd : tfloat64 the log-likelihood of the final iteration.; converged : tbool True if the null model converged.; exploded : tbool True if the null model failed to converge due to numerical; explosion. hail.methods.skat(key_expr, weight_expr, y, x, covariates, logistic=False, max_size=46340, accuracy=1e-06, iterations=10000)[sour",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:76971,fault,fault,76971,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,['fault'],['fault']
Availability,"jects where the value for :attr:`.url`; is a :obj:`dict` containing key: value pairs, like ``region: url``.; region : :obj:`str`; Region from which to access data, available regions given in; :attr:`hail.experimental.DB._valid_regions`. Returns; -------; available_versions : :class:`list` of :class:`.DatasetVersion`; List of available versions of a class:`.Dataset` for region.; """"""; available_versions = []; for version in versions:; if version.in_region(name, region):; version.url = version.url[region]; available_versions.append(version); return available_versions. def __init__(self, url: Union[dict, str], version: Optional[str], reference_genome: Optional[str]):; self.url = url; self.version = version; self.reference_genome = reference_genome. def in_region(self, name: str, region: str) -> bool:; """"""Check if a :class:`.DatasetVersion` object is accessible in the; desired region. Parameters; ----------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset exists in the specified region.; """"""; current_version = self.version; available_regions = [k for k in self.url.keys()]; valid_region = region in available_regions; if not valid_region:; message = (; f'\nName: {name}\n'; f'Version: {current_version}\n'; f'This dataset exists but is not yet available in the'; f' {region} region bucket.\n'; f'Dataset is currently available in the'; f' {"", "".join(available_regions)} region bucket(s).\n'; f'Reach out to the Hail team at https://discuss.hail.is/'; f' to request this dataset in your region.'; ); warnings.warn(message, UserWarning, stacklevel=1); return valid_region. def maybe_index(self, indexer_key_expr: StructExpression, all_matches: bool) -> Optional[StructExpression]:; """"""Find the prefix of the given indexer expression that can index the; :class:`.DatasetVersion`, if it exists. Parameter",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:3658,avail,available,3658,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['avail'],['available']
Availability,"job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool) – If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool) – If True, set job to always run. Return type:; Self. Returns:; Same job object set to always run. cloudfuse(bucket, mount_point, *, read_only=True); Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. Warning; There are performance and cost implications of using gcsfuse; or blobfuse. Examples; Google Cloud Platform:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters:. bucket (str) – Name of the google storage bucket to mount or the path to an Azure container in the; format of <account>/<container>",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:2157,echo,echo,2157,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['echo'],['echo']
Availability,"join(agg_exprs). return KeyTable(self.hc, self._jvds.aggregateByKey(key_exprs, agg_exprs)). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(expr=oneof(strlike, listof(strlike)),; propagate_gq=bool); def annotate_alleles_expr(self, expr, propagate_gq=False):; """"""Annotate alleles with expression. .. include:: requireTGenotype.rst. **Examples**. To create a variant annotation ``va.nNonRefSamples: Array[Int]`` where the ith entry of; the array is the number of samples carrying the ith alternate allele:. >>> vds_result = vds.annotate_alleles_expr('va.nNonRefSamples = gs.filter(g => g.isCalledNonRef()).count()'). **Notes**. This method is similar to :py:meth:`.annotate_variants_expr`. :py:meth:`.annotate_alleles_expr` dynamically splits multi-allelic sites,; evaluates each expression on each split allele separately, and for each expression annotates with an array with one element per alternate allele. In the splitting, genotypes are downcoded and each alternate allele is represented; using its minimal representation (see :py:meth:`split_multi` for more details). :param expr: Annotation expression.; :type expr: str or list of str; :param bool propagate_gq: Propagate GQ instead of computing from (split) PL. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = "","".join(expr). jvds = self._jvdf.annotateAllelesExpr(expr, propagate_gq); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(expr=oneof(strlike, listof(strlike))); def annotate_genotypes_expr(self, expr):; """"""Annotate genotypes with expression. **Examples**. Convert the genotype schema to a :py:class:`~hail.expr.TStruct` with two fields ``GT`` and ``CASE_HET``:. >>> vds_result = vds.annotate_genotypes_expr('g = {GT: g.gt, CASE_HET: sa.pheno.isCase && g.isHet()}'). Assume a VCF is imported with ``generic=True`` and the resulting genotype schema; is a ``Struct`` and the field ``GTA`` is a ``Call`` type. Use the ``.toGenotype",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:8125,down,downcoded,8125,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['down'],['downcoded']
Availability,"k \}`, let :math:`p_i` be the probability that; a randomly chosen character is :math:`c_i`, e.g. the number of instances; of :math:`c_i` divided by :math:`n`. Then the base-2 Shannon entropy is; given by. .. math::. H = \sum_{i=1}^k p_i \log_2(p_i). Parameters; ----------; s : :class:`.StringExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""entropy"", tfloat64, s). @typecheck(x=expr_any, trunc=nullable(expr_int32)); def _showstr(x, trunc=None):; if trunc is None:; return _func(""showStr"", tstr, x); return _func(""showStr"", tstr, x, trunc). [docs]@typecheck(x=expr_any); def str(x) -> StringExpression:; """"""Returns the string representation of `x`. Examples; --------. >>> hl.eval(hl.str(hl.struct(a=5, b=7))); '{""a"":5,""b"":7}'. Parameters; ----------; x. Returns; -------; :class:`.StringExpression`; """"""; if x.dtype == tstr:; return x; else:; return _func(""str"", tstr, x). [docs]@typecheck(c=expr_call, i=expr_int32); def downcode(c, i) -> CallExpression:; """"""Create a new call by setting all alleles other than i to ref. Examples; --------; Preserve the third allele and downcode all other alleles to reference. >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters; ----------; c : :class:`.CallExpression`; A call.; i : :class:`.Expression` of type :py:data:`.tint32`; The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns; -------; :class:`.CallExpression`; """"""; return _func(""downcode"", tcall, c, i). @typecheck(pl=expr_array(expr_int32)); def gq_from_pl(pl) -> Int32Expression:; """"""Compute genotype quality from Phred-scaled probability likelihoods. Examples; --------. >>> hl.eval(hl.gq_from_pl([0, 69, 1035])); 69. Parameters; ----------; pl : :class:`.Expression` of ty",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:105023,down,downcode,105023,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['down'],['downcode']
Availability,"k out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that the dependent job should not run until; the previous job completes. Thus, under the covers a batch is a directed acyclic graph (DAG); of jobs.; In the example below, we have defined a Batch b with the name ‘hello’.; We use the method Batch.new_job() to create a job object which we call j and then; use the method BashJob.command() to tell Batch that we want to execute echo “hello world”.; However, at this point, Batch hasn’t actually run the job to print “hello world”. All we have; done is specified the jobs and the order in which they should be run. To actually execute the; Batch, we call Batch.run(). The name arguments to both Batch and; Job are used in the Batch Service UI.; >>> b = hb.Batch(name='hello'); >>> j = b.new_job(name='j1'); >>> j.command('echo ""hello world""'); >>> b.run(). Now that we know how to create a batch with a single job, we call Batch.new_job(); twice to create two jobs s and t which both will print a variant of hello world to stdout.; Calling b.run() executes the batch. By default, batches are executed by the LocalBackend; which runs jobs on your local computer. Therefore, even though these jobs can be run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:2767,echo,echo,2767,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability,"k that all variants vary or; filter out constant variants (for example, with the help of; :func:`.aggregators.stats`). If the :meth:`.global_position` on `locus_expr` is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that's; been ordered by `locus_expr`. Set `coord_expr` to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-``nan``, on the; same source as `locus_expr`, and ascending with respect to locus; position for each contig; otherwise the method will raise an error. Warning; -------; See the warnings in :meth:`row_correlation`. In particular, for large; matrices it may be preferable to run its stages separately. `entry_expr` and `locus_expr` are implicitly aligned by row-index, though; they need not be on the same source. If their sources differ in the number; of rows, an error will be raised; otherwise, unintended misalignment may; silently produce unexpected results. Parameters; ----------; entry_expr : :class:`.Float64Expression`; Entry-indexed numeric expression on matrix table.; locus_expr : :class:`.LocusExpression`; Row-indexed locus expression on a table or matrix table that is; row-aligned with the matrix table of `entry_expr`.; radius: :obj:`int` or :obj:`float`; Radius of window for row values.; coord_expr: :class:`.Float64Expression`, optional; Row-indexed numeric expression for the row value on the same table or; matrix table as `locus_expr`.; By default, the row value is given by the locus position.; block_size : :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`. Returns; -------; :class:`.BlockMatrix`; Windowed correlation matrix between variants.; Row and column indices correspond to matrix table variant index.; """"""; starts_and_stops = hl.linalg.utils.locus_windows(locus_expr, radius, coord_expr, _localize=False); start",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:140326,error,error,140326,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['error'],['error']
Availability,"k^{(2)}_{ij}} \coloneqq; \frac{\sum_{s \in S_{ij}}X_{is} X_{js}}{\sum_{s \in S_{ij}}; \widehat{\sigma^2_{is}} \widehat{\sigma^2_{js}}}. The estimator for identity-by-descent zero is given by:. .. math::. \widehat{k^{(0)}_{ij}} \coloneqq; \begin{cases}; \frac{\text{IBS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}}; \widehat{\mu_{is}}^2(1 - \widehat{\mu_{js}})^2; + (1 - \widehat{\mu_{is}})^2\widehat{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}. Note that, even if present, phase information is ignored by this method. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :func:`.pc_relate` differs from the reference implementation in a few; ways:. - if ``k`` is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). Under the PC-Relate model, kinship, :math:`\phi_{ij}`, ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their ki",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:7681,avail,available,7681,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['avail'],['available']
Availability,"kedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(StructExpression, self).__init__(x, type, indices, aggregations); self._fields: Dict[str, Expression] = {}; self._warn_on_shadowed_name = set(). for i, (f, t) in enumerate(self.dtype.items()):; if isinstance(self._ir, ir.MakeStruct):; expr = construct_expr(self._ir.fields[i][1], t, self._indices, self._aggregations); elif isinstance(self._ir, ir.SelectedTopLevelReference):; expr = construct_expr(; ir.ProjectedTopLevelReference(self._ir.ref.name, f, t), t, self._indices, self._aggregations; ); elif isinstance(self._ir, ir.SelectFields):; expr = construct_expr(ir.GetField(self._ir.old, f), t, self._indices, self._aggregations); else:; expr = construct_expr(ir.GetField(self._ir, f), t, self._indices, self._aggregations); self._set_field(f, expr). def _set_field(self, key, value):; if key not in self._fields:; # Avoid using hasattr on self. Each new field added will fall through to __getattr__,; # which has to build a nice error message.; if key in self.__dict__ or hasattr(super(), key):; self._warn_on_shadowed_name.add(key); else:; self.__dict__[key] = value; self._fields[key] = value. def _get_field(self, item):; if item in self._fields:; return self._fields[item]; else:; raise KeyError(get_nice_field_error(self, item)). def __getattribute__(self, item):; if item in super().__getattribute__('_warn_on_shadowed_name'):; warning(; f'Field {item} is shadowed by another method or attribute. '; f'Use [""{item}""] syntax to access the field.'; ); self._warn_on_shadowed_name.remove(item); return super().__getattribute__(item). def __getattr__(self, item):; raise AttributeError(get_nice_attr_error(self, item)). def __len__(self):; return len(self._fields). def __bool__(self):; return bool(len(self)). [docs] @typecheck_method(item=oneof(str, int, slice)); def __getitem__(self, item):; """"""Access a field of the struct by name or index. Examples; --------. >>> hl.eval(struct['a']); ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:44553,error,error,44553,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['error'],['error']
Availability,"key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 or vd_col_key.types[0] != hl.tstr:; error(f""expect variant data to have a single col key of type string, found {vd_col_key}""). if 'END' not in rd.entry or rd.END.dtype != hl.tint32:; error(""expect field 'END' in entry of reference data with type int32""). if check_data:; # check cols; ref_cols = rd.col_key.collect(); var_cols = vd.col_key.collect(); if len(ref_cols) != len(var_cols):; error(; f""mismatch in number of columns: reference data has {ref_cols} columns, variant data has {var_cols} columns""; ). if ref_cols != var_cols:; first_mismatch = 0; while ref_cols[first_mismatch] == var_cols[first_mismatch]:; first_mismatch += 1; error(; f""mismatch in columns keys: ref={ref_cols[first_mismatch]}, var={var_cols[first_mismatch]} at position {first_mismatch}""; ). # check locus distinctness; n_rd_rows = rd.count_rows(); n_distinct = rd.distinct_by_row().count_rows(). if n_distinct != n_rd_rows:; error(f'reference data loci are not distinct: found {n_rd_rows} rows, but {n_distinct} distinct loci'). # check END field; end_exprs = dict(; missing_end=hl.agg.filter(hl.is_missing(rd.END), hl.agg.take((rd.row_key, rd.col_key), 5)),; end_before_position=hl.agg.filter(rd.END < rd.locus.position, hl.agg.take((rd.row_key, rd.col_key), 5)),; ); if VariantDataset.ref_block_max_length_field in rd.globals:; rbml = rd[VariantDataset.ref_block_max_length_field]; end_exprs['blocks_too_long'] = hl.agg.filter(; rd.END - rd.loc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:9842,error,error,9842,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['error'],['error']
Availability,"kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(ys=listof(strlike),; covariates=listof(strlike),; root=strlike,; use_dosages=bool,; min_ac=integral,; min_af=numeric); def linreg_multi_pheno(self, ys, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0):; r""""""Test each variant for association with multiple phenotypes using linear regression. This method runs linear regression for multiple phenotypes more efficiently; than looping over :py:meth:`.linreg`. .. warning::. :py:meth:`.linreg_multi_pheno` uses the same set of samples for each phenotype,; namely the set of samples for which **all** phenotypes and covariates are defined. **Annotations**. With the default root, the following four variant annotations are added.; The indexing of these annotations corresponds to that of ``y``. - **va.linreg.beta** (*Array[Double]*) -- array of fit genotype coefficients, :math:`\hat\beta_1`; - **va.linreg.se** (*Array[Double]*) -- array of estimated standard errors, :math:`\widehat{\mathrm{se}}`; - **va.linreg.tstat** (*Array[Double]*) -- array of :math:`t`-statistics, equal to :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; - **va.linreg.pval** (*Array[Double]*) -- array of :math:`p`-values. :param ys: list of one or more response expressions.; :type covariates: list of str. :param covariates: list of covariate expressions.; :type covariates: list of str. :param str root: Variant annotation path to store result of linear regression. :param bool use_dosages: If true, use dosage genotypes rather than hard call genotypes. :param int min_ac: Minimum alternate allele count. :param float min_af: Minimum alternate allele frequency. :return: Variant dataset with linear regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.linregMultiPheno(jarray(Env.jvm().java.lang.String, ys),; jarray(Env.jvm().java.lang.String, covariates), root, use_dosages, min_ac,; min_af); return VariantDataset(self.hc, jvds). [docs] ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:111015,error,errors,111015,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,"l and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.; (#7608) hl.grep; now has a show argument that allows users to either print the; results (default) or return a dictionary of the results. hailctl dataproc. (#7717) Throw error; when mispelling arguments instead of silently quitting. Version 0.2.28; Released 2019-11-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailct",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:80478,down,down,80478,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['down'],['down']
Availability,"l pointer exception (manifesting as; scala.MatchError: null) when reading data from requester pays; buckets.; (#12739) Fix; hl.plot.cdf, hl.plot.pdf, and hl.plot.joint_plot which; were broken by changes in Hail and changes in bokeh.; (#12735) Fix; (#11738) by allowing; user to override default types in to_pandas.; (#12760) Mitigate; some JVM bytecode generation errors, particularly those related to; too many method parameters.; (#12766) Fix; (#12759) by; loosening parsimonious dependency pin.; (#12732) In Query on; Batch, fix bug that sometimes prevented terminating a pipeline using; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop* methods now properly support creation and; modification time.; (#12566) Improve; error message when combining incompatibly indexed fields in certain; operations including array indexing. Version 0.2.108; Released 2023-1-12. New Features. (#12576); hl.import_bgen and hl.export_bgen now support compression; with Zstd. Bug fixes. (#12585); hail.ggplots that have more than one legend group or facet are; now interactive. If such a plot has enough legend entries that the; legend would be taller than the plot, the legend will now be; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:39219,checkpoint,checkpoint,39219,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['checkpoint'],['checkpoint']
Availability,"l probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. **Annotations**. :py:meth:`~hail.HailContext.import_bgen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .bgen files to import.; :type path: str or list of str. :param float tolerance: If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1. :param sample_file: Sample file.; :type sample_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :return: Variant dataset imported from .bgen file.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importBgens(jindexed_seq_args(path), joption(sample_file),; tolerance, joption(min_partitions)); return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; sample_file=nullable(strlike),; tolerance=numeric,; min_partitions=nullable(integral),; chromosome=nullable(strlike)); def import_gen(self, path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None):; """"""Import .gen file(s) as variant dataset. **Examples**. Read a .gen file and a .sample file and write to a .vds file:. >>> (hc.import_gen('data/example.gen', sample_file='data/example.sample'); ... .write('output/gen_example1.vds')). Load multiple files at the same time with :ref:`Hadoop glob patterns <sec-hadoop-glob>`:. >>> (hc.import_gen('data/example.chr*.gen', sample_file='data/example.sample'); ... .write('output/gen_example2.vds')). **Notes**. For more information on the .gen file format, see `here <http://www.stats.ox.ac.uk/%7Emarchini/software/gwas/file_format.html#mozTocId40300>`__. To ensure that the .gen file(s) and ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:8068,toler,tolerance,8068,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['toler'],['tolerance']
Availability,"l pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions.; The second, “global correlation” stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within bp_window_size base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is n_locally_pruned_variants / block_size.; The third, “global pruning” stage applies maximal_independent_set(); to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; keep_higher_maf is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on BlockMatrix.from_entry_expr with; regard to memory and Hadoop replication errors. Parameters:. call_expr (CallExpression) – Entry-indexed call expression on a matrix table with row-indexed; variants and column-indexed samples.; r2 (float) – Squared correlation threshold (exclusive upper bound).; Must be in the range [0.0, 1.0].; bp_window_size (int) – Window size in base pairs (inclusive upper bound).; memory_per_core (int) – Memory in MB per core for local pruning queue.; keep_higher_maf (int) – If True, break ties at each step of the global pruning stage by; preferring to keep variants with higher minor allele frequency.; block_size (int, optional) – Block size for block matrices in the second stage.; Default given by BlockMatrix.default_block_size(). Returns:; Table – Table of a maximal independent set of variants. hail.methods.compute_charr(ds, min_af=0.05, max_af=0.95, min_dp=10, max_dp=100, min_gq=20, ref_AF=None)[source]; Compute CHARR, the DNA sample contamination estimator. Danger; This functionality is experimental. It m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:45580,error,errors,45580,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability,"l row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:36690,toler,tolerance,36690,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['toler'],['tolerance']
Availability,"l use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Release 2022-10-18. Bug Fixes. (#12305): Fixed a; rare crash reading tables/matrixtables with _intervals. Version 0.2.102; Released 2022-10-06. New Features. (#12218) Missing; values are now supported in primitive columns in Table.to_pandas.; (#12254); Cross-product-style legends for data groups have been replaced with; factored ones (consistent with ggplot2’s implementation) for; hail.ggplot.geom_point, and support has been added for custom; legend group labels.; (#12268); VariantDataset now implements union_rows for combining; datasets with the same samples but disjoint variants. Bug Fixes. (#12278) Fixed bug; made more likely by 0.2.101 in which Hail errors when interacting; with a NumPy integer or floating point type.; (#12277) Fixed bug; in reading tables/matrixtables with partition intervals that led to; error or segfault. Version 0.2.101; Released 2022-10-04. New Features. (#12218) Support; missing values in primitive columns in Table.to_pandas.; (#12195) Add a; impute_sex_chr_ploidy_from_interval_coverage to impute sex ploidy; directly from a coverage MT.; (#12222); Query-on-Batch pipelines now add worker jobs to the same batch as the; driver job instead of producing a new batch per stage.; (#12244) Added; support for custom labels for per-group legends to; hail.ggplot.geom_point via the legend_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Deprecations. (#12213) The; batch_size parameter of vds.new_combiner is deprecated in; favor of gvcf",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:44878,error,error,44878,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"l#mozTocId40300>`__. To ensure that the .gen file(s) and .sample file are correctly prepared for import:. - If there are only 5 columns before the start of the genotype probability data (chromosome field is missing), you must specify the chromosome using the ``chromosome`` parameter. - No duplicate sample IDs are allowed. The first column in the .sample file is used as the sample ID ``s``. Also, see section in :py:meth:`~hail.HailContext.import_bgen` linked :ref:`here <gpfilters>` for information about Hail's genotype probability representation. **Annotations**. :py:meth:`~hail.HailContext.import_gen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .gen files to import.; :type path: str or list of str. :param str sample_file: The sample file. :param float tolerance: If the sum of the genotype probabilities for a genotype differ from 1.0 by more than the tolerance, set the genotype to missing. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param chromosome: Chromosome if not listed in the .gen file.; :type chromosome: str or None. :return: Variant dataset imported from .gen and .sample files.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importGens(jindexed_seq_args(path), sample_file, joption(chromosome), joption(min_partitions),; tolerance); return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(paths=oneof(strlike, listof(strlike)),; key=oneof(strlike, listof(strlike)),; min_partitions=nullable(int),; impute=bool,; no_header=bool,; comment=nullable(strlike),; delimiter=strlike,; missing=strlike,; types=dictof(strlike, Type),; quote=nullable(char)); def import_table(self, paths, key=[], min_partitions=None, impute=False, no_header=False,; comment=None, delimiter=""\t"", missing=""NA"", types={}, quote=None):;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:9984,toler,tolerance,9984,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['toler'],['tolerance']
Availability,"l.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:124601,error,error,124601,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,"l.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:78459,error,errors,78459,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"l; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Table Tutorial. View page source. Table Tutorial; Table is Hail’s distributed analogue of a data frame or SQL table. It will be familiar if you’ve used R or pandas, but Table differs in 3 important ways:. It is distributed. Hail tables can store far more data than can fit on a single computer.; It carries global fields.; It is keyed. A Table has two different kinds of fields:. global fields; row fields. Importing and Reading; Hail can import data from many sources: TSV and CSV files, JSON files, FAM files, databases, Spark, etc. It can also read (and write) a native Hail format.; You can read a dataset with hl.read_table. It take a path and returns a Table. ht stands for Hail Table.; We’ve provided a method to download and import the MovieLens dataset of movie ratings in the Hail native format. Let’s read it!. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=https://dx.doi.org/10.1145/2827872. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_movie_lens('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/03-tables.html:1399,down,download,1399,docs/0.2/tutorials/03-tables.html,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html,1,['down'],['download']
Availability,"l=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). indices = local_df_y.index.to_list(); results = [None] * len(indices). inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:17148,checkpoint,checkpoint,17148,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,12,['checkpoint'],['checkpoint']
Availability,"l[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present valu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:27251,toler,tolerance,27251,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,"l_n.take(5); [5, 5, 5, 5, 5]. >>> mt_filt = mt_filt.annotate_rows(row_n = hl.agg.count()); >>> mt_filt.row_n.take(5); [5, 5, 5, 5, 5]. 3. Annotating a new entry field will not annotate filtered entries. >>> mt_filt = mt_filt.annotate_entries(y = 1); >>> mt_filt.aggregate_entries(hl.agg.sum(mt_filt.y)); 50. 4. If all the entries in a row or column of a matrix table are; filtered, the row or column remains. >>> mt_filt.filter_entries(False).count(); (10, 10). See Also; --------; :meth:`unfilter_entries`, :meth:`compute_entry_filter_stats`; """"""; base, cleanup = self._process_joins(expr); analyze('MatrixTable.filter_entries', expr, self._entry_indices). m = MatrixTable(ir.MatrixFilterEntries(base._mir, ir.filter_predicate_with_keep(expr._ir, keep))); return cleanup(m). [docs] def unfilter_entries(self):; """"""Unfilters filtered entries, populating fields with missing values. Returns; -------; :class:`MatrixTable`. Notes; -----; This method is used in the case that a pipeline downstream of :meth:`filter_entries`; requires a fully dense (no filtered entries) matrix table. Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See Also; --------; :meth:`filter_entries`, :meth:`compute_entry_filter_stats`; """"""; entry_ir = hl.if_else(; hl.is_defined(self.entry), self.entry, hl.struct(**{k: hl.missing(v.dtype) for k, v in self.entry.items()}); )._ir; return MatrixTable(ir.MatrixMapEntries(self._mir, entry_ir)). [docs] @typecheck_method(row_field=str, col_field=str); def compute_entry_filter_stats(self, row_field='entry_stats_row', col_field='entry_stats_col') -> 'MatrixTable':; """"""Compute statistics about the number and fraction of filtered entries. .. include:: _templates/experimental.rst. Parameters; ----------; row_field : :class:`str`; Name for computed row field (default: ``entry_stats_row``.; col_field : :class:`str`; Name for computed column field (default: ``entry_stats_col``. Returns; --",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:58820,down,downstream,58820,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['down'],['downstream']
Availability,"ld see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. Note; A trial billing project is automatically created for you with the name {USERNAME}-trial. Regions; Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:9393,echo,echo,9393,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['echo'],['echo']
Availability,"lds[1]),; 'add': int(fields[2]),; 'mult': int(fields[3])}; data.append(d); return json.dumps(data). # Get all the multiplication and addition table results. b = Batch(name='add-mult-table'). formatted_results = []. for x in range(3):; for y in range(3):; j = b.new_python_job(name=f'{x}-{y}'); add_result = j.call(add, x, y); mult_result = j.call(multiply, x, y); result = j.call(format_as_csv, x, y, add_result, mult_result); formatted_results.append(result.as_str()). cat_j = b.new_bash_job(name='concatenate'); cat_j.command(f'cat {"" "".join(formatted_results)} > {cat_j.output}'). csv_to_json_j = b.new_python_job(name='csv-to-json'); json_output = csv_to_json_j.call(csv_to_json, cat_j.output). b.write_output(j.as_str(), '/output/add_mult_table.json'); b.run(). Notes; Unlike the BashJob, a PythonJob returns a new; PythonResult for every invocation of PythonJob.call(). A; PythonResult can be used as an argument in subsequent invocations of; PythonJob.call(), as an argument in downstream python jobs,; or as inputs to other bash jobs. Likewise, InputResourceFile,; JobResourceFile, and ResourceGroup can be passed to; PythonJob.call(). Batch automatically detects dependencies between jobs; including between python jobs and bash jobs.; When a ResourceFile is passed as an argument, it is passed to the; function as a string to the local file path. When a ResourceGroup; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:2604,down,downstream,2604,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html,2,['down'],['downstream']
Availability,"le Requester Pays Buckets within a project that are acceptable; to access:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; return _fses[requester_pays_config].open(path, mode, buffer_size). [docs]def copy(src: str, dest: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hfs.copy",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:2709,error,error,2709,docs/0.2/_modules/hailtop/fs/fs_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html,2,['error'],['error']
Availability,"le identically, it is duplicated; in the result. All tables must have the same key names and types. They; must also have the same row types, unless the unify parameter is; True, in which case a field appearing in any table will be included; in the result, with missing values for tables that do not contain the; field. If a field appears in multiple tables with incompatible types,; like arrays and strings, then an error will be raised. Parameters:. tables (varargs of Table) – Tables to union.; unify (bool) – Attempt to unify table field. Returns:; Table – Table with all rows from each component table. unpersist()[source]; Unpersists this table from memory/disk.; Notes; This function will have no effect on a table that was not previously; persisted. Returns:; Table – Unpersisted table. write(output, overwrite=False, stage_locally=False, _codec_spec=None)[source]; Write to disk.; Examples; >>> table1.write('output/table1.ht', overwrite=True). Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. See also; read_table(). Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output.; overwrite (bool) – If True, overwrite an existing file at the destination. write_many(output, fields, *, overwrite=False, stage_locally=False, _codec_spec=None)[source]; Write fields to distinct tables.; Examples; >>> t = hl.utils.range_table(10); >>> t = t.annotate(a = t.idx, b = t.idx * t.idx, c = hl.str(t.idx)); >>> t.write_many('output-many', fields=('a', 'b', 'c'), overwrite=True); >>> hl.read_table('output-many/a').describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'a': int32; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> hl.read_table('output-many/a').show(); +----",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:78980,checkpoint,checkpoint,78980,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['checkpoint'],['checkpoint']
Availability,"le must match the key type of other.; This method does not change the schema of the table; it is a method of; filtering the matrix table to row keys not present in another table.; To restrict to rows whose key is present in other, use; semi_join_rows().; Examples; >>> ds_result = ds.anti_join_rows(rows_to_remove). It may be expensive to key the matrix table by the right-side key.; In this case, it is possible to implement an anti-join using a non-key; field as follows:; >>> ds_result = ds.filter_rows(hl.is_missing(rows_to_remove.index(ds['locus'], ds['alleles']))). See also; anti_join_rows(), filter_rows(), anti_join_cols(). cache()[source]; Persist the dataset in memory.; Examples; Persist the dataset in memory:; >>> dataset = dataset.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; MatrixTable – Cached dataset. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False, _drop_cols=False, _drop_rows=False)[source]; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; MatrixTable. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_matrix_table(). It is; possible to read the file at this path later with; read_matrix_table(). A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; write().; Examples; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'). choose_cols(indices)[source]; Choose a new set of columns from a list of old column indices.; Examples; Randomly shuffle column ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:18164,checkpoint,checkpoint,18164,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['checkpoint'],['checkpoint']
Availability,"le(s).; - AD: The filtered alleles' columns are eliminated, e.g., filtering alleles 1 and 2 transforms ``25,5,10,20`` to ``25,20``.; - DP: No change.; - PL: The filtered alleles' columns are eliminated and the remaining columns shifted so the minimum value is 0.; - GQ: The second-lowest PL (after shifting). **Downcode algorithm**. The downcode algorithm (``subset=False``) recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where downcodeing filtered alleles merges distinct genotypes, the minimum PL is used (since PL is on a log scale, this roughly corresponds to adding probabilities). The PLs; are then re-normalized (shifted) so that the most likely genotype has a PL of 0, and GT is set to this genotype.; If an allele is filtered, this algorithm acts similarly to :py:meth:`~hail.VariantDataset.split_multi`. The downcoding algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: The filtered alleles' columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded genotype, and shift so the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Expression Variables**. The following symbols are in scope for ``expr``:. - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; - ``aIndex`` (*Int*): the index of the allele being tested. The following symbols are in scope for ``annotation``:. - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; - ``aIndices`` (*Array[Int]*): the array of old indices (such that ``aIndices[newIndex] = oldIndex`` and ``aIndices[0] = 0",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:63801,down,downcoding,63801,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['down'],['downcoding']
Availability,"le, index_file=None)[source]; Load the reference sequence from a FASTA file.; Examples; Access the GRCh37 reference genome using get_reference():; >>> rg = hl.get_reference('GRCh37') . Add a sequence file:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') . Add a sequence file with the default index location:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') . Notes; This method can only be run once per reference genome. Use; has_sequence() to test whether a sequence is loaded.; FASTA and index files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37. FASTA file: gs://hail-common/references/human_g1k_v37.fasta.gz; Index file: gs://hail-common/references/human_g1k_v37.fasta.fai. GRCh38. FASTA file: gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz; Index file: gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai. Public download links are available; here. Parameters:. fasta_file (str) – Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file (None or str) – Path to FASTA index file. Must be uncompressed. If None, replace; the fasta_file’s extension with fai. contig_length(contig)[source]; Contig length. Parameters:; contig (str) – Contig name. Returns:; int – Length of contig. property contigs; Contig names. Returns:; list of str. classmethod from_fasta_file(name, fasta_file, index_file, x_contigs=[], y_contigs=[], mt_contigs=[], par=[])[source]; Create reference genome from a FASTA file. Parameters:. name (str) – Name for new reference genome.; fasta_file (str) – Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file (str) – Path to FASTA index file. Must be uncompressed.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contigs (str or list of str) – Contigs to be treated as Y chromosomes.; mt_contigs (str or list of str) – Contigs to be treated a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:5834,down,download,5834,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html,2,"['avail', 'down']","['available', 'download']"
Availability,"le2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_else(hl.agg.array_sum(implicated[tm.mendel_code]), [0, 0, 0]),; snp_errors=hl.or_else(; hl.agg.filter(hl.is_snp(tm.alleles[0], tm.alleles[1]), hl.agg.array_sum(implicated[tm.mendel_code])),; [0, 0, 0],; ),; ); .key_cols_by(); .cols(); ). table3 = table3.select(; xs=[; hl.struct(**{; ck_name: table3.father[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[0],; 'snp_errors': table3.snp_errors[0],; }),; hl.struct(**{; ck_name: table3.mother[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[1],; 'snp_errors': table3.snp_errors[1],; }),; hl.struct(**{; ck_name: table3.proband[ck_name],; 'fam_id': table3.fam_id,; 'errors': table3.all_errors[2],; 'snp_errors': table3.snp_errors[2],; }),; ]; ); table3 = table3.explode('xs'); table3 = table3.select(**table3.xs); table3 = (; table3.group_by(ck_name, 'fam_id'); .aggregate(errors=hl.agg.sum(table3.errors), snp_errors=hl.agg.sum(table3.snp_errors)); .key_by(ck_name); ). table4 = tm.select_rows(errors=hl.agg.count_where(hl.is_defined(tm.mendel_code))).rows(). return table1, table2, table3, table4. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree); def transmission_disequilibrium_test(dataset, pedigree) -> Table:; r""""""Performs the transmission disequilibrium test on trios. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------; Compute TDT association statistics and show the first two results:. >>> pedigr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:12182,error,errors,12182,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"lectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hstack([hl.nd.array([logit_avg]), hl.nd.zeros((hl.int32(m - 1)))]); mu = sigmoid(X @ b); score = X.T @ (y - mu); # Reshape so we do a rowwise multiply; fisher = X.T @ (X * (mu * (1 - mu)).reshape(-1, 1)); else:; # num covs used to fit null model.; m0 = null_fit.b.shape[0]; m_diff = m - m0. X0 = X[:, 0:m0]; X1 = X[:, m0:]. b = hl.nd.hstack([null_fit.b, hl.nd.zeros((m_diff,))]); mu = sigmoid(X @ b); score = hl.nd.hstack([null_fit.score, X1.T @ (y - mu)]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:39197,toler,tolerance,39197,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,"lele index. Its length is the same as the modified alleles; field. Downcode algorithm; We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles.; GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; split_multi_hts().; The downcode algorithm would produce the following:; GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. GT: Downcode filtered alleles to reference.; AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms 25,5,10,20 to 40,20.; DP: No change.; PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded genotype, and shift so; the overall minimum PL is 0.; GQ: The second-lowest PL (after shifting). Subset algorithm; We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles.; GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The subset algorithm subsets the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:26148,down,downcode,26148,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['down'],['downcode']
Availability,"leles per variant; -------------------; 2 alleles: 346 variants; ==============================; Variants per contig; -------------------; 20: 346 variants; ==============================; Allele type distribution; ------------------------; SNP: 301 alleles; Deletion: 27 alleles; Insertion: 18 alleles; ==============================. Parameters; ----------; mt : :class:`.MatrixTable` or :class:`.Table`; Matrix table with a variant (locus / alleles) row key.; show : :obj:`bool`; If ``True``, print results instead of returning them.; handler. Notes; -----; The result returned if `show` is ``False`` is a :class:`.Struct` with; five fields:. - `n_variants` (:obj:`int`): Number of variants present in the matrix table.; - `allele_types` (:obj:`dict` [:obj:`str`, :obj:`int`]): Number of alternate alleles in; each allele allele category.; - `contigs` (:obj:`dict` [:obj:`str`, :obj:`int`]): Number of variants on each contig.; - `allele_counts` (:obj:`dict` [:obj:`int`, :obj:`int`]): Number of variants broken down; by number of alleles (biallelic is 2, for example).; - `r_ti_tv` (:obj:`float`): Ratio of transition alternate alleles to; transversion alternate alleles. Returns; -------; :obj:`None` or :class:`.Struct`; Returns ``None`` if `show` is ``True``, or returns results as a struct.; """"""; require_row_key_variant(mt, 'summarize_variants'); if isinstance(mt, MatrixTable):; ht = mt.rows(); else:; ht = mt; allele_pairs = hl.range(1, hl.len(ht.alleles)).map(lambda i: (ht.alleles[0], ht.alleles[i])). def explode_result(alleles):; ref, alt = alleles; return (; hl.agg.counter(hl.allele_type(ref, alt)),; hl.agg.count_where(hl.is_transition(ref, alt)),; hl.agg.count_where(hl.is_transversion(ref, alt)),; ). (allele_types, nti, ntv), contigs, allele_counts, n_variants = ht.aggregate((; hl.agg.explode(explode_result, allele_pairs),; hl.agg.counter(ht.locus.contig),; hl.agg.counter(hl.len(ht.alleles)),; hl.agg.count(),; )); rg = ht.locus.dtype.reference_genome; if show:; summary = _Var",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:58194,down,down,58194,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['down'],['down']
Availability,"lement in `array` not smaller; than `elem`. This is a value between 0 and the length of `array`, inclusive; (if all elements in `array` are smaller than `elem`, the returned value is; the length of `array` or the index of the first missing value, if one; exists). If either `elem` or `array` is missing, the result is missing. Examples; --------. >>> a = hl.array([0, 2, 4, 8]). >>> hl.eval(hl.binary_search(a, -1)); 0. >>> hl.eval(hl.binary_search(a, 1)); 1. >>> hl.eval(hl.binary_search(a, 10)); 4. """"""; c = coercer_from_dtype(array.dtype.element_type); if not c.can_coerce(elem.dtype):; raise TypeError(; f""'binary_search': cannot search an array of type {array.dtype} for a value of type {elem.dtype}""; ); elem = c.coerce(elem); return hl.switch(elem).when_missing(hl.missing(hl.tint32)).default(_lower_bound(array, elem)). @typecheck(s=expr_str); def _escape_string(s):; return _func(""escapeString"", hl.tstr, s). @typecheck(left=expr_any, right=expr_any, tolerance=expr_float64, absolute=expr_bool); def _values_similar(left, right, tolerance=1e-6, absolute=False):; assert left.dtype == right.dtype; return (is_missing(left) & is_missing(right)) | (; (is_defined(left) & is_defined(right)) & _func(""valuesSimilar"", hl.tbool, left, right, tolerance, absolute); ). @typecheck(coords=expr_array(expr_array(expr_float64)), radius=expr_float64); def _locus_windows_per_contig(coords, radius):; rt = hl.ttuple(hl.tarray(hl.tint32), hl.tarray(hl.tint32)); return _func(""locus_windows_per_contig"", rt, coords, radius). [docs]@typecheck(a=expr_array(), seed=nullable(builtins.int)); def shuffle(a, seed: Optional[builtins.int] = None) -> ArrayExpression:; """"""Randomly permute an array. Example; -------. >>> hl.reset_global_randomness(); >>> hl.eval(hl.shuffle(hl.range(5))); [4, 0, 2, 1, 3]. Parameters; ----------; a : :class:`.ArrayExpression`; Array to permute.; seed : :obj:`int`, optional; Random seed. Returns; -------; :class:`.ArrayExpression`; """"""; return sorted(a, key=lambda _: hl.rand_unif(0",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:184844,toler,tolerance,184844,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,4,['toler'],['tolerance']
Availability,"les so that no two have a PI_HAT value greater than or equal to 0.6.; >>> pruned_vds = vds.ibd_prune(0.6). Prune samples so that no two have a PI_HAT value greater than or equal to 0.5, with a tiebreaking expression that ; selects cases over controls:; >>> pruned_vds = vds.ibd_prune(; ... 0.5,; ... tiebreaking_expr=""if (sa1.isCase && !sa2.isCase) -1 else if (!sa1.isCase && sa2.isCase) 1 else 0""). Notes; The variant dataset returned may change in near future as a result of algorithmic improvements. The current algorithm is very efficient on datasets with many small; families, less so on datasets with large families. Currently, the algorithm works by deleting the person from each family who has the highest number of relatives,; and iterating until no two people have a PI_HAT value greater than that specified. If two people within a family have the same number of relatives, the tiebreaking_expr; given will be used to determine which sample gets deleted.; The tiebreaking_expr namespace has the following variables available:. s1: The first sample id.; sa1: The annotations associated with s1.; s2: The second sample id.; sa2: The annotations associated with s2. The tiebreaking_expr returns an integer expressing the preference for one sample over the other. Any negative integer expresses a preference for keeping s1. Any positive integer expresses a preference for keeping s2. A zero expresses no preference. This function must induce a preorder on the samples, in particular:. tiebreaking_expr(sample1, sample2) must equal -1 * tie breaking_expr(sample2, sample1), which evokes the common sense understanding that if x < y then y > x`.; tiebreaking_expr(sample1, sample1) must equal 0, i.e. x = x; if sample1 is preferred to sample2 and sample2 is preferred to sample3, then sample1 must also be preferred to sample3. The last requirement is only important if you have three related samples with the same number of relatives and all three are related to one another. In cases like this ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:69472,avail,available,69472,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['avail'],['available']
Availability,"les**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in thi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:155585,error,errors,155585,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,"lf.variant_data: MatrixTable = variant_data. self.validate(check_data=False). [docs] def write(self, path, **kwargs):; """"""Write to `path`.""""""; self.reference_data.write(VariantDataset._reference_path(path), **kwargs); self.variant_data.write(VariantDataset._variants_path(path), **kwargs). [docs] def checkpoint(self, path, **kwargs) -> 'VariantDataset':; """"""Write to `path` and then read from `path`.""""""; self.write(path, **kwargs); return read_vds(path). [docs] def n_samples(self) -> int:; """"""The number of samples present.""""""; return self.reference_data.count_cols(). @property; def reference_genome(self) -> ReferenceGenome:; """"""Dataset reference genome. Returns; -------; :class:`.ReferenceGenome`; """"""; return self.reference_data.locus.dtype.reference_genome. [docs] @typecheck_method(check_data=bool); def validate(self, *, check_data: bool = True):; """"""Eagerly checks necessary representational properties of the VDS."""""". rd = self.reference_data; vd = self.variant_data. def error(msg):; raise ValueError(f'VDS.validate: {msg}'). rd_row_key = rd.row_key.dtype; if (; not isinstance(rd_row_key, hl.tstruct); or len(rd_row_key) != 1; or not rd_row_key.fields[0] == 'locus'; or not isinstance(rd_row_key.types[0], hl.tlocus); ):; error(f""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_key = vd.row_key.dtype; if (; not isinstance(vd_row_key, hl.tstruct); or len(vd_row_key) != 2; or not vd_row_key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:8410,error,error,8410,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['error'],['error']
Availability,"lic variants. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; >>> vds.split_multi().write('output/split.vds'). Notes; We will explain by example. Consider a hypothetical 3-allelic; variant:; A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi will create two biallelic variants (one for each; alternate allele) at the same position; A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT field is downcoded once for each; alternate allele. A call for an alternate allele maps to 1 in; the biallelic variant corresponding to itself and 0; otherwise. For example, in the example above, 0/2 maps to 0/0; and 0/1. The genotype 1/2 maps to 0/1 and 0/1.; The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the; sum of the other multiallelic entries.; The biallelic DP is the same as the multiallelic DP.; The biallelic PL entry for for a genotype g is the minimum; over PL entries for multiallelic genotypes that downcode to; g. For example, the PL for (A, T) at 0/1 is the minimum of the; PLs for 0/1 (50) and 1/2 (45), and thus 45.; Fixing an alternate allele and biallelic variant, downcoding; gives a map from multiallelic to biallelic alleles and; genotypes. The biallelic AD entry for an allele is just the; sum of the multiallelic AD entries for alleles that map to; that allele. Similarly, the biallelic PL entry for a genotype; is the minimum over multiallelic PL entries for genotypes that; map to that genotype.; By default, GQ is recomputed from PL. If propagate_gq=True; is passed, the biallelic GQ field is simply the multiallelic; GQ field, that is, genotype qualities are unchanged.; Here is a second example for a het non-ref; A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as; A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. VCF Info Fields; Hail does not split annotations in the info field. This means; that if a multiallelic site with inf",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:163239,down,downcode,163239,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['down'],['downcode']
Availability,"lihood function of \(h^2\) normalized over the discrete grid 0.01, 0.02, ..., 0.98, 0.99. The length of the array is 101 so that index i contains the likelihood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of \(h^2\) as follows. Let \(x_2\) be the maximum likelihood estimate of \(h^2\) and let \(x_ 1\) and \(x_3\) be just to the left and right of \(x_2\). Let \(y_1\), \(y_2\), and \(y_3\) be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. \[\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\frac{1}{2 \sigma^2}\]; The standard error \(\hat{\sigma}\) is then estimated by solving for \(\sigma\).; Note that the mean and standard deviation of the (discretized or continuous) distribution held in global.lmmreg.fit.normLkhdH2 will not coincide with \(\hat{h}^2\) and \(\hat{\sigma}\), since this distribution only becomes normal in the infinite sample limit. One can visually assess normality by plotting this distribution against a normal distribution with the same mean and standard deviation, or use this distribution to approximate credible intervals under a flat prior on \(h^2\).; Testing each variant for association; Fixing a single variant, we define:. \(v = n \times 1\) vector of genotypes, with missing genotypes imputed as the mean of called genotypes; \(X_v = \left[v | X \right] = n \times (1 + c)\) matrix concatenating \(v\) and \(X\); \(\beta_v = (\beta^0_v, \beta^1_v, \ldots, \beta^c_v) = (1 + c) \times 1\) vector of covariate coefficients. Fixing \(\delta\) at the global R",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:104260,error,error,104260,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,"ling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which con",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:71160,error,error,71160,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"ling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; in_autosome(). Auto – in autosome or in PAR or female child; HemiX – in non-PAR of X and male child; HemiY – in no",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:49988,error,error,49988,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['error']
Availability,"litatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard any; probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g., filtering alleles 1 and 2 transforms 25,5,10,20 to 25,20.; DP: No change.; PL: The filtered alleles’ columns are eliminated and the remaining columns shifted so the minimum value is 0.; GQ: The second-lowest PL (after shifting). Downcode algorithm; The downcode algorithm (subset=False) recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where downcodeing filtered alleles merges distinct genotypes, the minimum PL is used (since PL is on a log scale, this roughly corresponds to adding probabilities). The PLs; are then re-normalized (shifted) so that the most likely genotype has a PL of 0, and GT is set to this genotype.; If an allele is filtered, this algorithm acts similarly to split_multi().; The downcoding algorithm would produce the following:; GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. GT: Downcode filtered alleles to reference.; AD: The filtered alleles’ columns are eliminated and their value is added to the reference, e.g., filtering alleles 1 and 2 transforms 25,5,10,20 to 40,20.; DP: No change.; PL: Downcode filtered alleles to reference, combine PLs using minimum for each overloaded genotype, and shift so the overall minimum PL is 0.; GQ: The second-lowest PL (after shifting). Expression Variables; The following symbols are in scope for expr:. v (Variant): Variant; va: variant annotations; aIndex (Int): the index of the allele being tested. The following s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:49988,down,downcodeing,49988,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['down'],['downcodeing']
Availability,"lity distribution, :py:meth:`~hail.HailContext.import_bgen` automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the ``tolerance`` parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains. - :py:meth:`~hail.HailContext.import_bgen` normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. **Annotations**. :py:meth:`~hail.HailContext.import_bgen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .bgen files to import.; :type path: str or list of str. :param float tolerance: If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1. :param sample_file: Sample file.; :type sample_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :return: Variant dataset imported from .bgen file.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importBgens(jindexed_seq_args(path), joption(sample_file),; tolerance, joption(min_partitions)); return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; sample_file=nullable(strlike),; tolerance=numeric,; min_partitions=nullable(integral),; chromosome=nullable(strlike)); def import_gen(self, path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None):; """"""Import .gen file(s) as variant dataset. **Examples**. Read a .gen file and a .sample file and write to a .vds file:. >>> (hc.import_gen('data/example.gen', sample_file='data/example.sample'); ... .write('output/gen_example1.vds')). Load mu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:7668,toler,tolerance,7668,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['toler'],['tolerance']
Availability,"lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariates or n_covariates + 1, depending on improved null fit vs full fit; mu = sigmoid(X_bslice @ b); sqrtW = hl.sqrt(mu * (1 - mu)); q, r = hl.nd.qr(X * sqrtW.T.reshape(-1, 1)); h = (q * q).sum(1); coef = r[:m, :m]; residual = y - mu; dep = q[:, :m].T @ ((residual + (h * (0.5 - mu))) / sqrtW); delta_b_struct = hl.nd.solve_triangular(coef, dep.reshape(-1, 1), no_crash=True); exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution.reshape(-1). max_delta_b = nd_max(hl.abs(delta_b)). return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.annotate(n_iterations=0, log_lkhd=0, converged=False, exploded=False); return hl.experimental.loop(fit, dtype, 1, b). def _firth_test(null_fit, X, y, max_iterations, tolerance) -> StructExpression:; firth_improved_null_fit = _firth_fit(null_fit.b, X, y, max_iterations=max_iterations, tolerance=tolerance); dof = 1 # 1 variant. def cont(firth_improved_null_fit):; initial_b_full_model = hl.nd.hstack([firth_improved_null_fit.b, hl.nd.array([0.0])]); firth_fit = _firth_fit(initial_b_full_model, X, y, max_iterations=max_iterations, tolerance=tolerance). def cont2(firth_fit):; firth_chi_sq = 2 * (firth_fit.log_lkhd - firth_improved_null_fit.log_lkhd); firth_p = hl.pchisqtail(firth_chi_sq, dof). blank_struct = hl.struct(; beta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; );",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:45572,toler,tolerance,45572,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['toler'],['tolerance']
Availability,"llelic variants. .. include:: requireTGenotype.rst. **Examples**. >>> vds.split_multi().write('output/split.vds'). **Notes**. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT field is downcoded once for each; alternate allele. A call for an alternate allele maps to 1 in; the biallelic variant corresponding to itself and 0; otherwise. For example, in the example above, 0/2 maps to 0/0; and 0/1. The genotype 1/2 maps to 0/1 and 0/1. The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the; sum of the other multiallelic entries. The biallelic DP is the same as the multiallelic DP. The biallelic PL entry for for a genotype g is the minimum; over PL entries for multiallelic genotypes that downcode to; g. For example, the PL for (A, T) at 0/1 is the minimum of the; PLs for 0/1 (50) and 1/2 (45), and thus 45. Fixing an alternate allele and biallelic variant, downcoding; gives a map from multiallelic to biallelic alleles and; genotypes. The biallelic AD entry for an allele is just the; sum of the multiallelic AD entries for alleles that map to; that allele. Similarly, the biallelic PL entry for a genotype; is the minimum over multiallelic PL entries for genotypes that; map to that genotype. By default, GQ is recomputed from PL. If ``propagate_gq=True``; is passed, the biallelic GQ field is simply the multiallelic; GQ field, that is, genotype qualities are unchanged. Here is a second example for a het non-ref. .. code-block:: text. A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as. .. code-block:: text. A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. **VCF Info Fields**. Hail does not split annotations in the info field.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:209413,down,downcode,209413,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['down'],['downcode']
Availability,"llowing:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms 25,5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See pca() for more details.; Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; \(ij\) entry of the GRM \(MM^T\) is simply the dot product of rows; \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:28511,down,downcodes,28511,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['down'],['downcodes']
Availability,"lock_size=16)[source]¶; Test each variant for association with multiple phenotypes using linear regression.; This method runs linear regression for multiple phenotypes; more efficiently than looping over linreg(). This; method is more efficient than linreg_multi_pheno(); but doesn’t implicitly filter on allele count or allele; frequency. Warning; linreg3() uses the same set of samples for each phenotype,; namely the set of samples for which all phenotypes and covariates are defined. Annotations; With the default root, the following four variant annotations are added.; The indexing of the array annotations corresponds to that of y. va.linreg.nCompleteSamples (Int) – number of samples used; va.linreg.AC (Double) – sum of the genotype values x; va.linreg.ytx (Array[Double]) – array of dot products of each phenotype vector y with the genotype vector x; va.linreg.beta (Array[Double]) – array of fit genotype coefficients, \(\hat\beta_1\); va.linreg.se (Array[Double]) – array of estimated standard errors, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Array[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Array[Double]) – array of \(p\)-values. Parameters:; ys – list of one or more response expressions.; covariates (list of str) – list of covariate expressions.; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosage genotypes rather than hard call genotypes.; variant_block_size (int) – Number of variant regressions to perform simultaneously. Larger block size requires more memmory. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. linreg_burden(key_name, variant_keys, single_key, agg_expr, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; linear regression model. Important; The genotype_schema() must be of type TGenotype in order to u",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:83365,error,errors,83365,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,"long the reference genome. in_autosome; Returns True if the locus is on an autosome. in_autosome_or_par; Returns True if the locus is on an autosome or a pseudoautosomal region of chromosome X or Y. in_mito; Returns True if the locus is on mitochondrial DNA. in_x_nonpar; Returns True if the locus is in a non-pseudoautosomal region of chromosome X. in_x_par; Returns True if the locus is in a pseudoautosomal region of chromosome X. in_y_nonpar; Returns True if the locus is in a non-pseudoautosomal region of chromosome Y. in_y_par; Returns True if the locus is in a pseudoautosomal region of chromosome Y. sequence_context; Return the reference genome sequence at the locus. window; Returns an interval of a specified number of bases around the locus. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of record",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.LocusExpression.html:2050,error,error,2050,docs/0.2/hail.expr.LocusExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html,1,['error'],['error']
Availability,"lowing annotations:. - **va.wasSplit** (*Boolean*) -- true if this variant was; originally multiallelic, otherwise false.; - **va.aIndex** (*Int*) -- The original index of this; alternate allele in the multiallelic representation (NB: 1; is the first alternate allele or the only alternate allele; in a biallelic variant). For example, 1:100:A:T,C splits; into two variants: 1:100:A:T with ``aIndex = 1`` and; 1:100:A:C with ``aIndex = 2``. :param bool propagate_gq: Set the GQ of output (split); genotypes to be the GQ of the input (multi-allelic) variants; instead of recompute GQ as the difference between the two; smallest PL values. Intended to be used in conjunction with; ``import_vcf(store_gq=True)``. This option will be obviated; in the future by generic genotype schemas. Experimental.; :param bool keep_star_alleles: Do not filter out * alleles.; :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :return: A biallelic variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.splitMulti(propagate_gq, keep_star_alleles, max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree,; root=strlike); def tdt(self, pedigree, root='va.tdt'):; """"""Find transmitted and untransmitted variants; count per variant and; nuclear family. .. include:: requireTGenotype.rst. **Examples**. Compute TDT association results:. >>> pedigree = Pedigree.read('data/trios.fam'); >>> (vds.tdt(pedigree); ... .export_variants(""output/tdt_results.tsv"", ""Variant = v, va.tdt.*"")). **Notes**. The transmission disequilibrium test tracks the number of times the alternate allele is transmitted (t) or not transmitted (u) from a heterozgyous parent to an affected child under the null that the rate of such transmissions is 0.5. For variants where transmission is guaranteed (i.e., the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:212473,error,error,212473,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability,"ls are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the linear regression model using the supplied phenotype and covariates.; The model is that of :py:meth:`.linreg` with sample genotype ``gt`` replaced by the score in the sample; key table. For each key, missing scores are mean-imputed across all samples. The resulting **linear regression key table** has the following columns:. - value of ``key_name`` (*String*) -- descriptor of variant group key (key column); - **beta** (*Double*) -- fit coefficient, :math:`\hat\beta_1`; - **se** (*Double*) -- estimated standard error, :math:`\widehat{\mathrm{se}}`; - **tstat** (*Double*) -- :math:`t`-statistic, equal to :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; - **pval** (*Double*) -- :math:`p`-value. :py:meth:`.linreg_burden` returns both the linear regression key table and the sample key table. **Extended example**. Let's walk through these steps in the ``max()`` toy example above.; There are six samples with the following annotations:. +--------+-------+------+------+; | Sample | pheno | cov1 | cov2 |; +========+=======+======+======+; | A | 0 | 0 | -1 |; +--------+-------+------+------+; | B | 0 | 2 | 3 |; +--------+-------+------+------+; | C | 1 | 1 | 5 |; +--------+-------+------+------+; | D | 1 | -2 | 0 |; +--------+-------+------+------+; | E | 1 | -2 | -4 |; +--------+-------+------+------+; | F | 1 | 4 | 3 |; +--------+-------+------+------+. There are three variants with the following ``gt`` values:. +---------+---+---+---+---+---+---+; | Variant | A | B | C | D | E | F |; +=========+===",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:106187,error,error,106187,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability,"lt value is ‘standard’. Parameters:; memory (Union[str, int, None]) – Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (‘standard’). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in ‘us-central1’:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]]) – The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters:; is_spot (bool) – If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the job’s storage size.; Examples; Set the job’s disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional su",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:7346,avail,available,7346,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['avail'],['available']
Availability,"lter variants.; min_gq – Minimum genotype quality to filter variants; ref_AF – Reference AF expression. Necessary when the sample size is below 10,000. Returns:; Table. hail.methods.mendel_errors(call, pedigree)[source]; Find Mendel errors; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:48509,error,errors,48509,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability,"lter(ht.i == ht.j, keep=False). col_keys = hl.literal(mt.select_cols().key_cols_by().cols().collect(), dtype=tarray(mt.col_key.dtype)); return ht.key_by(i=col_keys[ht.i], j=col_keys[ht.j]).persist(). def _bad_mu(mu: Float64Expression, maf: float) -> BooleanExpression:; """"""Check if computed value for estimated individual-specific allele; frequency (mu) is not valid for estimating relatedness. Parameters; ----------; mu : :class:`.Float64Expression`; Estimated individual-specific allele frequency.; maf : :obj:`float`; Minimum individual-specific minor allele frequency. Returns; -------; :class:`.BooleanExpression`; ``True`` if `mu` is not valid for relatedness estimation, else ``False``.; """"""; return (mu <= maf) | (mu >= (1.0 - maf)) | (mu <= 0.0) | (mu >= 1.0). def _gram(M: BlockMatrix) -> BlockMatrix:; """"""Compute Gram matrix, `M.T @ M`. Parameters; ----------; M : :class:`.BlockMatrix`. Returns; -------; :class:`.BlockMatrix`; `M.T @ M`; """"""; return (M.T @ M).checkpoint(new_temp_file('pc_relate_bm/gram', 'bm')). def _dominance_encoding(g: Float64Expression, mu: Float64Expression) -> Float64Expression:; """"""Compute value for a single entry in dominance encoding of genotype matrix,; given the number of alternate alleles from the genotype matrix and the; estimated individual-specific allele frequency. Parameters; ----------; g : :class:`.Float64Expression`; Alternate allele count.; mu : :class:`.Float64Expression`; Estimated individual-specific allele frequency. Returns; -------; gd : :class:`.Float64Expression`; Dominance-coded entry for dominance-coded genotype matrix.; """"""; gd = (; hl.case(); .when(hl.is_nan(mu), 0.0); .when(g == 0.0, mu); .when(g == 1.0, 0.0); .when(g == 2.0, 1 - mu); .or_error('entries in genotype matrix must be 0.0, 1.0, or 2.0'); ); return gd. def _AtB_plus_BtA(A: BlockMatrix, B: BlockMatrix) -> BlockMatrix:; """"""Compute `(A.T @ B) + (B.T @ A)`, used in estimating IBD0 (k0). Parameters; ----------; A : :class:`.BlockMatrix`; B : :class:`.BlockMatri",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:15274,checkpoint,checkpoint,15274,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['checkpoint'],['checkpoint']
Availability,"ludes_end=True,; reference_genome=reference_genome,; ); ); else:; ht = ht.transmute(; interval=hl.interval(; hl.struct(seqname=ht['seqname'], position=ht['start']),; hl.struct(seqname=ht['seqname'], position=ht['end']),; includes_start=True,; includes_end=True,; ); ). ht = ht.key_by('interval'). return ht. [docs]@typecheck(; gene_symbols=nullable(sequenceof(str)),; gene_ids=nullable(sequenceof(str)),; transcript_ids=nullable(sequenceof(str)),; verbose=bool,; reference_genome=nullable(reference_genome_type),; gtf_file=nullable(str),; ); def get_gene_intervals(; gene_symbols=None, gene_ids=None, transcript_ids=None, verbose=True, reference_genome=None, gtf_file=None; ):; """"""Get intervals of genes or transcripts. Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable. On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz. Example; -------; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) # doctest: +SKIP. Parameters; ----------. gene_symbols : :obj:`list` of :class:`str`, optional; Gene symbols (e.g. PCSK9).; gene_ids : :obj:`list` of :class:`str`, optional; Gene IDs (e.g. ENSG00000223972).; transcript_ids : :obj:`list` of :class:`str`, optional; Transcript IDs (e.g. ENSG00000223972).; verbose : :obj:`bool`; If ``True``, print which genes and transcripts were matched in the GTF file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform). Returns; -------; :obj:`list` of :class:`.Interval`; """"""; if gene_symbols is None and gene_ids is None and trans",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html:7082,avail,available,7082,docs/0.2/_modules/hail/experimental/import_gtf.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html,2,['avail'],['available']
Availability,"lues,; # presumably because a good estimate of the Generalized Chi-Sqaured CDF is not significantly; # affected by chi-squared components with very tiny weights.; threshold = 1e-5 * eigenvalues.sum() / eigenvalues.shape[0]; w = hl.array(eigenvalues).filter(lambda y: y >= threshold); genchisq_data = hl.pgenchisq(; ht.Q,; w=w,; k=hl.nd.ones(hl.len(w), dtype=hl.tint32),; lam=hl.nd.zeros(hl.len(w)),; mu=0,; sigma=0,; min_accuracy=accuracy,; max_iterations=iterations,; ); ht = ht.select(; 'size',; # for reasons unknown, the R implementation calls this expression the Q statistic (which is; # *not* what they write in the paper); q_stat=ht.Q / 2 / ht.s2,; # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples'). [docs]@typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; null_max_iterations=int,; null_tolerance=float,; accuracy=numeric,; iterations=int,; ); def _logistic_skat(; group,; weight,; y,; x,; covariates,; max_size: int = 46340,; null_max_iterations: int = 25,; null_tolerance: float = 1e-6,; accuracy: float = 1e-6,; iterations: int = 10000,; ):; r""""""The logistic sequence kernel association test (SKAT). Logistic SKAT tests if the phenotype, `y`, is significantly associated with the genotype,; `x`. For :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the; model is given by:. .. math::. \begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}. The usual null hypothesis is :math:`\beta_1 = 0`. SKAT tests",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:86565,fault,fault,86565,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability,"lumn if `chromosome` is not defined) is the second; element.; - `varid` (:py:data:`.tstr`) -- The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; - `rsid` (:py:data:`.tstr`) -- The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. **Entry Fields**. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the highest probability.; - `GP` (:class:`.tarray` of :py:data:`.tfloat64`) -- Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the probabilities is a distance greater than the `tolerance`; parameter from 1.0. Otherwise, the probabilities are normalized to sum to; 1.0. For example, the input ``[0.98, 0.0, 0.0]`` will be normalized to; ``[1.0, 0.0, 0.0]``. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; GEN files to import.; sample_file : :class:`str`; Sample file to import.; tolerance : :obj:`float`; If the sum of the genotype probabilities for a genotype differ from 1.0; by more than the tolerance, set the genotype to missing.; min_partitions : :obj:`int`, optional; Number of partitions.; chromosome : :class:`str`, optional; Chromosome if not included in the GEN file; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding : :obj:`dict` of :class:`str` to :obj:`str`, optional; Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by `reference_genome`.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`. Returns; -------; :class:`.MatrixTable`; """"""; gen_table = import_lines(path, min_partitions); sample_table = import_lines(sample_file); rg = reference_genome.name if reference_genome else None; if contig_recoding is None:; contig_recoding = hl.empty_dict(hl.tstr, hl.tstr); else:; contig_recoding = hl.dict(contig_recoding). gen_table = gen_table",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:49933,toler,tolerance,49933,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['toler'],['tolerance']
Availability,"lumn-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:37150,toler,tolerance,37150,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['toler'],['tolerance']
Availability,"ly ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:125530,error,errors,125530,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,"ly reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:157218,error,errors,157218,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,"ly.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR). - HemiX -- in non-PAR of X, male child; - HemiY -- in non-PAR of Y, male child; - Auto -- otherwise (in autosome or PAR, or female child). Any refers to :m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:157541,error,error,157541,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability,"m: float64,; set: str; },; variant_qc: struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64,; p_value_excess_het: float64; }; }; --------------------------------------------------------; Source:; <hail.matrixtable.MatrixTable object at 0x7f0460fbcb50>; Index:; ['row']; --------------------------------------------------------. These statistics actually look pretty good: we don’t need to filter this dataset. Most datasets require thoughtful quality control, though. The filter_rows method can help!. Let’s do a GWAS!; First, we need to restrict to variants that are :. common (we’ll use a cutoff of 1%); not so far from Hardy-Weinberg equilibrium as to suggest sequencing error. [35]:. mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01). [36]:. mt = mt.filter_rows(mt.variant_qc.p_value_hwe > 1e-6). [37]:. print('Samples: %d Variants: %d' % (mt.count_cols(), mt.count_rows())). [Stage 37:> (0 + 1) / 1]. Samples: 250 Variants: 7774. These filters removed about 15% of sites (we started with a bit over 10,000). This is NOT representative of most sequencing datasets! We have already downsampled the full thousand genomes dataset to include more common variants than we’d expect by chance.; In Hail, the association tests accept column fields for the sample phenotype and covariates. Since we’ve already got our phenotype of interest (caffeine consumption) in the dataset, we are good to go:. [38]:. gwas = hl.linear_regression_rows(y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.row.describe(). [Stage 41:> (0 + 1) / 1]. --------------------------------------------------------; Type:; str",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:18413,error,error,18413,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['error'],['error']
Availability,"made to genotype probabilities in BGEN v1.1 files:. Since genotype probabilities are understood to define a probability distribution, import_bgen() automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the tolerance parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains.; import_bgen() normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. Annotations; import_bgen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .bgen files to import.; tolerance (float) – If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1.; sample_file (str or None) – Sample file.; min_partitions (int or None) – Number of partitions. Returns:Variant dataset imported from .bgen file. Return type:VariantDataset. import_gen(path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None)[source]¶; Import .gen file(s) as variant dataset.; Examples; Read a .gen file and a .sample file and write to a .vds file:; >>> (hc.import_gen('data/example.gen', sample_file='data/example.sample'); ... .write('output/gen_example1.vds')). Load multiple files at the same time with Hadoop glob patterns:; >>> (hc.import_gen('data/example.chr*.gen', sample_file='data/example.sample'); ... .write('output/gen_example2.vds')). Notes; For more information on the .gen file format, see here.; To ensure that the .gen file(s) and .sample file are correctly prepared for import:. If there are only 5 columns before the start of the genotype probability data (chromosome f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:10358,toler,tolerance,10358,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['toler'],['tolerance']
Availability,"mat specifiers and arguments. Missing values are printed as ``'null'`` except when using the; format flags `'b'` and `'B'` (printed as ``'false'`` instead). Parameters; ----------; f : :class:`.StringExpression`; Java `format string <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Formatter.html#syntax>`__.; args : variable-length arguments of :class:`.Expression`; Arguments to format. Returns; -------; :class:`.StringExpression`; """""". return _func(""format"", hl.tstr, f, hl.tuple(args)). [docs]@typecheck(x=expr_float64, y=expr_float64, tolerance=expr_float64, absolute=expr_bool, nan_same=expr_bool); def approx_equal(x, y, tolerance=1e-6, absolute=False, nan_same=False):; """"""Tests whether two numbers are approximately equal. Examples; --------; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters; ----------; x : :class:`.NumericExpression`; y : :class:`.NumericExpression`; tolerance : :class:`.NumericExpression`; absolute : :class:`.BooleanExpression`; If True, compute ``abs(x - y) <= tolerance``. Otherwise, compute; ``abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022)``.; nan_same : :class:`.BooleanExpression`; If True, then ``NaN == NaN`` will evaluate to True. Otherwise,; it will return False. Returns; -------; :class:`.BooleanExpression`; """""". return _func(""approxEqual"", hl.tbool, x, y, tolerance, absolute, nan_same). def _shift_op(x, y, op):; assert op in ('<<', '>>', '>>>'); t = x.dtype; if t == hl.tint64:; word_size = 64; zero = hl.int64(0); else:; word_size = 32; zero = hl.int32(0). indices, aggregations = unify_all(x, y); return hl.bind(; lambda x, y: (; hl.case(); .when(y >= word_size, hl.sign(x) if op == '>>' else zero); .when(y >= 0, construct_expr(ir.ApplyBinaryPrimOp(op, x._ir, y._ir), t, indices, aggregations)); .or_error('cannot shift by a negative value: ' + hl.str(x) + f"" {op} "" + hl.str(y)); ),; x,; y,; ). def _bit_op(x, y, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:177175,toler,tolerance,177175,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['toler'],['tolerance']
Availability,"mat,; see `here <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only **unphased** and **diploid** genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed. Before importing, ensure that:. - The sample file has the same number of samples as the BGEN file.; - No duplicate sample IDs are present. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns <sec-hadoop-glob>`. .. _gpfilters:. **Genotype probability (``gp``) representation**:. The following modifications are made to genotype probabilities in BGEN v1.1 files:. - Since genotype probabilities are understood to define a probability distribution, :py:meth:`~hail.HailContext.import_bgen` automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the ``tolerance`` parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains. - :py:meth:`~hail.HailContext.import_bgen` normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. **Annotations**. :py:meth:`~hail.HailContext.import_bgen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .bgen files to import.; :type path: str or list of str. :param float tolerance: If the sum of the probabilities for a; genotype differ from 1.0 by more than the tolerance, set; the genotype to missing. Only applicable if the BGEN files are v1.1. :param sample_file: Sample file.; :type sample_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:6900,toler,tolerance,6900,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['toler'],['tolerance']
Availability,"matically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each job j uses an f-string; to create a temporary output file j.ofile where the output to echo is redirected.; We then use all of the output files in the sink command by creating a string; with the temporary output file names for each job. A JobResourceFile; is a Batch-specific object that inherits from str. Therefore, you can use; JobResourceFile as if they were strings, which we do with the join; command for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:7197,echo,echo,7197,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability,"mber of partitions. Examples; --------. Repartition to 500 partitions:. >>> table_result = table1.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_table(tmp2).add_index(uid).key_by(uid); ht.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n).key_by().drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:93044,resilien,resilient-distributed-datasets-rdds,93044,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['resilien'],['resilient-distributed-datasets-rdds']
Availability,"me in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per-user jobs; be implicit file dependencies in the sink job (see the section on; file dependencies). The changes from the previous; example to make this happen are each job j uses an f-string; to create a temporary output file j.ofile where the output to echo is redirected.; We then use all of the output files in the sink command by creating a string; with the temporary output file names for each job. A JobResourceFile; is a Batch-specific object that inherits from str. Therefore, you can use; JobResourceFile as if they were strings, which we do with the join; command for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(na",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:7294,echo,echo,7294,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability,"me ploidy from a precomputed interval coverage MatrixTable. to_dense_mt(vds); Creates a single, dense MatrixTable from the split VariantDataset representation. to_merged_sparse_mt(vds, *[, ...]); Creates a single, merged sparse MatrixTable from the split VariantDataset representation. truncate_reference_blocks(ds, *[, ...]); Cap reference blocks at a maximum length in order to permit faster interval filtering. merge_reference_blocks(ds, equivalence_function); Merge adjacent reference blocks according to user equivalence criteria. lgt_to_gt(lgt, la); Transform LGT into GT using local alleles array. local_to_global(array, local_alleles, ...); Reindex a locally-indexed array to globally-indexed. store_ref_block_max_length(vds_path); Patches an existing VDS file to store the max reference block length for faster interval filters. Variant Dataset Combiner. VDSMetadata; The path to a Variant Dataset and the number of samples within. VariantDatasetCombiner; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. new_combiner(*, output_path, temp_path[, ...]); Create a new VariantDatasetCombiner or load one from save_path. load_combiner(path); Load a VariantDatasetCombiner from path. The data model of VariantDataset; A VariantDataset is the Hail implementation of a data structure called the; “scalable variant call representation”, or SVCR. The Scalable Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/index.html:3404,failure,failure-tolerant,3404,docs/0.2/vds/index.html,https://hail.is,https://hail.is/docs/0.2/vds/index.html,1,['failure'],['failure-tolerant']
Availability,"meters; ----------; dataset : :class:`.MatrixTable`; pedigree : :class:`.Pedigree`. Returns; -------; (:class:`.Table`, :class:`.Table`, :class:`.Table`, :class:`.Table`); """"""; source = call._indices.source; if not isinstance(source, MatrixTable):; raise ValueError(; ""'mendel_errors': expected 'call' to be an expression of 'MatrixTable', found {}"".format(; ""expression of '{}'"".format(source.__class__) if source is not None else 'scalar expression'; ); ). source = source.select_entries(__GT=call); dataset = require_biallelic(source, 'mendel_errors'); tm = trio_matrix(dataset, pedigree, complete_trios=True); tm = tm.select_entries(; mendel_code=hl.mendel_error_code(; tm.locus, tm.is_female, tm.father_entry['__GT'], tm.mother_entry['__GT'], tm.proband_entry['__GT']; ); ); ck_name = next(iter(source.col_key)); tm = tm.filter_entries(hl.is_defined(tm.mendel_code)); tm = tm.rename({'id': ck_name}). entries = tm.entries(). table1 = entries.select('fam_id', 'mendel_code'). t2 = tm.annotate_cols(errors=hl.agg.count(), snp_errors=hl.agg.count_where(hl.is_snp(tm.alleles[0], tm.alleles[1]))); table2 = t2.key_cols_by().cols(); table2 = table2.select(; pat_id=table2.father[ck_name],; mat_id=table2.mother[ck_name],; fam_id=table2.fam_id,; errors=table2.errors,; snp_errors=table2.snp_errors,; ); table2 = table2.group_by('pat_id', 'mat_id').aggregate(; fam_id=hl.agg.take(table2.fam_id, 1)[0],; children=hl.int32(hl.agg.count()),; errors=hl.agg.sum(table2.errors),; snp_errors=hl.agg.sum(table2.snp_errors),; ); table2 = table2.annotate(; errors=hl.or_else(table2.errors, hl.int64(0)), snp_errors=hl.or_else(table2.snp_errors, hl.int64(0)); ). # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; implicated = hl.literal(; [; [0, 0, 0], # dummy; [1, 1, 1],; [1, 1, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [1, 0, 1],; [0, 1, 1],; [0, 0, 1],; [0, 1, 1],; [0, 1, 1],; [1, 0, 1],; [1, 0, 1],; ],; dtype=hl.tarray(hl.tarray(hl.tint64)),; ). table3 = (; tm.annotate_cols(; all_errors=hl.or_el",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:10691,error,errors,10691,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. """"""; if max_iterations is None:; max_iterations = hl.literal(10_000); if min_accuracy is None:; min_accuracy = hl.literal(1e-5); return _func(""pgenchisq"", PGENCHISQ_RETURN_TYPE, x - mu, w, k, lam, sigma, max_iterations, min_accuracy). [docs]@typecheck(x=expr_float64, mu=expr_float64, sigma=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False) -> Float64Expression:; """"""The cumulative probability function of a normal distribution with mean; `mu` and standard deviation `sigma`. Returns cumulative probability of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; -----; Returns the left-tail probability `p` = Prob(:math:`Z < x`) with :math:`Z`; a normal random variable. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:71508,error,error,71508,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['error'],['error']
Availability,"mmand for strings. >>> b = hb.Batch(name='scatter-gather-2'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}"" > {j.ofile}'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command('cat {}'.format(' '.join([j.ofile for j in jobs]))); >>> b.run(). Nested Scatters; We can also create a nested scatter where we have a series of jobs per user.; This is equivalent to a nested for loop. In the example below, we instantiate a; new Batch object b. Then for each user in ‘Alice’, ‘Bob’, and ‘Dan’; we create new jobs for making the bed, doing laundry, and grocery shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The do_chores function takes a Batch object to add new jobs; to and a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines.; >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... do_chores(b, user); >>> b.run(). Lastly, we provide an example of a more complicated batch that has an initial; job, then scatters jobs per user, then has a series of gather / sink jobs; to wait for the per user jobs to be done before",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:9046,echo,echo,9046,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability,"mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautoso",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:157397,error,errors,157397,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,"mple:; >>> split_ds = hl.split_multi_hts(dataset); >>> split_ds = split_ds.annotate_rows(info = split_ds.info.annotate(AC = split_ds.info.AC[split_ds.a_index - 1])); >>> hl.export_vcf(split_ds, 'output/export.vcf') . The info field AC in data/export.vcf will have Number=1.; New Fields; split_multi_hts() adds the following fields:. was_split (bool) – True if this variant was originally; multiallelic, otherwise False.; a_index (int) – The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with a_index = 1 and 1:100:A:C; with a_index = 2. See also; split_multi(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left; aligned and have unique loci. This avoids a shuffle. If the assumption; is violated, an error is generated.; vep_root (str) – Top-level location of vep data. All variable-length VEP fields; (intergenic_consequences, motif_feature_consequences,; regulatory_feature_consequences, and transcript_consequences); will be split properly (i.e. a_index corresponding to the VEP allele_num).; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table – A biallelic variant dataset. hail.methods.summarize_variants(mt, show=True, *, handler=None)[source]; Summarize the variants present in a dataset and print the results.; Examples; >>> hl.summarize_variants(dataset) ; ==============================; Number of variants: 346; ==============================; Alleles per variant; -------------------; 2 alleles: 346 variants; ==============================; Variants per contig; -------------------; 20: 346",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:91595,error,error,91595,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['error']
Availability,"mputing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are filtered are missing.; - `new_to_old` (``array<int32>``) -- An array that maps new allele index to; the old allele index. Its length is the same as the modified `alleles`; field. **Downcode algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; :func:`.split_multi_hts`. The downcode algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded genotype, and shift so; the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Subset algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:159637,down,downcoding,159637,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['down'],['downcoding']
Availability,"mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl.is_defined(mt.PL),; hl.bind(; lambda unnorm: unnorm - hl.min(unnorm),; hl.range(0, hl.triangle(mt.alleles.length())).map(; lambda newi: hl.bind(; lambda newc: mt.PL[; hl.call(mt.new_to_old[newc[0]], mt.new_to_old[newc[1]]).unphased_diploid_gt_index(); ],; hl.unphased_diploid_gt_index_call(newi),; ); ),; ),; hl.missing(tarray(tint32)),; ); return mt.annotate_entries(; GT=hl.unphased_diploid_gt_index_call(hl.argmin(newPL, unique=True)),; AD=hl.if_else(; hl.is_defined(mt.AD),; hl.range(0, mt.alleles.length()).map(lambda newi: mt.AD[mt.new_to_old[newi]]),; hl.missing(tarray(tint32)),; ),; # DP unchanged; GQ=hl.gq_from_pl(newPL),; PL=newPL,; ); # otherwise downcode; else:; mt = mt.annotate_rows(__old_to_new_no_na=mt.old_to_new.map(lambda x: hl.or_else(x, 0))); newPL = hl.if_else(; hl.is_defined(mt.PL),; (; hl.range(0, hl.triangle(hl.len(mt.alleles))).map(; lambda newi: hl.min(; hl.range(0, hl.triangle(hl.len(mt.old_alleles))); .filter(; lambda oldi: hl.bind(; lambda oldc: hl.call(mt.__old_to_new_no_na[oldc[0]], mt.__old_to_new_no_na[oldc[1]]); == hl.unphased_diploid_gt_index_call(newi),; hl.unphased_diploid_gt_index_call(oldi),; ); ); .map(lambda oldi: mt.PL[oldi]); ); ); ),; hl.missing(tarray(tint32)),; ); return mt.annotate_entries(; GT=hl.call(mt.__old_to_new_no_na[mt.GT[0]], mt.__old_to_new_no_na[mt.GT[1]]),; AD=hl.if_else(; hl.is_defined(mt.AD),; (; hl.range(0, hl.len(mt.alleles)).map(; lambda newi: hl.sum(; hl.range(0, hl.len(mt.old_alleles)); .filter(lambda oldi: mt.__old_to_new_no_na[oldi] == newi); .map(lambda oldi: mt.AD[oldi]); ); ); ),; hl.missing(tarray(tint32)),; ),; # DP unchanged; GQ=hl.gq_from_pl(newPL),; PL=ne",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:163661,down,downcode,163661,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['down'],['downcode']
Availability,"mum value in the array. argmax(array[, unique]); Return the index of the maximum value in the array. corr(x, y); Compute the Pearson correlation coefficient between x and y. binary_search(array, elem); Binary search array for the insertion point of elem. hail.expr.functions.abs(x)[source]; Take the absolute value of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.abs(-5)); 5. >>> hl.eval(hl.abs([1.0, -2.5, -5.1])); [1.0, 2.5, 5.1]. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Returns:; NumericExpression, ArrayNumericExpression or NDArrayNumericExpression. hail.expr.functions.approx_equal(x, y, tolerance=1e-06, absolute=False, nan_same=False)[source]; Tests whether two numbers are approximately equal.; Examples; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters:. x (NumericExpression); y (NumericExpression); tolerance (NumericExpression); absolute (BooleanExpression) – If True, compute abs(x - y) <= tolerance. Otherwise, compute; abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022).; nan_same (BooleanExpression) – If True, then NaN == NaN will evaluate to True. Otherwise,; it will return False. Returns:; BooleanExpression. hail.expr.functions.bit_and(x, y)[source]; Bitwise and x and y.; Examples; >>> hl.eval(hl.bit_and(5, 3)); 1. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_or(x, y)[source]; Bitwise or x and y.; Examples; >>> hl.eval(hl.bit_or(5, 3)); 7. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_xor(x, y)[source]; Bitwise exclusive-",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:3833,toler,tolerance,3833,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,2,['toler'],['tolerance']
Availability,must be a single entry-indexed field; (not a list of fields).; * ``n_samples_exprs`` must be a single entry-indexed field; (not a list of fields).; * The ``phenotype`` field that keys the table returned by; :func:`.ld_score_regression` will have values corresponding to the; column keys of the input matrix table. This function returns a :class:`~.Table` with one row per set of summary; statistics passed to the ``chi_sq_exprs`` argument. The following; row-indexed fields are included in the table:. * **phenotype** (:py:data:`.tstr`) -- The name of the phenotype. The; returned table is keyed by this field. See the notes below for; details on the possible values of this field.; * **mean_chi_sq** (:py:data:`.tfloat64`) -- The mean chi-squared; test statistic for the given phenotype.; * **intercept** (`Struct`) -- Contains fields:. - **estimate** (:py:data:`.tfloat64`) -- A point estimate of the; intercept :math:`1 + Na`.; - **standard_error** (:py:data:`.tfloat64`) -- An estimate of; the standard error of this point estimate. * **snp_heritability** (`Struct`) -- Contains fields:. - **estimate** (:py:data:`.tfloat64`) -- A point estimate of the; SNP-heritability :math:`h_g^2`.; - **standard_error** (:py:data:`.tfloat64`) -- An estimate of; the standard error of this point estimate. Warning; -------; :func:`.ld_score_regression` considers only the rows for which both row; fields ``weight_expr`` and ``ld_score_expr`` are defined. Rows with missing; values in either field are removed prior to fitting the LD score; regression model. Parameters; ----------; weight_expr : :class:`.Float64Expression`; Row-indexed expression for the LD scores used to derive; variant weights in the model.; ld_score_expr : :class:`.Float64Expression`; Row-indexed expression for the LD scores used as covariates; in the model.; chi_sq_exprs : :class:`.Float64Expression` or :obj:`list` of; :class:`.Float64Expression`; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions f,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ld_score_regression.html:6747,error,error,6747,docs/0.2/_modules/hail/experimental/ld_score_regression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ld_score_regression.html,2,['error'],['error']
Availability,"n 0 and 1.; position: :class:`str`; Tells how to deal with different groups of data at same point. Options are ""stack"" and ""dodge"". Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomHistogram(; mapping,; min_val=min_val,; max_val=max_val,; bins=bins,; fill=fill,; color=color,; alpha=alpha,; position=position,; size=size,; ). # Computes the maximum entropy distribution whose cdf is within +- e of the; # staircase-shaped cdf encoded by min_x, max_x, x, y.; #; # x is an array of n x-coordinates between min_x and max_x, and y is an array; # of (n+1) y-coordinates between 0 and 1, both sorted. Together they encode a; # staircase-shaped cdf.; # For example, if min_x = 1, max_x=4, x=[2], y=[.2, .6], then the cdf is the; # staircase tracing the points; # (1, 0) - (1, .2) - (2, .2) - (2, .6) - (4, .6) - (4, 1); #; # Now consider the set of all possible cdfs within +-e of the one above. In; # other words, shift the staircase both up and down by e, capping above and; # below at 1 and 0, and consider all possible cdfs that lie in between. The; # distribution with maximum entropy whose cdf is between the two staircases; # is the one whose cdf is the graph constructed as follows: tie a rubber band; # to the points (min_x, 0) and (max_x, 1), place the middle between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.; def _max_entropy_cdf(min_x, max_x, x, y, e):; def poin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:14237,down,down,14237,docs/0.2/_modules/hail/ggplot/geoms.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html,2,['down'],['down']
Availability,"n API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:1351,down,downloads,1351,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,1,['down'],['downloads']
Availability,"n and hl.export_bgen now support compression; with Zstd. Bug fixes. (#12585); hail.ggplots that have more than one legend group or facet are; now interactive. If such a plot has enough legend entries that the; legend would be taller than the plot, the legend will now be; scrollable. Legend entries for such plots can be clicked to show/hide; traces on the plot, but this does not work and is a known issue that; will only be addressed if hail.ggplot is migrated off of plotly.; (#12584) Fixed bug; which arose as an assertion error about type mismatches. This was; usually triggered when working with tuples.; (#12583) Fixed bug; which showed an empty table for ht.col_key.show().; (#12582) Fixed bug; where matrix tables with duplicate col keys do not show properly.; Also fixed bug where tables and matrix tables with HTML unsafe column; headers are rendered wrong in Jupyter.; (#12574) Fixed a; memory leak when processing tables. Could trigger unnecessarily high; memory use and out of memory errors when there are many rows per; partition or large key fields.; (#12565) Fixed a bug; that prevented exploding on a field of a Table whose value is a; random value. Version 0.2.107; Released 2022-12-14. Bug fixes. (#12543) Fixed; hl.vds.local_to_global error when LA array contains non-ascending; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12497) Added; support for scales, nrow, and ncol arguments, as well as; grouped legends, to hail.ggplot.facet_wrap.; (#12471) Added; hailctl batch submit command to run local scripts inside batch; jobs.; (#12525) Add support; for passing arguments to hailctl batch submit.; (#12465) Batch jobs’; status now contains the region the job ran in. The job itself can; access which region it is in through the HAIL_REGION environment; variable.; (#12464) When using; Query-on-Batch, all jobs ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:40968,error,errors,40968,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"n boolean expressions or collections of booleans. all(*args); Check for all True in boolean expressions or collections of booleans. filter(f, collection); Returns a new collection containing elements where f returns True. sorted(collection[, key, reverse]); Returns a sorted array. find(f, collection); Returns the first element where f returns True. group_by(f, collection); Group collection elements into a dict according to a lambda function. fold(f, zero, collection); Reduces a collection with the given function f, provided the initial value zero. array_scan(f, zero, a); Map each element of a to cumulative value of function f, with initial value zero. reversed(x); Reverses the elements of a collection. keyed_intersection(*arrays, key); Compute the intersection of sorted arrays on a given key. keyed_union(*arrays, key); Compute the distinct union of sorted arrays on a given key. Numeric functions. abs(x); Take the absolute value of a numeric value, array or ndarray. approx_equal(x, y[, tolerance, absolute, ...]); Tests whether two numbers are approximately equal. bit_and(x, y); Bitwise and x and y. bit_or(x, y); Bitwise or x and y. bit_xor(x, y); Bitwise exclusive-or x and y. bit_lshift(x, y); Bitwise left-shift x by y. bit_rshift(x, y[, logical]); Bitwise right-shift x by y. bit_not(x); Bitwise invert x. bit_count(x); Count the number of 1s in the in the two's complement binary representation of x. exp(x). expit(x). is_nan(x). is_finite(x). is_infinite(x). log(x[, base]); Take the logarithm of the x with base base. log10(x). logit(x). sign(x); Returns the sign of a numeric value, array or ndarray. sqrt(x). int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. floor(x). ceil(x). uniroot(f, min, max, *[, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/index.html:6689,toler,tolerance,6689,docs/0.2/functions/index.html,https://hail.is,https://hail.is/docs/0.2/functions/index.html,1,['toler'],['tolerance']
Availability,"n by chromosome) and position (4th column if chromosome is not; defined). If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: contig with type; tstr and position with type tint32.; alleles (tarray of tstr) – Row key. An array; containing the alleles of the variant. The reference allele (4th column if; chromosome is not defined) is the first element of the array and the; alternate allele (5th column if chromosome is not defined) is the second; element.; varid (tstr) – The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; rsid (tstr) – The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. Entry Fields. GT (tcall) – The hard call corresponding to the genotype with; the highest probability.; GP (tarray of tfloat64) – Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the probabilities is a distance greater than the tolerance; parameter from 1.0. Otherwise, the probabilities are normalized to sum to; 1.0. For example, the input [0.98, 0.0, 0.0] will be normalized to; [1.0, 0.0, 0.0]. Parameters:. path (str or list of str) – GEN files to import.; sample_file (str) – Sample file to import.; tolerance (float) – If the sum of the genotype probabilities for a genotype differ from 1.0; by more than the tolerance, set the genotype to missing.; min_partitions (int, optional) – Number of partitions.; chromosome (str, optional) – Chromosome if not included in the GEN file; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of str to str, optional) – Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by reference_genome.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome. Returns:; MatrixTable. hail.methods.import_locus_intervals(path",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:18424,toler,tolerance,18424,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['toler'],['tolerance']
Availability,"n exception will be thrown.; The f argument should be a function taking one argument, an expression of; the element type of array, and returning an expression including; aggregation(s). The type of the aggregated expression returned by; array_agg() is an array of elements of the return type of f. Parameters:. f – Aggregation function to apply to each element of the exploded array.; array (ArrayExpression) – Array to aggregate. Returns:; ArrayExpression. hail.expr.aggregators.downsample(x, y, label=None, n_divisions=500)[source]; Downsample (x, y) coordinate datapoints. Parameters:. x (NumericExpression) – X-values to be downsampled.; y (NumericExpression) – Y-values to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the distribution of values. In; particular, for any value x = values(i) in the summary, we estimate that; there are ranks(i) values strictly less than x, and that there are; ranks(i+1) values less than or equal to x. For any value y (not; necessarily in the summary), we estimate CDF(y) to be ranks(i), where i; is such that values(i-1) < y ≤ values(i).; An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:33657,down,downsampled,33657,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['down'],['downsampled']
Availability,"n from the same Table.; - a tuple (str, NumericExpression) from the same Table. If passed as a tuple the first element is used as the hover label.; If no label or a single label is provided, then returns bokeh.plotting.figure; Otherwise returns a bokeh.models.layouts.Column containing:; - a bokeh.models.widgets.inputs.Select dropdown selection widget for labels; - a bokeh.plotting.figure containing the interactive scatter plot; Points will be colored by one of the labels defined in the label using the color scheme defined in; the corresponding entry of colors if provided (otherwise a default scheme is used). To specify your color; mapper, check the bokeh documentation; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions.; Hovering on points will display their coordinates, labels and any additional fields specified in hover_fields. Parameters:. x (NumericExpression or (str, NumericExpression)) – List of x-values to be plotted.; y (NumericExpression or (str, NumericExpression)) – List of y-values to be plotted.; label (Expression or Dict[str, Expression]], optional) – Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title (str, optional) – Title of the scatterplot.; xlabel (str, optional) – X-axis label.; ylabel (str, optional) – Y-axis label.; size (int) – Size of markers in screen space units.; legend (bool) – Whether or not to show the legend in the result",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:7356,down,down,7356,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['down'],['down']
Availability,"n of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probability p = Prob(\(Z < x\)) with \(Z\); a normal random variable. Defaults to a standard normal random variable. Parameters:. x (float or Expression of type tfloat64); mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:20952,fault,fault,20952,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['fault'],['fault']
Availability,"n self. if self.n_rows == 1:; index_expr = [0]; elif self.n_cols == 1:; index_expr = [1]; else:; index_expr = [1, 0]. return BlockMatrix(BlockMatrixBroadcast(self._bmir, index_expr, [self.n_cols, self.n_rows], self.block_size)). [docs] def densify(self):; """"""Restore all dropped blocks as explicit blocks of zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""; return Env.backend().persist_blockmatrix(self). [docs] def unpersist(self):; """"""Unpersists this block matrix from memory/disk. Notes; -----; This function will have no effect on a block matrix that was not previously; persisted. Returns; -------; :class:`.BlockMatrix`; Unpe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:42406,redundant,redundant,42406,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['redundant'],['redundant']
Availability,"n testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = (hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)) > 0.5). Test if the phenotype is significantly associated with the genotype:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 1.78e+02 | 1.68e-04 | 0 |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real dat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:92010,fault,fault,92010,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability,"n the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:156536,error,errors,156536,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,"n to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Gene",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:79362,error,error,79362,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"n {first_mismatch}""; ). # check locus distinctness; n_rd_rows = rd.count_rows(); n_distinct = rd.distinct_by_row().count_rows(). if n_distinct != n_rd_rows:; error(f'reference data loci are not distinct: found {n_rd_rows} rows, but {n_distinct} distinct loci'). # check END field; end_exprs = dict(; missing_end=hl.agg.filter(hl.is_missing(rd.END), hl.agg.take((rd.row_key, rd.col_key), 5)),; end_before_position=hl.agg.filter(rd.END < rd.locus.position, hl.agg.take((rd.row_key, rd.col_key), 5)),; ); if VariantDataset.ref_block_max_length_field in rd.globals:; rbml = rd[VariantDataset.ref_block_max_length_field]; end_exprs['blocks_too_long'] = hl.agg.filter(; rd.END - rd.locus.position + 1 > rbml, hl.agg.take((rd.row_key, rd.col_key), 5); ). res = rd.aggregate_entries(hl.struct(**end_exprs)). if res.missing_end:; error(; 'found records in reference data with missing END field\n '; + '\n '.join(str(x) for x in res.missing_end); ); if res.end_before_position:; error(; 'found records in reference data with END before locus position\n '; + '\n '.join(str(x) for x in res.end_before_position); ); blocks_too_long = res.get('blocks_too_long', []); if blocks_too_long:; error(; 'found records in reference data with blocks larger than `ref_block_max_length`\n '; + '\n '.join(str(x) for x in blocks_too_long); ). def _same(self, other: 'VariantDataset'):; return self.reference_data._same(other.reference_data) and self.variant_data._same(other.variant_data). [docs] def union_rows(*vdses):; """"""Combine many VDSes with the same samples but disjoint variants. **Examples**. If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:. >>> vds_paths = ['chr1.vds', 'chr2.vds'] # doctest: +SKIP; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) # doctest: +SKIP; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) # doctest: +SKIP. """""". fd = hl.vds.VariantDataset.ref_block_max_length_field; mts = [vds.reference_data for vds in vdses]; n_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:11169,error,error,11169,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['error'],['error']
Availability,"n ‘us-central1’:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]]) – The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters:; is_spot (bool) – If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the job’s storage size.; Examples; Set the job’s disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, jobs requesting one or more cores receive; 5 GiB of storage for the root file system /. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at /io. Batch automatically writes all ResourceFile to; /io.; The default storage size is 0 Gi. The minimum storage size is ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:7886,echo,echo,7886,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['echo'],['echo']
Availability,"n-imputes missing values and tests pairs of variants in a different order than PLINK.; Be sure to provide enough disk space per worker because ld_prune() persists up to 3 copies of the data to both memory and disk.; The amount of disk space required will depend on the size and minor allele frequency of the input data and the prune parameters r2 and window. The number of bytes stored in memory per variant is about nSamples / 4 + 50. Warning; The variants in the pruned set are not guaranteed to be identical each time ld_prune() is run. We recommend running ld_prune() once and exporting the list of LD pruned variants using; export_variants() for future use. Parameters:; r2 (float) – Maximum \(R^2\) threshold between two variants in the pruned set within a given window.; window (int) – Width of window in base-pairs for computing pair-wise \(R^2\) values.; memory_per_core (int) – Total amount of memory available for each core in MB. If unsure, use the default value.; num_cores (int) – The number of cores available. Equivalent to the total number of workers times the number of cores per worker. Returns:Variant dataset filtered to those variants which remain after LD pruning. Return type:VariantDataset. linreg(y, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0)[source]¶; Test each variant for association using linear regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run linear regression per variant using a phenotype and two covariates stored in sample annotations:; >>> vds_result = vds.linreg('sa.pheno.height', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The linreg() method computes, for each variant, statistics of; the \(t\)-test for the genotype coefficient of the linear function; of best fit from sample genotype and covariates to quantitative; phenotype or case-control status. Hail only includes samples for which; phenotype and all covariates are defined. For each variant",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:77859,avail,available,77859,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['avail'],['available']
Availability,"n-major format; to row-major format before writing.; If ``False``, write blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path). writer = BlockMatrixNativeWriter(path, overwrite, force_row_major, stage_locally); Env.backend().execute(BlockMatrixWrite(self._bmir, writer)). [docs] @typecheck_method(path=str, overwrite=bool, force_row_major=bool, stage_locally=bool); def checkpoint(self, path, overwrite=False, force_row_major=False, stage_locally=False):; """"""Checkpoint the block matrix. .. include:: ../_templates/write_warning.rst. Parameters; ----------; path: :class:`str`; Path for output file.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; force_row_major: :obj:`bool`; If ``True``, transform blocks in column-major format; to row-major format before checkpointing.; If ``False``, checkpoint blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Bl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:21371,checkpoint,checkpoint,21371,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['checkpoint'],['checkpoint']
Availability,"n_axes:; # the expected axes of aggregated expressions are the expected axes + axes aggregated over; expected_agg_axes = expected_axes.union(aggregation_axes). for agg in aggregations:; assert isinstance(agg, Aggregation); refs = get_refs(*agg.exprs); agg_axes = agg.agg_axes(). # check for stray indices; unexpected_agg_axes = agg_axes - expected_agg_axes; if unexpected_agg_axes:; # one or more out-of-scope fields; bad_refs = []; for name, inds in refs.items():; bad_axes = inds.axes.intersection(unexpected_agg_axes); if bad_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0. errors.append(; ExpressionException(; ""scope violation: '{caller}' supports aggregation over indices {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}"".format(; caller=caller,; expected=list(aggregation_axes),; axes=list(agg_axes),; stray=list(unexpected_agg_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)); for name, inds in bad_refs; ),; ); ); ); else:; errors.append(ExpressionException(""'{}' does not support aggregation"".format(caller))). for w in warnings:; warning('{}'.format(w.msg)); if errors:; for e in errors:; error('{}'.format(e.msg)); raise errors[0]. @typecheck(expression=expr_any); def eval_timed(expression):; """"""Evaluate a Hail expression, returning the result and the times taken for; each stage in the evaluation process. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; (Any, dict); Result of evaluating `expression` and a dictionary of the timings; """""". from hail.utils.java import Env. analyze('eval', expression, Indices(expression._indices.source)); if expression._indices.source is None:; ir_type = expression._ir.typ; expression_type = expression.dtype; if ir_type != expression.dtype:; raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:4206,error,errors,4206,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html,2,['error'],['errors']
Availability,"n_type):; """"""Add global annotations from Python objects. **Examples**. Add populations as a global annotation:; ; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). **Notes**. This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given ``annotation``; parameter. :param str path: annotation path starting in 'global'. :param annotation: annotation to add to global. :param annotation_type: Hail type of annotation; :type annotation_type: :py:class:`.Type`. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". annotation_type._typecheck(annotation). annotated = self._jvds.annotateGlobal(annotation_type._convert_to_j(annotation), annotation_type._jtype, path); assert annotated.globalSignature().typeCheck(annotated.globalAnnotation()), 'error in java type checking'; return VariantDataset(self.hc, annotated). [docs] @handle_py4j; @typecheck_method(expr=oneof(strlike, listof(strlike))); def annotate_samples_expr(self, expr):; """"""Annotate samples with expression. **Examples**. Compute per-sample GQ statistics for hets:. >>> vds_result = (vds.annotate_samples_expr('sa.gqHetStats = gs.filter(g => g.isHet()).map(g => g.gq).stats()'); ... .export_samples('output/samples.txt', 'sample = s, het_gq_mean = sa.gqHetStats.mean')). Compute the list of genes with a singleton LOF per sample:. >>> variant_annotations_table = hc.import_table('data/consequence.tsv', impute=True).key_by('Variant'); >>> vds_result = (vds.annotate_variants_table(variant_annotations_table, root='va.consequence'); ... .annotate_variants_expr('va.isSingleton = gs.map(g => g.nNonRefAlleles()).sum() == 1'); ... .annotate_samples_expr('sa.LOF_genes = gs.filter(g => va.isSingleton && g.isHet() && va.consequence == ""LOF"").map(g => va.gene).collect()')). To create an annotation for ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:13480,error,error,13480,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability,"n` of type :py:class:`.tarray` of :py:data:`.tint32`; A degrees of freedom parameter for each non-central chi-square term.; lam : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A non-centrality parameter for each non-central chi-square term. We use `lam` instead; of `lambda` because the latter is a reserved word in Python.; mu : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; sigma : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; max_iterations : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. """"""; if max_iterations is None:; max_iterations = hl.literal(10_000); if min_accuracy is None:; min_accuracy = hl.literal(1e-5); ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:70700,error,error,70700,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['error'],['error']
Availability,"nce sequence. Use ReferenceGenome.add_sequence() to; load and attach a reference sequence to a reference genome.; Returns None if contig and position are not valid coordinates in; reference_genome. Parameters:. contig (Expression of type tstr) – Locus contig.; position (Expression of type tint32) – Locus position.; before (Expression of type tint32, optional) – Number of bases to include before the locus of interest. Truncates at; contig boundary.; after (Expression of type tint32, optional) – Number of bases to include after the locus of interest. Truncates at; contig boundary.; reference_genome (str or ReferenceGenome) – Reference genome to use. Must have a reference sequence available. Returns:; StringExpression. hail.expr.functions.mendel_error_code(locus, is_female, father, mother, child)[source]; Compute a Mendelian violation code for genotypes.; >>> father = hl.call(0, 0); >>> mother = hl.call(1, 1); >>> child1 = hl.call(0, 1) # consistent; >>> child2 = hl.call(0, 0) # Mendel error; >>> locus = hl.locus('2', 2000000). >>> hl.eval(hl.mendel_error_code(locus, True, father, mother, child1)); None. >>> hl.eval(hl.mendel_error_code(locus, True, father, mother, child2)); 7. Note; Ignores call phasing, and assumes diploid and biallelic. Haploid calls for; hemiploid samples on sex chromosomes also are acceptable input. Notes; In the table below, the copy state of a locus with respect to a trio is; defined as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; LocusExpression.in_autosome():. Auto – in autosome or in PAR, or in non-PAR of X and female child; HemiX – in non-PAR of X and male child; HemiY – in non-PAR of Y and male child. Any refers to the set { HomRef, Het, HomVar, NoCall } and ~; denotes complement in this set. Code; Dad; Mom; Kid; Copy State; Implicated. 1; HomVar; HomVar; Het; Auto; Dad, Mom, Kid. 2; HomRef; HomRef; Het; Auto; Dad, Mom, Kid. 3; HomRef; ~HomRef; HomVar; Au",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:19991,error,error,19991,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['error'],['error']
Availability,"nctional.; (#5611) Fix; hl.nirvana crash. Experimental. (#5524) Add; summarize functions to Table, MatrixTable, and Expression.; (#5570) Add; hl.agg.approx_cdf aggregator for approximate density calculation.; (#5571) Add log; parameter to hl.plot.histogram.; (#5601) Add; hl.plot.joint_plot, extend functionality of hl.plot.scatter.; (#5608) Add LD score; simulation framework.; (#5628) Add; hl.experimental.full_outer_join_mt for full outer joins on; MatrixTables. Version 0.2.11; Released 2019-03-06. New features. (#5374) Add default; arguments to hl.add_sequence for running on GCP.; (#5481) Added; sample_cols method to MatrixTable.; (#5501) Exposed; MatrixTable.unfilter_entries. See filter_entries; documentation for more information.; (#5480) Added; n_cols argument to MatrixTable.head.; (#5529) Added; Table.{semi_join, anti_join} and; MatrixTable.{semi_join_rows, semi_join_cols, anti_join_rows, anti_join_cols}.; (#5528) Added; {MatrixTable, Table}.checkpoint methods as wrappers around; write / read_{matrix_table, table}. Bug fixes. (#5416) Resolved; issue wherein VEP and certain regressions were recomputed on each; use, rather than once.; (#5419) Resolved; issue with import_vcf force_bgz and file size checks.; (#5427) Resolved; issue with Table.show and dictionary field types.; (#5468) Resolved; ordering problem with Expression.show on key fields that are not; the first key.; (#5492) Fixed; hl.agg.collect crashing when collecting float32 values.; (#5525) Fixed; hl.trio_matrix crashing when complete_trios is False. Version 0.2.10; Released 2019-02-15. New features. (#5272) Added a new; ‘delimiter’ option to Table.export.; (#5251) Add utility; aliases to hl.plot for output_notebook and show.; (#5249) Add; histogram2d function to hl.plot module.; (#5247) Expose; MatrixTable.localize_entries method for converting to a Table; with an entries array.; (#5300) Add new; filter and find_replace arguments to hl.import_table and; hl.import_vcf to apply regex and substitutio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:98616,checkpoint,checkpoint,98616,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['checkpoint'],['checkpoint']
Availability,"nd ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`~.LocusExpression.in_autosome`. - Auto -- ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6782,error,errors,6782,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"nd has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0.2.69. Added the option to specify either remote_tmpdir or bucket when using the ServiceBackend. Version 0.2.68. Fixed copying a directory from GCS when using the LocalBackend; Fixed writing files to GCS when the bucket name starts with a “g” or an “s”; Fixed the error “Argument list too long” when using the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ when using the; LocalBackend; Fixed executing a PythonJob when using the LocalBackend. Version 0.2.65. Added PythonJob; Added new Job.memory inputs lowmem, standard, and highmem corresponding to ~1Gi/core, ~4Gi/core, and ~7Gi/core respectively.; Job.storage is now interpreted as the desired extra storage mounted at /io in addition to the default root filesystem / when; using the ServiceBackend. The root filesystem is allocated 5Gi for all jobs except 1.25Gi for 0.25 core jobs and 2.5Gi for 0.5 core jobs.; Changed how we bill for storage when using the ServiceBack",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:5382,error,error,5382,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,2,['error'],['error']
Availability,"nd table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`~.LocusExpression.in_autosome`. - Auto -- in autosome or in PAR or female child; - HemiX -- in non-PAR of X and male child; - HemiY -- in non-PAR of Y and male child. `Any` refers to",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6923,error,errors,6923,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"nd; LocalBackend; ServiceBackend; ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; ServiceBackend. View page source. ServiceBackend. class hailtop.batch.backend.ServiceBackend(*args, billing_project=None, bucket=None, remote_tmpdir=None, google_project=None, token=None, regions=None, gcs_requester_pays_configuration=None, gcs_bucket_allow_list=None); Bases: Backend[Batch]; Backend that executes batches on Hail’s Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named “my-billing-account”; and stores temporary intermediate files in “gs://my-bucket/temporary-files”.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1209,echo,echo,1209,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,2,['echo'],['echo']
Availability,"ndard deviation of the normal term.; max_iterations : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. """"""; if max_iterations is None:; max_iterations = hl.literal(10_000); if min_accuracy is None:; min_accuracy = hl.literal(1e-5); return _func(""pgenchisq"", PGENCHISQ_RETURN_TYPE, x - mu, w, k, lam, sigma, max_iterations, min_accuracy). [docs]@typecheck(x=expr_float64, mu=expr_float64, sigma=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False) -> Float64Expression:; """"""The cumulative probability function of a normal distribution with mean; `mu` and standard deviation `sigma`. Returns cumulative probability of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:71261,error,error,71261,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['error'],['error']
Availability,"ndition_ab = '''let ab = g.ad[1] / g.ad.sum() in; ((g.isHomRef && ab <= 0.1) ||; (g.isHet && ab >= 0.25 && ab <= 0.75) ||; (g.isHomVar && ab >= 0.9))'''; vds = vds.filter_genotypes(filter_condition_ab). In [38]:. post_qc_call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)'); print('post QC call rate is %.3f' % post_qc_call_rate). post QC call rate is 0.955. Variant QC is a bit more of the same: we can use the; variant_qc; method to produce a variety of useful statistics, plot them, and filter. In [39]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; }; }. The; cache; is used to optimize some of the downstream operations. In [40]:. vds = vds.variant_qc().cache(). In [41]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; }; }. In [42]:. variant_df = vds.variants_table().to_pandas(). plt.clf(); plt.subplot(2, 2, 1); variantgq_means = variant_df[""va.qc.gqMean""]; plt.hist(variantg",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:18612,down,downstream,18612,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['down'],['downstream']
Availability,"ne entry in some data_field_name array for every row in; the inputs.; The multi_way_zip_join() method assumes that inputs have distinct; keys. If any input has duplicate keys, the row value that is included; in the result array for that key is undefined. Parameters:. tables (list of Table) – A list of tables to combine; data_field_name (str) – The name of the resulting data field; global_field_name (str) – The name of the resulting global field. n_partitions()[source]; Returns the number of partitions in the table.; Examples; Range tables can be constructed with an explicit number of partitions:; >>> ht = hl.utils.range_table(100, n_partitions=10); >>> ht.n_partitions(); 10. Small files are often imported with one partition:; >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True); >>> ht2.n_partitions(); 1. The min_partitions argument to import_table() forces more partitions, but it can; produce empty partitions. Empty partitions do not affect correctness but introduce; unnecessary extra bookkeeping that slows down the pipeline.; >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True, min_partitions=10); >>> ht2.n_partitions(); 10. Returns:; int – Number of partitions. naive_coalesce(max_partitions)[source]; Naively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> table_result = table1.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; Table – Table with at most max_partitions partitions. order_by(*exprs)[source]; Sort by the specified fields, defaulting",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:47323,down,down,47323,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['down'],['down']
Availability,"ne)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.pca(entry_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on numeric columns derived from a; matrix table.; Examples; For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls.; >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; This method does not automatically mean-center or normalize each column.; If desired, such transformations should be incorporated in entry_expr.; Hail will return an error if entry_expr evaluates to missing, nan, or; infinity on any entry. Notes; PCA is run on the columns of the numeric matrix obtained by evaluating; entry_expr on each entry of the matrix table, or equivalently on the rows; of the transposed numeric matrix \(M\) referenced below.; PCA computes the SVD. \[M = USV^T\]; where columns of \(U",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:16520,toler,tolerance,16520,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['toler'],['tolerance']
Availability,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}'). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:8240,error,error,8240,docs/0.2/_modules/hail/utils/hadoop_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html,2,['error'],['error']
Availability,"netic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """""". valid_regions = {'us', 'us-central1', 'europe-west1'}; if region not in valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {valid_clouds}.'; ). datasets = get_datasets_metadata(); names = set([dataset for dataset in datasets]); if name not in names:; raise ValueError(f'{name} is not a dataset available in the' f' repository.'). versions = set(dataset['version'] for dataset in datasets[name]['versions']); if version not in versions:; raise ValueError(; f'Version {version!r} not available for dataset' f' {name!r}.\n' f'Available versions: {versions}.'; ). reference_genomes =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:2281,avail,available,2281,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,2,['avail'],['available']
Availability,"new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets:; gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: use_exome_default_intervals=True and; use_genome_default_intervals=True.; The combiner serializes itself to save_path so that it can be restarted after failure. Parameters:. save_path (str) – The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path (str) – The location to store the new VariantDataset.; temp_path (str) – The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome (ReferenceGenome) – The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor (int) – The number of Variant Datasets to combine at once.; target_records (int) – The target number of variants per partition.; gvcf_batch_size (int) – The number of GVCFs to ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:2361,avail,available,2361,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,1,['avail'],['available']
Availability,"nfiguration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions (list of str, optional) – List of regions to run jobs in when using the Batch backend. Use ANY_REGION to specify any region is allowed; or use None to use the underlying default regions from the hailctl environment configuration. For example, use; hailctl config set batch/regions region1,region2 to set the default regions to use.; gcs_bucket_allow_list – A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use “cold” storage. Should look like [""bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argument, sets the default reference genome to the argument. Returns:; ReferenceGenome. hail.get_reference(name)[source]; Returns the reference genome corresponding to name.; Notes; Hail’s built-in references are 'GRCh37', GRCh38', 'GRCm38', and; 'CanFam3'.; The contig names and lengths come from the GATK resource bundle:; human_g1k_v37.dict; and Homo_sapiens_assembly38.dict.; If name='default', the value of default_reference() is returned. Parameters:; name (str) – Nam",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:7242,error,error,7242,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['error'],['error']
Availability,"nfo). Notes; -----; For usage of the `f` argument, see the :func:`.filter_alleles`; documentation. :func:`.filter_alleles_hts` requires the dataset have the GATK VCF schema,; namely the following entry fields in this order:. .. code-block:: text. GT: call; AD: array<int32>; DP: int32; GQ: int32; PL: array<int32>. Use :meth:`.MatrixTable.select_entries` to rearrange these fields if; necessary. The following new fields are generated:. - `old_locus` (``locus``) -- The old locus, before filtering and computing; the minimal representation.; - `old_alleles` (``array<str>``) -- The old alleles, before filtering and; computing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are filtered are missing.; - `new_to_old` (``array<int32>``) -- An array that maps new allele index to; the old allele index. Its length is the same as the modified `alleles`; field. **Downcode algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; :func:`.split_multi_hts`. The downcode algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:159084,Down,Downcode,159084,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['Down'],['Downcode']
Availability,"ng Data with the Sequence Kernel Association Test.; Row weights must be non-negative. Rows with missing weights are ignored. In; the R package skat—which assumes rows are variants—default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field AF, one can use the expression:; >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response y must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively.; The resulting Table provides the group’s key (id), thenumber of; rows in the group (size), the variance component score q_stat, the SKAT; p-value, and a fault flag. For the toy example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC; 3; 4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. k",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:80560,fault,fault,80560,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['fault'],['fault']
Availability,"ng an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=dict(y=y, covariates=covariates),; entry_exprs=dict(x=x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])). mt = mt.annotate_globals(; **mt.aggregate_cols(; hl.struct(; yvec=hl.agg.collect(hl.float(mt.y)),; covmat=hl.agg.collect(mt.covariates.map(hl.float)),; n=hl.agg.count(),; ),; _localize=False,; ); ); mt =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:64347,toler,tolerance,64347,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,6,['toler'],['tolerance']
Availability,"ng caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [4]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr.write('Done!\n'). All files are present and accounted for!. Loading data from disk¶; Hail has its own internal data representation, called a Variant Dataset; (VDS). This is both an on-disk file format and a Python; object. See the; overview for a complete story.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:1893,down,download,1893,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,2,['down'],"['download', 'downloads']"
Availability,"ng) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement in this set. Code; Dad; Mom; Kid; Copy State. 1; HomVar; HomVar; Het; Auto. 2; HomRef; HomRef; Het; Auto. 3; HomRef; ! HomRef; HomVar; Auto. 4; ! HomRef; HomRef; HomVar; Auto. 5; HomRef; HomRef; HomVar; Auto. 6; HomVar; ! HomVar; HomRef; Auto. 7; ! HomVar; HomVar; HomRef; Auto. 8; HomVar; HomVar; HomRef; Auto. 9; Any; HomVar; HomRef; HemiX. 10; Any; HomRef; HomVar; HemiX. 11; HomVar; Any; HomRef; HemiY. 12; HomRef; Any; HomVar; HemiY. This method only considers children with two parents and a defined sex.; PAR is currently defined with respect to reference; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:126454,error,error,126454,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,"ngle hail session are inserted into; the same batch instead of one batch per action.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:42962,error,error,42962,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"ning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; split_multi_hts(), which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See also; split_multi_hts(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table. hail.methods.split_multi_hts(ds, keep_star=False",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:85626,down,downcode,85626,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['down'],['downcode']
Availability,"ning the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; accuracy (float) – Accuracy achieved by the Davies algorithm if fault value is zero.; iterations (int) – Maximum number of iterations attempted by the Davies algorithm. Returns:; Table – Table of SKAT results. hail.methods.lambda_gc(p_value, approximate=True)[source]; Compute genomic inflation factor (lambda GC) from an Expression of p-values. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Parameters:. p_value (NumericExpression) – Row-indexed numeric expression of p-values.; approximate (bool) – If False, computes exact lambda GC (slower and uses more memory). Returns:; float – Genomic inflation factor (lambda genomic control). ha",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:82241,toler,tolerance,82241,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['toler'],['tolerance']
Availability,"nner dimension.; This function splits a single matrix multiplication into split_on_inner smaller matrix multiplications,; does the smaller multiplications, checkpoints them with names defined by file_name_prefix, and adds them; together. This is useful in cases when the multiplication of two large matrices results in a much smaller matrix. Parameters:. b (numpy.ndarray or BlockMatrix); splits (int (keyword only argument)) – The number of smaller multiplications to do.; path_prefix (str (keyword only argument)) – The prefix of the path to write the block matrices to. If unspecified, writes to a tmpdir. Returns:; BlockMatrix. unpersist()[source]; Unpersists this block matrix from memory/disk.; Notes; This function will have no effect on a block matrix that was not previously; persisted. Returns:; BlockMatrix – Unpersisted block matrix. write(path, overwrite=False, force_row_major=False, stage_locally=False)[source]; Writes the block matrix. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before writing.; If False, write blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static write_from_entry_expr(entry_expr, path, overwrite=False, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Writes a block matrix from a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:44858,checkpoint,checkpoint,44858,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['checkpoint'],['checkpoint']
Availability,"nning pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail query but can be used standalone for Hail Batch users by; import hailtop.fs as hfs. Deprecations. (#12929) Hail no; longer officially supports Python 3.7.; (#12917) The; hail-az scheme for referencing blobs in ABS is now deprecated and; will be removed in an upcoming release. Bug Fixes. (#12913) Fixed bug; in hail.ggplot where all legend entri",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:33652,error,errors,33652,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"nnotate_rows(auto_or_x_par=dataset.locus.in_autosome() | dataset.locus.in_x_par()); dataset = dataset.filter_rows(dataset.auto_or_x_par | dataset.locus.in_x_nonpar()). hom_ref = 0; het = 1; hom_var = 2. auto = 2; hemi_x = 1. # kid, dad, mom, copy, t, u; config_counts = [; (hom_ref, het, het, auto, 0, 2),; (hom_ref, hom_ref, het, auto, 0, 1),; (hom_ref, het, hom_ref, auto, 0, 1),; (het, het, het, auto, 1, 1),; (het, hom_ref, het, auto, 1, 0),; (het, het, hom_ref, auto, 1, 0),; (het, hom_var, het, auto, 0, 1),; (het, het, hom_var, auto, 0, 1),; (hom_var, het, het, auto, 2, 0),; (hom_var, het, hom_var, auto, 1, 0),; (hom_var, hom_var, het, auto, 1, 0),; (hom_ref, hom_ref, het, hemi_x, 0, 1),; (hom_ref, hom_var, het, hemi_x, 0, 1),; (hom_var, hom_ref, het, hemi_x, 1, 0),; (hom_var, hom_var, het, hemi_x, 1, 0),; ]. count_map = hl.literal({(c[0], c[1], c[2], c[3]): [c[4], c[5]] for c in config_counts}). tri = trio_matrix(dataset, pedigree, complete_trios=True). # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; father_is_het = tri.father_entry.GT.is_het(); parent_is_valid_het = (father_is_het & tri.auto_or_x_par) | (tri.mother_entry.GT.is_het() & ~father_is_het). copy_state = hl.if_else(tri.auto_or_x_par | tri.is_female, 2, 1). config = (; tri.proband_entry.GT.n_alt_alleles(),; tri.father_entry.GT.n_alt_alleles(),; tri.mother_entry.GT.n_alt_alleles(),; copy_state,; ). tri = tri.annotate_rows(counts=agg.filter(parent_is_valid_het, agg.array_sum(count_map.get(config)))). tab = tri.rows().select('counts'); tab = tab.transmute(t=tab.counts[0], u=tab.counts[1]); tab = tab.annotate(chi_sq=((tab.t - tab.u) ** 2) / (tab.t + tab.u)); tab = tab.annotate(p_value=hl.pchisqtail(tab.chi_sq, 1.0)). return tab.cache(). [docs]@typecheck(; mt=MatrixTable,; pedigree=Pedigree,; pop_frequency_prior=expr_float64,; min_gq=int,; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:18631,error,error,18631,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['error']
Availability,"nor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions. - The second, ""global correlation"" stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within `bp_window_size` base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is ``n_locally_pruned_variants / block_size``. - The third, ""global pruning"" stage applies :func:`.maximal_independent_set`; to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; `keep_higher_maf` is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; -------; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on `BlockMatrix.from_entry_expr` with; regard to memory and Hadoop replication errors. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression on a matrix table with row-indexed; variants and column-indexed samples.; r2 : :obj:`float`; Squared correlation threshold (exclusive upper bound).; Must be in the range [0.0, 1.0].; bp_window_size: :obj:`int`; Window size in base pairs (inclusive upper bound).; memory_per_core : :obj:`int`; Memory in MB per core for local pruning queue.; keep_higher_maf: :obj:`int`; If ``True``, break ties at each step of the global pruning stage by; preferring to keep variants with higher minor allele frequency.; block_size: :obj:`int`, optional; Block size for block matrices in the second stage.; Default given by :meth:`.BlockMatrix.default_block_size`. Returns; -------; :class:`.Table`; Table of a maximal independent set of variants.; """"""; if block_size is None:; block_size = BlockMatrix.default_block_size(). if not 0.0 <= r2 <= 1:; raise ValueError(f'r2 must be in the ran",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:168667,error,errors,168667,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['error'],['errors']
Availability,"not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:113405,down,downcode,113405,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['down'],['downcode']
Availability,"notate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.aiotools.copy will always show a progress bar when; --verbose is passed. hailctl dataproc. (#11710) support; pass-through arguments to connect. Bug fixes. (#11792) Resolved; issue where corrupted tables could be created with whole-stage code; generation enabled. Version 0.2.93; Release 2022-03-27. Beta features. Several issues with the beta version of Hail Query on Hail Batch are; addressed in this ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:49440,error,error,49440,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"notated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (String) – Readable representation of Mendel error. Second table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:124663,error,error,124663,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['error'],['error']
Availability,"notated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:155949,error,error,155949,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability,"notation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False, computes exact lambda GC (slower and uses more memory). Returns; -------; :obj:`float`; Genomic inflation factor (lambda genomic control).; """"""; raise_unless_row_indexed('lambda_gc', p_value); t = table_source('lambda_gc', p_value); med_chisq = _lambda_gc_agg(p_value, approximate); return t.aggregate(med_chisq). @typecheck(p_value=expr_numeric, approximate=bool); def _lambda_gc_agg(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109630,toler,tolerance,109630,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,"novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). def call_hemi(kid_pp, parent, parent_pp, kid_ad_ratio):; p_data_given_dn = parent_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (parent_pp[1] + parent_pp[2]) * kid_pp[2] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (parent.DP) < min_dp_ratio) | (kid_ad_ratio < min_child_ab), failure); .when((hl.sum(parent.AD) == 0), failure); .when(parent.AD[1] / hl.sum(parent.AD) > max_parent_ab, failure); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.3, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:31871,failure,failure,31871,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['failure'],['failure']
Availability,"ns in the new table is equal to the number of; partitions containing the first `n` rows. Parameters; ----------; n : int; Number of rows to include. Returns; -------; :class:`.Table`; Table limited to the first `n` rows.; """""". return Table(ir.TableHead(self._tir, n)). [docs] @typecheck_method(n=int); def tail(self, n) -> 'Table':; """"""Subset table to last `n` rows. Examples; --------; Subset to the last three rows:. >>> table_result = table1.tail(3); >>> table_result.count(); 3. Notes; -----. The number of partitions in the new table is equal to the number of; partitions containing the last `n` rows. Parameters; ----------; n : int; Number of rows to include. Returns; -------; :class:`.Table`; Table including the last `n` rows.; """""". return Table(ir.TableTail(self._tir, n)). [docs] @typecheck_method(p=numeric, seed=nullable(int)); def sample(self, p, seed=None) -> 'Table':; """"""Downsample the table by keeping each row with probability ``p``. Examples; --------. Downsample the table to approximately 1% of its rows. >>> table1.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+; >>> small_table1 = table1.sample(0.75, seed=0); >>> small_table1.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 3 | 70 | ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:89987,Down,Downsample,89987,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['Down'],['Downsample']
Availability,ns; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Int32Expression. View page source. Int32Expression. class hail.expr.Int32Expression[source]; Expression of type tint32.; Attributes. dtype; The data type of the expression. Methods. __add__(other); Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other); Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other); Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other); Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __le__(other); Less-than-or-equals comparison.; Examples; >>> hl.eval(x <,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Int32Expression.html:1282,error,error,1282,docs/0.2/hail.expr.Int32Expression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int32Expression.html,1,['error'],['error']
Availability,ns; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Int64Expression. View page source. Int64Expression. class hail.expr.Int64Expression[source]; Expression of type tint64.; Attributes. dtype; The data type of the expression. Methods. __add__(other); Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other); Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other); Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other); Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __le__(other); Less-than-or-equals comparison.; Examples; >>> hl.eval(x <,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Int64Expression.html:1282,error,error,1282,docs/0.2/hail.expr.Int64Expression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Int64Expression.html,1,['error'],['error']
Availability,"ns=int); def downsample(x, y, label=None, n_divisions=500) -> ArrayExpression:; """"""Downsample (x, y) coordinate datapoints. Parameters; ----------; x : :class:`.NumericExpression`; X-values to be downsampled.; y : :class:`.NumericExpression`; Y-values to be downsampled.; label : :class:`.StringExpression` or :class:`.ArrayExpression`; Additional data for each (x, y) coordinate. Can pass in multiple fields in an :class:`.ArrayExpression`.; n_divisions : :obj:`int`; Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns; -------; :class:`.ArrayExpression`; Expression for downsampled coordinate points (x, y). The element type of the array is; :class:`.ttuple` of :py:data:`.tfloat64`, :py:data:`.tfloat64`, and :class:`.tarray` of :py:data:`.tstr`; """"""; if label is None:; label = hl.missing(hl.tarray(hl.tstr)); elif isinstance(label, StringExpression):; label = hl.array([label]); return _agg_func(; 'Downsample', [x, y, label], tarray(ttuple(tfloat64, tfloat64, tarray(tstr))), init_op_args=[n_divisions]; ). @typecheck(expr=expr_any, n=expr_int32); def _reservoir_sample(expr, n):; return _agg_func('ReservoirSample', [expr], tarray(expr.dtype), [n]). [docs]@typecheck(gp=expr_array(expr_float64)); def info_score(gp) -> StructExpression:; r""""""Compute the IMPUTE information score. Examples; --------; Calculate the info score per variant:. >>> gen_mt = hl.import_gen('data/example.gen', sample_file='data/example.sample'); >>> gen_mt = gen_mt.annotate_rows(info_score = hl.agg.info_score(gen_mt.GP)). Calculate group-specific info scores per variant:. >>> gen_mt = hl.import_gen('data/example.gen', sample_file='data/example.sample'); >>> gen_mt = gen_mt.annotate_cols(is_case = hl.rand_bool(0.5)); >>> gen_mt = gen_mt.annotate_rows(info_score = hl.agg.group_by(gen_mt.is_case, hl.agg.info_score(gen_mt.GP))). Notes; -----; The result of :func:`.info_score` is a struct with two fields:. - `score` (``float64``) -- Info score.; - `n_i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:45024,Down,Downsample,45024,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,1,['Down'],['Downsample']
Availability,"nsecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:52606,error,error,52606,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"nsion='vcf.bgz'); info('importing VCF and writing to matrix table...'); hl.import_vcf(cluster_readable_vcf, min_partitions=16, reference_genome='GRCh38').write(; matrix_table_path, overwrite=True; ). tmp_sample_annot = os.path.join(tmp_dir, 'HGDP_annotations.txt'); source = resources['HGDP_annotations']; info(f'downloading HGDP annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['HGDP_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene_annot), gene_annotations_path); hl.hadoop_copy(local_path_uri(tmp_vcf), vcf_path); info('Done!'); else:; info('HGDP files found'). [docs]def get_movie_lens(output_dir, overwrite: bool = False):; """"""Download public Movie Lens dataset. Notes; -----; The download is about 6M. See the; `MovieLens website <https://grouplens.org/datasets/movielens/100k/>`__; for more information about this dataset. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite existing files/directories at those locations.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). paths = [os.path.join(output_dir, x) for x in ['movies.ht', 'ratings.ht', 'users.ht']]; if overwrite or any(not _dir_exists(fs, f) for f in paths):; init_temp_dir(); source = resources['movie_lens_100k']; tmp_path = os.path.join(tmp_dir, 'ml-100k.zip'); info(f'downloading MovieLens-100k data ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_path); with zipfile.ZipFile(tmp_path, 'r') as z:; z.extractall(tmp_dir). user_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.user'); movie_table_path = os.path.join(tmp_dir, 'm",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:6278,Down,Download,6278,docs/0.2/_modules/hail/utils/tutorial.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html,1,['Down'],['Download']
Availability,"ns. (#9482); ArrayExpression.head has been deprecated in favor of; ArrayExpression.first. Version 0.2.57; Released 2020-09-03. New features. (#9343) Implement the; KING method for relationship inference as hl.methods.king. Version 0.2.56; Released 2020-08-31. New features. (#9308) Add; hl.enumerate in favor of hl.zip_with_index, which is now deprecated.; (#9278) Add; ArrayExpression.grouped, a function that groups hail arrays into; fixed size subarrays. Performance. (#9373)(#9374); Decrease amount of memory used when slicing or filtering along a; single BlockMatrix dimension. Bug fixes. (#9304) Fix crash in; run_combiner caused by inputs where VCF lines and BGZ blocks; align. hailctl dataproc. (#9263) Add support; for --expiration-time argument to hailctl dataproc start.; (#9263) Add support; for --no-max-idle, no-max-age, --max-age, and; --expiration-time to hailctl dataproc --modify. Version 0.2.55; Released 2020-08-19. Performance. (#9264); Table.checkpoint now uses a faster LZ4 compression scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed err",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:65823,checkpoint,checkpoint,65823,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['checkpoint'],['checkpoint']
Availability,"nt and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; in_autosome(). Auto – in autosome or in PAR or female child; HemiX – in non-PAR of X and male child; HemiY – in non-PAR of Y and male child. Any refers to the set { HomRef, Het, HomVar, NoCall } and ~; denotes complement in this set. Code; Dad; Mom; Kid; Copy State | Implicated. 1; HomVar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:50178,error,errors,50178,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,['error'],['errors']
Availability,"nt of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:71641,error,errors,71641,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"nt); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we’ve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:10419,checkpoint,checkpoint,10419,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability,"ntheses appropriately. A single ‘&’ denotes logical AND and a single ‘|’ denotes logical OR. [9]:. users.aggregate(hl.agg.filter((users.occupation == 'writer') | (users.occupation == 'executive'), hl.agg.count())). [9]:. 77. [10]:. users.aggregate(hl.agg.filter((users.sex == 'F') | (users.occupation == 'executive'), hl.agg.count())). [10]:. 302. hist; As we saw in the first tutorial, hist can be used to build a histogram over numeric data. [11]:. hist = users.aggregate(hl.agg.hist(users.age, 10, 70, 60)); hist. [11]:. Struct(bin_edges=[10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0], bin_freq=[1, 1, 0, 5, 3, 6, 5, 14, 18, 23, 32, 27, 37, 28, 33, 38, 34, 35, 36, 32, 39, 25, 28, 26, 17, 27, 21, 19, 17, 22, 21, 10, 21, 13, 23, 15, 12, 14, 20, 19, 20, 20, 6, 12, 4, 11, 6, 9, 3, 3, 9, 3, 2, 3, 2, 3, 1, 0, 2, 5], n_smaller=1, n_larger=1). [12]:. p = hl.plot.histogram(hist, legend='Age'); show(p). take and collect; There are a few aggregators for collecting values. take localizes a few values into an array. It has an optional ordering.; collect localizes all values into an array.; collect_as_set localizes all unique values into a set. [13]:. users.aggregate(hl.agg.take(users.occupation, 5)). [13]:. ['technician', 'other', 'writer', 'technician', 'other']. [14]:. users.aggregate(hl.agg.take(users.age, 5, ordering=-users.age)). [14]:. [73, 70, 70, 70, 69]. Warning! Aggregators like collect and counter return Python objects and can fail with out of memory errors if you apply them to collections that are too large (e.g. all 50 trillion genotypes in the UK Biobank dataset). [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/04-aggregation.html:6353,error,errors,6353,docs/0.2/tutorials/04-aggregation.html,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html,1,['error'],['errors']
Availability,"ntiles simultaneously. Returns; -------; :class:`.NumericExpression`; Upper bound on error of quantile estimates.; """""". def compute_sum(cdf):; s = hl.sum(; hl.range(0, hl.len(cdf._compaction_counts)).map(lambda i: cdf._compaction_counts[i] * (2 ** (2 * i))); ); return s / (cdf.ranks[-1] ** 2). def update_grid_size(p, s):; return 4 * hl.sqrt(hl.log(2 * p / failure_prob) / (2 * s)). def compute_grid_size(s):; return hl.fold(lambda p, i: update_grid_size(p, s), 1 / failure_prob, hl.range(0, 5)). def compute_single_error(s, failure_prob=failure_prob):; return hl.sqrt(hl.log(2 / failure_prob) * s / 2). def compute_global_error(s):; return hl.rbind(compute_grid_size(s), lambda p: 1 / p + compute_single_error(s, failure_prob / p)). if all_quantiles:; return hl.rbind(cdf, lambda cdf: hl.rbind(compute_sum(cdf), compute_global_error)); else:; return hl.rbind(cdf, lambda cdf: hl.rbind(compute_sum(cdf), compute_single_error)). def _error_from_cdf_python(cdf, failure_prob, all_quantiles=False):; """"""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :obj:`dict`; Result of :func:`.approx_cdf` aggregator, evaluated to a python dict; failure_prob: :obj:`float`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :obj:`float`; Upper bound on error of quantile estimates.; """"""; import math. s = 0; for i in builtins.range(builtins.len(cdf._compaction_counts)):; s += cdf._compaction_counts[i] << (2 * i); s = s / (cdf.ranks[-1] ** 2). def update_grid_size(p):; return 4 * math.sqrt(math.log(2 * p / failure_prob) / (2 * s)). def compute_grid_size(s):; p = 1 / failure_prob; for _ in builtins.range(5):; p = update_grid_size(p); return p. def compute_single_error(s, failure_prob=failure_prob):; return math.sqrt(math.log(2 / failure_prob) * s / 2). if s == 0:; # no com",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:6569,error,error,6569,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['error'],['error']
Availability,"ntinuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool, optional; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use `None` to collect all points.; missing_label: str; Label to use when a point is missing data for a categorical label. Returns; -------; :class:`.GridPlot`; """"""; # Collect data; hover_fields = {} if hover_fields is None else hover_fields. label_by_col: Dict[str, Expression]; if label is None:; label_by_col = {}; elif isinstance(label, Expression):; label_by_col = {'label': label}; else:; assert isinstance(label, dict); label_by_col = label. if isinstance(colors, ColorMapper):; colors_by_col = {'label': colors}; else:; colors_by_col = colors; if isinstance(x, NumericExpression):; _x = ('x', x); else:; _x = x. if isinstance(y, NumericExpression):; _y = ('y', y); else:; _y = y. label_cols = list(label_by_col.keys()); source_pd = _collect_scatter_plot_data(; _x,; _y,; fields={**hover_fields, **label_by_col},; n_divisions=_downsampling_factor('join_plot', n_divisions, collect_all),; missing_labe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:39732,down,downsample,39732,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['down'],['downsample']
Availability,"ntinuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool, optional; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use `None` to collect all points.; missing_label: str; Label to use when a point is missing data for a categorical label. Returns; -------; :class:`bokeh.models.Plot` if no label or a single label was given, otherwise :class:`bokeh.models.layouts.Column`; """"""; hover_fields = {} if hover_fields is None else hover_fields. label_by_col: Dict[str, Expression]; if label is None:; label_by_col = {}; elif isinstance(label, Expression):; label_by_col = {'label': label}; else:; assert isinstance(label, dict); label_by_col = label. if isinstance(colors, ColorMapper):; colors_by_col = {'label': colors}; else:; colors_by_col = colors. label_cols = list(label_by_col.keys()); if isinstance(x, NumericExpression):; _x = ('x', x); else:; _x = x. if isinstance(y, NumericExpression):; _y = ('y', y); else:; _y = y. source_pd = _collect_scatter_plot_data(; _x,; _y,; fields={**hover_fields, **label_by_col},; n_divi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:32829,down,downsample,32829,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['down'],['downsample']
Availability,"ntry_indexed('skat/x', x). analyze('skat/key_expr', key_expr, mt._row_indices); analyze('skat/weight_expr', weight_expr, mt._row_indices); analyze('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109064,toler,tolerance,109064,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['toler'],['tolerance']
Availability,"nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; in_autosome(). Auto – in autosome or in PAR or female child; HemiX – in non-PAR of X and male child; HemiY – in non-PAR of Y and male child. Any refers to the set { HomRef, Het, HomVar, NoCall } and ~; denotes complement in this set. Code; Dad; Mom; Kid; Copy State | Implicated. 1; HomVar; HomVar; Het; Auto; Dad, Mom, Kid. 2; HomRef; HomRef; Het; Auto; Dad, Mom, Kid. 3; HomRef; ~HomRef; HomVar; Auto; Dad, Kid. 4; ~HomRef; HomRef; HomVar; Auto; Mom, Kid. 5; HomRef; HomRef; HomVar; Auto; Kid. 6; HomVar; ~HomVar; HomRef; Auto; Dad, Kid. 7; ~HomVar; HomVar; HomRef; Au",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:50465,error,errors,50465,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,['error'],['errors']
Availability,"numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probability p = Prob(\(Z < x\)) with \(Z\); a normal random variable. Defaults to a standard normal random variable. Parameters:. x (float or Expression of type tfloat64); mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:21068,error,error,21068,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['error'],['error']
Availability,"numpy.ndarray of the same dimensions:; >>> a = np.fromfile('/local/file').reshape((10, 20)) . Notes; This method, analogous to numpy.tofile,; produces a binary file of float64 values in row-major order, which can; be read by functions such as numpy.fromfile; (if a local file) and BlockMatrix.fromfile().; Binary files produced and consumed by tofile() and; fromfile() are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; BlockMatrix.write() and BlockMatrix.read() to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent.; The number of entries must be less than \(2^{31}\). Parameters:; uri (str, optional) – URI of binary output file. See also; to_numpy(). tree_matmul(b, *, splits, path_prefix=None)[source]; Matrix multiplication in situations with large inner dimension.; This function splits a single matrix multiplication into split_on_inner smaller matrix multiplications,; does the smaller multiplications, checkpoints them with names defined by file_name_prefix, and adds them; together. This is useful in cases when the multiplication of two large matrices results in a much smaller matrix. Parameters:. b (numpy.ndarray or BlockMatrix); splits (int (keyword only argument)) – The number of smaller multiplications to do.; path_prefix (str (keyword only argument)) – The prefix of the path to write the block matrices to. If unspecified, writes to a tmpdir. Returns:; BlockMatrix. unpersist()[source]; Unpersists this block matrix from memory/disk.; Notes; This function will have no effect on a block matrix that was not previously; persisted. Returns:; BlockMatrix – Unpersisted block matrix. write(path, overwrite=False, force_row_major=False, stage_locally=False)[source]; Writes the block matrix. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; over",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:44035,checkpoint,checkpoints,44035,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['checkpoint'],['checkpoints']
Availability,"numpy; Collects the block matrix into a NumPy ndarray. to_table_row_major; Returns a table where each row represents a row in the block matrix. tofile; Collects and writes data to a binary file. tree_matmul; Matrix multiplication in situations with large inner dimension. unpersist; Unpersists this block matrix from memory/disk. write; Writes the block matrix. write_from_entry_expr; Writes a block matrix from a matrix table entry expression. property T; Matrix transpose. Returns:; BlockMatrix. abs()[source]; Element-wise absolute value. Returns:; BlockMatrix. property block_size; Block size. Returns:; int. cache()[source]; Persist this block matrix in memory.; Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; BlockMatrix – Cached block matrix. ceil()[source]; Element-wise ceiling. Returns:; BlockMatrix. checkpoint(path, overwrite=False, force_row_major=False, stage_locally=False)[source]; Checkpoint the block matrix. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before checkpointing.; If False, checkpoint blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static default_block_size()[source]; Default block side length. densify()[source]; Restore all dropped blocks as explicit blocks of zeros. Returns:; BlockMatrix. diagonal()[source]; Extracts diagonal elements as a row vector. Returns:; BlockMatrix. property element_type; The type of the elements. entries(keyed=True)[source]; Returns a table with the indices and value of each block matrix entry.; Examples; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[5, 7], [2, 8]]), 2)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:10872,checkpoint,checkpoint,10872,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['checkpoint'],['checkpoint']
Availability,"o Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions; SwitchBuilder. View page source. SwitchBuilder. class hail.expr.builders.SwitchBuilder[source]; Class for generating conditional trees based on value of an expression.; Examples; >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:1473,error,error,1473,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,1,['error'],['error']
Availability,"o evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:20330,error,error,20330,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['error'],['error']
Availability,"o my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global rando",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:4262,avail,available,4262,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['avail'],['available']
Availability,"oat – Convert value to a Float.; toInt(): Int – Convert value to an Integer.; toLong(): Long – Convert value to a Long. Float¶. abs(): Float – Returns the absolute value of a number.; max(a: Float): Float – Returns the maximum value.; min(a: Float): Float – Returns the minimum value.; signum(): Int – Returns the sign of a number (1, 0, or -1).; toDouble(): Double – Convert value to a Double.; toFloat(): Float – Convert value to a Float.; toInt(): Int – Convert value to an Integer.; toLong(): Long – Convert value to a Long. Genotype¶; A Genotype is a Hail data type representing a genotype in the Variant Dataset. It is referred to as g in the expression language. ad: Array[Int] – allelic depth for each allele. call(): Call – the integer gt = k*(k+1)/2 + j for call j/k (0 = 0/0, 1 = 0/1, 2 = 1/1, 3 = 0/2, etc.). dosage: Double – the expected number of non-reference alleles based on genotype probabilities. dp: Int – the total number of informative reads. fakeRef: Boolean – True if this genotype was downcoded in split_multi(). This can happen if a 1/2 call is split to 0/1, 0/1. fractionReadsRef(): Double – the ratio of ref reads to the sum of all informative reads. gp: Array[Double] – the linear-scaled probabilities. gq: Int – the difference between the two smallest PL entries. gt: Int – the integer gt = k*(k+1)/2 + j for call j/k (0 = 0/0, 1 = 0/1, 2 = 1/1, 3 = 0/2, etc.). gtj(): Int – the index of allele j for call j/k (0 = ref, 1 = first alt allele, etc.). gtk(): Int – the index of allele k for call j/k (0 = ref, 1 = first alt allele, etc.). isCalled(): Boolean – True if the genotype is not ./.. isCalledNonRef(): Boolean – True if either g.isHet or g.isHomVar is true. isHet(): Boolean – True if this call is heterozygous. isHetNonRef(): Boolean – True if this call is j/k with j>0. isHetRef(): Boolean – True if this call is 0/k with k>0. isHomRef(): Boolean – True if this call is 0/0. isHomVar(): Boolean – True if this call is j/j with j>0. isLinearScale: Boolean – True ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/types.html:36421,down,downcoded,36421,docs/0.1/types.html,https://hail.is,https://hail.is/docs/0.1/types.html,1,['down'],['downcoded']
Availability,"ob, the PLINK binary file root, the association results; with at least two columns (SNP and P), and the chromosome for which to do the clumping for.; The return value is the new BashJob created.; def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. We use the image hailgenetics/genetics which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK.; We explicitly tell PLINK to only use 1Gi of memory because PLINK defaults to using half; of the machine’s memory. PLINK’s memory-available detection mechanism is unfortunately; unaware of the memory limit imposed by Batch. Not specifying resource requirements; correctly can cause performance degradations with PLINK.; PLINK creates a hard-coded file plink.clumped. We have to move that file to a temporary; Batch file {c.clumped} in order to use that file in downstream jobs. Merge Clumping Results; The third function concatenates all of the clumping results per chromosome into a single file; with one header line. The inputs are the Batch for which to create a new BashJob; and a list containing all of the individual clumping results files. We use the ubuntu:22.04; Docker image for this job. The return value is the new BashJob created.; def merge(batch, results):; """"""; Merge clumped results files together; """"""; merger = batch.new_job(name='merge-results'); merger.image('ubuntu:22.04'); if results:; merger.command(f'''; head -n 1 {results[0]} > {merger.ofile}; for result in {"" "".join(results)}; do; tail -n +2 ""$result"" >> {merger.ofile}; done; sed -i -e '",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:8917,avail,available,8917,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['avail'],['available']
Availability,object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-parti,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:73206,fault,fault,73206,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['fault', 'toler']","['fault', 'tolerant']"
Availability,"obs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and B",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:2109,down,downloads,2109,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['down'],['downloads']
Availability,"obs whose parents have succeeded.; (#12845) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0.2.69. Added the option to specify either remote_tmpdir or bucket when using the Service",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:4219,avail,available,4219,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['avail'],['available']
Availability,"ocs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.expressions.expression_utils. Source code for hail.expr.expressions.expression_utils; from typing import Dict, Set. from hail.typecheck import setof, typecheck. from ...ir import MakeTuple; from ..expressions import Expression, ExpressionException, expr_any; from .indices import Aggregation, Indices. @typecheck(caller=str, expr=Expression, expected_indices=Indices, aggregation_axes=setof(str), broadcast=bool); def analyze(caller: str, expr: Expression, expected_indices: Indices, aggregation_axes: Set = set(), broadcast=True):; from hail.utils import error, warning. indices = expr._indices; source = indices.source; axes = indices.axes; aggregations = expr._aggregations. warnings = []; errors = []. expected_source = expected_indices.source; expected_axes = expected_indices.axes. if source is not None and source is not expected_source:; bad_refs = []; for name, inds in get_refs(expr).items():; if inds.source is not expected_source:; bad_refs.append(name); errors.append(; ExpressionException(; ""'{caller}': source mismatch\n""; "" Expected an expression from source {expected}\n""; "" Found expression derived from source {actual}\n""; "" Problematic field(s): {bad_refs}\n\n""; "" This error is commonly caused by chaining methods together:\n""; "" >>> ht.distinct().select(ht.x)\n\n""; "" Correct usage:\n""; "" >>> ht = ht.distinct()\n""; "" >>> ht = ht.select(ht.x)"".format(; caller=caller, expected=expected_source, actual=source, bad_refs=list(bad_refs); ); ); ). # check for stray indices by subtracting expected axes from observed; if broadcast:; unexpected_axes = axes - expected_axes; strictness = ''; else:; unexpected_axes = axes if axes != expected_axes else set(); strictness = 'str",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:1150,error,errors,1150,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html,2,['error'],['errors']
Availability,"ocs]def get_1kg(output_dir, overwrite: bool = False):; """"""Download subset of the `1000 Genomes <http://www.internationalgenome.org/>`__; dataset and sample annotations. Notes; -----; The download is about 15M. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite any existing files/directories at `output_dir`.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). matrix_table_path = os.path.join(output_dir, '1kg.mt'); vcf_path = os.path.join(output_dir, '1kg.vcf.bgz'); sample_annotations_path = os.path.join(output_dir, '1kg_annotations.txt'); gene_annotations_path = os.path.join(output_dir, 'ensembl_gene_annotations.txt'). if (; overwrite; or not _dir_exists(fs, matrix_table_path); or not _file_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, '1kg.vcf.bgz'); source = resources['1kg_matrix_table']; info(f'downloading 1KG VCF ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, resources['1kg_matrix_table'], tmp_vcf); cluster_readable_vcf = _copy_to_tmp(fs, local_path_uri(tmp_vcf), extension='vcf.bgz'); info('importing VCF and writing to matrix table...'); hl.import_vcf(cluster_readable_vcf, min_partitions=16).write(matrix_table_path, overwrite=True). tmp_sample_annot = os.path.join(tmp_dir, '1kg_annotations.txt'); source = resources['1kg_annotations']; info(f'downloading 1KG annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['1kg_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:2866,down,downloading,2866,docs/0.2/_modules/hail/utils/tutorial.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html,2,['down'],['downloading']
Availability,"oded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariates or n_covariates + 1, depending on improved null fit vs full fit; mu = sigmoid(X_bslice @ b); sqrtW = hl.sqrt(mu * (1 - mu)); q, r = hl.nd.qr(X * sqrtW.T.reshape(-1, 1)); h = (q * q).sum(1); coef = r[:m, :m]; residual = y - mu; dep = q[:, :m].T @ ((residual + (h * (0.5 - mu))) / sqrtW); delta_b_struct = hl.nd.solve_triangular(coef, dep.reshape(-1, 1), no_crash=True); exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution.reshape(-1). max_delta_b = nd_max(hl.abs(delta_b)). return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.annotate(n_iterations=0, log_lkhd=0, converged=False, exploded=False); return hl.experimental.loop(fit, dtype, 1, b). def _firth_test(null_fit, X, y, max_iterations, tolerance) -> StructExpression:; firth_improved_null_fit = _firth_fit(null_fit.b, X, y, max_iterations=max_iterations, tolerance=tolerance); dof = 1 # 1 variant. def cont(firth_improved_null_fit):; initial_b_full_model = hl.nd.hstack([firth_improved_null_fit.b, hl.nd.array([0.0])]); firth_fit = _firth_fit(initial_b_full_model, X, y, max_iterations=max_iterations, tolerance=tolerance). def cont2(firth_fit):; firth_chi_sq = 2 * (firth_fit.log_lkhd - firth_improved_null_fit.log_lkhd); firth_p = hl.pchisqtail(firth_chi_sq, dof). blank_struct = hl.struct(; beta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:45453,toler,tolerance,45453,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,ods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Float32Expression. View page source. Float32Expression. class hail.expr.Float32Expression[source]; Expression of type tfloat32.; Attributes. dtype; The data type of the expression. Methods. __add__(other); Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other); Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other); Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other); Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __le__(other); Less-than-or-equals comparison.; Examples; >>> hl.eval(x <,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Float32Expression.html:1292,error,error,1292,docs/0.2/hail.expr.Float32Expression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html,1,['error'],['error']
Availability,ods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Float64Expression. View page source. Float64Expression. class hail.expr.Float64Expression[source]; Expression of type tfloat64.; Attributes. dtype; The data type of the expression. Methods. __add__(other); Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other); Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other); Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other); Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __le__(other); Less-than-or-equals comparison.; Examples; >>> hl.eval(x <,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Float64Expression.html:1292,error,error,1292,docs/0.2/hail.expr.Float64Expression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float64Expression.html,1,['error'],['error']
Availability,"of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:5900,error,error,5900,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['error']
Availability,"of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail query but can be used standalone for Hail Batch users by; import hailtop.fs as hfs. Deprecations. (#12929) Hail no; longer officially supports Python 3.7.; (#12917) The; hail-az scheme for referencing blobs in ABS is now deprecated and; will be removed in an upcoming release. Bug Fixes. (#12913) Fixed bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point.; (#12901); hl.Struct now has a correct and useful implementation of; pprint. Version 0.2.115; Rele",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:33809,error,errors,33809,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"of this ndarray. For an n-dimensional array a,; a[i_0, …, i_n-1, i_n] = a.T[i_n, i_n-1, …, i_0].; Same as self.transpose().; See also transpose(). Returns:; NDArrayExpression. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html:2294,error,error,2294,docs/0.2/hail.expr.NDArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html,1,['error'],['error']
Availability,"of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probability p = Prob(\(Z < x\)) with \(Z\); a normal random variable. Defaults to a standard normal random variable. Parameters:. x (float or Expression of type tfloat64); mu (float or Expression o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:20832,error,error,20832,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['error'],['error']
Availability,"ole. summarize; Compute and print summary information about the expression. take; Collect the first n records of an expression. __eq__(other)[source]; Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other)[source]; Return self>=value. __gt__(other)[source]; Return self>value. __le__(other)[source]; Return self<=value. __lt__(other)[source]; Return self<value. __ne__(other)[source]; Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True)[source]; Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>)[source]; Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True)[source]; Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Expression-1.html:2052,error,error,2052,docs/0.2/hail.expr.Expression-1.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html,2,['error'],['error']
Availability,"om hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _hwe_normalized_blanczos(call_expr, k, compute_loadings). return pca(hwe_normalize(call_expr), k, compute_loadings). [docs]@typecheck(entry_expr=expr_float64, k=int, compute_loadings=bool); def pca(entry_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysis (PCA) on numeric columns derived from a; matrix table. Examples; --------. For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls. >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; -------; This method does **not** automatically mean-center or normalize each column.; If desired, such transformations should be incorporated in `entry_expr`. Hail will return an error if `entry_expr` evaluates to missing, nan, or; infinity on any entry. Notes; -----. PCA is run on the columns of the numeric matrix obtained by evaluating; `entry_expr` on each entry of the matrix table, or equivalently on the rows; of the **transposed** numeric matrix :math:`M` referenced below. PCA computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in; :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors; (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:5195,error,error,5195,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['error'],['error']
Availability,"om pyspark.sql import SQLContext. from hail.dataset import VariantDataset; from hail.expr import Type; from hail.java import *; from hail.keytable import KeyTable; from hail.stats import UniformDist, TruncatedBetaDist, BetaDist; from hail.utils import wrap_to_list. [docs]class HailContext(object):; """"""The main entry point for Hail functionality. .. warning::; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the :py:meth:`.HailContext.stop` method.; ; If passing in a Spark context, ensure that the configuration parameters ``spark.sql.files.openCostInBytes``; and ``spark.sql.files.maxPartitionBytes`` are set to as least 50GB. :param sc: Spark context, one will be created if None.; :type sc: :class:`.pyspark.SparkContext`. :param appName: Spark application identifier. :param master: Spark cluster master. :param local: Local resources to use. :param log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change conf",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:1486,error,error,1486,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['error'],['error']
Availability,"om their corresponding Hail types. To output a desired; Description, Number, and/or Type value in a FORMAT or INFO field or to; specify FILTER lines, use the `metadata` parameter to supply a dictionary; with the relevant information. See; :func:`get_vcf_metadata` for how to obtain the; dictionary corresponding to the original VCF, and for info on how this; dictionary should be structured. The output VCF header will also contain CONTIG lines; with ID, length, and assembly fields derived from the reference genome of; the dataset. The output VCF header will `not` contain lines added by external tools; (such as bcftools and GATK) unless they are explicitly inserted using the; `append_to_header` parameter. Warning; -------. INFO fields stored at VCF import are `not` automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating `info`, downstream; tools which may produce erroneous results. The solution is to create new; fields in `info` or overwrite existing fields. For example, in order to; produce an accurate `AC` field, one can run :func:`.variant_qc` and copy; the `variant_qc.AC` field to `info.AC` as shown below. >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) # doctest: +SKIP; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; -------; Do not export to a path that is being read from in the same pipeline. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; output : :class:`str`; Path of .vcf or .vcf.bgz file to write.; append_to_header : :class:`str`, optional; Path of file to append to VCF header.; parallel : :class:`str`, optional; If ``'header_per_shard'``, return a set of VCF files (one per; partition) rather than serially concatenating these files. If; ``'separate_header'``, re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:19366,down,downstream,19366,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['down'],['downstream']
Availability,"omVar; HomRef; HemiX. 10; Any; HomRef; HomVar; HemiX. 11; HomVar; Any; HomRef; HemiY. 12; HomRef; Any; HomVar; HemiY. This method only considers children with two parents and a defined sex.; PAR is currently defined with respect to reference; GRCh37:. X: 60001 - 2699520, 154931044 - 155260560; Y: 10001 - 2649520, 59034050 - 59363566. Parameters:pedigree (Pedigree) – Sample pedigree. Returns:Four tables with Mendel error statistics. Return type:(KeyTable, KeyTable, KeyTable, KeyTable). min_rep(max_shift=100)[source]¶; Gives minimal, left-aligned representation of alleles. Note that this can change the variant position.; Examples; 1. Simple trimming of a multi-allelic site, no change in variant position; 1:10000:TAA:TAA,AA => 1:10000:TA:T,A; 2. Trimming of a bi-allelic site leading to a change in position; 1:10000:AATAA,AAGAA => 1:10002:T:G. Parameters:max_shift (int) – maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. Return type:VariantDataset. naive_coalesce(max_partitions)[source]¶; Naively descrease the number of partitions. Warning; naive_coalesce() simply combines adjacent partitions to achieve the desired number. It does not attempt to rebalance, unlike repartition(), so it can produce a heavily unbalanced dataset. An unbalanced dataset can be inefficient to operate on because the work is not evenly distributed across partitions. Parameters:max_partitions (int) – Desired number of partitions. If the current number of partitions is less than max_partitions, do nothing. Returns:Variant dataset with the number of partitions equal to at most max_partitions. Return type:VariantDataset. num_partitions()[source]¶; Number of partitions.; Notes; The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. P",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:128238,error,error,128238,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,"om_ndarray(((1 / S) * V).T, block_size=block_size) @ g.T; beta = beta.checkpoint(new_temp_file('pc_relate_bm/beta', 'bm')). # Compute matrix of individual-specific AF estimates (mu), shape (m, n); mu = 0.5 * (BlockMatrix.from_ndarray(V * S, block_size=block_size) @ beta).T; # Replace entries in mu with NaN if invalid or if corresponding GT is missing (no contribution from that variant); mu = mu._apply_map2(; lambda _mu, _g: hl.if_else(_bad_mu(_mu, min_individual_maf) | hl.is_nan(_g), nan, _mu),; g,; sparsity_strategy='NeedsDense',; ); mu = mu.checkpoint(new_temp_file('pc_relate_bm/mu', 'bm')). # Compute kinship matrix (phi), shape (n, n); # Where mu is NaN (missing), set variance and centered AF to 0 (no contribution from that variant); variance = _replace_nan(mu * (1.0 - mu), 0.0).checkpoint(new_temp_file('pc_relate_bm/variance', 'bm')); centered_af = _replace_nan(g - (2.0 * mu), 0.0); phi = _gram(centered_af) / (4.0 * _gram(variance.sqrt())); phi = phi.checkpoint(new_temp_file('pc_relate_bm/phi', 'bm')); ht = phi.entries().rename({'entry': 'kin'}); ht = ht.annotate(k0=hl.missing(hl.tfloat64), k1=hl.missing(hl.tfloat64), k2=hl.missing(hl.tfloat64)). if statistics in ['kin2', 'kin20', 'all']:; # Compute inbreeding coefficient and dominance encoding of GT matrix; f_i = (2.0 * phi.diagonal()) - 1.0; gd = g._apply_map2(lambda _g, _mu: _dominance_encoding(_g, _mu), mu, sparsity_strategy='NeedsDense'); normalized_gd = gd - (variance * (1.0 + f_i)). # Compute IBD2 (k2) estimate; k2 = _gram(normalized_gd) / _gram(variance); ht = ht.annotate(k2=k2.entries()[ht.i, ht.j].entry). if statistics in ['kin20', 'all']:; # Get the numerator used in IBD0 (k0) computation (IBS0), compute indicator matrices for homozygotes; hom_alt = g._apply_map2(; lambda _g, _mu: hl.if_else((_g != 2.0) | hl.is_nan(_mu), 0.0, 1.0), mu, sparsity_strategy='NeedsDense'; ); hom_ref = g._apply_map2(; lambda _g, _mu: hl.if_else((_g != 0.0) | hl.is_nan(_mu), 0.0, 1.0), mu, sparsity_strategy='NeedsDense'; ); ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:20834,checkpoint,checkpoint,20834,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['checkpoint'],['checkpoint']
Availability,"ome {rg.name!r} has multiple X contigs, this is not supported in 'impute_sex_chr_ploidy_from_interval_coverage'""; ); chr_x = rg.x_contigs[0]; if len(rg.y_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple Y contigs, this is not supported in 'impute_sex_chr_ploidy_from_interval_coverage'""; ); chr_y = rg.y_contigs[0]. mt = mt.annotate_rows(contig=mt.interval.start.contig); mt = mt.annotate_cols(__mean_dp=hl.agg.group_by(mt.contig, hl.agg.sum(mt.sum_dp) / hl.agg.sum(mt.interval_size))). mean_dp_dict = mt.__mean_dp; auto_dp = mean_dp_dict.get(normalization_contig, 0.0); x_dp = mean_dp_dict.get(chr_x, 0.0); y_dp = mean_dp_dict.get(chr_y, 0.0); per_sample = mt.transmute_cols(; autosomal_mean_dp=auto_dp,; x_mean_dp=x_dp,; x_ploidy=2 * x_dp / auto_dp,; y_mean_dp=y_dp,; y_ploidy=2 * y_dp / auto_dp,; ); info(""'impute_sex_chromosome_ploidy': computing and checkpointing coverage and karyotype metrics""); return per_sample.cols().checkpoint(new_temp_file('impute_sex_karyotype', extension='ht')). [docs]@typecheck(; vds=VariantDataset,; calling_intervals=oneof(Table, expr_array(expr_interval(expr_locus()))),; normalization_contig=str,; use_variant_dataset=bool,; ); def impute_sex_chromosome_ploidy(; vds: VariantDataset, calling_intervals, normalization_contig: str, use_variant_dataset: bool = False; ) -> Table:; """"""Impute sex chromosome ploidy from depth of reference or variant data within calling intervals. Returns a :class:`.Table` with sample ID keys, with the following fields:. - ``autosomal_mean_dp`` (*float64*): Mean depth on calling intervals on normalization contig.; - ``x_mean_dp`` (*float64*): Mean depth on calling intervals on X chromosome.; - ``x_ploidy`` (*float64*): Estimated ploidy on X chromosome. Equal to ``2 * x_mean_dp / autosomal_mean_dp``.; - ``y_mean_dp`` (*float64*): Mean depth on calling intervals on chromosome.; - ``y_ploidy`` (*float64*): Estimated ploidy on Y chromosome. Equal to ``2 * y_mean_db / autosomal_mean_dp``. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/methods.html:12255,checkpoint,checkpoint,12255,docs/0.2/_modules/hail/vds/methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html,2,['checkpoint'],['checkpoint']
Availability,"ome)), is_negative_strand=tbool). if not rg.has_liftover(dest_reference_genome.name):; raise TypeError(; """"""Reference genome '{}' does not have liftover to '{}'.; Use 'add_liftover' to load a liftover chain file."""""".format(rg.name, dest_reference_genome.name); ). expr = _func(method_name, rtype, x, to_expr(min_match, tfloat64)); if not include_strand:; expr = expr.result; return expr. [docs]@typecheck(; f=func_spec(1, expr_float64),; min=expr_float64,; max=expr_float64,; max_iter=builtins.int,; epsilon=builtins.float,; tolerance=builtins.float,; ); def uniroot(f: Callable, min, max, *, max_iter=1000, epsilon=2.2204460492503131e-16, tolerance=1.220703e-4):; """"""Finds a root of the function `f` within the interval `[min, max]`. Examples; --------. >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; -----; `f(min)` and `f(max)` must not have the same sign. If no root can be found, the result of this call will be `NA` (missing). :func:`.uniroot` returns an estimate for a root with accuracy; `4 * epsilon * abs(x) + tolerance`. 4*EPSILON*abs(x) + tol. Parameters; ----------; f : function ( (arg) -> :class:`.Float64Expression`); Must return a :class:`.Float64Expression`.; min : :class:`.Float64Expression`; max : :class:`.Float64Expression`; max_iter : `int`; The maximum number of iterations before giving up.; epsilon : `float`; The scaling factor in the accuracy of the root found.; tolerance : `float`; The constant factor in approximate accuracy of the root found. Returns; -------; :class:`.Float64Expression`; The root of the function `f`.; """""". # Based on:; # https://github.com/wch/r-source/blob/e5b21d0397c607883ff25cca379687b86933d730/src/library/stats/src/zeroin.c. def error_if_missing(x):; res = f(x); return case().when(is_defined(res), res).or_error(format(""'uniroot': value of f(x) is missing for x = %.1e"", x)). wrapped_f = hl.experimental.define_function(error_if_missing, 'float'). def uniroot(recur, a, b, c, fa, fb, fc, prev, iterations_remaining):; tol = 2 ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:172899,toler,tolerance,172899,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['toler'],['tolerance']
Availability,"on 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:88522,error,error,88522,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"on API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Using the expression language to slice, dice, and query genetic data. View page source. Using the expression language to slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr.write('Done!\n'). Downloading data (~50M) from Google Storage...; Downl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:1648,error,error,1648,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['error'],['error']
Availability,"on(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----; The aggregation scope includes all column fields and global fields. See Also; --------; :meth:`.result`. Parameters; ----------; named_exprs : varargs of :class:`.Expression`; Aggregation expressions. Returns; -------; :class:`.GroupedMatrixTable`; """"""; if self._row_keys is not None:; raise Not",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:8479,down,downstream,8479,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['down'],['downstream']
Availability,"on). Arithmetic with a scalar will; apply the operation to each element of the ndarray.; Attributes. T; Reverse the dimensions of this ndarray. dtype; The data type of the expression. ndim; The number of dimensions of this ndarray. shape; The shape of this ndarray. Methods. sum; Sum out one or more axes of an ndarray. property T; Reverse the dimensions of this ndarray. For an n-dimensional array a,; a[i_0, …, i_n-1, i_n] = a.T[i_n, i_n-1, …, i_0].; Same as self.transpose().; See also transpose(). Returns:; NDArrayExpression. __add__(other)[source]; Positionally add an array or a scalar. Parameters:; other (NumericExpression or NDArrayNumericExpression) – Value or ndarray to add. Returns:; NDArrayNumericExpression – NDArray of positional sums. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other)[source]; Positionally divide by a ndarray or a scalar using floor division. Parameters:; other (NumericExpression or NDArrayNumericExpression). Returns:; NDArrayNumericExpression. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __matmul__(other)[source]; Matrix multiplication: a @ b, semantically equivalent to NumPy matmul. If a and b are vectors,; the vector dot product is performed, returning a NumericExpression. If a and b are both 2-dimensional; matrices, this performs normal matrix multiplication. If a and b have more than 2 dimensions, they are; treated as multi-dimensional stacks of 2-dimensional matrices. Matrix multiplication is applied element-wise; across the higher dimensions",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:2233,error,error,2233,docs/0.2/hail.expr.NDArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html,1,['error'],['error']
Availability,"on_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.pca(entry_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on numeric columns derived from a; matrix table.; Examples; For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls.; >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; This method does not automatically mean-center or normalize each column.; If desired, such transformations should be incorporated in entry_expr.; Hail will return an error if entry_expr evaluates to missing, nan, or; infinity on any entry. Notes; PCA is run on the columns of the numeric matrix obtained by evaluating; entry_expr on each entry of the matrix table, or equivalently on the rows; of the transposed nume",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:16353,toler,tolerance,16353,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,2,['toler'],['tolerance']
Availability,"onary keyed by results of f. hail.expr.functions.fold(f, zero, collection)[source]; Reduces a collection with the given function f, provided the initial value zero.; Examples; >>> a = [0, 1, 2]. >>> hl.eval(hl.fold(lambda i, j: i + j, 0, a)); 3. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f.; collection (ArrayExpression or SetExpression). Returns:; Expression. hail.expr.functions.array_scan(f, zero, a)[source]; Map each element of a to cumulative value of function f, with initial value zero.; Examples; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f.; a (ArrayExpression). Returns:; ArrayExpression. hail.expr.functions.reversed(x)[source]; Reverses the elements of a collection.; Examples; >>> a = ['The', 'quick', 'brown', 'fox']; >>> hl.eval(hl.reversed(a)); ['fox', 'brown', 'quick', 'The']. Parameters:; x (ArrayExpression or StringExpression) – Array or string expression. Returns:; Expression. hail.expr.functions.keyed_intersection(*arrays, key)[source]; Compute the intersection of sorted arrays on a given key.; Requires sorted arrays with distinct keys. Warning; Experimental. Does not support downstream randomness. Parameters:. arrays; key. Returns:; ArrayExpression. hail.expr.functions.keyed_union(*arrays, key)[source]; Compute the distinct union of sorted arrays on a given key.; Requires sorted arrays with distinct keys. Warning; Experimental. Does not support downstream randomness. Parameters:. exprs; key. Returns:; ArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/collections.html:13586,down,downstream,13586,docs/0.2/functions/collections.html,https://hail.is,https://hail.is/docs/0.2/functions/collections.html,2,['down'],['downstream']
Availability,"oncept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_table(tmp2).add_index(uid).key_by(uid); ht.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n).key_by().drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n). return Table(; ir.TableRepartition(; self._tir, n, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'Table':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> table_result = table1.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:93901,checkpoint,checkpoint,93901,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['checkpoint'],['checkpoint']
Availability,"ond table: errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the “.fmendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; father (String) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:125722,error,errors,125722,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,onding to the; column keys of the input matrix table. This function returns a :class:`~.Table` with one row per set of summary; statistics passed to the ``chi_sq_exprs`` argument. The following; row-indexed fields are included in the table:. * **phenotype** (:py:data:`.tstr`) -- The name of the phenotype. The; returned table is keyed by this field. See the notes below for; details on the possible values of this field.; * **mean_chi_sq** (:py:data:`.tfloat64`) -- The mean chi-squared; test statistic for the given phenotype.; * **intercept** (`Struct`) -- Contains fields:. - **estimate** (:py:data:`.tfloat64`) -- A point estimate of the; intercept :math:`1 + Na`.; - **standard_error** (:py:data:`.tfloat64`) -- An estimate of; the standard error of this point estimate. * **snp_heritability** (`Struct`) -- Contains fields:. - **estimate** (:py:data:`.tfloat64`) -- A point estimate of the; SNP-heritability :math:`h_g^2`.; - **standard_error** (:py:data:`.tfloat64`) -- An estimate of; the standard error of this point estimate. Warning; -------; :func:`.ld_score_regression` considers only the rows for which both row; fields ``weight_expr`` and ``ld_score_expr`` are defined. Rows with missing; values in either field are removed prior to fitting the LD score; regression model. Parameters; ----------; weight_expr : :class:`.Float64Expression`; Row-indexed expression for the LD scores used to derive; variant weights in the model.; ld_score_expr : :class:`.Float64Expression`; Row-indexed expression for the LD scores used as covariates; in the model.; chi_sq_exprs : :class:`.Float64Expression` or :obj:`list` of; :class:`.Float64Expression`; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions for chi-squared; statistics resulting from genome-wide association; studies (GWAS).; n_samples_exprs: :class:`.NumericExpression` or :obj:`list` of; :class:`.NumericExpression`; One or more row-indexed (if table) or entry-indexed; (if matrix table) expressions ,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ld_score_regression.html:7007,error,error,7007,docs/0.2/_modules/hail/experimental/ld_score_regression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ld_score_regression.html,2,['error'],['error']
Availability,"one. Attributes. dtype; The data type of the expression. Methods. __add__(other); Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __and__(other)[source]; Return True if the left and right arguments are True.; Examples; >>> hl.eval(t & f); False. >>> hl.eval(t & na); None. >>> hl.eval(f & na); False. The & and | operators have higher priority than comparison; operators like ==, <, or >. Parentheses are often; necessary:; >>> x = hl.literal(5). >>> hl.eval((x < 10) & (x > 2)); True. Parameters:; other (BooleanExpression) – Right-side operand. Returns:; BooleanExpression – True if both left and right are True. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other); Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other); Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other); Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __invert__()[source]; Return the boolean negation.; Examples; >>> hl.eval",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:1927,error,error,1927,docs/0.2/hail.expr.BooleanExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html,1,['error'],['error']
Availability,"one``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """""". valid_regions = {'us', 'us-central1', 'europe-west1'}; if region not in valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {valid_clouds}.'; ). datasets = get_datasets_metadata(); names = set([dataset for dataset in datasets]); if name not in names:; raise ValueError(f'{name} is not a dataset available in the' f' repository.'). versions = set(dataset['version'] for dataset in datasets[name]['versions']); if version not in versions:; raise ValueError(; f'Version {version!r} not available for dataset' f' {name!r}.\n' f'Available versions: {versions}.'; ). reference_genomes = set(dataset['reference_genome'] for dataset in datasets[name]['versions']); if reference_genome not in reference_genomes:; raise ValueError(; f'Reference genome build {reference_genome!r} not'; f' available for dataset {name!r}.\n'; f'Available reference genome builds:'; f' {reference_genomes}.'; ). clouds = set(k for dataset in datasets[name]['versions'] for k in dataset['url'].keys()); if cloud not in clouds:; raise ValueError(f'Cloud platform {cloud!r} not available for dataset {name}.\nAvailable platforms: {clouds}.'). regions = set(k for dataset in datasets[name]['versions'] for k in dataset['url'][cloud].keys()); if region not in regions:; r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:2984,avail,available,2984,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,2,['avail'],['available']
Availability,"only visible in the body of the let, the; expression following in. You can assign multiple variables. Variable; assignments are separated by and. Each variable is visible in the; right hand side of the following variables as well as the body of the; let. For example:. In [23]:. hc.eval_expr_typed('''; let a = 5; and b = a + 1; in a * b; '''). Out[23]:. (30, Int). Conditionals¶; Unlike other languages, conditionals in Hail return a value. The arms of; the conditional must have the same type. The predicate must be of type; Boolean. If the predicate is missing, the value of the entire; conditional is missing. Here are some simple examples. In [24]:. hc.eval_expr_typed('if (true) 1 else 2'). Out[24]:. (1, Int). In [25]:. hc.eval_expr_typed('if (false) 1 else 2'). Out[25]:. (2, Int). In [26]:. hc.eval_expr_typed('if (NA: Boolean) 1 else 2'). Out[26]:. (None, Int). The if and else branches need to return the same type. The below; expression is invalid. In [27]:. # Uncomment and run the below code to see the error message. # hc.eval_expr_typed('if (true) 1 else ""two""'). Compound Types¶; Hail has several compound types: -; Array[T] -; Set[T] - Dict[K,; V] -; Aggregable[T] -; Struct; T, K and V here mean any type, including other compound; types. Hail’s Array[T] objects are similar to Python’s lists, except; they must be homogenous: that is, each element must be of the same type.; Arrays are 0-indexed. Here are some examples of simple array; expressions.; Array literals are constructed with square brackets. In [28]:. hc.eval_expr_typed('[1, 2, 3, 4, 5]'). Out[28]:. ([1, 2, 3, 4, 5], Array[Int]). Arrays are indexed with square brackets and support Python’s slice; syntax. In [29]:. hc.eval_expr_typed('let a = [1, 2, 3, 4, 5] in a[0]'). Out[29]:. (1, Int). In [30]:. hc.eval_expr_typed('let a = [1, 2, 3, 4, 5] in a[1:3]'). Out[30]:. ([2, 3], Array[Int]). In [31]:. hc.eval_expr_typed('let a = [1, 2, 3, 4, 5] in a[1:]'). Out[31]:. ([2, 3, 4, 5], Array[Int]). In [32]:. hc.eval_expr_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:7161,error,error,7161,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,1,['error'],['error']
Availability,"ons. **Examples**. Repartition the variant dataset to have 500 partitions:. >>> vds_result = vds.repartition(500). **Notes**. Check the current number of partitions with :py:meth:`.num_partitions`. The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. When a variant dataset with :math:`M` variants is first imported, each of the :math:`k` partition will contain about :math:`M/k` of the variants. Since each partition has some computational overhead, decreasing the number of partitions can improve performance after significant filtering. Since it's recommended to have at least 2 - 4 partitions per core, increasing the number of partitions can allow one to take advantage of more cores. Partitions are a core concept of distributed computation in Spark, see `here <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__ for details. With ``shuffle=True``, Hail does a full shuffle of the data and creates equal sized partitions. With ``shuffle=False``, Hail combines existing partitions to avoid a full shuffle. These algorithms correspond to the ``repartition`` and ``coalesce`` commands in Spark, respectively. In particular, when ``shuffle=False``, ``num_partitions`` cannot exceed current number of partitions. :param int num_partitions: Desired number of partitions, must be less than the current number if ``shuffle=False``. :param bool shuffle: If true, use full shuffle to repartition. :return: Variant dataset with the number of partitions equal to at most ``num_partitions``; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.coalesce(num_partitions, shuffle); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(max_partitions=integral); def naive_coalesce(self, max_partitions):; """"""Naively descrease the number of partitions. .. warning ::. :py:meth:`~hail.Va",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:192415,resilien,resilient-distributed-datasets-rdds,192415,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['resilien'],['resilient-distributed-datasets-rdds']
Availability,"onsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_agg; correctly. Performance Improvements. (#8413) Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as opposed to only regular python strings.; (#8198) Improved; matrix multiplication interoperation between hail; NDArrayExpression and numpy. Bug fixes. (#8279) Fix a bug; where hl.agg.approx_cdf failed inside of a group_cols_by.; (#8275) Fix bad error; message coming from mt.make_table() when keys are missing.; (#8274) Fix memory; leak in hl.export_bgen.; (#8273) Fix segfault; caused by hl.agg.downsample inside of an array_agg or; group_by. hailctl dataproc. (#8253); hailctl dataproc now supports new flags; --requester-pays-allow-all and; --requester-pays-allow-buckets. This will configure your hail; installation to be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:75537,error,error,75537,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"ontain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:49915,error,errors,49915,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability,"ontains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:156607,error,errors,156607,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,"on’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile if intervals were; created conditionally.; (#10746) Various; compiler improvements, decrease likelihood of ClassTooLarge; errors.; (#10829) Fix a bug; where hl.missing and CaseBuilder.or_error failed if their; type was a struct containing a field starting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays would have their; first elements corrupted and long arrays would cause segfaults.; (#10523) Fixed bu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:57309,error,errors,57309,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). Under the PC-Relate model, kinship, :math:`\phi_{ij}`, ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, :math:`k^{(2)}_{ij}`,; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs. - Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation. - ""Third degree relatives"" are those pairs sharing; :math:`2^{-3} = 12.5 %` of their genetic material, the results of; PCRelate are often too noisy to reliably distinguish these pairs from; higher-degree-relative-pairs or unrelated pairs. Note that :math:`g_{is}` is the number of alternate alleles. Hence, for; multi-allelic variants, a value of 2 may indicate two distinct alternative; alleles rather than a homozygous variant genotype. To enforce the latter,; either filter or split multi-allelic variants first. The resulting table has the first 3, 4, 5, or 6 fields below, depending on; the `statistics` parameter:. - `i` (``col_key.dtype``) -- First sample. (key field); - `j` (``col_key.dtype``) -- Second sample. (key field); - `kin` (:py:data:`.tfloat64`) -- Kinship estimate, :math:`\widehat{\phi_{ij}}`.; - `ibd2` (:py:data:`.tfloat64`) -- IBD2 estimate, :math:`\widehat{k^{(2)}_{ij}}`.; - `ibd0` (:py:data:`.tfloat64`) -- IBD0 estimate, :math:`\widehat{k^{(0)}_{ij}}`.; - `ibd1` (:py:data:`.tfloat64`) -- IBD1 estimate, :math:`\widehat{k^{(1)}_{ij}}`. Here ``col_key`` refers to the column key of the source matrix table,; and ``col_key.dtype`",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:9257,reliab,reliably,9257,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['reliab'],['reliably']
Availability,"or Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:27155,degraded,degraded,27155,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['degraded'],['degraded']
Availability,"or each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the linear regression model using the supplied phenotype and covariates.; The model is that of linreg() with sample genotype gt replaced by the score in the sample; key table. For each key, missing scores are mean-imputed across all samples.; The resulting linear regression key table has the following columns:. value of key_name (String) – descriptor of variant group key (key column); beta (Double) – fit coefficient, \(\hat\beta_1\); se (Double) – estimated standard error, \(\widehat{\mathrm{se}}\); tstat (Double) – \(t\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); pval (Double) – \(p\)-value. linreg_burden() returns both the linear regression key table and the sample key table.; Extended example; Let’s walk through these steps in the max() toy example above.; There are six samples with the following annotations:. Sample; pheno; cov1; cov2. A; 0; 0; -1. B; 0; 2; 3. C; 1; 1; 5. D; 1; -2; 0. E; 1; -2; -4. F; 1; 4; 3. There are three variants with the following gt values:. Variant; A; B; C; D; E; F. 1:1:A:C; 0; 1; 0; 0; 0; 1. 1:2:C:T; .; 2; .; 2; 0; 0. 1:3:G:C; 0; .; 1; 1; 1; . The va.genes annotation of type Set[String] on example_burden.vds was created; using annotate_variants_table() with product=True on the interval list:; 1	1	2	+	geneA; 1	2	2	+	geneB; 1	1	3	+	geneC. So there are three overlapping genes: gene A contains two variants,; gene B contains one variant, and gene C contains all three variants. gene; 1:1:A:C; 1:2:C:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:87658,error,error,87658,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,"or not isinstance(calling_intervals.key[0].dtype, hl.tinterval); or calling_intervals.key[0].dtype.point_type != vds.reference_data.locus.dtype; ):; raise ValueError(; f""'impute_sex_chromosome_ploidy': expect calling_intervals to be list of intervals or""; f"" table with single key of type interval<locus>, found table with key: {key_dtype}""; ). rg = vds.reference_data.locus.dtype.reference_genome. par_boundaries = []; for par_interval in rg.par:; par_boundaries.append(par_interval.start); par_boundaries.append(par_interval.end). # segment on PAR interval boundaries; calling_intervals = hl.segment_intervals(calling_intervals, par_boundaries). # remove intervals overlapping PAR; calling_intervals = calling_intervals.filter(; hl.all(lambda x: ~x.overlaps(calling_intervals.interval), hl.literal(rg.par)); ). # checkpoint for efficient multiple downstream usages; info(""'impute_sex_chromosome_ploidy': checkpointing calling intervals""); calling_intervals = calling_intervals.checkpoint(new_temp_file(extension='ht')). interval = calling_intervals.key[0]; (any_bad_intervals, chrs_represented) = calling_intervals.aggregate((; hl.agg.any(interval.start.contig != interval.end.contig),; hl.agg.collect_as_set(interval.start.contig),; )); if any_bad_intervals:; raise ValueError(; ""'impute_sex_chromosome_ploidy' does not support calling intervals that span chromosome boundaries""; ). if len(rg.x_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple X contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ); if len(rg.y_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple Y contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ). kept_contig_filter = hl.array(chrs_represented).map(lambda x: hl.parse_locus_interval(x, reference_genome=rg)); vds = VariantDataset(; hl.filter_intervals(vds.reference_data, kept_contig_filter),; hl.filter_intervals(vds.variant_data, kept_contig_filter),; ). if use_var",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/methods.html:15065,checkpoint,checkpoint,15065,docs/0.2/_modules/hail/vds/methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html,2,['checkpoint'],['checkpoint']
Availability,"or point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Ad",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:94637,down,downgrade,94637,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['down'],['downgrade']
Availability,"or.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extendi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:157081,error,errors,157081,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,"orage in read-only mode. Return type:; Self. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the job’s CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where the optional suffix is m representing millicpu.; Omitting a suffix means the value is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the job’s CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None]) – Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job) – Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ...",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:4321,echo,echo,4321,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['echo'],['echo']
Availability,"ords of expr. fraction(predicate); Compute the fraction of records where predicate is True. hardy_weinberg_test(expr[, one_sided]); Performs test of Hardy-Weinberg equilibrium. explode(f, array_agg_expr); Explode an array or set expression to aggregate the elements of all records. filter(condition, aggregation); Filter records according to a predicate. inbreeding(expr, prior); Compute inbreeding statistics on calls. call_stats(call, alleles); Compute useful call statistics. info_score(gp); Compute the IMPUTE information score. hist(expr, start, end, bins); Compute binned counts of a numeric expression. linreg(y, x[, nested_dim, weight]); Compute multivariate linear regression statistics. corr(x, y); Computes the Pearson correlation coefficient between x and y. group_by(group, agg_expr); Compute aggregation statistics stratified by one or more groups. array_agg(f, array); Aggregate an array element-wise using a user-specified aggregation function. downsample(x, y[, label, n_divisions]); Downsample (x, y) coordinate datapoints. approx_cdf(expr[, k, _raw]); Produce a summary of the distribution of values. hail.expr.aggregators.collect(expr)[source]; Collect records into an array.; Examples; Collect the ID field where HT is greater than 68:; >>> table1.aggregate(hl.agg.filter(table1.HT > 68, hl.agg.collect(table1.ID))); [2, 3]. Notes; The element order of the resulting array is not guaranteed, and in some; cases is non-deterministic.; Use collect_as_set() to collect unique items. Warning; Collecting a large number of items can cause out-of-memory exceptions. Parameters:; expr (Expression) – Expression to collect. Returns:; ArrayExpression – Array of all expr records. hail.expr.aggregators.collect_as_set(expr)[source]; Collect records into a set.; Examples; Collect the unique ID field where HT is greater than 68:; >>> table1.aggregate(hl.agg.filter(table1.HT > 68, hl.agg.collect_as_set(table1.ID))); {2, 3}. Note that when collecting a set-typed field with collect_as_set",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:2733,down,downsample,2733,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['down'],['downsample']
Availability,"ore=0, after=0, reference_genome='default')[source]; Return the reference sequence at a given locus.; Examples; Return the reference allele for 'GRCh37' at the locus '1:45323':; >>> hl.eval(hl.get_sequence('1', 45323, reference_genome='GRCh37')) ; ""T"". Notes; This function requires reference genome has an attached; reference sequence. Use ReferenceGenome.add_sequence() to; load and attach a reference sequence to a reference genome.; Returns None if contig and position are not valid coordinates in; reference_genome. Parameters:. contig (Expression of type tstr) – Locus contig.; position (Expression of type tint32) – Locus position.; before (Expression of type tint32, optional) – Number of bases to include before the locus of interest. Truncates at; contig boundary.; after (Expression of type tint32, optional) – Number of bases to include after the locus of interest. Truncates at; contig boundary.; reference_genome (str or ReferenceGenome) – Reference genome to use. Must have a reference sequence available. Returns:; StringExpression. hail.expr.functions.mendel_error_code(locus, is_female, father, mother, child)[source]; Compute a Mendelian violation code for genotypes.; >>> father = hl.call(0, 0); >>> mother = hl.call(1, 1); >>> child1 = hl.call(0, 1) # consistent; >>> child2 = hl.call(0, 0) # Mendel error; >>> locus = hl.locus('2', 2000000). >>> hl.eval(hl.mendel_error_code(locus, True, father, mother, child1)); None. >>> hl.eval(hl.mendel_error_code(locus, True, father, mother, child2)); 7. Note; Ignores call phasing, and assumes diploid and biallelic. Haploid calls for; hemiploid samples on sex chromosomes also are acceptable input. Notes; In the table below, the copy state of a locus with respect to a trio is; defined as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; LocusExpression.in_autosome():. Auto – in autosome or in PAR, or in non-PAR of X and female child; HemiX – in non",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:19679,avail,available,19679,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['avail'],['available']
Availability,"ory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht.select_globals(); return ht; mt = matrix_table_source('skat/x', x); raise_unless_entry_indexed('skat/x', x). analyze('skat/key_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:107175,toler,tolerance,107175,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,"ost frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, your; Java installation must be an arm64 installation. Using x86_64 Java; with Hail on Apple M1 or M2 will cause SIGILL errors. If you have an; Apple M1 or Apple M2 and /usr/libexec/java_home -V does not; include (arm64), you must switch to an arm64 version of the JVM.; (#14022) Fix; (#13937) caused by; faulty library code in the Google Cloud Storage API Java client; library.; (#13812) Permit; hailctl batch submit to accept relative paths. Fix; (#13785).; (#13885) Hail; Query-on-Batch previously used Class A Operations for all interaction; with blobs. This change ensures that QoB only uses Class A Operations; when necessary.; (#14127); hailctl dataproc start ... --dry-run now uses shell escapes such; that, after copied and pasted into a shell, the gcloud command; works as expected.; (#14062) Fix; (#14052) which; caused incorrect results for identity by descent in Query-on-Batch.; (#14122) Ensure that; stack traces are transmitted from workers to the driver to the; client.; (#14105) When a VCF; contains missing values in array fields, Hail now suggests using; array_elements_required=False. Deprecations. (#13987) Deprecate; default_reference parameter to hl.init, users should use; hl.default_reference with an argument to set new default; references usually shortly after hl.init. Version 0.2.126; Release",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:19439,fault,faulty,19439,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['fault'],['faulty']
Availability,"otype is significantly associated with the genotype:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 1.78e+02 | 1.68e-04 | 0 |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:92686,fault,fault,92686,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability,"otype. To enforce the latter,; either filter or split multi-allelic variants first.; The resulting table has the first 3, 4, 5, or 6 fields below, depending on; the statistics parameter:. i (col_key.dtype) – First sample. (key field); j (col_key.dtype) – Second sample. (key field); kin (tfloat64) – Kinship estimate, \(\widehat{\phi_{ij}}\).; ibd2 (tfloat64) – IBD2 estimate, \(\widehat{k^{(2)}_{ij}}\).; ibd0 (tfloat64) – IBD0 estimate, \(\widehat{k^{(0)}_{ij}}\).; ibd1 (tfloat64) – IBD1 estimate, \(\widehat{k^{(1)}_{ij}}\). Here col_key refers to the column key of the source matrix table,; and col_key.dtype is a struct containing the column key fields.; There is one row for each pair of distinct samples (columns), where i; corresponds to the column of smaller column index. In particular, if the; same column key value exists for \(n\) columns, then the resulting; table will have \(\binom{n-1}{2}\) rows with both key fields equal to; that column key value. This may result in unexpected behavior in downstream; processing. Parameters:. call_expr (CallExpression) – Entry-indexed call expression.; min_individual_maf (float) – The minimum individual-specific minor allele frequency.; If either individual-specific minor allele frequency for a pair of; individuals is below this threshold, then the variant will not; be used to estimate relatedness for the pair.; k (int, optional) – If set, k principal component scores are computed and used.; Exactly one of k and scores_expr must be specified.; scores_expr (ArrayNumericExpression, optional) – Column-indexed expression of principal component scores, with the same; source as call_expr. All array values must have the same positive length,; corresponding to the number of principal components, and all scores must; be non-missing. Exactly one of k and scores_expr must be specified.; min_kinship (float, optional) – If set, pairs of samples with kinship lower than min_kinship are excluded; from the results.; statistics (str) – Set of st",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:20388,down,downstream,20388,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['down'],['downstream']
Availability,"ough` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:62588,toler,tolerance,62588,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['toler'],['tolerance']
Availability,"ow throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop* methods now properly support creation and; modification time.; (#12566) Improve; error message when combining incompatibly indexed fields in certain; operations including array indexing. Version 0.2.108; Released 2023-1-12. New Features. (#12576); hl.import_bgen and hl.export_bgen now support compression; with Zstd. Bug fixes. (#12585); hail.ggplots that have more than one legend group or facet are; now interactive. If such a plot has enough legend entries that the; legend would be taller than the plot, the legend will now be; scrollable. Legend entries for such plots can be clicked to show/hide; traces on the plot, but this does not work and is a known issue that; will only be addressed if hail.ggplot is migrated off of plotly.; (#12584) Fixed bug; which arose as an assertion error about type mismatches. This was; usually triggered when working with tuples.; (#12583) Fixed bug; which showed an empty table for ht.col_key.show().; (#12582) Fixed bug; where matrix tables with duplicate col keys do not show properly.; Also fixed bug where tables and matrix tables with HTML unsafe column; headers are rendered wrong in Jupyter.; (#12574) Fixed a; memory leak when processing tables. Could trigger unnecessarily high; memory use and out of memory errors when there are many rows per; partition or large key fields.; (#12565) Fixed a bug; that prevented exploding on a field of a Table whose value is a; random value. Version 0.2.107; Released 2022-12-14. Bug fixes. (#12543) Fixed; hl.vds.local_to_global error when LA array contains non-ascending; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:40497,error,error,40497,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"ow to aggregate over subsets.); We can do this with the Table.aggregate method.; A call to aggregate has two parts:. The expression to aggregate over (e.g. a field of a Table).; The aggregator to combine the values into the summary. Hail has a large suite of aggregators for summarizing data. Let’s see some in action!. count; Aggregators live in the hl.agg module. The simplest aggregator is count. It takes no arguments and returns the number of values aggregated. [1]:. import hail as hl; from bokeh.io import output_notebook,show; output_notebook(); hl.init(). hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:01.799 Hail: INFO: Movie Lens files found!. [2]:. users.aggregate(hl.agg.count()). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 943. [3]:. users.count(). [3]:. 943. stats; stats computes useful statistics about a numeric expression at once. There are also aggregators for mean, min, max, sum, product and array_sum. [4]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""94043""; 323""M""""writer""""32067""; 424""M""""technician""""43537""; 533""F""""other""""15213""; 642""M""""executive""""98101""; 757""M""""administrator""""91344""; 836""M""""administrator""""05201""; 929""M""""student""""01002""; 1053""M",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/04-aggregation.html:2315,avail,available,2315,docs/0.2/tutorials/04-aggregation.html,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html,1,['avail'],['available']
Availability,"ows are samples, columns are the different covariates; ht = ht.annotate_globals(; covmat=hl.nd.array(ht.samples.map(lambda s: [s[cov_name] for cov_name in cov_field_names])); ). # yvecs is a list of sample-length vectors, one for each dependent variable.; ht = ht.annotate_globals(yvecs=[hl.nd.array(ht.samples[y_name]) for y_name in y_field_names]). # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; def fit_null(yvec):; def error_if_not_converged(null_fit):; return (; hl.case(); .when(; ~null_fit.exploded,; (; hl.case(); .when(null_fit.converged, null_fit); .or_error(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:59569,toler,tolerance,59569,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['toler'],['tolerance']
Availability,"ows in the group (size), the variance component score q_stat, the SKAT; p-value, and a fault flag. For the toy example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC; 3; 4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:81131,fault,fault,81131,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['fault'],['fault']
Availability,"p glob patterns in file names.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of str to str, optional) – Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by reference_genome.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome. hail.methods.import_fam(path, quant_pheno=False, delimiter='\\\\s+', missing='NA')[source]; Import a PLINK FAM file into a Table.; Examples; Import a tab-separated; FAM file; with a case-control phenotype:; >>> fam_kt = hl.import_fam('data/case_control_study.fam'). Import a FAM file with a quantitative phenotype:; >>> fam_kt = hl.import_fam('data/quantitative_study.fam', quant_pheno=True). Notes; In Hail, unlike PLINK, the user must explicitly distinguish between; case-control and quantitative phenotypes. Importing a quantitative; phenotype with quant_pheno=False will return an error; (unless all values happen to be 0, 1, 2, or -9):; The resulting Table will have fields, types, and values that are interpreted as missing. fam_id (tstr) – Family ID (missing = “0”); id (tstr) – Sample ID (key column); pat_id (tstr) – Paternal ID (missing = “0”); mat_id (tstr) – Maternal ID (missing = “0”); is_female (tstr) – Sex (missing = “NA”, “-9”, “0”). One of:. is_case (tbool) – Case-control phenotype (missing = “0”, “-9”,; non-numeric or the missing argument, if given.; quant_pheno (tfloat64) – Quantitative phenotype (missing = “NA” or; the missing argument, if given. Warning; Hail will interpret the value “-9” as a valid quantitative phenotype, which; differs from default PLINK behavior. Use missing='-9' to interpret this; value as missing. Parameters:. path (str) – Path to FAM file.; quant_pheno (bool) – If True, phenotype is interpreted as quantitative.; delimiter (str) – Field delimiter regex.; missing (str) – The string used to denote missing values. For case-control, 0, -9, and; non-numeric are also t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:15405,error,error,15405,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['error'],['error']
Availability,"p-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions; CaseBuilder. View page source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html:1384,error,error,1384,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html,1,['error'],['error']
Availability,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:104061,error,errors,104061,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,5,"['avail', 'error']","['available', 'error', 'errors']"
Availability,"p.float64); keep = np.full_like(x, False, dtype=np.bool_). # State variables:; # (fx, fy) is most recently fixed point on max-ent cdf; fx, fy = min_x, 0; li, ui = 0, 0; j = 1. def slope_from_fixed(i, upper):; xi, yi = point_on_bound(i, upper); return (yi - fy) / (xi - fx). def fix_point_on_result(i, upper):; nonlocal fx, fy, new_y, keep; xi, yi = point_on_bound(i, upper); fx, fy = xi, yi; new_y[i] = fy; keep[i] = True. min_slope = slope_from_fixed(li, upper=False); max_slope = slope_from_fixed(ui, upper=True). # Consider a line l from (fx, fy) to (x[j], y?). As we increase y?, l first; # bumps into the upper staircase at (x[ui], y[ui] + e), and as we decrease; # y?, l first bumps into the lower staircase at (x[li], y[li+1] - e).; # We track the min and max slopes l can have while staying between the; # staircases, as well as the points li and ui where the line must bend if; # forced too high or too low. while True:; lower_slope = slope_from_fixed(j, upper=False); upper_slope = slope_from_fixed(j, upper=True); if upper_slope < min_slope:; # Line must bend down at x[li]. We know the max-entropy cdf passes; # through this point, so record it in new_y, keep.; # This becomes the new fixed point, and we must restart the scan; # from there.; fix_point_on_result(li, upper=False); j = li + 1; if j >= len(x):; break; li, ui = j, j; min_slope = slope_from_fixed(li, upper=False); max_slope = slope_from_fixed(ui, upper=True); j += 1; continue; elif lower_slope > max_slope:; # Line must bend up at x[ui]. We know the max-entropy cdf passes; # through this point, so record it in new_y, keep.; # This becomes the new fixed point, and we must restart the scan; # from there.; fix_point_on_result(ui, upper=True); j = ui + 1; if j >= len(x):; break; li, ui = j, j; min_slope = slope_from_fixed(li, upper=False); max_slope = slope_from_fixed(ui, upper=True); j += 1; continue; if j >= len(x):; break; if upper_slope < max_slope:; ui = j; max_slope = upper_slope; if lower_slope > min_slope:; li",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:16513,down,down,16513,docs/0.2/_modules/hail/ggplot/geoms.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html,2,['down'],['down']
Availability,"package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluste",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:2331,down,download,2331,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['down'],['download']
Availability,"park 3.0.; (#9549) Add; ignore_in_sample_frequency flag to hl.de_novo.; (#9501) Configurable; cache size for BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9474) Add; ArrayExpression.first and ArrayExpression.last.; (#9459) Add; StringExpression.join, an analogue to Python’s str.join.; (#9398) Hail will now; throw HailUserErrors if the or_error branch of a; CaseBuilder is hit. Bug fixes. (#9503) NDArrays can; now hold arbitrary data types, though only ndarrays of primitives can; be collected to Python.; (#9501) Remove memory; leak in BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9424); hl.experimental.writeBlockMatrices didn’t correctly support; overwrite flag. Performance improvements. (#9506); hl.agg.ndarray_sum will now do a tree aggregation. hailctl dataproc. (#9502) Fix hailctl; dataproc modify to install dependencies of the wheel file.; (#9420) Add; --debug-mode flag to hailctl dataproc start. This will enable; heap dumps on OOM errors.; (#9520) Add support; for requester pays buckets to hailctl dataproc describe. Deprecations. (#9482); ArrayExpression.head has been deprecated in favor of; ArrayExpression.first. Version 0.2.57; Released 2020-09-03. New features. (#9343) Implement the; KING method for relationship inference as hl.methods.king. Version 0.2.56; Released 2020-08-31. New features. (#9308) Add; hl.enumerate in favor of hl.zip_with_index, which is now deprecated.; (#9278) Add; ArrayExpression.grouped, a function that groups hail arrays into; fixed size subarrays. Performance. (#9373)(#9374); Decrease amount of memory used when slicing or filtering along a; single BlockMatrix dimension. Bug fixes. (#9304) Fix crash in; run_combiner caused by inputs where VCF lines and BGZ blocks; align. hailctl dataproc. (#9263) Add support; for --expiration-time argument to hailctl dataproc start.; (#9263) Add support; for --no-max-idle, no-max-age, --max-age, and; --expiration-time to hailctl ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:64752,error,errors,64752,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"passed as named expressions. Parameters:. exprs (args of str or Expression) – Row fields to group by.; named_exprs (keyword args of Expression) – Row-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix. Can be used to call GroupedMatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a column field and computing the number of non-reference calls; as an entry field:; >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.stats(dataset.pheno.height).mean)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html:5573,down,downstream,5573,docs/0.2/hail.GroupedMatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html,1,['down'],['downstream']
Availability,"past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem implementations. File Format. The native file format version is now 1.7.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.118; Released 2023-06-13. New Features. (#13140) Enable; hail-az and Azure Blob Storage https URLs to contain SAS; tokens to enable bearer-auth style file access to Azure storage.; (#13129) Allow; subnet to be passed through to gcloud in hailctl. Bug Fixes. (#13126); Query-on-Batch pipelines with one partition are now retried when they; encounter transient errors.; (#13113); hail.ggplot.geom_point now displays a legend group for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:31841,error,error,31841,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"path_uri(tmp_vcf), extension='vcf.bgz'); info('importing VCF and writing to matrix table...'); hl.import_vcf(cluster_readable_vcf, min_partitions=16).write(matrix_table_path, overwrite=True). tmp_sample_annot = os.path.join(tmp_dir, '1kg_annotations.txt'); source = resources['1kg_annotations']; info(f'downloading 1KG annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['1kg_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene_annot), gene_annotations_path); hl.hadoop_copy(local_path_uri(tmp_vcf), vcf_path); info('Done!'); else:; info('1KG files found'). [docs]def get_hgdp(output_dir, overwrite: bool = False):; """"""Download subset of the `Human Genome Diversity Panel; <https://www.internationalgenome.org/data-portal/data-collection/hgdp/>`__; dataset and sample annotations. Notes; -----; The download is about 30MB. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite any existing files/directories at `output_dir`.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). matrix_table_path = os.path.join(output_dir, 'HGDP.mt'); vcf_path = os.path.join(output_dir, 'HGDP.vcf.bgz'); sample_annotations_path = os.path.join(output_dir, 'HGDP_annotations.txt'); gene_annotations_path = os.path.join(output_dir, 'ensembl_gene_annotations.txt'). if (; overwrite; or not _dir_exists(fs, matrix_table_path); or not _file_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, 'HGDP.vcf.bgz'); source = resources['HGDP_matrix_tab",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:4042,Down,Download,4042,docs/0.2/_modules/hail/utils/tutorial.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html,1,['Down'],['Download']
Availability,"peared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.; (#7608) hl.grep; now has a show argument that allows users to either print the; results (default) or return a dictionary of the results. hailctl dataproc. (#7717) Throw error; when mispelling arguments instead of silently quitting. Version 0.2.28; Released 2019-11-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to mis",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:80197,error,errors,80197,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:156109,error,errors,156109,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,"perator.""). def __hash__(self):; return super(Expression, self).__hash__(). def __repr__(self):; return f'<{self.__class__.__name__} of type {self.dtype}>'. [docs] def __eq__(self, other):; """"""Returns ``True`` if the two expressions are equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for equality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are equal.; """"""; return self._compare_op(""=="", other). [docs] def __ne__(self, other):; """"""Returns ``True`` if the two expressions are not equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for inequality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are not equal.; """"""; return self._compare_op(""!="", other). def _to_table(self, name):; name, ds = self._to_relational(name); if isinstance(ds, hail.MatrixTable):; entries = ds.key_cols_by().entries(); entries = entries.order_by(*ds.row_key); return name, entries.select(name); else:; if len(ds.key) != 0:; ds = ds.order_by(*ds.key); return name, ds.select(name). def _to_relational(self, fallback_name):; source = self._indices.source; axes = self._indices.axes; if not self._aggregations.empty():; raise NotImplementedError('cannot convert aggregated expression to table'). if source is None:; return fallback_name, hl.Table.parallelize([hl.struct(**{fallback_name: self})], n_partitions=1). name = source._fields_inverse.get(self); top_level = name is not None; if not top_le",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:23992,error,error,23992,docs/0.2/_modules/hail/expr/expressions/base_expression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html,2,['error'],['error']
Availability,"phenotype is significantly associated with the genotype:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.76e+02 | 1.23e-05 | 0 |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:76964,fault,fault,76964,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability,"ples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Index into the tuple.; Examples; >>> hl.eval(tup[1]); 1. Parameters:; item (int) – Element index. Returns:; Expression. __gt__(other); Return self>value. __le__(other); Return self<=value. __len__()[source]; Returns the length of the tuple.; Examples; >>> len(tup); 3. Returns:; int. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. count(value)[source]; Do not use this method.; This only exists for compatibility with the Python Sequence abstract; base class. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus allele",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.TupleExpression.html:2126,error,error,2126,docs/0.2/hail.expr.TupleExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.TupleExpression.html,1,['error'],['error']
Availability,"ples; Create a simple batch with one job and execute it:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command('echo ""hello world""'); >>> b.run(). Parameters:. dry_run (bool) – If True, don’t execute code.; verbose (bool) – If True, print debugging output.; delete_scratch_on_exit (bool) – If True, delete temporary directories with intermediate files.; backend_kwargs (Any) – See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str) – Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. Warning; To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; All JobResourceFile are temporary files and must be written; to a permanent location using write_output() if the output needs; to be saved. Parameters:. resource (Resource) – Resource to be written to a file.; dest (str) – Destination file path. For a single ResourceFile, this will; simply be des",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:11389,echo,echo,11389,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['echo'],['echo']
Availability,"plit those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; split_multi_hts(), which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See also; split_multi_hts(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table. hail.methods.split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False)[source]; Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput seq",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:85808,down,downcode,85808,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['down'],['downcode']
Availability,"porary file t.ofile:; >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'). Write the temporary file t.ofile to a permanent location; >>> b.write_output(j.ofile, 'hello.txt'). Execute the DAG:; >>> b.run(). Notes; This class should never be created directly by the user. Use Batch.new_job(); or Batch.new_bash_job() instead.; Methods. command; Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the job’s command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; This method can be called more than once. It’s behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command.; To declare a resource file of type JobResourceFile, use either; the get attribute syntax of job.{identifier} or the get item syntax of; job[‘identifier’]. If an object for that identifier doesn’t exist,; then one will be created automatically (only allowed in the; command() method). The identifier name can be any",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:1644,echo,echo,1644,docs/batch/api/batch/hailtop.batch.job.BashJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html,2,['echo'],['echo']
Availability,"port_vcf to actually create tabix files when requested.; (#12020) Fix bug in; hl.experimental.densify which manifested as an AssertionError; about dtypes. Version 0.2.97; Released 2022-06-30. New Features. (#11756); hb.BatchPoolExecutor and Python jobs both now also support async; functions. Bug fixes. (#11962) Fix error; (logged as (#11891)); in VCF combiner when exactly 10 or 100 files are combined.; (#11969) Fix; import_table and import_lines to use multiple partitions when; force_bgz is used.; (#11964) Fix; erroneous “Bucket is a requester pays bucket but no user project; provided.” errors in Google Dataproc by updating to the latest; Dataproc image version. Version 0.2.96; Released 2022-06-21. New Features. (#11833); hl.rand_unif now has default arguments of 0.0 and 1.0. Bug fixes. (#11905) Fix; erroneous FileNotFoundError in glob patterns; (#11921) and; (#11910) Fix file; clobbering during text export with speculative execution.; (#11920) Fix array; out of bounds error when tree aggregating a multiple of 50; partitions.; (#11937) Fixed; correctness bug in scan order for Table.annotate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:48337,error,error,48337,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"pot (bool) – If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the job’s storage size.; Examples; Set the job’s disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, jobs requesting one or more cores receive; 5 GiB of storage for the root file system /. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at /io. Batch automatically writes all ResourceFile to; /io.; The default storage size is 0 Gi. The minimum storage size is 0 Gi and the; maximum storage size is 64 Ti. If storage is set to a value between 0 Gi; and 10 Gi, the storage request is rounded up to 10 Gi. All values are; rounded up to the nearest Gi. Parameters:; storage (Union[str, int, None]) – Units are in bytes if storage is an int. If None, use the; default storage size for the ServiceBackend (0 Gi). Return type:; Self. Returns:; Same job object with storage set. timeout(timeout); Set the maximum amount of time this job can run for in seconds.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.timeout(10); ... .command(f'echo ""hello""')). Parameters:; timeout (Union[int, float, None]) – Maximum amount of time in seconds for a job to run before being killed.; If None, there is no timeout. Return type:; Self. Returns:; Same job object set with a timeout in seconds. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:9563,echo,echo,9563,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['echo'],['echo']
Availability,"pression`; """""". return _func(""format"", hl.tstr, f, hl.tuple(args)). [docs]@typecheck(x=expr_float64, y=expr_float64, tolerance=expr_float64, absolute=expr_bool, nan_same=expr_bool); def approx_equal(x, y, tolerance=1e-6, absolute=False, nan_same=False):; """"""Tests whether two numbers are approximately equal. Examples; --------; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters; ----------; x : :class:`.NumericExpression`; y : :class:`.NumericExpression`; tolerance : :class:`.NumericExpression`; absolute : :class:`.BooleanExpression`; If True, compute ``abs(x - y) <= tolerance``. Otherwise, compute; ``abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022)``.; nan_same : :class:`.BooleanExpression`; If True, then ``NaN == NaN`` will evaluate to True. Otherwise,; it will return False. Returns; -------; :class:`.BooleanExpression`; """""". return _func(""approxEqual"", hl.tbool, x, y, tolerance, absolute, nan_same). def _shift_op(x, y, op):; assert op in ('<<', '>>', '>>>'); t = x.dtype; if t == hl.tint64:; word_size = 64; zero = hl.int64(0); else:; word_size = 32; zero = hl.int32(0). indices, aggregations = unify_all(x, y); return hl.bind(; lambda x, y: (; hl.case(); .when(y >= word_size, hl.sign(x) if op == '>>' else zero); .when(y >= 0, construct_expr(ir.ApplyBinaryPrimOp(op, x._ir, y._ir), t, indices, aggregations)); .or_error('cannot shift by a negative value: ' + hl.str(x) + f"" {op} "" + hl.str(y)); ),; x,; y,; ). def _bit_op(x, y, op):; if x.dtype == hl.tint32 and y.dtype == hl.tint32:; t = hl.tint32; else:; t = hl.tint64; coercer = coercer_from_dtype(t); x = coercer.coerce(x); y = coercer.coerce(y). indices, aggregations = unify_all(x, y); return construct_expr(ir.ApplyBinaryPrimOp(op, x._ir, y._ir), t, indices, aggregations). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64), y=expr_oneof(expr_int32, expr_int64)); def bit_and(x, y):; """"""Bitwise and `x` and `y`. E",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:177613,toler,tolerance,177613,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['toler'],['tolerance']
Availability,"prox filters the eigenvalues,; # presumably because a good estimate of the Generalized Chi-Sqaured CDF is not significantly; # affected by chi-squared components with very tiny weights.; threshold = 1e-5 * eigenvalues.sum() / eigenvalues.shape[0]; w = hl.array(eigenvalues).filter(lambda y: y >= threshold); genchisq_data = hl.pgenchisq(; ht.Q,; w=w,; k=hl.nd.ones(hl.len(w), dtype=hl.tint32),; lam=hl.nd.zeros(hl.len(w)),; mu=0,; sigma=0,; min_accuracy=accuracy,; max_iterations=iterations,; ); ht = ht.select(; 'size',; # for reasons unknown, the R implementation calls this expression the Q statistic (which is; # *not* what they write in the paper); q_stat=ht.Q / 2 / ht.s2,; # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples'). [docs]@typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; null_max_iterations=int,; null_tolerance=float,; accuracy=numeric,; iterations=int,; ); def _logistic_skat(; group,; weight,; y,; x,; covariates,; max_size: int = 46340,; null_max_iterations: int = 25,; null_tolerance: float = 1e-6,; accuracy: float = 1e-6,; iterations: int = 10000,; ):; r""""""The logistic sequence kernel association test (SKAT). Logistic SKAT tests if the phenotype, `y`, is significantly associated with the genotype,; `x`. For :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the; model is given by:. .. math::. \begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}. The usual null hypothesis is :math:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:86545,fault,fault,86545,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability,"pt term in beta; S = hl.nd.hstack((sqrt_n_samples, S0))._persist(); # Recover V from pc_scores with inv(S0); V0 = (pc_scores * (1 / S0))._persist(); # Set all entries in first column of V to 1/sqrt(n), for intercept term in beta; ones_normalized = hl.nd.full((V0.shape[0], 1), (1 / S[0])); V = hl.nd.hstack((ones_normalized, V0)). # Compute matrix of regression coefficients for PCs (beta), shape (k, m); beta = BlockMatrix.from_ndarray(((1 / S) * V).T, block_size=block_size) @ g.T; beta = beta.checkpoint(new_temp_file('pc_relate_bm/beta', 'bm')). # Compute matrix of individual-specific AF estimates (mu), shape (m, n); mu = 0.5 * (BlockMatrix.from_ndarray(V * S, block_size=block_size) @ beta).T; # Replace entries in mu with NaN if invalid or if corresponding GT is missing (no contribution from that variant); mu = mu._apply_map2(; lambda _mu, _g: hl.if_else(_bad_mu(_mu, min_individual_maf) | hl.is_nan(_g), nan, _mu),; g,; sparsity_strategy='NeedsDense',; ); mu = mu.checkpoint(new_temp_file('pc_relate_bm/mu', 'bm')). # Compute kinship matrix (phi), shape (n, n); # Where mu is NaN (missing), set variance and centered AF to 0 (no contribution from that variant); variance = _replace_nan(mu * (1.0 - mu), 0.0).checkpoint(new_temp_file('pc_relate_bm/variance', 'bm')); centered_af = _replace_nan(g - (2.0 * mu), 0.0); phi = _gram(centered_af) / (4.0 * _gram(variance.sqrt())); phi = phi.checkpoint(new_temp_file('pc_relate_bm/phi', 'bm')); ht = phi.entries().rename({'entry': 'kin'}); ht = ht.annotate(k0=hl.missing(hl.tfloat64), k1=hl.missing(hl.tfloat64), k2=hl.missing(hl.tfloat64)). if statistics in ['kin2', 'kin20', 'all']:; # Compute inbreeding coefficient and dominance encoding of GT matrix; f_i = (2.0 * phi.diagonal()) - 1.0; gd = g._apply_map2(lambda _g, _mu: _dominance_encoding(_g, _mu), mu, sparsity_strategy='NeedsDense'); normalized_gd = gd - (variance * (1.0 + f_i)). # Compute IBD2 (k2) estimate; k2 = _gram(normalized_gd) / _gram(variance); ht = ht.annotate(k2=k2.entries()",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:20414,checkpoint,checkpoint,20414,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['checkpoint'],['checkpoint']
Availability,"ption strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.aiotools.copy will always show a progress bar when; --verbose is passed. hailctl dataproc. (#11710) support; pass-through arguments to connect. Bug fixes. (#11792) Resolved; issue where corrupted tables could be created with whole-stage code; generation enabled. Version 0.2.93; Release 2022-03-27. Beta features. Several issues with the beta version of Hail Query on Hail Batch are; addressed in this release. Version 0.2.92; Release 2022-03-25. New features. (#11613) Add; hl.ggplot support",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:49540,error,error,49540,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"py is; given by. .. math::. H = \sum_{i=1}^k p_i \log_2(p_i). Parameters; ----------; s : :class:`.StringExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""entropy"", tfloat64, s). @typecheck(x=expr_any, trunc=nullable(expr_int32)); def _showstr(x, trunc=None):; if trunc is None:; return _func(""showStr"", tstr, x); return _func(""showStr"", tstr, x, trunc). [docs]@typecheck(x=expr_any); def str(x) -> StringExpression:; """"""Returns the string representation of `x`. Examples; --------. >>> hl.eval(hl.str(hl.struct(a=5, b=7))); '{""a"":5,""b"":7}'. Parameters; ----------; x. Returns; -------; :class:`.StringExpression`; """"""; if x.dtype == tstr:; return x; else:; return _func(""str"", tstr, x). [docs]@typecheck(c=expr_call, i=expr_int32); def downcode(c, i) -> CallExpression:; """"""Create a new call by setting all alleles other than i to ref. Examples; --------; Preserve the third allele and downcode all other alleles to reference. >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters; ----------; c : :class:`.CallExpression`; A call.; i : :class:`.Expression` of type :py:data:`.tint32`; The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns; -------; :class:`.CallExpression`; """"""; return _func(""downcode"", tcall, c, i). @typecheck(pl=expr_array(expr_int32)); def gq_from_pl(pl) -> Int32Expression:; """"""Compute genotype quality from Phred-scaled probability likelihoods. Examples; --------. >>> hl.eval(hl.gq_from_pl([0, 69, 1035])); 69. Parameters; ----------; pl : :class:`.Expression` of type :class:`.tarray` of :obj:`.tint32`. Returns; -------; :class:`.Expression` of type :py:data:`.tint32`; """"""; return _func(""gqFromPL"", tint32, pl). [docs]@typecheck(n=expr_int32); def tr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:105229,down,downcode,105229,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['down'],['downcode']
Availability,"py(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene_annot), gene_annotations_path); hl.hadoop_copy(local_path_uri(tmp_vcf), vcf_path); info('Done!'); else:; info('HGDP files found'). [docs]def get_movie_lens(output_dir, overwrite: bool = False):; """"""Download public Movie Lens dataset. Notes; -----; The download is about 6M. See the; `MovieLens website <https://grouplens.org/datasets/movielens/100k/>`__; for more information about this dataset. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite existing files/directories at those locations.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). paths = [os.path.join(output_dir, x) for x in ['movies.ht', 'ratings.ht', 'users.ht']]; if overwrite or any(not _dir_exists(fs, f) for f in paths):; init_temp_dir(); source = resources['movie_lens_100k']; tmp_path = os.path.join(tmp_dir, 'ml-100k.zip'); info(f'downloading MovieLens-100k data ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_path); with zipfile.ZipFile(tmp_path, 'r') as z:; z.extractall(tmp_dir). user_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.user'); movie_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.item'); ratings_table_path = os.path.join(tmp_dir, 'ml-100k', 'u.data'); assert os.path.exists(user_table_path); assert os.path.exists(movie_table_path); assert os.path.exists(ratings_table_path). user_cluster_readable = _copy_to_tmp(fs, local_path_uri(user_table_path), extension='txt'); movie_cluster_readable = _copy_to_tmp(fs, local_path_uri(movie_table_path), 'txt'); ratings_cluster_readable = _copy_to_tmp(fs, local_path_uri(ratings_table_path), 'txt'). [movies_path, ratings_path, users_path] = paths. genres = [; 'Action',; 'Adventure',; 'Animation',; ""Children's"",; 'Comedy',; 'Crime',; 'Documentary',; 'Drama',; 'Fantasy',; 'Film-Noir',; 'Horror',; 'Musical',; 'Mystery',; 'Romance',; 'Sci-Fi',",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:6969,down,downloading,6969,docs/0.2/_modules/hail/utils/tutorial.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html,2,['down'],['downloading']
Availability,"py:meth:`.ld_prune` `persists <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ up to 3 copies of the data to both memory and disk.; The amount of disk space required will depend on the size and minor allele frequency of the input data and the prune parameters ``r2`` and ``window``. The number of bytes stored in memory per variant is about ``nSamples / 4 + 50``. .. warning::. The variants in the pruned set are not guaranteed to be identical each time :py:meth:`.ld_prune` is run. We recommend running :py:meth:`.ld_prune` once and exporting the list of LD pruned variants using; :py:meth:`.export_variants` for future use. :param float r2: Maximum :math:`R^2` threshold between two variants in the pruned set within a given window. :param int window: Width of window in base-pairs for computing pair-wise :math:`R^2` values. :param int memory_per_core: Total amount of memory available for each core in MB. If unsure, use the default value. :param int num_cores: The number of cores available. Equivalent to the total number of workers times the number of cores per worker. :return: Variant dataset filtered to those variants which remain after LD pruning.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.ldPrune(r2, window, num_cores, memory_per_core); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(force_local=bool); def ld_matrix(self, force_local=False):; """"""Computes the linkage disequilibrium (correlation) matrix for the variants in this VDS. **Examples**. >>> ld_mat = vds.ld_matrix(). **Notes**. Each entry (i, j) in the LD matrix gives the :math:`r` value between variants i and j, defined as; `Pearson's correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; :math:`\\rho_{x_i,x_j}` between the two genotype vectors :math:`x_i` and :math:`x_j`. .. math::. \\rho_{x_i,x_j} = \\frac{\\mathrm{Cov}(X_i,X_j)}{\\sigma_{X_i} \\sigma_{X_j}}. Also note that varian",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:95417,avail,available,95417,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['avail'],['available']
Availability,"quilibrium. :param bool force_block: Force using Spark's BlockMatrix to compute kinship (advanced). :param bool force_gramian: Force using Spark's RowMatrix.computeGramian to compute kinship (advanced). :return: Realized Relationship Matrix for all samples.; :rtype: :py:class:`KinshipMatrix`; """"""; return KinshipMatrix(self._jvdf.rrm(force_block, force_gramian)). [docs] @handle_py4j; @typecheck_method(other=vds_type,; tolerance=numeric); def same(self, other, tolerance=1e-6):; """"""True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values. **Examples**. This will return True:. >>> vds.same(vds). **Notes**. The ``tolerance`` parameter sets the tolerance for equality when comparing floating-point fields. More precisely, :math:`x` and :math:`y` are equal if. .. math::. \abs{x - y} \leq tolerance * \max{\abs{x}, \abs{y}}. :param other: variant dataset to compare against; :type other: :class:`.VariantDataset`. :param float tolerance: floating-point tolerance for equality. :rtype: bool; """""". return self._jvds.same(other._jvds, tolerance). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(root=strlike,; keep_star=bool); def sample_qc(self, root='sa.qc', keep_star=False):; """"""Compute per-sample QC metrics. .. include:: requireTGenotype.rst. **Annotations**. :py:meth:`~hail.VariantDataset.sample_qc` computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; ``sa.qc.<identifier>`` (or ``<root>.<identifier>`` if a non-default root was passed):. +---------------------------+--------+----------------------------------------------------------+; | Name | Type | Description |; +===========================+========+==========================================================+; | ``callRate`` | Double | Fraction of genotypes called |; +---------------------------+--------+----------------------------------------------------------+; | ``nHomRef`` | Int | Number o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:196732,toler,tolerance,196732,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['toler'],['tolerance']
Availability,"r identity-by-descent two is given by:. .. math::. \\widehat{k^{(2)}_{ij}} := \\frac{\sum_{s \in S_{ij}}X_{is} X_{js}}{\sum_{s \in S_{ij}}\\widehat{\\sigma^2_{is}} \\widehat{\\sigma^2_{js}}}. The estimator for identity-by-descent zero is given by:. .. math::. \\widehat{k^{(0)}_{ij}} :=; \\begin{cases}; \\frac{\\text{IBS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}} \\widehat{\\mu_{is}}^2(1 - \\widehat{\\mu_{js}})^2 + (1 - \\widehat{\\mu_{is}})^2\\widehat{\\mu_{js}}^2}; & \\widehat{\phi_{ij}} > 2^{-5/2} \\\\; 1 - 4 \\widehat{\phi_{ij}} + k^{(2)}_{ij}; & \\widehat{\phi_{ij}} \le 2^{-5/2}; \\end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \\widehat{k^{(1)}_{ij}} := 1 - \\widehat{k^{(2)}_{ij}} - \\widehat{k^{(0)}_{ij}}. **Details**. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :py:meth:`~hail.VariantDataset.pc_relate` differs from the reference; implementation in a couple key ways:. - the principal components analysis does not use an unrelated set of; individuals. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). **Notes**. The ``block_size`` controls memory usage and parallelism. If it is large; enough to hold an entire sample-by-sample matrix of 64-bit doubles in; memory, then only one Spark worker node can be used to compute matrix; operations. If it is too small, communication overhead will begin to; dominate the computation's time. The author has found that on Google; Dataproc (where each core has about 3.75GB of memory), setting; ``block_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:174055,avail,available,174055,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['avail'],['available']
Availability,r improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:73458,error,error,73458,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"r of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR). - HemiX -- in non-PAR of X, male child; - HemiY -- in non-PAR of Y, male child; - Auto -- otherwise (in autosome or PAR, or female child). Any refers to :math:`\{ HomRef, Het, HomVar, NoCall \}` and ! denotes complement in this set. +--------+------------+------------+----------+------------------+; |Code | Dad | Mom | Kid | Copy State |; +========+============+============+==========+==================+; | 1 | HomVar | HomVar | Het | Auto |; +--------+------------+------------+----------+------------------+; | 2 | HomRef | HomRef | Het | Auto |; +--------+------------+------------+----------+------------------+; | 3 | HomRef | ! HomRef | HomVar | Auto",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:158036,error,error,158036,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability,"r quality control. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Examples; Compute sample QC metrics and remove low-quality samples:; >>> dataset = hl.sample_qc(dataset, name='sample_qc'); >>> filtered_dataset = dataset.filter_cols((dataset.sample_qc.dp_stats.mean > 20) & (dataset.sample_qc.r_ti_tv > 1.5)). Notes; This method computes summary statistics per sample from a genetic matrix and stores; the results as a new column-indexed struct field in the matrix, named based on the; name parameter.; If mt contains an entry field DP of type tint32, then the; field dp_stats is computed. If mt contains an entry field GQ of type; tint32, then the field gq_stats is computed. Both dp_stats; and gq_stats are structs with with four fields:. mean (float64) – Mean value.; stdev (float64) – Standard deviation (zero degrees of freedom).; min (int32) – Minimum value.; max (int32) – Maximum value. If the dataset does not contain an entry field GT of type; tcall, then an error is raised. The following fields are always; computed from GT:. call_rate (float64) – Fraction of calls not missing or filtered.; Equivalent to n_called divided by count_rows().; n_called (int64) – Number of non-missing calls.; n_not_called (int64) – Number of missing calls.; n_filtered (int64) – Number of filtered entries.; n_hom_ref (int64) – Number of homozygous reference calls.; n_het (int64) – Number of heterozygous calls.; n_hom_var (int64) – Number of homozygous alternate calls.; n_non_ref (int64) – Sum of n_het and n_hom_var.; n_snp (int64) – Number of SNP alternate alleles.; n_insertion (int64) – Number of insertion alternate alleles.; n_deletion (int64) – Number of deletion alternate alleles.; n_singleton (int64) – Number of private alleles. Reference alleles are never counted as singletons, even if; every other allele at a site is non-reference.; n_transition (int64) – Number of transition (A-G, C-T) alternate alleles.; n_transversion (int64",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:66414,error,error,66414,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['error']
Availability,"r that identifier doesn’t exist,; then one will be created automatically (only allowed in the; command() method). The identifier name can be any valid Python; identifier such as ofile5000.; All JobResourceFile are temporary files and must be written to; a permanent location using Batch.write_output() if the output; needs to be saved.; Only resources can be referred to in commands. Referencing a; batch.Batch or Job will result in an error. Parameters:; command (str) – A bash command. Return type:; BashJob. Returns:; Same job object with command appended. declare_resource_group(**mappings); Declare a resource group for a job.; Examples; Declare a resource group:; >>> b = Batch(); >>> input = b.read_input_group(bed='data/example.bed',; ... bim='data/example.bim',; ... fam='data/example.fam'); >>> j = b.new_job(); >>> j.declare_resource_group(tmp1={'bed': '{root}.bed',; ... 'bim': '{root}.bim',; ... 'fam': '{root}.fam',; ... 'log': '{root}.log'}); >>> j.command(f'plink --bfile {input} --make-bed --out {j.tmp1}'); >>> b.run() . Warning; Be careful when specifying the expressions for each file as this is Python; code that is executed with eval!. Parameters:; mappings (Dict[str, Any]) – Keywords (in the above example tmp1) are the name(s) of the; resource group(s). File names may contain arbitrary Python; expressions, which will be evaluated by Python eval. To use the; keyword as the file name, use {root} (in the above example {root}; will be replaced with tmp1). Return type:; BashJob. Returns:; Same job object with resource groups set. image(image); Set the job’s docker image.; Examples; Set the job’s docker image to ubuntu:22.04:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.image('ubuntu:22.04'); ... .command(f'echo ""hello""')); >>> b.run() . Parameters:; image (str) – Docker image to use. Return type:; BashJob. Returns:; Same job object with docker image set. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:4244,echo,echo,4244,docs/batch/api/batch/hailtop.batch.job.BashJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html,2,['echo'],['echo']
Availability,"r, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmethod; def get_running():; """"""Return the running Hail context in this Python session. **Example**. .. doctest::; :options: +SKIP. >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. :return: Current Hail context.; :rtype: :class:`.HailContext`; """""". return Env.hc(). @property; def version(self):; """"""Return the version of Hail associated with this HailContext. :rtype: str; """"""; return self._jhc.version(). [docs] @handle_py4j; @typecheck_method(regex=strlike,; path=oneof(strlike, listof(strlike)),; max_count=integral); def grep(self, regex, path, max_count=100):; """"""Grep big files, like, really fast. **Examples**. Print all lines containing the string ``hello`` in *file.txt*:. >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in *file1.txt* and *file2.txt*:. >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). **Background**. :py:meth:`~hail.HailContext.grep` mimics the basic functionality of Unix ``grep`` in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:3991,recover,recovery,3991,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['recover'],['recovery']
Availability,"r>) – The old alleles, before filtering and; computing the minimal representation.; old_to_new (array<int32>) – An array that maps old allele index to; new allele index. Its length is the same as old_alleles. Alleles that; are filtered are missing.; new_to_old (array<int32>) – An array that maps new allele index to; the old allele index. Its length is the same as the modified alleles; field. Downcode algorithm; We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles.; GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; split_multi_hts().; The downcode algorithm would produce the following:; GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. GT: Downcode filtered alleles to reference.; AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms 25,5,10,20 to 40,20.; DP: No change.; PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded genotype, and shift so; the overall minimum PL is 0.; GQ: The second-lowest PL (after shifting). Subset algorithm; We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference alle",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:25780,down,downcoding,25780,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['down'],['downcoding']
Availability,"r_entries(filter_condition_ab); mt = hl.variant_qc(mt).cache(); common_mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01); gwas = hl.linear_regression_rows(y=common_mt.CaffeineConsumption, x=common_mt.GT.n_alt_alleles(), covariates=[1.0]); pca_eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(common_mt.GT). [Stage 16:> (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height=400)). 2-D histogram; For visualizing relationships between variables in large datasets (where scatter plots may be less informative since they highlight outliers), the histogram_2d() function will create a heatmap with the number of observations in each section of a 2-d grid based on two variables. [10]:. p = hl.plot.histogram2d(pca_scores.scores[0], pca_scores.scores[1]); show(p). Q-Q (Quantile-Quantile); The qq() function requires either a Python type or a Hail field containing p-values to be plotted. This function also allows for downsampling. [11]:. p = hl.plot.qq(gwas.p_value, n_divisions=None); p2 = hl.plot.qq(gwas.p_value, n_divisions=75). show(gridplot([p, p2], ncols=2, width=400, height=400)). Manhattan; The manhattan() function requires a Hail field containing p-values. [12]:. p = hl.plot.manhattan(gwas.p_value); show(p). We can also pass in a dictionary of fields that we would like to show up as we hover ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/08-plotting.html:6139,down,downsampled,6139,docs/0.2/tutorials/08-plotting.html,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html,1,['down'],['downsampled']
Availability,"rabilities. Version 0.2.80; Release 2021-12-15. New features. (#11077); hl.experimental.write_matrix_tables now returns the paths of the; written matrix tables. hailctl dataproc. (#11157) Updated; Dataproc image version to mitigate the Log4j vulnerability.; (#10900) Added; --region parameter to hailctl dataproc submit.; (#11090) Teach; hailctl dataproc describe how to read URLs with the protocols; s3 (Amazon S3), hail-az (Azure Blob Storage), and file; (local file system) in addition to gs (Google Cloud Storage). Version 0.2.79; Release 2021-11-17. Bug fixes. (#11023) Fixed bug; in call decoding that was introduced in version 0.2.78. New features. (#10993) New; function p_value_excess_het. Version 0.2.78; Release 2021-10-19. Bug fixes. (#10766) Don’t throw; out of memory error when broadcasting more than 2^(31) - 1 bytes.; (#10910) Filters on; key field won’t be slowed down by uses of; MatrixTable.localize_entries or Table.rename.; (#10959) Don’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile if intervals were; created conditionally.; (#10746) Various; compiler improvements, decrease likelihood of ClassTooLarge; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:56287,error,error,56287,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"rage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of col",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:22455,down,downsamples,22455,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['down'],['downsamples']
Availability,"random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). indices = local_df_y.index.to_list(); results = [None] * len(indices). inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. output = hb.concatenate(b, results); b.write_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:16672,checkpoint,checkpoints,16672,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoints']
Availability,"random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). results = []. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint_batching.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:14555,checkpoint,checkpoints,14555,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoints']
Availability,"rce for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before writing.; If False, write blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static write_from_entry_expr(entry_expr, path, overwrite=False, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Writes a block matrix from a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:45855,down,downsamples,45855,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['down'],['downsamples']
Availability,"rce; if not isinstance(source, Table):; raise ValueError(; ""'maximal_independent_set' expects an expression of 'Table'. Found {}"".format(; ""expression of '{}'"".format(source.__class__) if source is not None else 'scalar expression'; ); ). if i._indices.source != j._indices.source:; raise ValueError(; ""'maximal_independent_set' expects arguments `i` and `j` to be expressions of the same Table. ""; ""Found\n{}\n{}"".format(i, j); ). node_t = i.dtype. if tie_breaker:; wrapped_node_t = ttuple(node_t); left_id = Env.get_uid(); right_id = Env.get_uid(); left = construct_variable(left_id, wrapped_node_t); right = construct_variable(right_id, wrapped_node_t); tie_breaker_expr = hl.float64(tie_breaker(left[0], right[0])); tie_breaker_ir = tie_breaker_expr._ir; t, _ = source._process_joins(i, j, tie_breaker_expr); else:; left_id, right_id, tie_breaker_ir = None, None, None; t, _ = source._process_joins(i, j). edges = t.select(__i=i, __j=j).key_by().select('__i', '__j'); edges = edges.checkpoint(new_temp_file()). mis_nodes = hl.set(; construct_expr(; ir.ArrayMaximalIndependentSet(edges.collect(_localize=False)._ir, left_id, right_id, tie_breaker_ir),; hl.tarray(node_t),; ); ). nodes = edges.select(node=[edges.__i, edges.__j]); nodes = nodes.explode(nodes.node); nodes = nodes.annotate_globals(mis_nodes=mis_nodes); nodes = nodes.filter(nodes.mis_nodes.contains(nodes.node), keep); nodes = nodes.select_globals(); if keyed:; return nodes.key_by('node').distinct(); return nodes. def require_col_key_str(dataset: MatrixTable, method: str):; if not len(dataset.col_key) == 1 or dataset[next(iter(dataset.col_key))].dtype != hl.tstr:; raise ValueError(; f""Method '{method}' requires column key to be one field of type 'str', found ""; f""{list(str(x.dtype) for x in dataset.col_key.values())}""; ). def require_table_key_variant(ht, method):; if (; list(ht.key) != ['locus', 'alleles']; or not isinstance(ht['locus'].dtype, tlocus); or not ht['alleles'].dtype == tarray(tstr); ):; raise ValueError(; """,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/misc.html:6424,checkpoint,checkpoint,6424,docs/0.2/_modules/hail/methods/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html,2,['checkpoint'],['checkpoint']
Availability,"rder_fields:; globals_order = list(self.globals); if list(other.globals) != globals_order:; other = other.select_globals(*globals_order). row_order = list(self.row); if list(other.row) != row_order:; other = other.select(*row_order). if self._type != other._type:; print(f'Table._same: types differ:\n {self._type}\n {other._type}'); return False. left = self; left = left.select_globals(left_globals=left.globals); left = left.group_by(key=left.key).aggregate(left_row=hl.agg.collect(left.row_value)). right = other; right = right.select_globals(right_globals=right.globals); right = right.group_by(key=right.key).aggregate(right_row=hl.agg.collect(right.row_value)). t = left.join(right, how='outer'). mismatched_globals, mismatched_rows = t.aggregate(; hl.tuple((; hl.or_missing(~_values_similar(t.left_globals, t.right_globals, tolerance, absolute), t.globals),; hl.agg.filter(; ~hl.all(; hl.is_defined(t.left_row),; hl.is_defined(t.right_row),; _values_similar(t.left_row, t.right_row, tolerance, absolute),; ),; hl.agg.take(t.row, 10),; ),; )); ). columns, _ = shutil.get_terminal_size((80, 10)). def pretty(obj):; pretty_str = pprint.pformat(obj, width=columns); return ''.join(' ' + line for line in pretty_str.splitlines(keepends=True)). is_same = True; if mismatched_globals is not None:; print(f""""""Table._same: globals differ:; Left:; {pretty(mismatched_globals.left_globals)}; Right:; {pretty(mismatched_globals.right_globals)}""""""); is_same = False. if len(mismatched_rows) > 0:; print('Table._same: rows differ:'); for r in mismatched_rows:; print(f"""""" Row mismatch at key={r.key}:; Left:; {pretty(r.left_row)}; Right:; {pretty(r.right_row)}""""""); is_same = False. return is_same. [docs] def collect_by_key(self, name: str = 'values') -> 'Table':; """"""Collect values for each unique key into an array. .. include:: _templates/req_keyed_table.rst. Examples; --------; >>> t1 = hl.Table.parallelize([; ... {'t': 'foo', 'x': 4, 'y': 'A'},; ... {'t': 'bar', 'x': 2, 'y': 'B'},; ... {'t': 'bar',",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:129312,toler,tolerance,129312,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['toler'],['tolerance']
Availability,"rectly list all files in a directory, not; just the first 1000. This could manifest in an import_table or; import_vcf which used a glob expression. In such a case, only the; first 1000 files would have been included in the resulting Table or; MatrixTable.; (#13550); hl.utils.range_table(n) now supports all valid 32-bit signed; integer values of n.; (#13500) In; Query-on-Batch, the client-side Python code will not try to list; every job when a QoB batch fails. This could take hours for; long-running pipelines or pipelines with many partitions. Deprecations. (#13275) Hail no; longer officially supports Python 3.8.; (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new; n_rows parameter. Version 0.2.120; Released 2023-07-27. New Features. (#13206) The VDS; Combiner now works in Query-on-Batch. Bug Fixes. (#13313) Fix bug; introduced in 0.2.119 which causes a serialization error when using; Query-on-Spark to read a VCF which is sorted by locus, with split; multi-allelics, in which the records sharing a single locus do not; appear in the dictionary ordering of their alternate alleles.; (#13264) Fix bug; which ignored the partition_hint of a Table; group-by-and-aggregate.; (#13239) Fix bug; which ignored the HAIL_BATCH_REGIONS argument when determining in; which regions to schedule jobs when using Query-on-Batch.; (#13253) Improve; hadoop_ls and hfs.ls to quickly list globbed files in a; directory. The speed improvement is proportional to the number of; files in the directory.; (#13226) Fix the; comparison of an hl.Struct to an hl.struct or field of type; tstruct. Resolves; (#13045) and; (Hail#13046).; (#12995) Fixed bug; causing poor performance and memory leaks for; MatrixTable.annotate_rows aggregations. Version 0.2.119; Released 2023-06-28. New Features. (#12081) Hail now; uses Zstandard as the default; compression algorithm for table and matrix table storage. Reducing; file size around 20% in most cases.; (#12988) Arbitrary; aggregations ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:29340,error,error,29340,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"rectly prepared for import:. - If there are only 5 columns before the start of the genotype probability data (chromosome field is missing), you must specify the chromosome using the ``chromosome`` parameter. - No duplicate sample IDs are allowed. The first column in the .sample file is used as the sample ID ``s``. Also, see section in :py:meth:`~hail.HailContext.import_bgen` linked :ref:`here <gpfilters>` for information about Hail's genotype probability representation. **Annotations**. :py:meth:`~hail.HailContext.import_gen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*String*) -- 3rd column of .gen file if chromosome present, otherwise 2nd column. :param path: .gen files to import.; :type path: str or list of str. :param str sample_file: The sample file. :param float tolerance: If the sum of the genotype probabilities for a genotype differ from 1.0 by more than the tolerance, set the genotype to missing. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param chromosome: Chromosome if not listed in the .gen file.; :type chromosome: str or None. :return: Variant dataset imported from .gen and .sample files.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importGens(jindexed_seq_args(path), sample_file, joption(chromosome), joption(min_partitions),; tolerance); return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(paths=oneof(strlike, listof(strlike)),; key=oneof(strlike, listof(strlike)),; min_partitions=nullable(int),; impute=bool,; no_header=bool,; comment=nullable(strlike),; delimiter=strlike,; missing=strlike,; types=dictof(strlike, Type),; quote=nullable(char)); def import_table(self, paths, key=[], min_partitions=None, impute=False, no_header=False,; comment=None, delimiter=""\t"", missing=""NA"", types={}, quote=None):; """"""Import delimited text file (text table) as key table. The resulting key ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:10084,toler,tolerance,10084,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['toler'],['tolerance']
Availability,"reference genome. Returns; -------; :class:`.ReferenceGenome`; """"""; return self.reference_data.locus.dtype.reference_genome. [docs] @typecheck_method(check_data=bool); def validate(self, *, check_data: bool = True):; """"""Eagerly checks necessary representational properties of the VDS."""""". rd = self.reference_data; vd = self.variant_data. def error(msg):; raise ValueError(f'VDS.validate: {msg}'). rd_row_key = rd.row_key.dtype; if (; not isinstance(rd_row_key, hl.tstruct); or len(rd_row_key) != 1; or not rd_row_key.fields[0] == 'locus'; or not isinstance(rd_row_key.types[0], hl.tlocus); ):; error(f""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_key = vd.row_key.dtype; if (; not isinstance(vd_row_key, hl.tstruct); or len(vd_row_key) != 2; or not vd_row_key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 or vd_col_key.types[0] != hl.tstr:; error(f""expect variant data to have a single col key of type string, found {vd_col_key}""). if 'END' not in rd.entry or rd.END.dtype != hl.tint32:; error(""expect field 'END' in entry of reference data with type int32""). if check_data:; # check cols; ref_cols = rd.col_key.collect(); var_cols = vd.col_key.collect(); if len(ref_cols) != len(var_cols):; error(; f""mismatch in number of columns: reference data has {ref_cols} columns, variant data has {var_cols} columns""; ). if ref_cols != var_cols:; first_mismatch = 0; while ref_cols[first_mismatch] == var_cols[first_mismatch]:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:9015,error,error,9015,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['error'],['error']
Availability,"refs = get_refs(*agg.exprs); agg_axes = agg.agg_axes(). # check for stray indices; unexpected_agg_axes = agg_axes - expected_agg_axes; if unexpected_agg_axes:; # one or more out-of-scope fields; bad_refs = []; for name, inds in refs.items():; bad_axes = inds.axes.intersection(unexpected_agg_axes); if bad_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0. errors.append(; ExpressionException(; ""scope violation: '{caller}' supports aggregation over indices {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}"".format(; caller=caller,; expected=list(aggregation_axes),; axes=list(agg_axes),; stray=list(unexpected_agg_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)); for name, inds in bad_refs; ),; ); ); ); else:; errors.append(ExpressionException(""'{}' does not support aggregation"".format(caller))). for w in warnings:; warning('{}'.format(w.msg)); if errors:; for e in errors:; error('{}'.format(e.msg)); raise errors[0]. @typecheck(expression=expr_any); def eval_timed(expression):; """"""Evaluate a Hail expression, returning the result and the times taken for; each stage in the evaluation process. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; (Any, dict); Result of evaluating `expression` and a dictionary of the timings; """""". from hail.utils.java import Env. analyze('eval', expression, Indices(expression._indices.source)); if expression._indices.source is None:; ir_type = expression._ir.typ; expression_type = expression.dtype; if ir_type != expression.dtype:; raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); ir = expression._ir; else:; uid = Env.get_uid(); ir = expression._indices.source.select_globals(**{uid: expression}).index_globals()[uid]._ir. return Env.backend().execute(MakeTuple([ir]), timed=True)[0]. [docs]@typech",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:4406,error,errors,4406,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html,2,['error'],['errors']
Availability,"region parameter,'; f' received: region={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {valid_clouds}.'; ). datasets = get_datasets_metadata(); names = set([dataset for dataset in datasets]); if name not in names:; raise ValueError(f'{name} is not a dataset available in the' f' repository.'). versions = set(dataset['version'] for dataset in datasets[name]['versions']); if version not in versions:; raise ValueError(; f'Version {version!r} not available for dataset' f' {name!r}.\n' f'Available versions: {versions}.'; ). reference_genomes = set(dataset['reference_genome'] for dataset in datasets[name]['versions']); if reference_genome not in reference_genomes:; raise ValueError(; f'Reference genome build {reference_genome!r} not'; f' available for dataset {name!r}.\n'; f'Available reference genome builds:'; f' {reference_genomes}.'; ). clouds = set(k for dataset in datasets[name]['versions'] for k in dataset['url'].keys()); if cloud not in clouds:; raise ValueError(f'Cloud platform {cloud!r} not available for dataset {name}.\nAvailable platforms: {clouds}.'). regions = set(k for dataset in datasets[name]['versions'] for k in dataset['url'][cloud].keys()); if region not in regions:; raise ValueError(; f'Region {region!r} not available for dataset'; f' {name!r} on cloud platform {cloud!r}.\n'; f'Available regions: {regions}.'; ). path = [; dataset['url'][cloud][region]; for dataset in datasets[name]['versions']; if all([dataset['version'] == version, dataset['reference_genome'] == reference_genome]); ]; assert len(path) == 1; path = path[0]; if path.startswith('s3://'):; try:; dataset = _read_dataset(path); except hl.utils.java.FatalError:; dataset = _read_dataset(path.replace('s3://', 's3a://')); else:; dataset = _read_dataset(path); return dataset. © Copyright 2015-2024, Hail Te",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:3505,Avail,Available,3505,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,1,['Avail'],['Available']
Availability,"regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selectin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:62374,toler,tolerance,62374,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,"relation values. To avoid this, first check that all variants vary or; filter out constant variants (for example, with the help of; aggregators.stats()).; If the global_position() on locus_expr is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that’s; been ordered by locus_expr.; Set coord_expr to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-nan, on the; same source as locus_expr, and ascending with respect to locus; position for each contig; otherwise the method will raise an error. Warning; See the warnings in row_correlation(). In particular, for large; matrices it may be preferable to run its stages separately.; entry_expr and locus_expr are implicitly aligned by row-index, though; they need not be on the same source. If their sources differ in the number; of rows, an error will be raised; otherwise, unintended misalignment may; silently produce unexpected results. Parameters:. entry_expr (Float64Expression) – Entry-indexed numeric expression on matrix table.; locus_expr (LocusExpression) – Row-indexed locus expression on a table or matrix table that is; row-aligned with the matrix table of entry_expr.; radius (int or float) – Radius of window for row values.; coord_expr (Float64Expression, optional) – Row-indexed numeric expression for the row value on the same table or; matrix table as locus_expr.; By default, the row value is given by the locus position.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Returns:; BlockMatrix – Windowed correlation matrix between variants.; Row and column indices correspond to matrix table variant index. hail.methods.ld_prune(call_expr, r2=0.2, bp_window_size=1000000, memory_per_core=256, keep_higher_maf=True, block_size=None)[source]; Returns a maximal subset of variants that are nearly uncor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:41921,error,error,41921,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['error']
Availability,"rencing ABS blobs is now deprecated and will be removed in a future release. Version 0.2.114. (#12780) PythonJobs now handle arguments with resources nested inside dicts and lists.; (#12900) Reading data from public blobs is now supported in Azure. Version 0.2.113. (#12780) The LocalBackend now supports always_run jobs. The LocalBackend will no longer immediately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; (#12845) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Larg",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:3711,error,errors,3711,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,2,['error'],"['erroring', 'errors']"
Availability,"ression) -> StructExpression:; """"""Get index from compatible version of annotation dataset. Checks for compatible indexed values from each :class:`.DatasetVersion`; in :attr:`.Dataset.versions`, where `key_expr` is the row key struct; from the dataset to be annotated. Parameters; ----------; key_expr : :class:`.StructExpression`; Row key struct from relational object to be annotated. Returns; -------; :class:`.StructExpression`; Struct of compatible indexed values.; """"""; all_matches = 'unique' not in self.key_properties; compatible_indexed_values = [; (version.maybe_index(key_expr, all_matches), version.version); for version in self.versions; if version.maybe_index(key_expr, all_matches) is not None; ]; if len(compatible_indexed_values) == 0:; versions = [f'{(v.version, v.reference_genome)}' for v in self.versions]; raise ValueError(; f'Could not find compatible version of {self.name} for user'; f' dataset with key {key_expr.dtype}.\n'; f'This annotation dataset is available for the following'; f' versions and reference genome builds: {"", "".join(versions)}.'; ); else:; indexed_values = sorted(compatible_indexed_values, key=lambda x: x[1])[-1]. if len(compatible_indexed_values) > 1:; info(; f'index_compatible_version: More than one compatible version'; f' exists for annotation dataset: {self.name}. Rows have been'; f' annotated with version {indexed_values[1]}.'; ); return indexed_values[0]. [docs]class DB:; """"""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Reg",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:8699,avail,available,8699,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['avail'],['available']
Availability,"result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input va",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:27561,toler,tolerance,27561,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,"result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:7530,toler,tolerance,7530,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['toler'],['tolerance']
Availability,"rg/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_table(tmp2).add_index(uid).key_by(uid); ht.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n).key_by().drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n). return Table(; ir.TableRepartition(; self._tir, n, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'Table':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> table_result = table1.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than or equal to `max_partitions`, do nothing. Ret",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:93990,checkpoint,checkpoint,93990,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['checkpoint'],['checkpoint']
Availability,"riant. The reference allele (4th column if; chromosome is not defined) is the first element of the array and the; alternate allele (5th column if chromosome is not defined) is the second; element.; varid (tstr) – The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; rsid (tstr) – The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. Entry Fields. GT (tcall) – The hard call corresponding to the genotype with; the highest probability.; GP (tarray of tfloat64) – Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the probabilities is a distance greater than the tolerance; parameter from 1.0. Otherwise, the probabilities are normalized to sum to; 1.0. For example, the input [0.98, 0.0, 0.0] will be normalized to; [1.0, 0.0, 0.0]. Parameters:. path (str or list of str) – GEN files to import.; sample_file (str) – Sample file to import.; tolerance (float) – If the sum of the genotype probabilities for a genotype differ from 1.0; by more than the tolerance, set the genotype to missing.; min_partitions (int, optional) – Number of partitions.; chromosome (str, optional) – Chromosome if not included in the GEN file; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of str to str, optional) – Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by reference_genome.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome. Returns:; MatrixTable. hail.methods.import_locus_intervals(path, reference_genome='default', skip_invalid_intervals=False, contig_recoding=None, **kwargs)[source]; Import a locus interval list as a Table.; Examples; Add the row field capture_region indicating inclusion in; at least one locus interval from capture_intervals.txt:; >>> intervals = hl.import_locus_intervals('data/capture_intervals.txt', reference_gen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:18702,toler,tolerance,18702,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['toler'],['tolerance']
Availability,"riant:; A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi will create two biallelic variants (one for each; alternate allele) at the same position; A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT field is downcoded once for each; alternate allele. A call for an alternate allele maps to 1 in; the biallelic variant corresponding to itself and 0; otherwise. For example, in the example above, 0/2 maps to 0/0; and 0/1. The genotype 1/2 maps to 0/1 and 0/1.; The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the; sum of the other multiallelic entries.; The biallelic DP is the same as the multiallelic DP.; The biallelic PL entry for for a genotype g is the minimum; over PL entries for multiallelic genotypes that downcode to; g. For example, the PL for (A, T) at 0/1 is the minimum of the; PLs for 0/1 (50) and 1/2 (45), and thus 45.; Fixing an alternate allele and biallelic variant, downcoding; gives a map from multiallelic to biallelic alleles and; genotypes. The biallelic AD entry for an allele is just the; sum of the multiallelic AD entries for alleles that map to; that allele. Similarly, the biallelic PL entry for a genotype; is the minimum over multiallelic PL entries for genotypes that; map to that genotype.; By default, GQ is recomputed from PL. If propagate_gq=True; is passed, the biallelic GQ field is simply the multiallelic; GQ field, that is, genotype qualities are unchanged.; Here is a second example for a het non-ref; A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as; A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. VCF Info Fields; Hail does not split annotations in the info field. This means; that if a multiallelic site with info.AC value [10, 2] is; split, each split site will contain the same array [10,; 2]. The provided allele index annotation va.aIndex can be used; to select the value corresponding to the split allele’s; position:; >>> vds_result = (vd",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:163411,down,downcoding,163411,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['down'],['downcoding']
Availability,"ricExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariates or n_covariates + 1, depending on improved null fit vs full fit; mu = sigmoid(X_bslice @ b); sqrtW = hl.sqrt(mu * (1 - mu)); q, r = hl.nd.qr(X * sqrtW.T.reshape(-1, 1)); h = (q * q).sum(1); coef = r[:m, :m]; residual = y - mu; dep = q[:, :m].T @ ((residual + (h * (0.5 - mu))) / sqrtW); delta_b_struct = hl.nd.solve_triangular(coef, dep.reshape(-1, 1), no_crash=True); exploded = delta_b_struct.failed; delta_b = delta_b_struct.solution.reshape(-1). max_delta_b = nd_max(hl.abs(delta_b)). return hl.bind(cont, exploded, delta_b, max_delta_b). if max_iterations == 0:; return blank_struct.annotate(n_iterations=0, log_lkhd=0, converged=False, exploded=False); return hl.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:44371,toler,tolerance,44371,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,"ring) – Paternal ID.; mother (String) – Maternal ID.; nChildren (Int) – Number of children in this nuclear family.; nErrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement in this set. Code; Dad; Mom; Kid; Copy State. 1",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:125954,error,errors,125954,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,"rite=True). tmp_sample_annot = os.path.join(tmp_dir, '1kg_annotations.txt'); source = resources['1kg_annotations']; info(f'downloading 1KG annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['1kg_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene_annot), gene_annotations_path); hl.hadoop_copy(local_path_uri(tmp_vcf), vcf_path); info('Done!'); else:; info('1KG files found'). [docs]def get_hgdp(output_dir, overwrite: bool = False):; """"""Download subset of the `Human Genome Diversity Panel; <https://www.internationalgenome.org/data-portal/data-collection/hgdp/>`__; dataset and sample annotations. Notes; -----; The download is about 30MB. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite any existing files/directories at `output_dir`.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). matrix_table_path = os.path.join(output_dir, 'HGDP.mt'); vcf_path = os.path.join(output_dir, 'HGDP.vcf.bgz'); sample_annotations_path = os.path.join(output_dir, 'HGDP_annotations.txt'); gene_annotations_path = os.path.join(output_dir, 'ensembl_gene_annotations.txt'). if (; overwrite; or not _dir_exists(fs, matrix_table_path); or not _file_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, 'HGDP.vcf.bgz'); source = resources['HGDP_matrix_table']; info(f'downloading HGDP VCF ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, resources['HGDP_matrix_table'], tmp_vcf); cluster_readable_vcf = _copy_to_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:4222,down,download,4222,docs/0.2/_modules/hail/utils/tutorial.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html,2,['down'],['download']
Availability,"rix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; HailContext. View page source. HailContext¶. class hail.HailContext(sc=None, app_name='Hail', master=None, local='local[*]', log='hail.log', quiet=False, append=False, parquet_compression='snappy', min_block_size=1, branching_factor=50, tmp_dir='/tmp')[source]¶; The main entry point for Hail functionality. Warning; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the HailContext.stop() method.; If passing in a Spark context, ensure that the configuration parameters spark.sql.files.openCostInBytes; and spark.sql.files.maxPartitionBytes are set to as least 50GB. Parameters:; sc (pyspark.SparkContext) – Spark context, one will be created if None.; appName – Spark application identifier.; master – Spark cluster master.; local – Local resources to use.; log – Log path.; quiet (bool) – Don’t write logging information to standard error.; append – Write to end of log file instead of overwriting.; parquet_compression – Level of on-disk annotation compression.; min_block_size – Minimum file split size in MB.; branching_factor – Branching factor for tree aggregation.; tmp_dir – Temporary directory for file merging. Variables:sc (pyspark.SparkContext) – Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_running; Return the running Hail context in this Python session. grep; Grep big files, like, really fast. import_bgen; Import .bgen file(s) as variant dataset. import_gen; Import .gen file(s) as variant dataset. import_plink; Import PLINK binary file (BED, BIM, FAM",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:1290,error,error,1290,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['error'],['error']
Availability,"rixTable or Table'). def _dataset_by_name(self, name: str) -> Dataset:; """"""Retrieve :class:`Dataset` object by name. Parameters; ----------; name : :obj:`str`; Name of dataset. Returns; -------; :class:`Dataset`; """"""; if name not in self.__by_name:; raise ValueError(; f'{name} not found in annotation database,'; f' you may list all known dataset names'; f' with available_datasets'; ); return self.__by_name[name]. def _annotate_gene_name(self, rel: Union[TableRows, MatrixRows]) -> Tuple[str, Union[TableRows, MatrixRows]]:; """"""Annotate row lens with gene name if annotation dataset is gene; keyed. Parameters; ----------; rel : :class:`TableRows` or :class:`MatrixRows`; Row lens of relational object to be annotated. Returns; -------; :class:`tuple`; """"""; gene_field = Env.get_uid(); gencode = self.__by_name['gencode'].index_compatible_version(rel.key); return gene_field, rel.annotate(**{gene_field: gencode.gene_name}). def _check_availability(self, names: Iterable) -> None:; """"""Check if datasets given in `names` are available in the annotation; database instance. Parameters; ----------; names : :obj:`iterable`; Names to check.; """"""; unavailable = [x for x in names if x not in self.__by_name.keys()]; if unavailable:; raise ValueError(f'datasets: {unavailable} not available' f' in the {self.region} region.'). [docs] @typecheck_method(rel=oneof(table_type, matrix_table_type), names=str); def annotate_rows_db(self, rel: Union[Table, MatrixTable], *names: str) -> Union[Table, MatrixTable]:; """"""Add annotations from datasets specified by name to a relational; object. List datasets with :attr:`~.available_datasets`. An interactive query builder is available in the; `Hail Annotation Database documentation; </docs/0.2/annotation_database_ui.html>`_. Examples; --------; Annotate a :class:`.MatrixTable` with ``gnomad_lof_metrics``:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') # doctest: +SKIP. Annotate a :clas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:14021,avail,available,14021,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['avail'],['available']
Availability,"rk 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can now import a matrix table with no; columns.; (#5724); hl.rand_norm2d samples from a two dimensional random normal. Bug fixes. (#5885) Fix; Table.to_spark in the presence of fields of tuples.; (#5882)(#5886); Fix BlockMatrix conversion methods to correctly handle filtered; entries.; (#5884)(#4874); Fix longstanding crash when reading Hail data files under certain; conditions.; (#5855)(#5786); Fix hl.mendel_errors incorrectly reporting children counts in the; presence of entry filtering.; (#5830)(#5835); Fix Nirvana support; (#5773) Fix; hl.sample_qc to use correct number of total rows when calculating; call rate.; (#5763)(#5764); Fix hl.agg.array_agg to work inside mt.annotate_rows and; similar functions.; (#5770) Hail now uses; the correct unicode string encoding which resolves a number of issues; when a Table or MatrixTable has a key field containing unicode; characters.; (#5692) When; keyed is True, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:95906,failure,failure,95906,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['error', 'failure']","['error', 'failure']"
Availability,"rm. '; f'Valid region, cloud combinations are'; f' {DB._valid_combinations}.'; ); if config is not None and url is not None:; raise ValueError(; f'Only specify one of the parameters url and' f' config, received: url={url} and config={config}'; ); if config is None:; if url is None:; config = get_datasets_metadata(); else:; session = external_requests_client_session(); response = retry_response_returning_functions(session.get, url); config = response.json(); assert isinstance(config, dict); elif not isinstance(config, dict):; raise ValueError(f'expected a dict mapping dataset names to ' f'configurations, but found {config}'); config = {k: v for k, v in config.items() if 'annotation_db' in v}; self.region = region; self.cloud = cloud; self.url = url; self.config = config; self.__by_name = {; k: Dataset.from_name_and_json(k, v, region, cloud); for k, v in config.items(); if Dataset.from_name_and_json(k, v, region, cloud) is not None; }. @property; def available_datasets(self) -> List[str]:; """"""List of names of available annotation datasets. Returns; -------; :obj:`list`; List of available annotation datasets.; """"""; return sorted(self.__by_name.keys()). @staticmethod; def _row_lens(rel: Union[Table, MatrixTable]) -> Union[TableRows, MatrixRows]:; """"""Get row lens from relational object. Parameters; ----------; rel : :class:`Table` or :class:`MatrixTable`. Returns; -------; :class:`TableRows` or :class:`MatrixRows`; """"""; if isinstance(rel, MatrixTable):; return MatrixRows(rel); elif isinstance(rel, Table):; return TableRows(rel); else:; raise ValueError('annotation database can only annotate Hail' ' MatrixTable or Table'). def _dataset_by_name(self, name: str) -> Dataset:; """"""Retrieve :class:`Dataset` object by name. Parameters; ----------; name : :obj:`str`; Name of dataset. Returns; -------; :class:`Dataset`; """"""; if name not in self.__by_name:; raise ValueError(; f'{name} not found in annotation database,'; f' you may list all known dataset names'; f' with available_da",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:12393,avail,available,12393,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['avail'],['available']
Availability,"rm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:111144,error,error,111144,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,"rom the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Release 2022-10-18. Bug Fixes. (#12305): Fixed a; rare crash reading tables/matrixtables with _intervals. Version 0.2.102; Released 2022-10-06. New Features. (#12218) Missing; values are now supported in primitive columns in Table.to_pandas.; (#12254); Cross-product-style legends for data groups have been replaced with; factored ones (consistent with ggplot2’s implementation) for; hail.ggplot.geom_point, and support has been added for custom; legend group labels.; (#12268); VariantDatase",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:43558,error,errors,43558,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"ror code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:157001,error,errors,157001,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,"ror for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_agg; correctly. Performance Improvements. (#8413) Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as opposed to only regular python strings.; (#8198) Improved; matrix multiplication interoperation between hail; NDArrayExpression and numpy. Bug fixes. (#8279) Fix a bug; where hl.agg.approx_cdf failed inside of a group_cols_by.; (#8275) Fix bad error; message co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:74579,error,error,74579,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"ror(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); ht = ht.annotate(covs_and_x=hl.nd.hstack([ht.covmat, ht.x.reshape((-1, 1))])). def run_test(yvec, null_fit):; if test == 'score':; return logistic_score_test(ht.covs_and_x, yvec, null_fit); if test == 'firth':; return _firth_test(null_fit, ht.covs_and_x, yvec, max_iterations=max_iterations, tolerance=tolerance). test_fit = logreg_fit(ht.covs_and_x, yvec, null_fit, max_iterations=max_iterations, tolerance=tolerance); if test == 'wald':; return wald_test(ht.covs_and_x, test_fit); assert test == 'lrt', test; return lrt_test(ht.covs_and_x, null_fit, test_fit). ht = ht.select(; logistic_regression=hl.starmap(run_test, hl.zip(ht.yvecs, ht.null_fits)), **{f: ht[f] for f in row_fields}; ); assert 'null_fits' not in row_fields; assert 'logistic_regression' not in row_fields. if not y_is_list:; assert all(f not in row_fields for f in ht.null_fits[0]); assert all(f not in row_fields for f in ht.logistic_regression[0]); ht = ht.select_globals(**ht.null_fits[0]); return ht.transmute(**ht.logistic_regression[0]); ht = ht.select_globals('null_fits'); return ht. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:60172,toler,tolerance,60172,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['toler'],['tolerance']
Availability,"rrors (Int) – Number of Mendel errors in this nuclear family.; nSNP (Int) – Number of Mendel errors at SNPs in this nuclear family. Third table: errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement in this set. Code; Dad; Mom; Kid; Copy State. 1; HomVar; HomVar; Het; Auto. 2; HomRef; HomRef; Het; Auto. 3; HomRef; ! HomRef; HomVar; Auto. 4; ! HomRef; HomRef; HomV",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:126066,error,error,126066,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,"rs; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; er",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:48757,error,errors,48757,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability,"rsion Compatibility Policy; Change Log. Batch. Cookbooks; Random Forest Model. View page source. Random Forest Model. Introduction; We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Random Forest Function; The inputs to the random forest function are two data frame files. df_x; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. df_y is the path to a file containing a Pandas data; frame where the variables in the data frame are the observed and expected variant; count ratio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:1391,checkpoint,checkpointing,1391,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpointing']
Availability,"rsion` has two constructors: :func:`.from_json` and; :func:`.get_region`. Parameters; ----------; url : :obj:`dict` or :obj:`str`; Nested dictionary of URLs containing key: value pairs, like; ``cloud: {region: url}`` if using :func:`.from_json` constructor,; or a string with the URL from appropriate region if using the; :func:`.get_region` constructor.; version : :obj:`str`, optional; String of dataset version, if not ``None``.; reference_genome : :obj:`str`, optional; String of dataset reference genome, if not ``None``.; """""". @staticmethod; def from_json(doc: dict, cloud: str) -> Optional['DatasetVersion']:; """"""Create :class:`.DatasetVersion` object from dictionary. Parameters; ----------; doc : :obj:`dict`; Dictionary containing url and version keys.; Value for url is a :obj:`dict` containing key: value pairs, like; ``cloud: {region: url}``.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`.DatasetVersion` if available on cloud platform, else ``None``.; """"""; assert 'url' in doc, doc; assert 'version' in doc, doc; assert 'reference_genome' in doc, doc; if cloud in doc['url']:; return DatasetVersion(doc['url'][cloud], doc['version'], doc['reference_genome']); else:; return None. @staticmethod; def get_region(name: str, versions: List['DatasetVersion'], region: str) -> List['DatasetVersion']:; """"""Get versions of a :class:`.Dataset` in the specified region, if they; exist. Parameters; ----------; name : :obj:`str`; Name of dataset.; versions : :class:`list` of :class:`.DatasetVersion`; List of DatasetVersion objects where the value for :attr:`.url`; is a :obj:`dict` containing key: value pairs, like ``region: url``.; region : :obj:`str`; Region from which to access data, available regions given in; :attr:`hail.experimental.DB._valid_regions`. Returns; -------; available_versions : :class:`list` of :class:`.DatasetVersion`; List of available versions of a class:`.Dataset` for region.; """"""; available_version",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:2040,avail,available,2040,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['avail'],['available']
Availability,"rtain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:103071,error,error,103071,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,3,"['error', 'failure', 'toler']","['error', 'failure', 'tolerance']"
Availability,"run jobs that haven’t already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we’ll append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesn’t exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.append(tsv_result). Add Batching of Jobs; If we have a lot of short running jobs, then we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the resul",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:9145,checkpoint,checkpoint,9145,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability,"s (list of str) – Contig names.; lengths (dict of str to int) – Dict of contig names to contig lengths.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contigs (str or list of str) – Contigs to be treated as Y chromosomes.; mt_contigs (str or list of str) – Contigs to be treated as mitochondrial DNA.; par (list of tuple of (str, int, int)) – List of tuples with (contig, start, end). Attributes. contigs; Contig names. global_positions_dict; Get a dictionary mapping contig names to their global genomic positions. lengths; Dict of contig name to contig length. mt_contigs; Mitochondrial contigs. name; Name of reference genome. par; Pseudoautosomal regions. x_contigs; X contigs. y_contigs; Y contigs. Methods. add_liftover; Register a chain file for liftover. add_sequence; Load the reference sequence from a FASTA file. contig_length; Contig length. from_fasta_file; Create reference genome from a FASTA file. has_liftover; True if a liftover chain file is available from this reference genome to the destination reference. has_sequence; True if the reference sequence has been loaded. locus_from_global_position; "". read; Load reference genome from a JSON file. remove_liftover; Remove liftover to dest_reference_genome. remove_sequence; Remove the reference sequence. write; ""Write this reference genome to a file in JSON format. add_liftover(chain_file, dest_reference_genome)[source]; Register a chain file for liftover.; Examples; Access GRCh37 and GRCh38 using get_reference():; >>> rg37 = hl.get_reference('GRCh37') ; >>> rg38 = hl.get_reference('GRCh38') . Add a chain file from 37 to 38:; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) . Notes; This method can only be run once per reference genome. Use; has_liftover() to test whether a chain file has been registered.; The chain file format is described; here.; Chain files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37 to GRCh38; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:3460,avail,available,3460,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html,1,['avail'],['available']
Availability,"s = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:49756,error,errors,49756,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,['error'],['errors']
Availability,"s a single value or a list, :func:`.linear_regression_rows`; considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which **all** response variables; and covariates are defined. If `y` is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; -----; With the default root and `y` a single expression, the following row-indexed; fields are added. - **<row key fields>** (Any) -- Row key fields.; - **<pass_through fields>** (Any) -- Row fields in `pass_through`.; - **n** (:py:data:`.tint32`) -- Number of columns used.; - **sum_x** (:py:data:`.tfloat64`) -- Sum of input values `x`.; - **y_transpose_x** (:py:data:`.tfloat64`) -- Dot product of response; vector `y` with the input vector `x`.; - **beta** (:py:data:`.tfloat64`) --; Fit effect coefficient of `x`, :math:`\hat\beta_1` below.; - **standard_error** (:py:data:`.tfloat64`) --; Estimated standard error, :math:`\widehat{\mathrm{se}}_1`.; - **t_stat** (:py:data:`.tfloat64`) -- :math:`t`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}_1`.; - **p_value** (:py:data:`.tfloat64`) -- :math:`p`-value. If `y` is a list of expressions, then the last five fields instead have type; :class:`.tarray` of :py:data:`.tfloat64`, with corresponding indexing of; the list and each array. If `y` is a list of lists of expressions, then `n` and `sum_x` are of type; ``array<float64>``, and the last five fields are of type; ``array<array<float64>>``. Index into these arrays with; ``a[index_in_outer_list, index_in_inner_list]``. For example, if; ``y=[[a], [b, c]]`` then the p-value for ``b`` is ``p_value[1][0]``. In the statistical genetics example above, the input variable `x` encodes; genotype as the number of alternate alleles (0, 1, or 2). For each variant; (row), genotype is tested for association with height controlling for age; and sex, by fitting the linear regression model:. .. math:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:9740,error,error,9740,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['error'],['error']
Availability,"s an array of the cumulative sum of values in the array. argmin(array[, unique]); Return the index of the minimum value in the array. argmax(array[, unique]); Return the index of the maximum value in the array. corr(x, y); Compute the Pearson correlation coefficient between x and y. binary_search(array, elem); Binary search array for the insertion point of elem. hail.expr.functions.abs(x)[source]; Take the absolute value of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.abs(-5)); 5. >>> hl.eval(hl.abs([1.0, -2.5, -5.1])); [1.0, 2.5, 5.1]. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Returns:; NumericExpression, ArrayNumericExpression or NDArrayNumericExpression. hail.expr.functions.approx_equal(x, y, tolerance=1e-06, absolute=False, nan_same=False)[source]; Tests whether two numbers are approximately equal.; Examples; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters:. x (NumericExpression); y (NumericExpression); tolerance (NumericExpression); absolute (BooleanExpression) – If True, compute abs(x - y) <= tolerance. Otherwise, compute; abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022).; nan_same (BooleanExpression) – If True, then NaN == NaN will evaluate to True. Otherwise,; it will return False. Returns:; BooleanExpression. hail.expr.functions.bit_and(x, y)[source]; Bitwise and x and y.; Examples; >>> hl.eval(hl.bit_and(5, 3)); 1. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_or(x, y)[source]; Bitwise or x and y.; Examples; >>> hl.eval(hl.bit_or(5, 3)); 7. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:3734,toler,tolerance,3734,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['toler'],['tolerance']
Availability,"s an int. If None,; use the default value for the ServiceBackend (‘standard’). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in ‘us-central1’:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]]) – The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters:; is_spot (bool) – If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the job’s storage size.; Examples; Set the job’s disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:7431,avail,available,7431,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['avail'],['available']
Availability,"s at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we’ve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the genome. The; complete code is provided here for you",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:10577,checkpoint,checkpoint,10577,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability,"s follows:; >>> ds_result = ds.filter_rows(hl.is_missing(rows_to_remove.index(ds['locus'], ds['alleles']))). See also; anti_join_rows(), filter_rows(), anti_join_cols(). cache()[source]; Persist the dataset in memory.; Examples; Persist the dataset in memory:; >>> dataset = dataset.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; MatrixTable – Cached dataset. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False, _drop_cols=False, _drop_rows=False)[source]; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; MatrixTable. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_matrix_table(). It is; possible to read the file at this path later with; read_matrix_table(). A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; write().; Examples; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'). choose_cols(indices)[source]; Choose a new set of columns from a list of old column indices.; Examples; Randomly shuffle column order:; >>> import random; >>> indices = list(range(dataset.count_cols())); >>> random.shuffle(indices); >>> dataset_reordered = dataset.choose_cols(indices). Take the first ten columns:; >>> dataset_result = dataset.choose_cols(list(range(10))). Parameters:; indices (list of int) – List of old column indices. Returns:; MatrixTable. property col; Returns a struct expression of all column-indexed fields, including keys.; Examples; Get all column field names:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:18750,checkpoint,checkpoint,18750,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['checkpoint'],['checkpoint']
Availability,"s functions. locus(contig, pos[, reference_genome]); Construct a locus expression from a chromosome and position. locus_from_global_position(global_pos[, ...]); Constructs a locus expression from a global position and a reference genome. locus_interval(contig, start, end[, ...]); Construct a locus interval expression. parse_locus(s[, reference_genome]); Construct a locus expression by parsing a string or string expression. parse_variant(s[, reference_genome]); Construct a struct with a locus and alleles by parsing a string. parse_locus_interval(s[, reference_genome, ...]); Construct a locus interval expression by parsing a string or string expression. variant_str(*args); Create a variant colon-delimited string. call(*alleles[, phased]); Construct a call expression. unphased_diploid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/index.html:13414,down,downcode,13414,docs/0.2/functions/index.html,https://hail.is,https://hail.is/docs/0.2/functions/index.html,1,['down'],['downcode']
Availability,"s nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR). - HemiX -- in non-PAR of X, male child; - HemiY -- in non-PAR of Y, male child; - Auto -- otherwise (in autosome or PAR, or female child). Any refers to :math:`\{ HomRef, Het, HomVar, NoCall \}` and ! denotes complement",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:157612,error,error,157612,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability,"s of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):; >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:; >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:; >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:; >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:49186,error,error,49186,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['error']
Availability,"s of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; SwitchBuilder – Mutates and returns self. when_missing(then)[source]; Add a test for missingness. If the base expression is missing,; returns then. Parameters:; then (Expression). Returns:; SwitchBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:1909,error,error,1909,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,1,['error'],['error']
Availability,"s of rows to keep. Must be non-empty and increasing. Returns:; BlockMatrix. floor()[source]; Element-wise floor. Returns:; BlockMatrix. classmethod from_entry_expr(entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Creates a block matrix using a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use write_from_entry_expr() directly to; avoid writing the result twice. See write_from_entry_expr() for; further documentation. Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use write_from_entry_expr() to write to external storage.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:21928,error,error,21928,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['error'],['error']
Availability,"s passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have any non-builtin packages that are used by unapplied installed; in your image. You can use docker.build_python_image() to build a; Python image with additional Python packages installed that is compatible; with Python jobs.; Here are some tips to make sure your function can be used with Batch:. Only reference top-level modules in your functions: like numpy or pandas.; If you get a serialization error, try moving your imports into your function.; Instead of serializing a complex class, determine what information is essential; and only serialize that, perhaps as a dict or array. Parameters:. unapplied (Callable) – A reference to a Python function to execute.; args (Union[PythonResult, ResourceFile, ResourceGroup, List[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Tuple[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any], ...], Dict[str, Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Any]) – Positional arguments to the Python function. Must be either a builtin; Python object, a Resource, or a Dill serializable object.; kwargs (Union[PythonResult, ResourceFile, ResourceGroup, List[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:4018,error,error,4018,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html,2,['error'],['error']
Availability,"s size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone. - s2 : :obj:`.tfloat64`, the variance of the residuals, :math:`\sigma^2` in the paper. - null_fit:. - b : :obj:`.tndarray` vector of coefficients. - score : :obj:`.tndarray` vector of score statistics. - fisher : :obj:`.tndarray` matrix of fisher statistics. - mu : :obj:`.tndarray` the expected value under the null model. - n_iterations : :obj:`.tint32` the number of iterations before termination. - log_lkhd : :obj:`.tfloat64` the log-likelihood of the final iteration. - converged : :obj:`.tbool` True if the null model converged. - exploded : :obj:`.tbool` True if the null model failed to converge due to numerical; ex",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:96374,fault,fault,96374,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['fault'],['fault']
Availability,"s the temporary file; root given to all files in the resource group ({root} when declaring the resource group). Clumping By Chromosome; The second function performs clumping for a given chromosome. The input arguments are the Batch; for which to create a new BashJob, the PLINK binary file root, the association results; with at least two columns (SNP and P), and the chromosome for which to do the clumping for.; The return value is the new BashJob created.; def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. We use the image hailgenetics/genetics which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK.; We explicitly tell PLINK to only use 1Gi of memory because PLINK defaults to using half; of the machine’s memory. PLINK’s memory-available detection mechanism is unfortunately; unaware of the memory limit imposed by Batch. Not specifying resource requirements; correctly can cause performance degradations with PLINK.; PLINK creates a hard-coded file plink.clumped. We have to move that file to a temporary; Batch file {c.clumped} in order to use that file in downstream jobs. Merge Clumping Results; The third function concatenates all of the clumping results per chromosome into a single file; with one header line. The inputs are the Batch for which to create a new BashJob; and a list containing all of the individual clumping results files. We use the ubuntu:22.04; Docker image for this job. The return value is the new BashJob created.; def merge(batch, results):; """"""; Merge clumped results fi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:8654,avail,available,8654,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['avail'],['available']
Availability,"s(""VQSRTranche99.5..."")``. Variants that are flagged as ""PASS"" ; will have no filters applied; for these variants, ``va.filters.isEmpty()`` is true. Thus, ; filtering to PASS variants can be done with :py:meth:`.VariantDataset.filter_variants_expr`; as follows:; ; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). **Annotations**. - **va.filters** (*Set[String]*) -- Set containing all filters applied to a variant. ; - **va.rsid** (*String*) -- rsID of the variant.; - **va.qual** (*Double*) -- Floating-point number in the QUAL field.; - **va.info** (*Struct*) -- All INFO fields defined in the VCF header; can be found in the struct ``va.info``. Data types match the type; specified in the VCF header, and if the declared ``Number`` is not; 1, the result will be stored as an array. :param path: VCF file(s) to read.; :type path: str or list of str. :param bool force: If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB. :param bool force_bgz: If True, load .gz files as blocked gzip files (BGZF). :param header_file: File to load VCF header from. If not specified, the first file in path is used.; :type header_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations or; genotypes. :param bool store_gq: If True, store GQ FORMAT field instead of computing from PL. Only applies if ``generic=False``. :param bool pp_as_pl: If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if ``generic=False``. :param bool skip_bad_ad: If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if ``generic=False``. :param bool generic: If True, read the genotype with a generic schema. :param call_fields: FORMAT fi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:24794,down,downstream,24794,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['down'],['downstream']
Availability,"s). class Dataset:; """"""Dataset object constructed from name, description, url, key_properties,; and versions specified in JSON configuration file or a provided :obj:`dict`; mapping dataset names to configurations. Parameters; ----------; name : :obj:`str`; Name of dataset.; description : :obj:`str`; Brief description of dataset.; url : :obj:`str`; Cloud URL to access dataset.; key_properties : :class:`set` of :obj:`str`; Set containing key property strings, if present. Valid properties; include ``'gene'`` and ``'unique'``.; versions : :class:`list` of :class:`.DatasetVersion`; List of :class:`.DatasetVersion` objects.; """""". @staticmethod; def from_name_and_json(name: str, doc: dict, region: str, cloud: str) -> Optional['Dataset']:; """"""Create :class:`.Dataset` object from dictionary. Parameters; ----------; name : :obj:`str`; Name of dataset.; doc : :obj:`dict`; Dictionary containing dataset description, url, key_properties, and; versions.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`Dataset`, optional; If versions exist for region returns a :class:`.Dataset` object,; else ``None``.; """"""; assert 'annotation_db' in doc, doc; assert 'key_properties' in doc['annotation_db'], doc['annotation_db']; assert 'description' in doc, doc; assert 'url' in doc, doc; assert 'versions' in doc, doc; key_properties = set(x for x in doc['annotation_db']['key_properties'] if x is not None); versions = [; DatasetVersion.from_json(x, cloud); for x in doc['versions']; if DatasetVersion.from_json(x, cloud) is not None; ]; versions_in_region = DatasetVersion.get_region(name, versions, region); if versions_in_region:; return Dataset(name, doc['description'], doc['url'], key_properties, versions_in_region). def __init__(self, name: str, description: str, url: str, key_properties: Set[str], versions: Li",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:6261,avail,available,6261,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['avail'],['available']
Availability,"s, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for ``+`` and ``*``, place the; block matrix operand first; for ``-``, ``/``, and ``@``, first convert; the ndarray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read('d.bm') # doctest: +SKIP; >>> (c @ d).write('cd.bm') # doctest: +SKIP; >>> a = BlockMatrix.read('a.bm') # doctest: +SKIP; >>> e = a @ BlockMatrix.read('cd.bm') # doctest: +SKIP. **Indexing and slicing**. Block matrices also support NumPy-style 2-dimensional; `indexing and slicing <https://docs.scipy.org/doc/numpy/user/basics.indexing.html>`__,; with two differences.; First, slices ``start:stop:step`` must be non-empty with positive ``step``.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional. For example, for a block matrix ``bm`` with 10 r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:5169,resilien,resilience,5169,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['resilien'],['resilience']
Availability,"s. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR). - HemiX -- in non-PAR of X, male child; - HemiY -- in non-PAR of Y, male child; - Auto -- otherwise (in autosome or PAR, or female child). Any refers to :math:`\{ HomRef, Het, HomVar, NoCall \}` and ! denotes complement in this set. +--------+------------+------------+----------+------------------+; |Code | Dad | Mom | Kid | Copy State |; +========+=======",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:157670,error,error,157670,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability,"s.; """"""; x = wrap_to_list(x); if len(x) == 0:; raise ValueError(""linreg: must have at least one covariate in `x`""). hl.methods.statgen._warn_if_no_intercept('linreg', x). if weight is not None:; sqrt_weight = hl.sqrt(weight); y = sqrt_weight * y; x = [sqrt_weight * xi for xi in x]. k = len(x); x = hl.array(x). res_type = hl.tstruct(; xty=hl.tarray(hl.tfloat64),; beta=hl.tarray(hl.tfloat64),; diag_inv=hl.tarray(hl.tfloat64),; beta0=hl.tarray(hl.tfloat64),; ). temp = _agg_func('LinearRegression', [y, x], res_type, [k, hl.int32(nested_dim)]). k0 = nested_dim; covs_defined = hl.all(lambda cov: hl.is_defined(cov), x); tup = hl.agg.filter(covs_defined, hl.tuple([hl.agg.count_where(hl.is_defined(y)), hl.agg.sum(y * y)])); n = tup[0]; yty = tup[1]. def result_from_agg(linreg_res, n, k, k0, yty):; xty = linreg_res.xty; beta = linreg_res.beta; diag_inv = linreg_res.diag_inv; beta0 = linreg_res.beta0. def dot(a, b):; return hl.sum(a * b). d = n - k; rss = yty - dot(xty, beta); rse2 = rss / d # residual standard error squared; se = (rse2 * diag_inv) ** 0.5; t = beta / se; p = t.map(lambda ti: 2 * hl.pT(-hl.abs(ti), d, True, False)); rse = hl.sqrt(rse2). d0 = k - k0; xty0 = xty[:k0]; rss0 = yty - dot(xty0, beta0); r2 = 1 - rss / rss0; r2adj = 1 - (1 - r2) * (n - k0) / d; f = (rss0 - rss) * d / (rss * d0); p0 = hl.pF(f, d0, d, False, False). return hl.struct(; beta=beta,; standard_error=se,; t_stat=t,; p_value=p,; multiple_standard_error=rse,; multiple_r_squared=r2,; adjusted_r_squared=r2adj,; f_stat=f,; multiple_p_value=p0,; n=n,; ). global _result_from_linreg_agg_f; if _result_from_linreg_agg_f is None:; _result_from_linreg_agg_f = hl.experimental.define_function(; result_from_agg, res_type, hl.tint64, hl.tint32, hl.tint32, hl.tfloat64, _name=""linregResFromAgg""; ). return _result_from_linreg_agg_f(temp, n, k, k0, yty). [docs]@typecheck(x=expr_float64, y=expr_float64); def corr(x, y) -> Float64Expression:; """"""Computes the; `Pearson correlation coefficient <https://en.wikipedia.o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:54546,error,error,54546,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['error'],['error']
Availability,"s.dtype.element_type); stream_req = stream_f(ctx_var); make_prod_ir = stream_req._ir; if isinstance(make_prod_ir.typ, hl.tarray):; make_prod_ir = ir.ToStream(make_prod_ir); t = stream_req.dtype.element_type. key_typ = hl.tstruct(**{k: t[k] for k in key}); vals_typ = hl.tarray(t). key_uid = Env.get_uid(); vals_uid = Env.get_uid(). key_var = construct_variable(key_uid, key_typ); vals_var = construct_variable(vals_uid, vals_typ). join_ir = join_f(key_var, vals_var); zj = ir.ToArray(ir.StreamZipJoinProducers(contexts._ir, ctx_uid, make_prod_ir, key, key_uid, vals_uid, join_ir._ir)); indices, aggs = unify_all(contexts, stream_req, join_ir); return construct_expr(zj, zj.typ, indices, aggs). [docs]@typecheck(arrays=expr_oneof(expr_stream(expr_any), expr_array(expr_any)), key=sequenceof(builtins.str)); def keyed_intersection(*arrays, key):; """"""Compute the intersection of sorted arrays on a given key. Requires sorted arrays with distinct keys. Warning; -------; Experimental. Does not support downstream randomness. Parameters; ----------; arrays; key. Returns; -------; :class:`.ArrayExpression`; """"""; return _union_intersection_base(; 'keyed_intersection',; arrays,; key,; lambda key_var, vals_var: hl.tuple((key_var, vals_var)),; lambda res: res.filter(lambda x: hl.fold(lambda acc, elt: acc & hl.is_defined(elt), True, x[1])).map(; lambda x: x[1].first(); ),; ). [docs]@typecheck(arrays=expr_oneof(expr_stream(expr_any), expr_array(expr_any)), key=sequenceof(builtins.str)); def keyed_union(*arrays, key):; """"""Compute the distinct union of sorted arrays on a given key. Requires sorted arrays with distinct keys. Warning; -------; Experimental. Does not support downstream randomness. Parameters; ----------; exprs; key. Returns; -------; :class:`.ArrayExpression`; """"""; return _union_intersection_base(; 'keyed_union',; arrays,; key,; lambda keys_var, vals_var: hl.fold(; lambda acc, elt: hl.coalesce(acc, elt), hl.missing(vals_var.dtype.element_type), vals_var; ),; lambda res: res,; ). [d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:145843,down,downstream,145843,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['down'],['downstream']
Availability,"s; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Get the i*th* allele.; Examples; Index with a single integer:; >>> hl.eval(call[0]); 0. >>> hl.eval(call[1]); 1. Parameters:; item (int or Expression of type tint32) – Allele index. Returns:; Expression of type tint32. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains_allele(allele)[source]; Returns true if the call has one or more called alleles of the given index.; >>> c = hl.call(0, 3). >>> hl.eval(c.contains_allele(3)); True. >>> hl.eval(c.contains_allele(1)); False. Returns:; BooleanExpression. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.CallExpression.html:3006,error,error,3006,docs/0.2/hail.expr.CallExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.CallExpression.html,1,['error'],['error']
