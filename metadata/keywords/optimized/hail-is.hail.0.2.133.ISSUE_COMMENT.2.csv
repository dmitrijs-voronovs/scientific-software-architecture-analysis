quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Energy Efficiency,"AndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.49\n2022-11-15 20:31:41.231 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.49 response 200\n2022-11-15 20:31:41.330 ServiceBackend$: INFO: result 49 complete - 8157265 bytes\n2022-11-15 20:31:41.330 ServiceBackend$: INFO: all results complete\n2022-11-15 20:31:41.331 root: INFO: executed D-Array [table_aggregate_singlestage] in 1m23.1s\n2022-11-15 20:31:41.331 root: INFO: RegionPool: REPORT_THRESHOLD: 8.2M allocated (192.0K blocks / 8.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.335 root: INFO: RegionPool: REPORT_THRESHOLD: 16.2M allocated (192.0K blocks / 16.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.339 root: INFO: RegionPool: REPORT_THRESHOLD: 24.2M allocated (192.0K blocks / 24.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.345 root: INFO: RegionPool: REPORT_THRESHOLD: 32.2M allocated (192.0K blocks / 32.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.351 root: INFO: RegionPool: REPORT_THRESHOLD: 40.2M allocated (192.0K blocks / 40.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.357 root: INFO: RegionPool: REPORT_THRESHOLD: 48.2M allocated (192.0K blocks / 48.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.362 root: INFO: RegionPool: REPORT_THRESHOLD: 56.2M allocated (192.0K blocks / 56.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.368 root: INFO: RegionPool: REPORT_THRESHOLD: 64.2M allocated (192.0K blocks / 64.0M chunks), regions.size = 3, 0 current java objects, thread ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:62394,allocate,allocated,62394,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['allocate'],['allocated']
Energy Efficiency,"Are you sure that you installed all the necessary packages listed here: https://hail.is/docs/0.2/install/linux.html ? In particular this kind of error can happen if you did not install openblas. In the future, please use https://discuss.hail.is for support questions, we don't monitor GitHub issues.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9939#issuecomment-786236682:277,monitor,monitor,277,https://hail.is,https://github.com/hail-is/hail/issues/9939#issuecomment-786236682,1,['monitor'],['monitor']
Energy Efficiency,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:106,monitor,monitoring,106,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336,5,['monitor'],['monitoring']
Energy Efficiency,"At least on GCP, the GKE Autopilot offering might also be worth a look? One advantage would be that you're only charged for the pods you use, not the nodes (https://cloud.google.com/kubernetes-engine/pricing).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11443#issuecomment-1062528734:112,charge,charged,112,https://hail.is,https://github.com/hail-is/hail/pull/11443#issuecomment-1062528734,1,['charge'],['charged']
Energy Efficiency,Azure monitor provides detailed insights: https://portal.azure.com/#view/Microsoft_Azure_Monitoring/AzureMonitoringBrowseBlade/~/storageInsights,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12994#issuecomment-1536270932:6,monitor,monitor,6,https://hail.is,https://github.com/hail-is/hail/issues/12994#issuecomment-1536270932,1,['monitor'],['monitor']
Energy Efficiency,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:569,schedul,scheduler,569,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683,3,"['monitor', 'schedul']","['monitoring', 'scheduler']"
Energy Efficiency,"Behond! The power of IR. All this PR does is move the RVB-based computation for filter entries to a fully IR-driven computation. Timing of:. ```; ds = hl.read_matrix_table('gnomad.mt'); ds = ds.filter_entries((ds.GQ > 40) & (ds.DP > 30)); ds._force_count_rows(); ```. run on a shard of gnomAD (~5GB). master:. ```; real	2m20.295s; user	0m0.421s; sys	0m0.034s; ```. filterentriesir:. ```; real	1m48.399s; user	0m0.411s; sys	0m0.046s; ```. (Two timings each, both +/- <1s.). 23% improvement.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3354#issuecomment-380514682:12,power,power,12,https://hail.is,https://github.com/hail-is/hail/pull/3354#issuecomment-380514682,1,['power'],['power']
Energy Efficiency,Block$1.apply(TorrentBroadcast.scala:222); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 11 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); 	at is.hail.utils.richUtil,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:3866,schedul,scheduler,3866,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['schedul'],['scheduler']
Energy Efficiency,Bump. This needs to go into production before we do massive scale tests since Standard_SSD charges per IO operation.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11123#issuecomment-990100370:91,charge,charges,91,https://hail.is,https://github.com/hail-is/hail/pull/11123#issuecomment-990100370,1,['charge'],['charges']
Energy Efficiency,"Buy it, use it, break it, fix it; Trash it, change it, mail - upgrade it; Charge it, point it, zoom it, press it; Snap it, work it, quick - erase it; Write it, cut it, paste it, save it; Load it, check it, quick - rewrite it; Plug it, play it, burn it, rip it; Drag and drop it, zip - unzip it; Lock it, fill it, call it, find it; View it, code it, jam - unlock it; Surf it, scroll it, pause it, click it; Cross it, crack it, switch - update it; Name it, read it, tune it, print it; Scan it, send it, fax - rename it; Touch it, bring it, pay it, watch it; Turn it, leave it, start - format it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2036#issuecomment-318518817:74,Charge,Charge,74,https://hail.is,https://github.com/hail-is/hail/pull/2036#issuecomment-318518817,1,['Charge'],['Charge']
Energy Efficiency,Can you give us a bit more information about your cluster? Are you using a scheduler like GridEngine or LSF? Are your files stored in NFS?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/902#issuecomment-251747734:75,schedul,scheduler,75,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251747734,1,['schedul'],['scheduler']
Energy Efficiency,"Can you take another look now?. I added two new fields to the jobs table to help with indexing and order bys. This should make the queries simpler and allow us to revert back to the old scheduler that Cotton wrote that was optimitzed. The regions_bits_rep is just a 0/1 for each region. So [us-east1, us-central1] could be ""1100000"". I also realized that I could aggregate the ready cores per user and then order them after unioning each user. I think this will perform better. From small tests, the autoscaler query seemed much better, but I'll want to do one last load test once you're okay with this approach.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1274632733:186,schedul,scheduler,186,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1274632733,1,['schedul'],['scheduler']
Energy Efficiency,Chris points out the `__uid_3` is created by `_same`. That's the entries array. There is no bug; just that `_same` was not intended for public consumption and therefore does some funky things.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13823#issuecomment-1765051388:143,consumption,consumption,143,https://hail.is,https://github.com/hail-is/hail/issues/13823#issuecomment-1765051388,1,['consumption'],['consumption']
Energy Efficiency,Cipher.java:2463) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLCipher$T12GcmReadCipherGenerator$GcmReadCipher.decrypt(SSLCipher.java:1606) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:262) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1404) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1372) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:966) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_392]; 	at sun.net.www.MeteredStream.read(MeteredStream.java:134) ~[?:1.8.0_392]; 	at java.io.FilterInputStream.read(FilterInputStream.java:133) ~[?:1.8.0_392]; 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3460) ~[?:1.8.0_392]; 	at com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:164) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_392]; 	at is.hail.relocated.com.google.cloud.storage.StorageByteChannels$ScatteringByteChannelFacade.read(StorageByteChannels.java:242) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.ApiaryUnbufferedReadableByteChannel.read(ApiaryUnbufferedReadableByteChannel.java:113) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:9370,Meter,MeteredStream,9370,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['Meter'],['MeteredStream']
Energy Efficiency,"Closing. I think using github's graphQL API is going to be able to give us everything we need with a single query, and reduce a bunch of shadow state tracking in CI",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14659#issuecomment-2299798018:119,reduce,reduce,119,https://hail.is,https://github.com/hail-is/hail/pull/14659#issuecomment-2299798018,1,['reduce'],['reduce']
Energy Efficiency,Commit message: ; ```; Fixed bug in TextTableReader caused by unsafe ArrayBuilder use. ; Bug occurred for text tables with a number of columns equal to a power of 2; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1127#issuecomment-263676448:154,power,power,154,https://hail.is,https://github.com/hail-is/hail/pull/1127#issuecomment-263676448,1,['power'],['power']
Energy Efficiency,"Conceptually, if we're not just interested in missingness, then unboxedGT is useful only in route to nNonRefAlleles (and equal to it if we've split). E.g., the simplest way to extend regression to multi-allelic is to use nNonRefAlleles...though thinking about this further, it's upsettingly asymmetric in the case where the ref allele is the minor allele, so perhaps we should just force deliberate choice of splitting, esp if we're moving toward implementing that less painfully under the hood. Or do regression per alternate allele while maintaining multi-allelic form. (edited). We currently compute nNonRefAlleles from unboxedGT through GTPair, which allocates per genotype. For example, PCA currently requires splitting, but uses nNonRefAlleles. And IBD currently requires splitting but allocates per genotype via:; ```; def countRefs(gtIdx: Int): Int = {; val gt = Genotype.gtPair(gtIdx); indicator(gt.j == 0) + indicator(gt.k == 0); ```. So it may make sense to add `unboxedNNonRefAlleles` that avoids allocation, but doesn't require splitting for these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468:655,allocate,allocates,655,https://hail.is,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468,2,['allocate'],['allocates']
Energy Efficiency,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812:1012,efficient,efficient,1012,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812,1,['efficient'],['efficient']
Energy Efficiency,"D 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:9097,schedul,scheduler,9097,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['schedul'],['scheduler']
Energy Efficiency,D.scala:631); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:631); 	at is.hail.io.RichRDDRegionValue$.writeRowsPartition(RowStore.scala:510); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:1806,schedul,scheduler,1806,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['schedul'],['scheduler']
Energy Efficiency,DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2244); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2269); 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029); 	at is.hail.backend.spark.SparkBackend.parallelizeAndComputeWithIndex(SparkBackend.scala:355); 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:43); 	at __C16,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:6240,schedul,scheduler,6240,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['schedul'],['scheduler']
Energy Efficiency,Dan pointed out that a flat version of this could also be achieved with `hl.range(nd.shape[0]).map(lambda i: nd[i])`. That could of course also be generalized to higher dimensional ndarrays as well. I wonder how efficient it would be though. Would be nice if deforesting could naturally cross boundary between arrays and ndarrays.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7979#issuecomment-579782403:212,efficient,efficient,212,https://hail.is,https://github.com/hail-is/hail/issues/7979#issuecomment-579782403,1,['efficient'],['efficient']
Energy Efficiency,"Deploy commits don't need to cleanup which adds some latency to this PR. We should probably use xargs -P4 to delete instances 4-way parallel. This PR is ~46 minutes, including all the cleanup time, where as deploys are 46 minutes *without the cleanup time*. Notice two things: (1) the service backend is again the critical path (2) some local backend tests took quite a while to get scheduled. Seems fishy to me that it took ~16 minutes to find a core for the local backend tests to run on. Anyway, seems good to use more fine-grained parallelism. This should help keep the cluster large-ish and turning over fast so that users get a great experience during the work day. ---. A deploy commit:. <img width=""2032"" alt=""Screen Shot 2023-05-17 at 17 30 55"" src=""https://github.com/hail-is/hail/assets/106194/9c00365e-1079-451c-bd85-e10561e715c1"">. This PR:. <img width=""2032"" alt=""Screen Shot 2023-05-17 at 17 34 40"" src=""https://github.com/hail-is/hail/assets/106194/fa7751be-3986-4361-89ea-e322760176bf"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1552119464:383,schedul,scheduled,383,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1552119464,1,['schedul'],['scheduled']
Energy Efficiency,Doesn't the batch driver need to schedule the `deploy_batch` job to update itself? How can it eventually get to deploying the new / compatible code?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13810#issuecomment-1812763995:33,schedul,schedule,33,https://hail.is,https://github.com/hail-is/hail/pull/13810#issuecomment-1812763995,1,['schedul'],['schedule']
Energy Efficiency,"Downgrading from high priority to normal priority because konrad isn't actually blocked by this. The issue is probably caused by hail downloading the FASTA file once per task. Consider:; ```; import hail as hl; rg = hl.get_reference('GRCh38'); rg.add_sequence('gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz',; 'gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai'); ht = hl.read_table('gs://konradk/liftover_test/gnomad_exomes.ht'); ht.annotate(context=ht.locus.sequence_context(before=1, after=1))._force_count(); ```. `gnomad_exomes.ht` has about 10000 partitions. If you execute this on a 100 node cluster, you'll see that workers will have many copies of the FASTA file:. ```; dking@dk-sw-sczv:~$ du -sh /tmp/hail.*/*.fasta; 3.1G	/tmp/hail.iNLnbdai1pJe/00000.fasta; 3.1G	/tmp/hail.Psc430xLLmdE/00000.fasta; 3.1G	/tmp/hail.RNWZxuNSm6h2/00000.fasta; 3.1G	/tmp/hail.rxwJfyieiIie/00000.fasta; 3.1G	/tmp/hail.w79BrNc7RXOz/00000.fasta; 3.1G	/tmp/hail.yqgUhdCe5I6I/00000.fasta; ```. I think the issue is that a ReferenceGenome is allocated once per shipped JVM bytecode pack. A ReferenceGenome has a FASTAReader which has a SerializableReferenceSequenceFile. That roughly means we allocate one SerializableReferenceSequenceFile per-task. As the tasks:worker ratio gets large, this becomes infeasible. If we move the reference genome management to some broadcasted object, we can ensure it's once per-JVM (in fact one per-JVM for a whole slew of tasks). I'll look into this more at some point soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5371#issuecomment-464225703:1052,allocate,allocated,1052,https://hail.is,https://github.com/hail-is/hail/issues/5371#issuecomment-464225703,2,['allocate'],"['allocate', 'allocated']"
Energy Efficiency,"During implementation, I noticed some behaviors that were not fully specified. If an allele is filtered, we must address the genotypes that reference that allele. A genotype consists of five parts. | Part | Description | Action |; | --- | --- | --- |; | GT | the hard call | if the filtered allele is `a` then `forall b.` `b/a` and `a/b` are converted, respectively, to `b/0` and `0/b` |; | AD | allele depth | the filtered allele's column is eliminated, e.g. filtering allele 1 transforms `[25,5,20]` to `[25,20]` |; | DP | number of informative reads | no change |; | PL | Phred-likelihoods for each allele pair | convert the allele-pair & likelihood pairs (e.g. `(0/1, 10)`) according to the GT rule. This yields a bag of (possibly duplicated) allele-likelihood pairs. We reduce back to unique allele-pairs by taking the `min` likelihood for each allele-pair |; | GQ | genotype quality | set to the second lowest value in the modified PL |. I'm a tad uneasy about the actions for AD, DP, and PL. For AD, should we shift the depth to the reference? For DP, should we subtract the removed depths? For PL, `min` should be ok when the values have differing orders of magnitude, but if two values are similar, should we convert to probabilities, sum, and convert back?. @cseed @monkollek @konradjk",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/551#issuecomment-240788265:775,reduce,reduce,775,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240788265,1,['reduce'],['reduce']
Energy Efficiency,Example error:. ```; Caused by: is.hail.relocated.org.json4s.MappingException: No usable value for value_parameter_names; No usable value for str; Did not find value which can be converted into java.lang.String; 	at is.hail.relocated.org.json4s.reflect.package$.fail(package.scala:53); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:638); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:689); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:688); 	at scala.PartialFunction.$anonfun$runWith$1$adapted(PartialFunction.scala:145); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at scala.collection.TraversableLike.collect(TraversableLike.scala:407); 	at scala.collection.TraversableLike.collect$(TraversableLike.scala:405); 	at scala.collection.AbstractTraversable.collect(Traversable.scala:108); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:688); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:767); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$extract$10(Extraction.scala:462); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$customOrElse$1(Extraction.scala:780); 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127); 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126); 	at scala.PartialFunction$$anon$1.applyOrElse(PartialFunction.scala:257); 	at is.hail.relocated.org.json4s.Extraction$.customOrElse(Extraction.scala:780); 	at is.hail.relocated.org.json4s.Extraction$.extract(Extraction.scala:454); 	at is.hail.relocated.org.json4s.Extraction$.org$json4s$Extraction$$extractDete,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14579#issuecomment-2163457890:698,adapt,adapted,698,https://hail.is,https://github.com/hail-is/hail/pull/14579#issuecomment-2163457890,1,['adapt'],['adapted']
Energy Efficiency,"F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.49 response 200\n2022-11-15 20:31:41.330 ServiceBackend$: INFO: result 49 complete - 8157265 bytes\n2022-11-15 20:31:41.330 ServiceBackend$: INFO: all results complete\n2022-11-15 20:31:41.331 root: INFO: executed D-Array [table_aggregate_singlestage] in 1m23.1s\n2022-11-15 20:31:41.331 root: INFO: RegionPool: REPORT_THRESHOLD: 8.2M allocated (192.0K blocks / 8.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.335 root: INFO: RegionPool: REPORT_THRESHOLD: 16.2M allocated (192.0K blocks / 16.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.339 root: INFO: RegionPool: REPORT_THRESHOLD: 24.2M allocated (192.0K blocks / 24.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.345 root: INFO: RegionPool: REPORT_THRESHOLD: 32.2M allocated (192.0K blocks / 32.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.351 root: INFO: RegionPool: REPORT_THRESHOLD: 40.2M allocated (192.0K blocks / 40.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.357 root: INFO: RegionPool: REPORT_THRESHOLD: 48.2M allocated (192.0K blocks / 48.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.362 root: INFO: RegionPool: REPORT_THRESHOLD: 56.2M allocated (192.0K blocks / 56.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.368 root: INFO: RegionPool: REPORT_THRESHOLD: 64.2M allocated (192.0K blocks / 64.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.374 root: INFO: RegionPool: REPORT_THRESHOLD: 72.2M allocated (192.0K blocks / 72.0M chunks), regions.size = 3, 0 current java objects, thread ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:62577,allocate,allocated,62577,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['allocate'],['allocated']
Energy Efficiency,"FO: 7: 7028; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 8: 9060; 2023-09-27 16:43:10.390 JVMEntryway: INFO: Yielding control to the QoB Job.; 2023-09-27 16:43:10.393 Worker$: INFO: is.hail.backend.service.Worker 09526a168d57dac1a26f8caa4ab49593931ed2ef; 2023-09-27 16:43:10.394 Worker$: INFO: running job 7028/9060 at root gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g= with scratch directory '/batch/83e7aee9e9244f6884b8a84ea81b4c7a'; 2023-09-27 16:43:10.398 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-27 16:43:10.779 WorkerTimer$: INFO: readInputs took 384.743327 ms.; 2023-09-27 16:43:10.779 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-27 16:43:10.787 : INFO: RegionPool: REPORT_THRESHOLD: 2.2M allocated (192.0K blocks / 2.0M chunks), regions.size = 3, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.3M allocated (192.0K blocks / 2.1M chunks), regions.size = 3, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.3M allocated (256.0K blocks / 2.1M chunks), regions.size = 4, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.4M allocated (320.0K blocks / 2.1M chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.286 : INFO: RegionPool: REPORT_THRESHOLD: 28.8M allocated (576.0K blocks / 28.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.657 : INFO: RegionPool: REPORT_THRESHOLD: 30.8M allocated (576.0K blocks / 30.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.683 : INFO: RegionPool: REPORT_THRESHOLD: 32.8M allocated (576.0K blocks / 32.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:3799,allocate,allocated,3799,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['allocate'],['allocated']
Energy Efficiency,"Finally got a lead on Christina's bug:; ```; # hailctl dataproc submit dk foo.py; Submitting to cluster 'dk'...; gcloud command:; gcloud dataproc jobs submit pyspark foo.py \; --cluster=dk \; --files= \; --py-files=/var/folders/cq/p_l4jm3x72j7wkxqxswccs180000gq/T/pyscripts_yg_wzlu0.zip \; --properties=; Job [66c1d088108948b2b76bb607f61d7b3f] submitted.; Waiting for job output...; Initializing Spark and Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:982,allocate,allocate,982,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815,1,['allocate'],['allocate']
Energy Efficiency,"Fixed, thanks for the push, that was easy to do and is so much better than the old thing. I can't delete the powerpoints from their old location on github yet since the current website still links to them, but after 0.2.32 release I'll delete the old ones.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7950#issuecomment-577856190:109,power,powerpoints,109,https://hail.is,https://github.com/hail-is/hail/pull/7950#issuecomment-577856190,1,['power'],['powerpoints']
Energy Efficiency,"Fixing MakeNDArray to not double allocate cut my laptop benchmark time by more than 50%, it's only a bit slower than main now (~5-10%), so we should probably try and get it in. Feel free to review again when you get a chance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10001#issuecomment-781709108:33,allocate,allocate,33,https://hail.is,https://github.com/hail-is/hail/pull/10001#issuecomment-781709108,1,['allocate'],['allocate']
Energy Efficiency,For posterity this is what goes wrong if I don't create a fresh OOS for each object:; ```. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:132,schedul,scheduler,132,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307,8,['schedul'],['scheduler']
Energy Efficiency,"For posterity, the proposed mitigation is to promote a job request to a number of cores necessary to get the requested storage, much in the same way that we do for memory. If that requires more cores than can be allocated in our shared pools, we reject the request. In such a scenario, the user should alter their job to use a job-private instance with sufficient disk space.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14522#issuecomment-2130377307:212,allocate,allocated,212,https://hail.is,https://github.com/hail-is/hail/issues/14522#issuecomment-2130377307,1,['allocate'],['allocated']
Energy Efficiency,Got it green. Going to benchmark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12981#issuecomment-1540483249:7,green,green,7,https://hail.is,https://github.com/hail-is/hail/pull/12981#issuecomment-1540483249,1,['green'],['green']
Energy Efficiency,"Great! So here's what the docs look like now:; https://hail.is/docs/devel/methods/genetics.html#hail.methods.nirvana. Here's the Python source:; https://github.com/hail-is/hail/blob/master/python/hail/methods/qc.py. You can see the built docs of this PR by clicking on Details next to the passing 2.2.0 test, and then clicking on Docs, e.g.:; https://ci.hail.is/viewLog.html?buildId=63354&buildTypeId=HailSourceCode_PRsOnly_HailTestJarSpark220&tab=report_project8_Docs. I'd appreciate if you could:; - ensure the docs are still accurate and add information on what version(s) of Nirvana is compatible.; - update the schema in the documentation to match your changes in Scala; - try running the same pipeline with a few block sizes to see whether its reasonable to reduce the default block size so that users will get more parallelism by default. I suspect a user with a 1 million variant VCF would prefer running 100 cores with 10k variants each to 2 cores with 500k variants each. I'd be surprised if the per-block overhead is so high to outweigh the benefit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3266#issuecomment-379138339:764,reduce,reduce,764,https://hail.is,https://github.com/hail-is/hail/pull/3266#issuecomment-379138339,1,['reduce'],['reduce']
Energy Efficiency,"Hail does not support heterogeneous arrays: found list with elements of types [dtype('int32'), dtype('str')] . The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 584, in arg_check; return checker.check(arg, function_name, arg_name); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_typecheck.py"", line 80, in check; raise TypecheckFailure from e; hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in <module>; cli.main(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main; run(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file; runpy.run_path(target, run_name=""__main__""); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path; return _run_module_code(code, init_globals, run_name,; File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code; _run_code(code, mod_globals, init_globals,; File ""/home/edmund/.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:3044,adapt,adapter,3044,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['adapt'],['adapter']
Energy Efficiency,"Hail only accepts BGEN files with 8-bit probabilities, which is the most common (and a very space-efficient) representation. You can generate this yourself using the `-bgen-bits 8` argument in qctool. Hope this helps!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8545#issuecomment-613143370:98,efficient,efficient,98,https://hail.is,https://github.com/hail-is/hail/issues/8545#issuecomment-613143370,1,['efficient'],['efficient']
Energy Efficiency,Hand deploy successful. Monitoring logs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8424#issuecomment-607417090:24,Monitor,Monitoring,24,https://hail.is,https://github.com/hail-is/hail/pull/8424#issuecomment-607417090,1,['Monitor'],['Monitoring']
Energy Efficiency,Here is a straight-line pipeline that replicates the high memory use. In my experience this can get up to 100GiB of RAM use. https://gist.github.com/danking/3432deabd997ce08515b2088e202a039. The VDS file is privileged. Next steps:. - [ ] replicate on a public VDS like the HGDP/1KG VDS.; - [ ] delete as much code as possible from this file to reduce the possible causes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13606#issuecomment-1717799683:344,reduce,reduce,344,https://hail.is,https://github.com/hail-is/hail/issues/13606#issuecomment-1717799683,1,['reduce'],['reduce']
Energy Efficiency,"Here is an example of the trailing slash issue:. What kibana sees:. ""GET /monitoring/kibana/ui/fonts/inter_ui/Inter-UI-Bold.woff2 HTTP/1.1"" 200 94840 ""https://internal.hail.is/monitoring/kibana/app/kibana"". config:. ```; location /monitoring/kibana/ {; proxy_pass http://kibana/;; }; ```. It may not be inconsistent with trailing / on proxy_pass stripping the url, but it sure is confusing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540622757:74,monitor,monitoring,74,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540622757,3,['monitor'],['monitoring']
Energy Efficiency,"Here's a link with an absolute time window: https://cloudlogging.app.goo.gl/gXAWZpZtUiV8jphXA. This is the assertion's stack trace:; ```; at scala.Predef$.assert(Predef.scala:208); at is.hail.QoBOutputStreamManager.createOutputStream(QoBAppender.scala:38); at org.apache.logging.log4j.core.appender.OutputStreamManager.getOutputStream(OutputStreamManager.java:165); at org.apache.logging.log4j.core.appender.OutputStreamManager.writeToDestination(OutputStreamManager.java:250); at org.apache.logging.log4j.core.appender.OutputStreamManager.flushBuffer(OutputStreamManager.java:283); at org.apache.logging.log4j.core.appender.OutputStreamManager.flush(OutputStreamManager.java:294); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.directEncodeEvent(AbstractOutputStreamAppender.java:217); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.tryAppend(AbstractOutputStreamAppender.java:208); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:199); at org.apache.logging.log4j.core.config.AppenderControl.tryCallAppender(AppenderControl.java:161); ```. And the line of our code that triggers the logger appender:; ```; is.hail.JVMEntryway$2.run(JVMEntryway.java:139); ```. On that line, we should have already evaluated line 97:; ```; QoBOutputStreamManager.changeFileInAllAppenders(logFile);; ```; Which updates the filename for all `QoBOutputStreamManager`s. We should be the only ones allocating `QoBOutputStreamManager` (it has no magic annotations, we don't pass its constructor anywhere). We should only allocate `QoBOutputStreamManager` in its associated object. We always put it into the map in `getInstance`. We don't synchronize the other methods though, so that could be the issue? If we have a stale version of that map?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13242#issuecomment-1703383030:1611,allocate,allocate,1611,https://hail.is,https://github.com/hail-is/hail/issues/13242#issuecomment-1703383030,1,['allocate'],['allocate']
Energy Efficiency,"Here's a little algorithm I think is a core piece of implementing producting interval joins efficiently. It takes an iterator of intervals, sorted lexicographically (by left endpoint, then right endpoint), and produces an iterator of `(Interval, Array[Interval])` pairs. The intervals `i` of the pairs `(i, a)` are guaranteed to be disjoint (and still sorted), and `a` will contain all intervals from the original iterator containing `i`. The algorithm maintains a min-heap of intervals, with the weight of an interval given by the right endpoint, i.e. the interval with least right endpoint is at the top of the heap. The following pseudocode shows how to produce each `(Interval, Array[Interval])` pair, given the input iterator `it`.; ```; i = it.next(); while (heap.top disjoint i); heap.pop(); heap.push(i); while (it.head.left == i.left); heap.push(it.next()); iOut := Interval(i.left, min(heap.top.right, it.head.left)); emit(iOut, heap.toArray); ```. It's easy to see that at the time of the last line, `heap` contains all and only those intervals which contain `iOut`, and that `iOut` is the intersection of the intervals in `heap`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4270#issuecomment-481240768:92,efficient,efficiently,92,https://hail.is,https://github.com/hail-is/hail/issues/4270#issuecomment-481240768,1,['efficient'],['efficiently']
Energy Efficiency,"Hey @danking,; Thanks so much for the advice. Your team has been very helpful and responsive. - I made the adjustment to my `create_intervals` function; - Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. I tried your suggestion of writing the data to a table first, but my `cols` field doesn't contain the computed HWE values. These are contained within the `entries` field. Table description is below. I tried modifying the code to what is shown below but I'm still having the same issue. Also tried increasing the RAM to max available per CPU. One thing I noticed is the `mt_hwe_vals` variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'ancestry': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'filters': set<str>; 'variant_qc': struct {; gq_stats: struct {; mean: float64, ; stdev: float64, ; min: float64, ; max: float64; }, ; call_rate: float64, ; n_called: int64, ; n_not_called: int64, ; n_filtered: int64, ; n_het: int64, ; n_non_ref: int64, ; het_freq_hwe: float64, ; p_value_hwe: float64, ; p_value_excess_het: float64; }; 'info': struct {; AC: array<int32>, ; AF: array<float64>, ; AN: int32, ; homozygote_count: array<int32>; }; 'a_index': int32; 'was_split': bool; ----------------------------------------; Entry fields:; 'hwe': struct {; het_freq_hwe: float64, ; p_value: float64; }; ----------------------------------------; Column key: ['ancestry']; Row key: ['locus', 'alleles']; ----------------------------------------; ```. ```python; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)). # T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1674895492:196,efficient,efficient,196,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1674895492,1,['efficient'],['efficient']
Energy Efficiency,"Hi, sorry we missed this -- clearly we're not monitoring issues well. We do support on the forum: https://discuss.hail.is. If this is still an open question, please make a post there!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9837#issuecomment-827850982:46,monitor,monitoring,46,https://hail.is,https://github.com/hail-is/hail/issues/9837#issuecomment-827850982,1,['monitor'],['monitoring']
Energy Efficiency,"Hm, weird. When I try these tests out against default I get:. ```; FatalError: batch id was 2271614; HailException: Hail off-heap memory exceeded maximum threshold: limit 2.25 GiB, allocated 3.35 GiB; Report: 3.4G allocated (192.0K blocks / 3.4G chunks), regions.size = 3, 0 current java objects, thread 9: pool-1-thread-2; is.hail.utils.HailException: Hail off-heap memory exceeded maximum threshold: limit 2.25 GiB, allocated 3.35 GiB; Report: 3.4G allocated (192.0K blocks / 3.4G chunks), regions.size = 3, 0 current java objects, thread 9: pool-1-thread-2; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.annotations.RegionPool.closeAndThrow(RegionPool.scala:58); 	at is.hail.annotations.RegionPool.incrementAllocatedBytes(RegionPool.scala:73); 	at is.hail.annotations.ChunkCache.newChunk(ChunkCache.scala:75); 	at is.hail.annotations.ChunkCache.getChunk(ChunkCache.scala:130); 	at is.hail.annotations.RegionPool.getChunk(RegionPool.scala:96); 	at is.hail.annotations.RegionMemory.allocateBigChunk(RegionMemory.scala:62); 	at is.hail.annotations.RegionMemory.allocate(RegionMemory.scala:96); 	at is.hail.annotations.Region.allocate(Region.scala:332); 	at __C35collect_distributed_array.__m61split_ToArray(Unknown Source); 	at __C35collect_distributed_array.__m54split_StreamFor(Unknown Source); 	at __C35collect_distributed_array.__m49begin_group_0(Unknown Source); 	at __C35collect_distributed_array.apply(Unknown Source); 	at __C35collect_distributed_array.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$2(BackendUtils.scala:31); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$1(BackendUtils.scala:30); 	at is.hail.backend.service.Worker$.$anonfun$main$13(Worker.scala:142); 	at scala.ru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11777#issuecomment-1110147573:181,allocate,allocated,181,https://hail.is,https://github.com/hail-is/hail/pull/11777#issuecomment-1110147573,4,['allocate'],['allocated']
Energy Efficiency,"Hmm. Tim, Chris, `test_tiny_driver_has_tiny_memory` fails on this PR because the client-side runs out of memory. It allocates more than 4GiB. That's weird because `test_tiny_driver_has_tiny_memory` is supposed to fail *in the driver* before you ever get data back into the client. Does this:; ```; hl.utils.range_table(100_000_000, 50).to_pandas(); ```; Execute in a way that doesn't place 5B integers in memory on the driver?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12981#issuecomment-1536794205:116,allocate,allocates,116,https://hail.is,https://github.com/hail-is/hail/pull/12981#issuecomment-1536794205,1,['allocate'],['allocates']
Energy Efficiency,"How are you running the tests? We have the following in `hail/testng.xml`:. ```; <suite name=""SuiteAll"" verbose=""1"">; <test name=""TestAll""> ; <packages> ; <package name=""is.hail.*"">; <exclude name=""is.hail.scheduler""></exclude>; <exclude name=""is.hail.backend.distributed""></exclude>; 	 </package>; </packages>; </test>; </suite>; ```. These tests are explicitly executed by `scheduler/testng.xml`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6734#issuecomment-514880263:206,schedul,scheduler,206,https://hail.is,https://github.com/hail-is/hail/pull/6734#issuecomment-514880263,2,['schedul'],['scheduler']
Energy Efficiency,"How big is the driver here? 4G?. The `50` arg here is partitions, not fields, so storing this in memory should be 800MB for an uncompressed copy. There should be 2-3 copies in memory max, and switching to a compressed buffer spec in CDA will reduce the size of the Array[Array[Byte]] returned from BackendUtils.collectDArray. Basically, I think this should work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12981#issuecomment-1540238954:242,reduce,reduce,242,https://hail.is,https://github.com/hail-is/hail/pull/12981#issuecomment-1540238954,1,['reduce'],['reduce']
Energy Efficiency,Huh. I added you back to broad-ctsa. I'm curious to nail down the requester pays issues. Mmm. The billing monitoring is a bit of a mess. The BQ table should really be in hail-vdc and terraform should be able to create the necessary billing sinks to dump billing data into BQ. I'm not exactly sure what the equivalent tools are in Azure to get billing information.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11053#issuecomment-964686794:106,monitor,monitoring,106,https://hail.is,https://github.com/hail-is/hail/pull/11053#issuecomment-964686794,1,['monitor'],['monitoring']
Energy Efficiency,"Huh. Well, this is a terrible error message, but the short answer is that Hail doesn't support reading directly from an HTTP(S) server. You can either download that file or use a dataset that is available in a cloud storage bucket. In general, you'll want to convert to Hail's native MatrixTable format before you do further analysis anyway. I'll fix this to give a more reasonable error message, but, in general, not all HTTP(S) servers support the Range header which means Hail can't efficiently read from all HTTP(S) servers. If you're looking for public datasets to experiment with, I strongly recommend using the Dense Hail MatrixTable of the HGDP+1KG dataset hosted for free by the three major clouds https://gnomad.broadinstitute.org/downloads#v3-hgdp-1kg.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12280#issuecomment-1270722614:486,efficient,efficiently,486,https://hail.is,https://github.com/hail-is/hail/issues/12280#issuecomment-1270722614,1,['efficient'],['efficiently']
Energy Efficiency,I added a new `trait BroadcastSerializable` that tries to verify classes implementing this trait are only serialized when broadcasting. It works by getting the current stack trace and verifying that serialization only happens within a call to a `broadcast` method on the class. `ReferenceGenome` and `RVDPartitioner` implement `BroadcastSerializable`. @chrisvittal This also reduces the size of the RDD broadcast in the VCF combiner pipeline.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5512#issuecomment-469069004:375,reduce,reduces,375,https://hail.is,https://github.com/hail-is/hail/pull/5512#issuecomment-469069004,1,['reduce'],['reduces']
Energy Efficiency,"I added the Google Cloud Monitoring datasource, which I think is the only thing missing out of this PR. If you're ok with this we should merge this PR and close mine.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10772#issuecomment-905873596:25,Monitor,Monitoring,25,https://hail.is,https://github.com/hail-is/hail/pull/10772#issuecomment-905873596,1,['Monitor'],['Monitoring']
Energy Efficiency,"I agree completely. No point in putting any energy into hail1. I think we can do that sooner rather than later. As soon as the current devel users (basically Robert, Konrad and maybe Laurent) can use hail2 instead of hail1, it goes, even if hail2 is still a bit rough and in flux. (It is devel, after all.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2588#issuecomment-352159175:44,energy,energy,44,https://hail.is,https://github.com/hail-is/hail/pull/2588#issuecomment-352159175,1,['energy'],['energy']
Energy Efficiency,"I agree with the confusion. However, the reason for this structure is because the JobPrivate Instances can't be in a ""pool"". So we decided when we discussed this two weeks ago that the monitor needed to be centralized and do the tasks we defined: monitor instances and handle events.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9772#issuecomment-738471689:185,monitor,monitor,185,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738471689,2,['monitor'],['monitor']
Energy Efficiency,"I agree. I think the regulation has to happen in two ways: if it receives too much work, it should reject the work so it doesn't get overloaded. If it is consistently getting too much work, it should reduce the size of the worker pool to generate less work for itself.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8149#issuecomment-592061652:200,reduce,reduce,200,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-592061652,1,['reduce'],['reduce']
Energy Efficiency,I also added a PV and PVC count monitor to Grafana,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6366#issuecomment-502703328:32,monitor,monitor,32,https://hail.is,https://github.com/hail-is/hail/pull/6366#issuecomment-502703328,1,['monitor'],['monitor']
Energy Efficiency,I also had to leave ensure_future(put_on_ready) in `deactivate` and `schedule`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7347#issuecomment-544694599:69,schedul,schedule,69,https://hail.is,https://github.com/hail-is/hail/pull/7347#issuecomment-544694599,1,['schedul'],['schedule']
Energy Efficiency,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418:278,monitor,monitoring,278,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418,2,"['monitor', 'reduce']","['monitoring', 'reduces']"
Energy Efficiency,I am unfamiliar with Hail's test infrastructure so it would be more time efficient for the maintainers to add a test themselves.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14700#issuecomment-2397555427:73,efficient,efficient,73,https://hail.is,https://github.com/hail-is/hail/pull/14700#issuecomment-2397555427,1,['efficient'],['efficient']
Energy Efficiency,"I am using a cluster with a PBS scheduler. Hail and my files are located in my home directory which is on a mounted NFS. The same NFS is mounted, and accessible, on the worker nodes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/902#issuecomment-251762194:32,schedul,scheduler,32,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251762194,1,['schedul'],['scheduler']
Energy Efficiency,"I can write an RFC for how to do this with regards to billing updates and the database. I don't think it's too difficult, but it will take a bit of work to add some new metadata that says whether a resources is `by_time` or `by_unit` and compute usage accordingly per billing update. If we are just using the bytes uploaded and downloaded that are tracked by the resource usage monitor, then I think we can do a first pass at adding this functionality. If we have to track by IP address, I don't know how to do that and would have to look into it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13428#issuecomment-1692021482:378,monitor,monitor,378,https://hail.is,https://github.com/hail-is/hail/issues/13428#issuecomment-1692021482,1,['monitor'],['monitor']
Energy Efficiency,I changed the scheduler query to be the exact same as the control loop query. I verified this had a big impact on scheduling efficiency.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1264174190:14,schedul,scheduler,14,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1264174190,2,['schedul'],"['scheduler', 'scheduling']"
Energy Efficiency,"I checked out your branch, ran `make install-hailctl`, started a cluster, connected to a notebook, and ran `hl.utils.range_table(1_000_000, 10000)._force_count()`. Did not see any monitor UI show up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7087#issuecomment-532861246:180,monitor,monitor,180,https://hail.is,https://github.com/hail-is/hail/pull/7087#issuecomment-532861246,1,['monitor'],['monitor']
Energy Efficiency,I couldn't figure out how to get the gcloud and gsutil binaries into the docker container. This adds about 400 MB to the docker image. I'll make a to-do item to figure out how to reduce the image size further.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7307#issuecomment-542398239:179,reduce,reduce,179,https://hail.is,https://github.com/hail-is/hail/pull/7307#issuecomment-542398239,1,['reduce'],['reduce']
Energy Efficiency,"I created a separate PR to prove to ourselves that it actually triggers the behavior: https://github.com/hail-is/hail/pull/13400. I'm skeptical this will trigger the behavior. `submit` will wait for the bunch to be durably added, right? In that case, there's a *happens before* relationship between adding the first bunch and adding the second bunch. I think we need 10s of bunches to add in parallel so that the db is experiencing enough load that at least one bunch has reserved its job indices but its jobs have not been added while at the same time *at least two jobs* bunch is getting scheduled.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13399#issuecomment-1671784481:590,schedul,scheduled,590,https://hail.is,https://github.com/hail-is/hail/pull/13399#issuecomment-1671784481,1,['schedul'],['scheduled']
Energy Efficiency,"I deleted the pod in question, and then I see these k8s events:; ```; 1m 1m 1 batch-12728-job-287-742170.15c2402851e4fd08 Pod Warning FailedScheduling default-scheduler Binding rejected: Operation cannot be fulfilled on pods/binding ""batch-12728-job-287-742170"": pod batch-12728-job-287-742170 is being deleted, cannot be assigned to a host; 1m 1m 1 batch-12728-job-287-742170.15c2402851ed033d Pod Warning FailedScheduling default-scheduler skip schedule deleting pod: batch-pods/batch-12728-job-287-742170; 1m 1m 1 batch-12728-job-287-742170.15c2402853df7089 Pod Normal Scheduled default-scheduler Successfully assigned batch-pods/batch-12728-job-287-742170 to gke-vdc-non-preemptible-pool-0106a51b-qz7f; 1m 1m 1 batch-12728-job-287-742170.15c24029e9da8a8e Pod Normal SuccessfulAttachVolume attachdetach-controller AttachVolume.Attach succeeded for volume ""pvc-167a0df6-d011-11e9-92a9-42010a800041"" ; 1m 1m 1 batch-12728-job-287-742170.15c2402aa05ad63c Pod spec.initContainers{setup} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Container image ""google/cloud-sdk:237.0.0-alpine"" already present on machine; 1m 1m 1 batch-12728-job-287-742170.15c2402aa4bf829c Pod spec.initContainers{setup} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 1m 1m 1 batch-12728-job-287-742170.15c2402aaf3284fb Pod spec.initContainers{setup} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 11s 11s 1 batch-12728-job-287-742170.15c2403c041fbfef Pod spec.containers{main} Normal Pulling kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f pulling image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 10s 10s 1 batch-12728-job-287-742170.15c2403c1523483f Pod spec.containers{main} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Successfully pulled image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 7s 7s 1 batch-12728-job-287-742170.15c2403ccfdae1f7 Pod spec.containers{main} Normal Created kubelet, gke-vdc-non-p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7016#issuecomment-529142368:159,schedul,scheduler,159,https://hail.is,https://github.com/hail-is/hail/issues/7016#issuecomment-529142368,5,"['Schedul', 'schedul']","['Scheduled', 'schedule', 'scheduler']"
Energy Efficiency,"I didn't realize there are two prices for spot instances versus standard instances. We can support both (we currently charge the non-preemptible price), but is that something we want to do? Should still be relatively straightforward. https://cloud.google.com/vpc/pricing-announce-external-ips",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13784#issuecomment-1904656977:118,charge,charge,118,https://hail.is,https://github.com/hail-is/hail/issues/13784#issuecomment-1904656977,1,['charge'],['charge']
Energy Efficiency,I didnt quite fix all of the select for updates when getting the instance state. Dont merge until I get a chance to fix it (computer ran out of battery),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7833#issuecomment-573028759:145,battery,battery,145,https://hail.is,https://github.com/hail-is/hail/pull/7833#issuecomment-573028759,1,['battery'],['battery']
Energy Efficiency,"I don't think it matters too much, but we should aim for consistency. In frozen mode, cancelling a batch will flip the database state and I think the canceller loop will cancel running jobs and pending jobs despite being in frozen mode. The scheduler will not schedule Ready jobs that are always_run though. If we did not allow cancellation in frozen mode, then the existing running jobs would run until completion which might cost users money that is unnecessary. I guess writing this out now I'm in favor of giving the users the option to cancel as it's not really impacted by the driver being in frozen mode and will save the user money. Although, the reason we'd put Batch in frozen mode is because there's a major problem and we don't want to add stress to the system. Thoughts now that I've written this out?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12145#issuecomment-1249545177:241,schedul,scheduler,241,https://hail.is,https://github.com/hail-is/hail/pull/12145#issuecomment-1249545177,2,['schedul'],"['schedule', 'scheduler']"
Energy Efficiency,"I get. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm ida dtherm tpr_shadow vnmi flexpriority; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm ida dtherm tpr_shadow vnmi flexpriority. Thanks,. Rob K.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1520#issuecomment-285804122:220,monitor,monitor,220,https://hail.is,https://github.com/hail-is/hail/issues/1520#issuecomment-285804122,2,['monitor'],['monitor']
Energy Efficiency,"I got frustrated with the query speeds, tried to figure out how I'd go and implement it efficiently if I were coding this from scratch, searched google for awhile and finally came across what I needed on either Stack Overflow or the MySQL docs. It's a pretty recent feature addition. v8.0.14.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13175#issuecomment-1603255819:88,efficient,efficiently,88,https://hail.is,https://github.com/hail-is/hail/pull/13175#issuecomment-1603255819,1,['efficient'],['efficiently']
Energy Efficiency,"I guess I'm OK accepting this as-is, since I expect us to have the real fix, adaptive branching, in soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11784#issuecomment-1105467838:77,adapt,adaptive,77,https://hail.is,https://github.com/hail-is/hail/pull/11784#issuecomment-1105467838,1,['adapt'],['adaptive']
Energy Efficiency,"I like this table much better! However, it's too wide. I don't know exactly the best way to shrink it down, but here's a few off the cuff thoughts:. I don't think I care about ""Live"", I can do that math myself (it's pending + active, right?). Can we shorten Instances to ""I"" and Cores to ""C"" with abbr tags a la `<abbr title=""Instances"">`?. I don't think I care about schedulable instances, for scheduling I really care about cores. I don't think I care about the cores column, right? ~~Isn't that a synonym for ""active cores""?~~ Ah versioning matters. Hmm. Can we maybe just do `XX / YY` and `ZZ%` columns? It's just too wide to quickly scan this table. I think the most important super-heading is ""Schedulable"", what do you think of putting that at the far left of the table?. If we swap ""Spot"" for ""Preemptible"" that will also shrink the width of the table.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13943#issuecomment-1789625920:368,schedul,schedulable,368,https://hail.is,https://github.com/hail-is/hail/pull/13943#issuecomment-1789625920,3,"['Schedul', 'schedul']","['Schedulable', 'schedulable', 'scheduling']"
Energy Efficiency,"I made some edits that I think helped to reduce the number of places that are aware of the notion of a default_region. It's really now just isolated to the `InstanceCollectionManager`, since that's the piece of code that's making the decision ""use the default region when the cluster is small"". I didn't quite like the pattern of retrieving a default from a `LocationMonitor` just to give it right back to the location monitor in the next line. I think this way the `LocationMonitor` API is much simpler, and we can actually remove its `default_location` method entirely as I believe it is no longer used. I can do that in a follow-up PR if you like this approach. One other thing is I wanted to articulate the distinction between the ""region CI needs its jobs in"" and the ""default region that batch will spin up workers in for small clusters"". While they are in practice the same, I found that treating them both as the ""default_region"" tied the logic around jobs for CI closely with internal Batch decisions and made it more confusing for me to reason about. I tried to separate out these two concepts so that in the future when jobs support region-specific scheduling it will be easier to excise the CI-specific code from the Batch scheduler. Another thing that I realized during this process is that Azure has regions and zones just like GCP, though they may differ slightly since we don't need to specify a zone for a VM and such. I am fine with using the term ""location"" to mean ""where we scheduled the VM, either zone or region depending on the cloud provider"", but I would also like to follow up with a sweep that makes this language more precise where possible. For example, the `possible_cloud_locations` function is really just `possible_cloud_regions`, and we could even go so far as mandating a `region` field in the global config instead of having `azure_location` and `gcp_region`, which are synonymous even though named differently. It also leads me to wonder why we only schedule in a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12078#issuecomment-1207095240:41,reduce,reduce,41,https://hail.is,https://github.com/hail-is/hail/pull/12078#issuecomment-1207095240,2,"['monitor', 'reduce']","['monitor', 'reduce']"
Energy Efficiency,I need to see the index page (with the build log) to try to fix this. Everything on that page is rightfully green,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4514#issuecomment-428408138:108,green,green,108,https://hail.is,https://github.com/hail-is/hail/issues/4514#issuecomment-428408138,1,['green'],['green']
Energy Efficiency,"I only saw kube-dns failing, although it is possible there were additional issues. Is k8s OK scheduling system pods on preemptibles? I hadn't considered it before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7784#issuecomment-571213114:93,schedul,scheduling,93,https://hail.is,https://github.com/hail-is/hail/pull/7784#issuecomment-571213114,1,['schedul'],['scheduling']
Energy Efficiency,I particularly like that a `PropertySuite` would reduce the possibility of human error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/649#issuecomment-241139977:49,reduce,reduce,49,https://hail.is,https://github.com/hail-is/hail/pull/649#issuecomment-241139977,1,['reduce'],['reduce']
Energy Efficiency,"I prefer reworking count, killing the genotypes parameter, so that it's always just a simple/efficient way to get (nSamples, nVariants, nGenotypes, nCalled, callRate). I don't see why a tuple is better than a dict.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1505#issuecomment-284860282:93,efficient,efficient,93,https://hail.is,https://github.com/hail-is/hail/issues/1505#issuecomment-284860282,1,['efficient'],['efficient']
Energy Efficiency,I put the WIP tag on this. I don't have the energy to debug any failures today. Will merge it on Monday.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9241#issuecomment-674186071:44,energy,energy,44,https://hail.is,https://github.com/hail-is/hail/pull/9241#issuecomment-674186071,1,['energy'],['energy']
Energy Efficiency,I read the assignment wrong. It should be in integral powers.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8679#issuecomment-622011492:54,power,powers,54,https://hail.is,https://github.com/hail-is/hail/pull/8679#issuecomment-622011492,1,['power'],['powers']
Energy Efficiency,"I suggest that you might be able to debug this by tweaking in ways which stimulate the; large-block and transition-between-chunks behavior more frequently, e.g. - reduce the chunk size to 256 bytes. - maybe do two malloc()s for each chunk, to try to force malloc() to give you non-contiguous; addresses.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3655#issuecomment-397121048:163,reduce,reduce,163,https://hail.is,https://github.com/hail-is/hail/pull/3655#issuecomment-397121048,1,['reduce'],['reduce']
Energy Efficiency,"I tested this on my branch that had a bunch of deadlock errors and those were replaced with CallError in schedule job because the job was running, cancelled, or instance not active.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7782#issuecomment-568575288:105,schedul,schedule,105,https://hail.is,https://github.com/hail-is/hail/pull/7782#issuecomment-568575288,1,['schedul'],['schedule']
Energy Efficiency,"I tested this with a hard-hitting batch that used a bunch of storage, looked through the UI and didn't get any 500s, and checked the logs on both the k8s pods and the instances for errors. I also commented out each part of the garbage collection loops and made sure everything got cleaned up. For example, commenting out the activity logs loop or the monitor instances loop with the deactivate API point not doing anything.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-942374466:351,monitor,monitor,351,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-942374466,1,['monitor'],['monitor']
Energy Efficiency,"I think fixing this will mask an issue where a 1 CPU job blocks the scheduling of smaller jobs. I'm gonna debug that first, then remove WIP.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9462#issuecomment-694395067:68,schedul,scheduling,68,https://hail.is,https://github.com/hail-is/hail/pull/9462#issuecomment-694395067,1,['schedul'],['scheduling']
Energy Efficiency,"I think maybe I'm overcomplicating the regions thing. Just not specifying regions clearly means you can schedule anywhere. `regions(None)` is confusing, but users should never do that directly. It will only happen when folks are programmatically generating jobs. People doing that are experts who will understand that `None` is just a stand in for ""any region"".",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1270359051:104,schedul,schedule,104,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1270359051,1,['schedul'],['schedule']
Energy Efficiency,I think that's exactly what happened. It rebuilt on its own accord and eventually turned green. Thanks!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1463#issuecomment-283841404:89,green,green,89,https://hail.is,https://github.com/hail-is/hail/pull/1463#issuecomment-283841404,1,['green'],['green']
Energy Efficiency,"I think this PR is just about as good as it's going to get for now. From looking at the Grafana API metrics, I think I was hitting the maximum scheduler throughput. The get running cancellable jobs is around 40ms each call for 5000 jobs while the getting the job head queue is 123ms. If the 40ms becomes a problem, then we can pull less records (see explanation below) or we can not do a json array agg and figure out the regions using bit shifting. When we did the load tests yesterday getting the job head queue was around 1-2 seconds with us each having 20k records. I think we just have to keep an eye on it. I did some further optimization of the scheduler by allowing it to pull up to 10000 jobs from the database to try and schedule before it hits its fair share of jobs scheduled. This helps a lot with efficiency to use the existing capacity if there are jobs further down the queue that are schedulable. I know it's a bit of a departure from what we've done in the past, but I think since we're going in order of fair share now and pulling more jobs from the database isn't that expensive, then this is fine. Happy to make this number 1000 even. 300 was too small though. Jobs at the front of the queue will eventually be able to run because the next iteration of the autoscaler will create the correct instances for those jobs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1276546928:143,schedul,scheduler,143,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1276546928,5,['schedul'],"['schedulable', 'schedule', 'scheduled', 'scheduler']"
Energy Efficiency,"I think this is a known scheduler bug in Spark 1.5, where cancelled executors are incorrectly counted as failed. This will be fixed by an upgrade that will be installed this week. As a temporary fix, I increased the failed job retry count to 30. You hit this, although I don't see any genuine errors in your job. This is exasperated by jobs where each partition takes a long time to run. You can make the partition size smaller by increasing the number of partitions. I suggest you try it again with `-n 1000`. I increased the retry count in `hail-new-vep` to 50.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/302#issuecomment-210903100:24,schedul,scheduler,24,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-210903100,1,['schedul'],['scheduler']
Energy Efficiency,I think we need it to be offline unless we're willing to tolerate up to 5-10 mins of not being able to cancel a batch and some alerts. The only parts that would be referencing the wrong tables are in the `Canceller` and `notify_batch_complete`. I think scheduling and MJC would just work because we update those stored procedures and don't change the child code. We can shut batch down though for the migration. Seems safest although more of a pain.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13810#issuecomment-1812841477:253,schedul,scheduling,253,https://hail.is,https://github.com/hail-is/hail/pull/13810#issuecomment-1812841477,1,['schedul'],['scheduling']
Energy Efficiency,"I think you're right. I tried a number of things, but I need something to key the column by, and a global has no concept a key (which is why it is a global). I found this very confusing. . Let's say mt.C contains phenotypes for samples 1..n. This is, in my mind, a distributed array, with someone fancy (non-integer) indexing support. Great, but I don't care about that, I just want a distributed array. I want to localize_entries, but this creates a hail Table, which drops my phenotypes, because that's now a table and not a matrix table (why! all I wanted was to create a new field in my MT with the result of a column aggregation per row). So the natural thing I reach to is storing my phenotypes elsewhere. I think: ""I want to continue benefitting from Hail query planner), so I try not to materialize the phenotypes in memory. If I say mt.annotate_globals(Y = mt.C) I expect that to just work, because in my mind, I took something that was a a distributed array, but with more powerful indexing support, and converted it to something that is even more array like, that I'm going to need to understand how to index myself (which I'm fine with since I'm moving the thing to globals). Alternatively, I could also expect that globals now contains a reference to a new table, that contains only the column index, and value (phenotype), which seems fine. Neither of these options happens. Instead, I need to realize the array in memory on my master, which seems like a potentially bad idea. The bigger problem though is that I want 1 change (simplify indexing or make a reference to the array), and I seem to need 3 (that + memory + loss of distribution). . In short: I want to be able to choose whether I realize the values in memory, not be forced into it. Let me know if there's something I missed!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9121#issuecomment-662693797:983,power,powerful,983,https://hail.is,https://github.com/hail-is/hail/issues/9121#issuecomment-662693797,1,['power'],['powerful']
Energy Efficiency,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:261,monitor,monitoring,261,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522,2,['monitor'],['monitoring']
Energy Efficiency,"I used filters for the following images when I've run the Azure cleanup script, but we should double check these make sense still in light of changing how we use ""cache"" and there aren't any additional images or ones that we don't want to delete that are in this list:. ```; --filter 'auth:.*' \; --filter 'base:.*' \; --filter 'base_spark_3_2:.*' \; --filter 'batch:.*' \; --filter 'batch-driver-nginx:.*' \; --filter 'batch-worker:.*' \; --filter 'benchmark:.*' \; --filter 'blog_nginx:.*' \; --filter 'ci:.*' \; --filter 'ci-intermediate:.*' \; --filter 'ci-utils:.*' \; --filter 'create_certs_image:.*' \; --filter 'echo:.*' \; --filter 'grafana:.*' \; --filter 'hail-base:.*' \; --filter 'hail-build:.*' \; --filter 'hail-buildkit:.*' \; --filter 'hail-run:.*' \; --filter 'hail-run-tests:.*' \; --filter 'hail-pip-installed-python37:.*' \; --filter 'hail-pip-installed-python38:.*' \; --filter 'hail-ubuntu:.*' \; --filter 'memory:.*' \; --filter 'monitoring:.*' \; --filter 'notebook:.*' \; --filter 'notebook_nginx:.*' \; --filter 'prometheus:.*' \; --filter 'service-base:.*' \; --filter 'service-java-run-base:.*' \; --filter 'test-ci:.*' \; --filter 'test-monitoring:.*' \; --filter 'test-benchmark:.*' \; --filter 'website:.*' \; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12211#issuecomment-1255120349:954,monitor,monitoring,954,https://hail.is,https://github.com/hail-is/hail/pull/12211#issuecomment-1255120349,2,['monitor'],['monitoring']
Energy Efficiency,I went down that route once before and the main issue is how to trigger the change in batch state to completed. I couldn't figure out how to do that correctly and efficiently.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039624594:163,efficient,efficiently,163,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039624594,1,['efficient'],['efficiently']
Energy Efficiency,I will reopen this once the billing and fair share scheduler go in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7553#issuecomment-558701848:51,schedul,scheduler,51,https://hail.is,https://github.com/hail-is/hail/pull/7553#issuecomment-558701848,1,['schedul'],['scheduler']
Energy Efficiency,"I'd also recommend that you leave files as BGEN if possible, as it's an extremely efficient format for encoding huge amounts of imputed genotype data.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946#issuecomment-405779805:82,efficient,efficient,82,https://hail.is,https://github.com/hail-is/hail/issues/3946#issuecomment-405779805,1,['efficient'],['efficient']
Energy Efficiency,"I'd propose closing this as a won't-fix. The PLINK format doesn't lend itself well to parallel import, and spending even an engineer-day supporting it in a way that's more efficient than what you wrote above doesn't seem worth it. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3975#issuecomment-422359120:172,efficient,efficient,172,https://hail.is,https://github.com/hail-is/hail/issues/3975#issuecomment-422359120,1,['efficient'],['efficient']
Energy Efficiency,I'm closing this in favor of keeping the fraction of cpus and memory fixed (and taking the max of the requests) until we're scheduling along multiple dimensions.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7498#issuecomment-556059479:124,schedul,scheduling,124,https://hail.is,https://github.com/hail-is/hail/pull/7498#issuecomment-556059479,1,['schedul'],['scheduling']
Energy Efficiency,"I'm confused by the stack depth problem. `reduce` isn't recursive, it forwards to `reduceLeft`:; ```scala; def reduceLeft[B >: A](op: (B, A) => B): B = {; if (isEmpty); throw new UnsupportedOperationException(""empty.reduceLeft""). var first = true; var acc: B = 0.asInstanceOf[B]. for (x <- self) {; if (first) {; acc = x; first = false; }; else acc = op(acc, x); }; acc; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6943#issuecomment-525037589:42,reduce,reduce,42,https://hail.is,https://github.com/hail-is/hail/pull/6943#issuecomment-525037589,4,['reduce'],"['reduce', 'reduceLeft']"
Energy Efficiency,"I'm currently running this branch of CI on a pull request of itself on my own fork of hail, and it nearly passes all tests except for hailtop_batch_* because of requester pays permissions issues and monitoring, because I don't have a service account in my project with all the permissions for broad-ctsa. So unfortunately haven't fully validated that it will _not_ merge a passing PR, but this seemed good enough that we can push it through for azure (since both of these errors are gcp-dependent). If this goes through I can put in a follow-up PR that mirrors the infra resources that CI needs in azure (blob storage, acr permissions, etc.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11053#issuecomment-964437539:199,monitor,monitoring,199,https://hail.is,https://github.com/hail-is/hail/pull/11053#issuecomment-964437539,1,['monitor'],['monitoring']
Energy Efficiency,"I'm going to add a clouds scope for RunImage steps today. Maybe that will help for the monitoring situation for now? Won't work for GCP but would be good for Azure. When we have what we want done for Thanksgiving, I can revisit this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11053#issuecomment-965093937:87,monitor,monitoring,87,https://hail.is,https://github.com/hail-is/hail/pull/11053#issuecomment-965093937,1,['monitor'],['monitoring']
Energy Efficiency,I'm having trouble with finding examples of codegen to improve for these types. It seems that a lot of our way of generating code here is to apply functions from our function registry and adapting them to use PCode seems to be a much larger project. I think I need to change how those work before I can do the rest of the PCode changes in such a way that benefits our code changes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8348#issuecomment-603354428:188,adapt,adapting,188,https://hail.is,https://github.com/hail-is/hail/pull/8348#issuecomment-603354428,1,['adapt'],['adapting']
Energy Efficiency,"I'm not seeing the leak. `MemoryBuffer.clear` only zeroes the `pos` and `end` variables, and all the allocated memory is in the java heap. If anything, maybe you want to do `cb.assign(lazyBuffer, Code._null)`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12040#issuecomment-1191668397:101,allocate,allocated,101,https://hail.is,https://github.com/hail-is/hail/pull/12040#issuecomment-1191668397,1,['allocate'],['allocated']
Energy Efficiency,"I'm not sure why this is still failing. It looks like the CI jobs are all succeeding, but the last jobs in the batch aren't getting scheduled. Building the base image on the worker is taking 2 minutes 25 seconds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8440#issuecomment-614876259:132,schedul,scheduled,132,https://hail.is,https://github.com/hail-is/hail/pull/8440#issuecomment-614876259,1,['schedul'],['scheduled']
Energy Efficiency,"I've added some more information. I haven't quite figured out a good way to present all this. There seems to be three distinct things:; - the mounting of secrets to paths in the pods (documented as code in `deployment.yaml`s); - the name of k8s secrets, their contents, and the meaning of the contents (specifically what the applications expect of it). The latter would be best documented with scripts that regenerate the secrets from some root secret. We can [programmatically generate oauth tokens](https://developer.github.com/v3/oauth_authorizations/) (which are different from personal access tokens) with username and password authentication. A recreation script could use one privileged key that has access to username/password for each hail test user. That is used to generate auth-tokens (we might need to adapt our code to use oauth tokens instead of personal access tokens). GCP service account keys can be generated programmatically. Unfortunately, there seems to be a little bit of work involved in using OAuth instead of personal access tokens. We have to register our ""app"". I can look into this sometime soon. I'll create an issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141:815,adapt,adapt,815,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141,1,['adapt'],['adapt']
Energy Efficiency,"I've updated this to allow stream consumers to choose an allocation strategy independently of the producer: the producer is written to allocate temporary regions that it owns, as needed, but the consumer can override that behavior to actually put all temporary allocations in a single region. This is implemented via the two abstract interfaces `StagedRegion` and `StagedOwnedRegion`, which have two implementations, one of which can create temporary regions, and one which cannot. The producer is written using the interface, and the consumer passes in one of the concrete implementations, determining the allocation strategy. Currently, only the non-allocating instance is ever used, which generates the same code we did before, so this should have zero run-time effect, even if we continue to update more stream nodes to use smarter region management. We can figure out separately how to introduce the other strategy (use less memory, but with more allocator overhead) in a controlled way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9106#issuecomment-664591165:135,allocate,allocate,135,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-664591165,1,['allocate'],['allocate']
Energy Efficiency,Is part of the issue that the jobs are slow and there's a lot of them or is it the scheduler not keeping up? One hack we could consider is partial core JVM jobs. This would trivially get us 4x utilization.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11902#issuecomment-1151330418:83,schedul,scheduler,83,https://hail.is,https://github.com/hail-is/hail/pull/11902#issuecomment-1151330418,1,['schedul'],['scheduler']
Energy Efficiency,"It seems odd that I have to get below the fold to see any discussion of the engineering work. I don't know how other engineers look for jobs, but I tend to look for the technical details of what the day-to-day would consist of. Maybe @cseed can weigh in? I don't want to tip the scales with a sample of size 1. This is what I can see on my monitor:; ![screen shot 2017-06-28 at 11 06 59 am](https://user-images.githubusercontent.com/106194/27644588-f27f6862-5bf1-11e7-8b2f-820a7ba31490.png). On my laptop screen it looks like this:; ![screen shot 2017-06-28 at 11 07 56 am](https://user-images.githubusercontent.com/106194/27644640-11e85c5e-5bf2-11e7-91c3-0dc9df976c58.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1948#issuecomment-311690040:340,monitor,monitor,340,https://hail.is,https://github.com/hail-is/hail/pull/1948#issuecomment-311690040,1,['monitor'],['monitor']
Energy Efficiency,It was just a way to try and reduce the duplication in the code. The correct thing to do is to use requests and not have the overhead of an asynchronous library for a simple client. We can have this discussion in #6244.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6243#issuecomment-498329233:29,reduce,reduce,29,https://hail.is,https://github.com/hail-is/hail/pull/6243#issuecomment-498329233,1,['reduce'],['reduce']
Energy Efficiency,"It would need BigQuery access to broad-ctsa, since that's where the data for the monitoring service lives. You can also look at the billing-monitor service account. I'm not sure much beyond that because I don't seem to be able to see broad-ctsa anymore in the console? And I only have IAM permissions in hail-vdc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11053#issuecomment-964628523:81,monitor,monitoring,81,https://hail.is,https://github.com/hail-is/hail/pull/11053#issuecomment-964628523,2,['monitor'],"['monitor', 'monitoring']"
Energy Efficiency,It's currently waiting 7.5 minutes for batch to finish running a trivial job. I think if we're waiting 7.5 minutes for jobs to get scheduled the problem is that the cluster is too small.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5638#issuecomment-474581928:131,schedul,scheduled,131,https://hail.is,https://github.com/hail-is/hail/pull/5638#issuecomment-474581928,1,['schedul'],['scheduled']
Energy Efficiency,"LAPACK/MKL documentation says `WORK` must be an array of length `max(1, LWORK)`. I think the ""official"" way to fix this is to leave `LWORK` as whatever the workspace query returns, and allocate the `WORK` arrray of size `max(1, LWORK)`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10227#issuecomment-807289921:185,allocate,allocate,185,https://hail.is,https://github.com/hail-is/hail/pull/10227#issuecomment-807289921,1,['allocate'],['allocate']
Energy Efficiency,Let's merge this so people don't get horrible error messages for now. I'll make an issue to make typecheck more powerful.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1826#issuecomment-301973652:112,power,powerful,112,https://hail.is,https://github.com/hail-is/hail/pull/1826#issuecomment-301973652,1,['power'],['powerful']
Energy Efficiency,"Let's not turn off memory logs entirely. As we progress to NIST 800-53 compliance we need to log requests that use credentials like this. Instead, let's reduce the volume of bytes per-request. How much money do we spend on logs? We have to view them as necessary cost of business, but one that we can reduce in terms of reducing log volume.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12355#issuecomment-1284220591:153,reduce,reduce,153,https://hail.is,https://github.com/hail-is/hail/pull/12355#issuecomment-1284220591,2,['reduce'],['reduce']
Energy Efficiency,Let's schedule time to discuss in the next few weeks.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14540#issuecomment-2108357152:6,schedul,schedule,6,https://hail.is,https://github.com/hail-is/hail/issues/14540#issuecomment-2108357152,1,['schedul'],['schedule']
Energy Efficiency,Let's try to reproduce with a CPU heavy workload using 16 cores. Then let's try to reduce CPU and RAM requests to 95% of actual requested.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13902#issuecomment-1781835000:83,reduce,reduce,83,https://hail.is,https://github.com/hail-is/hail/issues/13902#issuecomment-1781835000,1,['reduce'],['reduce']
Energy Efficiency,Looks fine to me. We pretty much just use MJS/MJC and rate limit logs from NGINX for monitoring,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11904#issuecomment-1164670659:85,monitor,monitoring,85,https://hail.is,https://github.com/hail-is/hail/pull/11904#issuecomment-1164670659,1,['monitor'],['monitoring']
Energy Efficiency,"Looks like the ops agent [also does logging](https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent) in addition to monitoring. The logging agent we're using now is considered legacy, we should probably switch everything over to this new agent.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13949#issuecomment-1789790392:130,monitor,monitoring,130,https://hail.is,https://github.com/hail-is/hail/pull/13949#issuecomment-1789790392,1,['monitor'],['monitoring']
Energy Efficiency,"Looks like we're getting some intermittent failures, monitoring.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11623#issuecomment-1074129063:53,monitor,monitoring,53,https://hail.is,https://github.com/hail-is/hail/pull/11623#issuecomment-1074129063,1,['monitor'],['monitoring']
Energy Efficiency,Map.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.collection.ExternalAppendOnlyMap.maybeSpill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:158); at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/801#issuecomment-247861703:4632,schedul,scheduler,4632,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703,2,['schedul'],['scheduler']
Energy Efficiency,Moved to Asana to be scheduled.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/442#issuecomment-613650798:21,schedul,scheduled,21,https://hail.is,https://github.com/hail-is/hail/issues/442#issuecomment-613650798,1,['schedul'],['scheduled']
Energy Efficiency,"My concern was that not finding a browser when expected would prevent the server from accepting connections, but you may be right. Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 1m default-scheduler Successfully assigned notebook-worker-9szt8 to gke-vdc-non-preemptible-pool-0106a51b-pgxq; Normal SuccessfulMountVolume 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq MountVolume.SetUp succeeded for volume ""default-token-xl2w9""; Normal Pulling 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq pulling image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Pulled 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Successfully pulled image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Created 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Created container; Normal Started 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq ; NAME READY STATUS RESTARTS AGE; notebook-worker-9szt8 0/1 Running 0 49s. Started container; Warning Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:208,Schedul,Scheduled,208,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218,2,"['Schedul', 'schedul']","['Scheduled', 'scheduler']"
Energy Efficiency,"My large test worked in my namespace. The docs were able to build. They're a bit confusing with the enum object, but I'm not sure how to easily fix it. The key things to look for are the scheduler query matches the sort order of the control loop query. If that's off, then instances will thrash. Once you're good with this then we can do a load test sometime tomorrow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1270642090:187,schedul,scheduler,187,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1270642090,1,['schedul'],['scheduler']
Energy Efficiency,"Now that we have the SQL query monitoring, I would love to also see just the simple comparison of the total number of queries we perform over a 1-minute period under high load. We have the tools now to see just what chunk of overall database communication we are cutting down on, which is an achievement in itself. Just need to run the test!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11346#issuecomment-1036284520:31,monitor,monitoring,31,https://hail.is,https://github.com/hail-is/hail/pull/11346#issuecomment-1036284520,1,['monitor'],['monitoring']
Energy Efficiency,"O: is.hail.backend.service.Worker 09526a168d57dac1a26f8caa4ab49593931ed2ef; 2023-09-27 16:43:10.394 Worker$: INFO: running job 7028/9060 at root gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g= with scratch directory '/batch/83e7aee9e9244f6884b8a84ea81b4c7a'; 2023-09-27 16:43:10.398 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-27 16:43:10.779 WorkerTimer$: INFO: readInputs took 384.743327 ms.; 2023-09-27 16:43:10.779 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-27 16:43:10.787 : INFO: RegionPool: REPORT_THRESHOLD: 2.2M allocated (192.0K blocks / 2.0M chunks), regions.size = 3, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.3M allocated (192.0K blocks / 2.1M chunks), regions.size = 3, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.3M allocated (256.0K blocks / 2.1M chunks), regions.size = 4, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.4M allocated (320.0K blocks / 2.1M chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.286 : INFO: RegionPool: REPORT_THRESHOLD: 28.8M allocated (576.0K blocks / 28.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.657 : INFO: RegionPool: REPORT_THRESHOLD: 30.8M allocated (576.0K blocks / 30.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.683 : INFO: RegionPool: REPORT_THRESHOLD: 32.8M allocated (576.0K blocks / 32.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.709 : INFO: RegionPool: REPORT_THRESHOLD: 32.8M allocated (576.0K blocks / 32.3M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:3977,allocate,allocated,3977,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['allocate'],['allocated']
Energy Efficiency,"OK, I improved the tests two ways:. 1. I allocate a random amount of memory in the region to start so things don't always start at offset 0. 2. I test addRegionValue adding a value at the top level and and a nested level (by allocating a non-unsafe Row when t == TStruct) so it calls through to RVB.addRow. I verified it would have caught the previous errors, and it caught another error (toOff was wrong in addRegionValue because we called currentOffset before allocateRoot). Hopefully good to go now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2299#issuecomment-336949521:41,allocate,allocate,41,https://hail.is,https://github.com/hail-is/hail/pull/2299#issuecomment-336949521,2,['allocate'],"['allocate', 'allocateRoot']"
Energy Efficiency,"OK, I moved the file format test changes to https://github.com/hail-is/hail/pull/11906. This change can go in independently, but #11906 will even out the test job times and make developer experience better. Service backend tests on #11904 which should be representative of a normal PR:. id | name | state | exit_code | duration; -- | -- | -- | -- | --; 118 | test_hail_python_service_backend_0 | Success | Success  | 24 minutes; 119 | test_hail_python_service_backend_1 | Success | Success  | 27 minutes; 120 | test_hail_python_service_backend_2 | Success | Success  | 24 minutes; 121 | test_hail_python_service_backend_3 | Success | Success  | 41 minutes; 122 | test_hail_python_service_backend_4 | Success | Success  | 21 minutes. Service backend tests on this PR (albeit with #11906 which evens out test times):. id | name | state | exit_code | duration; -- | -- | -- | -- | --; 118 | test_hail_python_service_backend_0 | Failed | Failure  (1) | 31 minutes; 119 | test_hail_python_service_backend_1 | Success | Success  | 31 minutes; 120 | test_hail_python_service_backend_2 | Success | Success  | 28 minutes; 121 | test_hail_python_service_backend_3 | Success | Success  | 33 minutes; 122 | test_hail_python_service_backend_4 | Success | Success  | 26 minutes. I think there is almost no effect on service backend test times! We should really see if there's a way to improve the autoscaler & schedule to achieve this on its own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11902#issuecomment-1152508508:1409,schedul,schedule,1409,https://hail.is,https://github.com/hail-is/hail/pull/11902#issuecomment-1152508508,1,['schedul'],['schedule']
Energy Efficiency,"OK, here's the most recent failure https://batch.hail.is/batches/8090848/jobs/21993. Don't be duped by my bad log message! There were zero transient errors. I added a log statement that increments the number of errors and prints that message after *every* error, even if it's not transient. . This time it was partition 20053 (we keep moving earlier?). I forgot to catch and rethrow the error with the toString of the input buffer, but I'm not sure there is much to learn from that anyway. FWIW, 20053 was successful in the two previous executions:; 1. https://batch.hail.is/batches/8069235/jobs/21993; 2. https://batch.hail.is/batches/8083195/jobs/21993. Interestingly the peak bytes are not consistent:; ```; 2023-10-24 19:59:47.756 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=58394624, peakBytesReadable=55.69 MiB, chunks requested=5513, cache hits=5501; 2023-10-24 19:59:47.759 : INFO: RegionPool: FREE: 55.7M allocated (7.7M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 9: pool-2-thread-1; ```; ```; 2023-11-08 19:42:40.000 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=61343744, peakBytesReadable=58.50 MiB, chunks requested=5513, cache hits=5501; 2023-11-08 19:42:40.000 : INFO: RegionPool: FREE: 58.5M allocated (10.5M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 10: pool-2-thread-2; ```. Whatever is causing this bug is rare. Approximately once every 31,000 partitions. The CDA IR is the same except for a couple iruid names and the order of the aggregators in the aggregator array is swapped (collect & take vs take & collect). AFAICT, the GCS Java library doesn't do any streaming verification of the hash. We could compute the CRC32c in a streaming manner and fail if/when we get to the end of the object, but this wouldn't work when we read intervals. I'm really mystified.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385:937,allocate,allocated,937,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385,2,['allocate'],['allocated']
Energy Efficiency,"OK, update from Google: they suggest we check if the preemptible quota is non-zero and assume that if it is non-zero preemptible is in use and if it is zero normal quota is in use. In very rare cases, people can increase and then reduce their preemptible quota and Hail will not work properly. Google wasn't interested in providing an API to detect this case. I'll change this PR accordingly sometime next week.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10588#issuecomment-868865354:230,reduce,reduce,230,https://hail.is,https://github.com/hail-is/hail/pull/10588#issuecomment-868865354,1,['reduce'],['reduce']
Energy Efficiency,"Ok, I'm not sure I'll get to it today, been a bit PR review over-allocated. I can definitely look Monday.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3974#issuecomment-410324020:65,allocate,allocated,65,https://hail.is,https://github.com/hail-is/hail/pull/3974#issuecomment-410324020,1,['allocate'],['allocated']
Energy Efficiency,"Ok, I've split out the element-wise special ops to their own test so now there are two logical groups. I also moved assert_eq and assert_close to the top level to reduce repetitive defs and `self.assertTrue(np.array_equal(...to.numpy(),...)`. Plus a bit more structure in the top docs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3671#issuecomment-392871508:163,reduce,reduce,163,https://hail.is,https://github.com/hail-is/hail/pull/3671#issuecomment-392871508,1,['reduce'],['reduce']
Energy Efficiency,"Ok. I thought about this some more. What you've implemented is essentially a ""taint"" in Kubernetes. I ultimately want both taints and something more complicated that will have to be integrated into the scheduler. I think for now your change is self contained and it will be easy to transform later on without too much complexity or breaking changes for users. I think if you want to rename ""label"" to ""taint"" then that might make it clearer what's going on. cc: @daniel-goldstein",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11879#issuecomment-1145343746:202,schedul,scheduler,202,https://hail.is,https://github.com/hail-is/hail/pull/11879#issuecomment-1145343746,1,['schedul'],['scheduler']
Energy Efficiency,"On the first sentence, RFC would be great, and yeah, having a by_time and by_unit would be generally useful. It might be nice to eventually charge a fixed per-job fee if pre-job costs begin to dominate for short lived jobs. ---. On the last sentence:. There are two major questions, the first of which is much higher priority. We probably need to do a bit of research, at least on the second question. . 1. How can we allow public Internet egress without risking untracked cost? I suspect we must track bytes and charge some, possibly very high, rate. 2. How can we allow public Internet egress at or near the real cost to us?. The second question is complex because Google's egress pricing is complex. To directly respond to your comment: I don't think we need to disaggregate by destination IP address, but we do need to disaggregate by destination ""type & location"". GeoIP _might_ allow us to do this in iptables, we should figure out what is and isn't possible and how hard it would be. ---. The following is distilled from [Network Pricing](https://cloud.google.com/vpc/network-pricing). There are six types of egress:; 1. VM-to-Internet; 2. VM-to-VM or VM-to-Google-Service (which are charged equally); 3. Spanner-to-VM; 4. VM-to-Spanner; 5. GCS-to-VM; 6. VM-to-GCS. Egress types (3) and (5) do not apply to us because hail-vdc does not have Spanner and user jobs cannot read from hail-vdc buckets. Egress types (4) and (6) are slightly ambiguous. We should create a support ticket to verify, but I believe they're charged just like (2). This means we are concerned with just two types of egress:. 1. VM-to-Internet; 2. VM-to-VM / VM-to-Google-Service. Each type has a different cost table based on the _destination location_. In these tables, the cheapest price applies, so, for example, for traffic form us-central1-a to us-central1-a the within-zone price applies, not the within-region price. 1. VM-to-Internet. Prices decrease with more usage.; 1. Standard Tier Networking. For the first 10",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13428#issuecomment-1692168526:140,charge,charge,140,https://hail.is,https://github.com/hail-is/hail/issues/13428#issuecomment-1692168526,2,['charge'],['charge']
Energy Efficiency,One thing I wasn't sure about was whether there should be a global async worker pool or whether each scheduler and the canceller can have their own.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9832#issuecomment-747040862:101,schedul,scheduler,101,https://hail.is,https://github.com/hail-is/hail/pull/9832#issuecomment-747040862,1,['schedul'],['scheduler']
Energy Efficiency,"Oops, sorry, your PR was for the monitoring namespace. I see the problem, CI doesn't have a route for `''`. I'll push a fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536246164:33,monitor,monitoring,33,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536246164,1,['monitor'],['monitoring']
Energy Efficiency,Our production images are mostly tagged with a `deploy-` prefix but there are the `third-party/images.txt` which we need to handle differently. ```; (base) dking@wm28c-761 gar-cleaner % k get pods -o json | jq -r '.items[].spec.containers[].image' | sort -u; ghost:3.0-alpine; prom/prometheus:v2.34.0; us-docker.pkg.dev/hail-vdc/hail/admin-pod:deploy-qd833uw7kcyn; us-docker.pkg.dev/hail-vdc/hail/auth:deploy-crsithjyoxfg; us-docker.pkg.dev/hail-vdc/hail/batch:deploy-kpd6nqk4t25o; us-docker.pkg.dev/hail-vdc/hail/batch:deploy-v1yv8cgd1003; us-docker.pkg.dev/hail-vdc/hail/blog_nginx:deploy-wnrqjf4h6qto; us-docker.pkg.dev/hail-vdc/hail/ci:deploy-du68h4bouvp9; us-docker.pkg.dev/hail-vdc/hail/envoyproxy/envoy:v1.22.3; us-docker.pkg.dev/hail-vdc/hail/grafana/grafana:9.1.4; us-docker.pkg.dev/hail-vdc/hail/monitoring:deploy-ljz4mgjf132m; us-docker.pkg.dev/hail-vdc/hail/notebook:deploy-gmftvyf0op87; us-docker.pkg.dev/hail-vdc/hail/notebook_nginx:deploy-n9uipfhjn3jg; us-docker.pkg.dev/hail-vdc/hail/website:deploy-gb1372nuge4g; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13441#issuecomment-1679603478:806,monitor,monitoring,806,https://hail.is,https://github.com/hail-is/hail/issues/13441#issuecomment-1679603478,1,['monitor'],['monitoring']
Energy Efficiency,"Prometheus storage was only 10Gb, so it filled up after 14 days. By default, Prometheus deletes logs after 15 days. I increased the storage size to 50Gb accordingly. I also decided to use this opportunity to switch Prometheus from a Deployment to a StatefulSet. This meant turning the PersistentVolume for PrometheusStorage in the monitoring.yaml file to a PersistentVolumeClaim within the Prometheus StatefulSet spec. However, the claim was configured to mount at the same location as the previous PersistentVolume, and I did not first delete the old PersistentVolume. As a result, the new 50Gb disk was not initially allocated. I resolved this by deleting the StatefulSet and the old PersistentVolume, then redeploying.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6483#issuecomment-505884681:331,monitor,monitoring,331,https://hail.is,https://github.com/hail-is/hail/issues/6483#issuecomment-505884681,2,"['allocate', 'monitor']","['allocated', 'monitoring']"
Energy Efficiency,"Pushed a couple more changes:; - Make apiVersions consistent, and bring them up to date; - Removed incorrect tolerations on CI and batch. A toleration means you can tolerate the given taint. So CI and batch were being scheduled on preemptibles which I didn't think we want.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7287#issuecomment-541333109:218,schedul,scheduled,218,https://hail.is,https://github.com/hail-is/hail/pull/7287#issuecomment-541333109,1,['schedul'],['scheduled']
Energy Efficiency,"Putting the WIP tag on. If you're good with this, then let's merge tomorrow morning so we don't have a new scheduler running overnight.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1276550134:107,schedul,scheduler,107,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1276550134,1,['schedul'],['scheduler']
Energy Efficiency,"RVDContextIsPointed$.point(package.scala:6); at is.hail.utils.package$.point(package.scala:593); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD.is$hail$sparkextras$ContextRDD$$sparkManagedContext(ContextRDD.scala:129); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:138); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Daniel King: JAR and ZIP are both deleted from google storage. for the record the bad SHA was 5702f5c6b299; ```. A discuss user [is seeing that the JVM crashes when he tries to use hail](http://discuss.hail.is/t/gcp-py4jnetworkerror-answer-from-java-side-is-empty/630):. ```; hl.init(); mt = hl.read_matrix_table('gs://hail-datasets/hail-data/1000_genomes_phase3_autosomes.GRCh37.mt'); mt.describe() ...; mt.rsid.show(5); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNet",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:3125,schedul,scheduler,3125,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186,1,['schedul'],['scheduler']
Energy Efficiency,"Re. the questions about the PCA step, I think you'll be beter off modifying `_hwe_normalized_blanczos`. For one thing, this ensures that PC-AiR always returns results in the same form as normal PCA. More importantly, `_hwe_normalized_blanczos` performs the SVD using a ""tall-skinny matrix"" representation, which is just a table of matrices (2d ndarrays). This is more efficient than using block matrices for several reasons that aren't directly relevant here. The result of the SVD is computed as local numpy ndarrays. Given these forms of the data, projecting the related sampled onto the computed PCs should be straightforward and efficient. But once everything is converted to tables and matrixtables, it's much harder and does a lot of redundant work. Let me know if you want to schedule a time to walk through the PCA internals and where you can plug in to them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14326#issuecomment-1967533230:368,efficient,efficient,368,https://hail.is,https://github.com/hail-is/hail/pull/14326#issuecomment-1967533230,3,"['efficient', 'schedul']","['efficient', 'schedule']"
Energy Efficiency,"Re: this interface:; ```scala; def apply(i: Int): Option[Int] = {; setGenotype(i); if (hasGT) Some(getGT) else None; }; ```; It's entirely for performance reasons. We never want to allocate or process `Option`s anywhere, and there's some overhead we can avoid with calling `setGenotype(i)` twice if we use two methods for `hasGtIdx(i: Int): Boolean ` and `getGtIdx(I: Int): Int`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2368#issuecomment-340901365:181,allocate,allocate,181,https://hail.is,https://github.com/hail-is/hail/pull/2368#issuecomment-340901365,1,['allocate'],['allocate']
Energy Efficiency,"Read through the comments/changes here and they all seem very reasonable. Jackie and I talked and it seems like this is a good approach for our requirements and timeline. A cleaner, more sophisticated approach could be taken in the long-term but this seems to meet our more immediate needs and will likely scale well enough, beyond other parts of our system which also need some love. Would love to see if we can reduce the subqueries and can give another review then.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1270332292:413,reduce,reduce,413,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1270332292,1,['reduce'],['reduce']
Energy Efficiency,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/585#issuecomment-241153168:1211,reduce,reduced,1211,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168,2,['reduce'],['reduced']
Energy Efficiency,"Rebased, this should be ready for review. For the moment, I made the worker type and cores not modifiable. We check if jobs can be scheduled on creation before insertion into the database, using memory/core, so changing the type or cores may make jobs in the database unable to be scheduled. The next step is having the scheduler and instance pool choose the right mix of instances for the workload (based on memory/cpu ratio and requested cores).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7606#issuecomment-557911497:131,schedul,scheduled,131,https://hail.is,https://github.com/hail-is/hail/pull/7606#issuecomment-557911497,3,['schedul'],"['scheduled', 'scheduler']"
Energy Efficiency,"Revised: You can switch the map and collect order to get more parallelism: groupBy, mapValues with computeUpperIndexBounds, collect, shift relative upper bound indices to absolute upper bound indices, zipWithIndex, feed into computeRectangles. Once we have durable partitionStarts on table, the whole pipeline can be done on the workers, with a final reduce to concatenate the blocksToKeep.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3185#issuecomment-379135882:351,reduce,reduce,351,https://hail.is,https://github.com/hail-is/hail/pull/3185#issuecomment-379135882,1,['reduce'],['reduce']
Energy Efficiency,"Right, I was trying to suggest that run_forever appears to catches the thrown errors, and that the issue happens because we hit read_timeout (120s), which is an upstream issue. . Tests: #5065. What I think is happening: Flask is a blocking server, so while it waits that 120s, nothing else can happen. The upstream issue isn't resolved, and it keeps blocking. You see this in the 2nd set in particular (~10 timeout requests happen). While the upstream issue should be solved, I think we should also follow Flask best practices, and preferably use Gunicorn + async/green-thread workers + N kernel threads/processes. . Additionally, I would recommend we test a move to Falcon (keeping Gunicorn). It should be far faster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4984#issuecomment-451216690:564,green,green-thread,564,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-451216690,1,['green'],['green-thread']
Energy Efficiency,"Sadness. I spent two hours writing a longer version of this in browser and lost this comment and all my previous comments due to errant click. So now I'm summarizing in an editor and pasting (the other comments may no longer be relevant):. If I understand correctly, you want to filter the entries table to only include pairs of indicies that on the same contig and within some radius of one another. And you want to compute the minimal set of blocks to cover these pairs, which seems at odds with coalescing intervals. Meditating on your code, I think the core mathematical function to pull out is:. ```; // positions is non-decreasing, radius is non-negative.; // for each index i, compute the largest index j such that position[j] - position[i] <= radius; def computeUpperIndexBounds(positions: Array[Int], radius: Int): Array[Int]; ```. Suppose we have a Table with two fields (both keys), the second of which has type Int. Group by the first field and collect values to get `groupedPositions: Array[Array[Int]]`. Then get the absolute first index of each group with:; ```; val firstIndices = groupedPositions.init.map(_.length).scanLeft(0L)(_ + _); ```; Then:; ```; val rightWindows: Array[(Int, Int)] = (firstIndices, groupedPositions).zipped.map { case (i, positions) =>; positions.zip(computeRightWindows(positions, radius).map(i + _) }. val blocksToKeep = computeRectangles(rightWindows.map( (i, j) => Array(i, j, i, j) ); ```. Since `i <= j` by construction, these are exactly the upper triangular blocks you need. Note that this approach is more general but also more efficient by operating directly on arrays of integers. The incoming table doesn't need to be indexed, just properly ordered. And you no longer need upperTriangularBlocks. It'd be great to also have computeLowerIndexBounds, which you could implement in terms of computeUpperIndexBounds on the reversed and negated array. What do you think? Let me know if I've misunderstood!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3185#issuecomment-379121683:1579,efficient,efficient,1579,https://hail.is,https://github.com/hail-is/hail/pull/3185#issuecomment-379121683,1,['efficient'],['efficient']
Energy Efficiency,Scheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203623,schedul,scheduler,203623,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"ServiceBackend/apiserver is known to not be working right now. It isn't being deployed or maintained. @johnc1231 and @catoverdrive were working on some tasks related to this. Few tasks:; - the global reference state in the JVM backend has to go, and needs to be stored in the Python client. This means reference information needs to be including along with queries.; - Table => CollectDArray lowering needs to be finished so apiserver can use the new `scheduler` to execute pipelines.; - Need to implement GoogleFS on the JVM side. I think someone just needs to take on ""get service backend working again"". As per our quarterly planning discussion, it might make sense to focus on upstream tasks for now (lowering, batch).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7068#issuecomment-531915477:452,schedul,scheduler,452,https://hail.is,https://github.com/hail-is/hail/issues/7068#issuecomment-531915477,1,['schedul'],['scheduler']
Energy Efficiency,"So I don't think this is quite correct. malloc(0) is implementation defined, it either returns a valid pointer which cannot be dereferenced, or 0: https://stackoverflow.com/a/2022402. memcpy with a 0 source is undefined: https://stackoverflow.com/questions/5243012/is-it-guaranteed-to-be-safe-to-perform-memcpy0-0-0. There is some debate in the comments to the first answer about whether any reasonable memcpy implementation would do anything if the length is 0. However, see the second answer in the link above: gcc improves optimization by assuming the undefined behavior cannot happen. So our options seem to be:. 1. Leave this change, but change Memory.malloc to allocate at least one byte (guaranteeing it never returns 0). 2. Revert this change, and assume memcpy with 0 length is safe and weaken the memcpy check. 3. Revert this change, don't assume memcpy of 0 length is safe, and have Memory.memcpy check the length before calling into the Unsafe memcpy implementation. I'm inclined to do 3 in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8970#issuecomment-645082713:667,allocate,allocate,667,https://hail.is,https://github.com/hail-is/hail/pull/8970#issuecomment-645082713,1,['allocate'],['allocate']
Energy Efficiency,"So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I don't have a strong feeling if you want to also add a Python-inspired while loop (although I personally would find the similarities misleading given the required differences, I understand others might feel differently). Your while loop should be naturally implementable in terms of mine, so I also suggest we focus on that first. Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Is Patrick's proposal for extra types written up anywhere? I don't like the idea of complicating the type hierarchy for internal bookkeeping like this. So I'm going to remark that in the code generator it is often natural to build data structures to aid the organization of the code generator, and those data structures need not need to be types/IRs. Given that Recur has to be in tail position, and you know exactly when you're existing the loop (branches that don't contain recur nodes). So the compilation looks like:. ```; set initial loop variables; # fall through into loop; Lloop:; ...; # recur; loop variables = new values; goto Lloop; Lan_exit_branch:; result = compile(branch); goto Lafter; Lanother_exit_branch:; result = compile(other_branch); goto Lafter; Lafter:; use result ...; ```. What I would do is ""peel"" off the ifs and lets (anything else?) that can sit in tail position and build a separate data structure for those nodes which I then traverse to emit the above code. Using the stream interface seems wrong to me also. What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. I'll comment more once I've looked over the code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125:100,power,powerful,100,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125,1,['power'],['powerful']
Energy Efficiency,"So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. FYI, you can't dev deploy monitoring. It's part of the infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-539927584:256,monitor,monitoring,256,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-539927584,1,['monitor'],['monitoring']
Energy Efficiency,"So there's a double regex substitution now in this version. I couldn't figure out how to avoid this without having nice error checking at the exact line there's a problem. For example, `j.command(f'{b}')` right now immediately errors with a nice error message. But if the error checking doesn't come until the massive parallel `_compile` in `Backend.run`, then it will be harder to tell where the error is. I thought about having a `debug_mode` which is on by default that does the double check while the `debug_mode` being off is more efficient.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10716#issuecomment-888459694:536,efficient,efficient,536,https://hail.is,https://github.com/hail-is/hail/pull/10716#issuecomment-888459694,1,['efficient'],['efficient']
Energy Efficiency,"Sorry for a fairly late comment on this PR, but I was wondering about the default configuration:. > CHANGELOG: Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to specify which cloud regions a job can run in. The default value is a job can run in any available region. We're looking forward to the functionality in this PR particularly because we're hoping that it'll allow us to schedule workers in the US, while our Batch deployment is in Australia. However, by default we really need to make sure that workers won't be scheduled in the US, to avoid accidental egress charges, as all our datasets are located in Australia. For processing gnomAD data (which is located in the US), spinning up workers colocated with the data would be fantastic though. Hence we'd really need a configurable default value on the deployment level, I believe:. - Generally allow scheduling in AU + US regions (specifically `australia-southeast1` and `us-central1`).; - By default, pick any region in AU only (in practice `australia-southeast1`).; - Allow jobs to explicitly specify to run in the US (in practice `us-central1`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1275427218:422,schedul,schedule,422,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1275427218,4,"['charge', 'schedul']","['charges', 'schedule', 'scheduled', 'scheduling']"
Energy Efficiency,Sorry for not getting this done quicker. There's two new soon to be PRs in the stack that you can see as commits here:; - [Add infrastructure for updates](https://github.com/hail-is/hail/pull/12010/commits/72ff68e628b97bae439d04d4cb45e8508941e8bb); - [Cleanup adding update id infrastructure](https://github.com/hail-is/hail/pull/12010/commits/6364402e965a4f33248eba21639642e14a6f82be). I'll make PRs for them on Monday once you give me the green light that no other major database changes are needed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1221109305:441,green,green,441,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1221109305,1,['green'],['green']
Energy Efficiency,"TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled be",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205353,schedul,scheduler,205353,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"Thanks for the list @agladstein, that was helpful. (I love the pictures in their docs!). Of that list, I don't think closest could be built using only intersect and merge. I have some thoughts about how we might implement closest, but that looks like the trickiest function to scale up. > But for a Hail user, it would be awesome to have those implemented in a way we can easily call!. Absolutely! We're moving to an organization where as much of the higher level functionality as possible is implemented in Python, in libraries of methods built using the powerful core Hail 0.2 language. I'm just thinking about how much needs to be added to the core language to enable something like a bedtools emulation library to be built.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3302#issuecomment-379294711:556,power,powerful,556,https://hail.is,https://github.com/hail-is/hail/issues/3302#issuecomment-379294711,1,['power'],['powerful']
Energy Efficiency,"Thanks for the review -- that's a much better approach! I've made the change. Happy to say that the patched version has just ingested a 46 million x 1200 VCF without a hitch and in just over an hour, and I'm very much looking forward to seeing what hail can do with the data tomorrow -- thanks for creating such a powerful system!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1066#issuecomment-258820833:314,power,powerful,314,https://hail.is,https://github.com/hail-is/hail/pull/1066#issuecomment-258820833,1,['power'],['powerful']
Energy Efficiency,"That command should be unaffected, but `test-gsa-key` in PR namespaces is no longer the all-powerful `test-665@hail-vdc.iam.gserviceaccount.com` (which I would like to make not all-powerful), but is now `testns-test-418@hail-vdc.iam.gserviceaccount.com` which probably won't have that permission. My bad for missing that. Two questions:. - ~~Shouldn't `delete_gcp_batch_instances` fail if the vm deletion commands fail?~~ Ah there's a `set +e`, we should make that less permissive; - Can that step instead use the batch identity in the PR namespace? The batch identity should by definition have the ability to delete VMs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13554#issuecomment-1737449758:92,power,powerful,92,https://hail.is,https://github.com/hail-is/hail/issues/13554#issuecomment-1737449758,2,['power'],['powerful']
Energy Efficiency,"The #1895 Fast Multiply PR is not merged yet, but the multiplication algorithm used there should be the foundation for a tree-aggregating multiply. In particular, [`BetterBlockMatrix.BlockMatrixMultiplyRDD`](https://github.com/danking/hail/blob/b4dda2386e342afe0da1cb809ce756bddd029074/src/main/scala/org/apache/spark/mllib/linalg/distributed/BetterBlockMatrix.scala). My thinking is to produce a layer of a new rdd, `BlockMatrixTreeMultiplyRDD`, which reduces the number of partitions by an order of magnitude in the manner given above. The description above doesn't account for the situation in which the smaller dimension has more than one block. In that situation, we would not perform any condensation of partitions along the smaller dimension (we can't! if we did there would be too few blocks in the output matrix, `C`) .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1975#issuecomment-313702288:453,reduce,reduces,453,https://hail.is,https://github.com/hail-is/hail/issues/1975#issuecomment-313702288,1,['reduce'],['reduces']
Energy Efficiency,"The current execution of ; ```; mt.group_rows_by(mt.gene); .aggregate(...); ```; will be emitted as a `MatrixMapRows` (to re-key) followed by a `MatrixAggregateRowsByKey`. This means that the dataset will be shuffled _in full_ to re-sort by gene, before doing the efficient collapsing in `MatrixAggregateRowsByKey`. This is really bad. We need to be doing map-side combines. The preferred execution would be one of two options:; 1. scan to compute the OrderedPartitioner for the new key. Aggregate to this partitioner.; 2. Aggregate to a HashPartitioner. Both of these things involve new map-side combiner architecture which we haven't built yet, but this is important.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3645#issuecomment-391497730:264,efficient,efficient,264,https://hail.is,https://github.com/hail-is/hail/issues/3645#issuecomment-391497730,1,['efficient'],['efficient']
Energy Efficiency,"The decoder should only be using generated C++ if the environment flag `ENABLE_CPP_CODEGEN` is set (off by default). The `Region` class is now backed by C++-allocated memory regions, though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-428377634:157,allocate,allocated,157,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-428377634,1,['allocate'],['allocated']
Energy Efficiency,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:189,monitor,monitor,189,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358,6,['monitor'],"['monitor', 'monitors']"
Energy Efficiency,"The formatting does become a bit much. This is black's preferred rendering:; ```; @app.command(); def deploy(; branch: Annotated[str, typer.Option(""--branch"", ""-b"", help=""Fully-qualified branch, e.g., hail-is/hail:feature"")],; steps: Annotated[; List[str],; typer.Option(""--steps"", ""-s"", help=""Comma or space-separated list of steps to run.""),; ],; excluded_steps: Annotated[; List[str],; typer.Option(; ""--excluded_steps"",; ""-e"",; help=""Comma or space-separated list of steps to forcibly exclude. Use with caution!"",; ),; ],; extra_config: Annotated[; List[str],; typer.Option(; ""--extra-config"",; ""-e"",; help=""Comma or space-separated list of key=value pairs to add as extra config parameters."",; ),; ],; open: Annotated[; bool,; typer.Option(""--open"", ""-o"", help=""Open the deploy batch page in a web browser.""),; ],; ):; pass. ```. We can reduce the noise a bit with aliases:; ```; from typing import Annotated as Ann, List; from typer import Opt. @app.command(); def deploy(; branch: Ann[str, Opt(""--branch"", ""-b"", help=""Fully-qualified branch, e.g., hail-is/hail:feature"")],; steps: Ann[; List[str],; Opt(""--steps"", ""-s"", help=""Comma or space-separated list of steps to run.""),; ],; excluded_steps: Ann[; List[str],; Opt(; ""--excluded_steps"",; ""-e"",; help=""Comma or space-separated list of steps to forcibly exclude. Use with caution!"",; ),; ],; extra_config: Ann[; List[str],; Opt(; ""--extra-config"",; ""-e"",; help=""Comma or space-separated list of key=value pairs to add as extra config parameters."",; ),; ],; open: Ann[; bool,; Opt(""--open"", ""-o"", help=""Open the deploy batch page in a web browser.""),; ],; ):; pass; ```. It seems to me that the benefits of real sub-commands and better dead-option linting is worth the extra noise in the function definition.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13109#issuecomment-1570514400:842,reduce,reduce,842,https://hail.is,https://github.com/hail-is/hail/pull/13109#issuecomment-1570514400,1,['reduce'],['reduce']
Energy Efficiency,The other thing I was thinking about is we're going to want to utilize the same infrastructure to address the use case where certain jobs can only run in certain regions due to changes in October for how GCP charges for multi-regional buckets and egress fees across regions even on the same continent. I want to think about how we code that a job can run on any pool in any region compared to only a specific region.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11879#issuecomment-1144858456:208,charge,charges,208,https://hail.is,https://github.com/hail-is/hail/pull/11879#issuecomment-1144858456,1,['charge'],['charges']
Energy Efficiency,"The problem is it takes more than 7 minutes to schedule a trivial CI job and then a trivial deploy job, I could set the retries or try-delay higher, but what else could correct mean?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5639#issuecomment-474817396:47,schedul,schedule,47,https://hail.is,https://github.com/hail-is/hail/pull/5639#issuecomment-474817396,1,['schedul'],['schedule']
Energy Efficiency,The reason for the circularity is because I added this code to get the monitoring in Grafana/Prometheus of instance costs. https://github.com/hail-is/hail/commit/8d4c7a22a19ae0a79527eae790d537cf020c1cca#diff-0eb2a9c198baaf288c71080bf314f821303f734f13b9b15bb9767cad8ba9bbe7R1012-R1022,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956513606:71,monitor,monitoring,71,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956513606,1,['monitor'],['monitoring']
Energy Efficiency,The reason the code ended up this way is because I was trying to eliminate resource usage monitoring for the JVM lifetime (regardless of whether jobs were actually running). I'll give this another shot without this constraint.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13098#issuecomment-1559641028:90,monitor,monitoring,90,https://hail.is,https://github.com/hail-is/hail/pull/13098#issuecomment-1559641028,1,['monitor'],['monitoring']
Energy Efficiency,The reason we didn't expose the other parameters was what if we had 16 core jobs waiting to be scheduled and then we changed the worker pool size to 4 cores.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9285#issuecomment-674986845:95,schedul,scheduled,95,https://hail.is,https://github.com/hail-is/hail/pull/9285#issuecomment-674986845,1,['schedul'],['scheduled']
Energy Efficiency,"There is a kubernetes controlled resource that has been exhausted: CPU. CPU is provided by nodes (`kubectl get nodes`). I agree, users should not see this. The core issue is lack of sufficient CPU. We can resolve this in one of two ways:. 1. add more non-preemptible nodes to the k8s cluster. 2. make the non-preemptible node pool auto-scale. For tutorials, we increase the size of the non-preemptible node pool before the tutorial begins. We shrink it again (to save money) when the tutorial is over. We could enable point two, but we now have two new issues:. 1. AFAIK, the auto-scaler only adds nodes when resources are exhausted. Ergo, we only add more nodes when a pod becomes unscheduleable. When this happens, we must wait for a node to spin up (a couple minutes) to provide sufficient CPU for the pod. There has been work towards teaching the auto-scaler about ""slack"" or ""buffer"" resources, but [the relevant PR was abandoned in Dec 2017](https://github.com/kubernetes/autoscaler/pull/77). 2. Once the node is alive, it needs to download the docker image so it can schedule the pod. We use a `DaemonSet` to ensure that the latest notebook images is always cached on all nodes. Hail's notebook images are very large. They take about two minutes to download. Actions we can take:. 1. Investigate a system for ensuring our cluster always sufficient CPU for another N notebooks. (i.e. have CPU slack); 2. Shrink our notebook images. We need to get off `conda` (for all our projects). Their `jupyter/scipy-notebook` image is 4.16GB. I think this is more of a long-term issue because point 1 is rather tricky and we have only a handful of users right now (so we can set a static cluster size that is sufficiently large).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5269#issuecomment-461152857:1074,schedul,schedule,1074,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461152857,1,['schedul'],['schedule']
Energy Efficiency,"These changes include a performance regression - instead of allocating memory once and filling in each member of the nested value (srvb) we are using struct constructors that allocate e.g. the locus an extra time and copy it into the container. I do not think it's worth creating constructors right now that prevent this regression -- the region value construction is much slower than the java calls here, and the right solution is coming down teh pike -- constructing containers using SStackStruct emit codes will have exactly the semantics we want.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10019#issuecomment-776197644:175,allocate,allocate,175,https://hail.is,https://github.com/hail-is/hail/pull/10019#issuecomment-776197644,1,['allocate'],['allocate']
Energy Efficiency,"This is absolutely Jackie's domain and I trust her instinct. A couple thoughts:; - In general, we should transition away from logs for observability. Logs should be for diagnosing bugs.; - Relatedly, the UI should surface the insights we need to understand the cluster.; - If information critical for observability is time-consuming to compute, then we should engineer an efficient way to track it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11357#issuecomment-1039240058:372,efficient,efficient,372,https://hail.is,https://github.com/hail-is/hail/pull/11357#issuecomment-1039240058,1,['efficient'],['efficient']
Energy Efficiency,This is barely for power users. I hate it. And it should die as soon as it can.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11244#issuecomment-1017965355:19,power,power,19,https://hail.is,https://github.com/hail-is/hail/pull/11244#issuecomment-1017965355,1,['power'],['power']
Energy Efficiency,"This is green now. Testing it fully is difficult since we don't have lowered import. However, manually testing both lowered and unlowered versions has produced bit-for-bit identical output, and [test_bgen_export_from_vcf](https://github.com/chrisvittal/hail/blob/e0de6b3f5c91820be94591e95d1a49b67c83cd7f/hail/python/test/hail/methods/test_impex.py#L1521-L1536) does use the lowered execution for export and passes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12185#issuecomment-1251670379:8,green,green,8,https://hail.is,https://github.com/hail-is/hail/pull/12185#issuecomment-1251670379,1,['green'],['green']
Energy Efficiency,This is green!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12706#issuecomment-1464139738:8,green,green,8,https://hail.is,https://github.com/hail-is/hail/pull/12706#issuecomment-1464139738,1,['green'],['green']
Energy Efficiency,"This is not because we forgot to unfreeze CI, we just have simply never added the dockerhub images to azure automatically. The couple that are there now (only 107 and 112) must have been uploaded manually. Because there are some build.yaml steps that run on deploy that are specific to the broad GCP instance (like maybe making a release), non-hail-vdc instances don't run the whole build.yaml pipeline on deploy, but a subset that are specified through terraform (this is how AUS and MS could decide to only deploy a subset of our services e.g. not monitoring. We somewhat recently added a step (separate from the `deploy` step) called `mirror_hailgenetics_images` that was entirely intended so that other hail deployments (including ourselves on Azure!) could pick up the images that we released to dockerhub. I never added that steps to the Azure CI's config. I have done that now. Somehow I had foreseen this incident happening and when it actually did any prior on it disappeared from my brain entirely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1572657390:550,monitor,monitoring,550,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1572657390,1,['monitor'],['monitoring']
Energy Efficiency,"This is what `hailctl` looks like:. ```. Usage: hailctl [OPTIONS] COMMAND [ARGS]... Manage and monitor hail deployments.  Options ;  --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified ;  shell. ;  [default: None] ;  --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified ;  shell, to copy it or customize the ;  installation. ;  [default: None] ;  --help Show this message and exit. ; ;  Commands ;  batch Manage batches running on the batch service managed by the Hail team. ;  config Manage Hail configuration. ;  curl Issue authenticated curl requests to Hail infrastructure. ;  version Print version information and exit. ; ; ```. This is what `hailctl batch submit --help` looks like:. ```. Usage: hailctl batch submit [OPTIONS] SCRIPT [ARGUMENTS]... Submit a batch with a single job that runs SCRIPT with the arguments ARGUMENTS.  Arguments ;  * script PATH Path to the script [default: None] [required] ;  arguments [ARGUMENTS]... [default: None] ; ;  Options ;  --files PATH Files or directories to add to the working directory of the ;  job. ;  [default: None] ;  --name TEXT The name of the batch. ;  --image-name TEXT Name of Docker image for the job ;  [default: (hailgenetics/hail)] ;  --ou",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13109#issuecomment-1561081921:95,monitor,monitor,95,https://hail.is,https://github.com/hail-is/hail/pull/13109#issuecomment-1561081921,3,"['monitor', 'power']","['monitor', 'powershell']"
Energy Efficiency,This needs to be recreated once #2519 (on which this depends) is adapted to handle GenomeReference.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2516#issuecomment-350801948:65,adapt,adapted,65,https://hail.is,https://github.com/hail-is/hail/pull/2516#issuecomment-350801948,1,['adapt'],['adapted']
Energy Efficiency,This should be the right way to turn it off: https://cloud.google.com/monitoring/settings/disable#disable-oagent. I think any small amount of data is from the VM startup time before we shut it off in the startup script,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14015#issuecomment-1814589546:70,monitor,monitoring,70,https://hail.is,https://github.com/hail-is/hail/pull/14015#issuecomment-1814589546,1,['monitor'],['monitoring']
Energy Efficiency,This should be working. I'll add the optimization to not double schedule jobs in another PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7833#issuecomment-572811460:64,schedul,schedule,64,https://hail.is,https://github.com/hail-is/hail/pull/7833#issuecomment-572811460,1,['schedul'],['schedule']
Energy Efficiency,This will probably behave better with this: https://github.com/hail-is/hail/pull/7636. The four was roughly chosen to match the k8s maximum pool size so there is space for test deployments. One problem we're seeing now is preemptible workloads get scheduled on non-preemptible nodes meaning there isn't space for non-preemptible test workloads.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7728#issuecomment-565619961:248,schedul,scheduled,248,https://hail.is,https://github.com/hail-is/hail/pull/7728#issuecomment-565619961,1,['schedul'],['scheduled']
Energy Efficiency,Tim is just really efficient! :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8004#issuecomment-580298708:19,efficient,efficient,19,https://hail.is,https://github.com/hail-is/hail/issues/8004#issuecomment-580298708,1,['efficient'],['efficient']
Energy Efficiency,"Tim, ok, to remove in this PR (in favor of the `allocateAndStoreString` method alone, which is parametrized on `Region` and `String`)? Don't want to surprise you during your review. ```scala; def allocate(region: Region, byteLength: Int): Long. def allocate(region: Code[Region], byteLength: Code[Int]): Code[Long]. def store(addr: Long, str: String). def store(addr: Code[Long], str: Code[String]): Code[Unit]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7904#issuecomment-576360896:48,allocate,allocateAndStoreString,48,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576360896,3,['allocate'],"['allocate', 'allocateAndStoreString']"
Energy Efficiency,TraversableOnce$class.aggregate(TraversableOnce.scala:214); 	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); 	at is.hail.sparkextras.ContextRDD$$anonfun$6$$anonfun$apply$11.apply(ContextRDD.scala:237); 	at is.hail.sparkextras.ContextRDD$$anonfun$6$$anonfun$apply$11.apply(ContextRDD.scala:235); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$6.apply(ContextRDD.scala:235); 	at is.hail.sparkextras.ContextRDD$$anonfun$6.apply(ContextRDD.scala:234); 	at scala.Function1$$anonfun$andThen$1.apply(Function1.scala:52); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; ```. _All_ of my workers had this error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:6145,schedul,scheduler,6145,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['schedul'],['scheduler']
Energy Efficiency,"Turns out we weren't running mypy on auth, gear, monitoring and website, so this triggered a bunch of lints. I fixed them in the last commit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12841#issuecomment-1496518298:49,monitor,monitoring,49,https://hail.is,https://github.com/hail-is/hail/pull/12841#issuecomment-1496518298,1,['monitor'],['monitoring']
Energy Efficiency,"Unless its most recent build failed in one of gcp or azure, it should merge over the course of a day (as Dan said, it can get beat out by other PRs. Was there a time where it was all green and not merging?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12251#issuecomment-1379531238:183,green,green,183,https://hail.is,https://github.com/hail-is/hail/pull/12251#issuecomment-1379531238,1,['green'],['green']
Energy Efficiency,"Update to this, tried running the same script with the bgen file as v1.2 instead (was v1.1 in initial posted issue), but it gives the same issue/stack trace:. ```; SparkException: Job aborted due to stage failure: Task 1.0 in stage 5.0 (TID 2681) had a not serializable result: is.hail.io.bgen.Bgen12GenotypeIterator; Serialization stack:; 	- object not serializable (class: is.hail.io.bgen.Bgen12GenotypeIterator, value: Bgen12GenotypeIterator(0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:; ```; ```; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:591,schedul,scheduler,591,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,4,['schedul'],['scheduler']
Energy Efficiency,"We need to expand union_cols (previously join) to take varargs of dataset, and to execute efficiently.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2664#issuecomment-355828218:90,efficient,efficiently,90,https://hail.is,https://github.com/hail-is/hail/pull/2664#issuecomment-355828218,1,['efficient'],['efficiently']
Energy Efficiency,"We seem to be miscommunicating. My issue is that this change uses a knob (pixel ratio) to control something seemingly unrelated to the knob (opacity). This isn't a browser thing, it's a monitor thing. I have two screens one with device pixel ratio (DPR) 1 and one with DPR 2. Regardless of browser, on the DPR 1 monitor, the lines appear thicker than on the DPR 2 monitor. They must appear thicker! The pixels are bigger and less dense. My suggestion is this: https://github.com/hail-is/hail/compare/master...danking:viz. That change sets line width to 1 and uses opacity to reduce the brightness of the lines. My proposal looks, to me, exactly the same as this PR (which I've dev deployed into my namespace: https://internal.hail.is/dking/site/). I've compared both on my DPR 2 monitor and my DPR 1 monitor. . The reason I prefer the change linked above is that it is self-describing: we want the thinest possible lines that the monitor supports and we want them to be a bit faint.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8964#issuecomment-645699918:186,monitor,monitor,186,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645699918,7,"['monitor', 'reduce']","['monitor', 'reduce']"
Energy Efficiency,"We'll probably switch to 2.12 when there's a PySpark release with 2.12, which there isn't in the 2.4 series (aside from one patch version (2.4.2, nothing else). This has been moved to Asana for task scheduling.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8009#issuecomment-613655651:199,schedul,scheduling,199,https://hail.is,https://github.com/hail-is/hail/issues/8009#issuecomment-613655651,1,['schedul'],['scheduling']
Energy Efficiency,"We, unfortunately, have no satisfactory performance measurement, target, or monitoring story. Currently, when someone makes a change that risks changing the performance of Hail, we first do local timings on large files (I have a 1GB and a 30GB file). If those are satisfactory, we additionally run some timings using a cluster on larger files. Generally, we are only comparing against latest master.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5075#issuecomment-454043009:76,monitor,monitoring,76,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-454043009,1,['monitor'],['monitoring']
Energy Efficiency,"What if all commands were proxied through the gateway api? We already have an authentication layer, with support for 2FA (though in an alpha state, doesn't handle fancier things like merging multiple social accounts). So permissions can be granted on a per-user basis. Currently we can define any scopes for our own APIs, or use Github's available API's (or pass both along to CI). . It may still be useful to have CI grab and validate a gateway-api generated token containing the necessary scopes, although this would add overhead, and may not be as necessary if CI is only available through the gateway api. Another alternative is that it validates against Auth0, but I'd like to reduce/remove that overhead if securely possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4556#issuecomment-449069425:682,reduce,reduce,682,https://hail.is,https://github.com/hail-is/hail/issues/4556#issuecomment-449069425,1,['reduce'],['reduce']
Energy Efficiency,Whatever is failing here is likely different from the interval pipeline failures seen in https://github.com/hail-is/hail/issues/13748 and related tickets because GVS team has confirmed that 0.2.126 reduces peak RAM usage from >50GB to 11GB.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1791064886:198,reduce,reduces,198,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1791064886,1,['reduce'],['reduces']
Energy Efficiency,"Why prefer nodeSelector to tolerations/taints?. I thought we would have two taints: preemptible, non-preemptible. Pods must tolerate at least one (in practice, pods will tolerate no more than one). If a pod has no toleration, it is unscheduable. In this setting, Pods must set nodeSelector to exactly one of preemptible, non-preemptible. If they specify no nodeSelector, they'll be scheduled anywhere. It seems like the main difference is if we forget to specify the kind of pod this is. I feel like we should prefer the unscheduable case, rather than silently working. Are there other motivations for changing to nodeSelector? If someone accidentally tolerates two taints, that will jump out more in code review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636#issuecomment-560586209:382,schedul,scheduled,382,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560586209,1,['schedul'],['scheduled']
Energy Efficiency,Will this actually get scheduled? I thought the max on our 2 core nodes was 1.8 or so.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9423#issuecomment-689115183:23,schedul,scheduled,23,https://hail.is,https://github.com/hail-is/hail/pull/9423#issuecomment-689115183,1,['schedul'],['scheduled']
Energy Efficiency,"Would it be useful to give people repr() feedback on lazy operations? Like ""scheduling x"" and ""executing x""? cc @tpoterba. Would be a separate PR, but related issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7828#issuecomment-572726743:76,schedul,scheduling,76,https://hail.is,https://github.com/hail-is/hail/issues/7828#issuecomment-572726743,1,['schedul'],['scheduling']
Energy Efficiency,"Yeah, it's all in the same JVM process so those variables would have no effect: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala#L97",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583453677:161,schedul,scheduler,161,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583453677,1,['schedul'],['scheduler']
Energy Efficiency,"Yeah, that's a good question, and probably something I should research, address on a Thursday. It would be nice if the structure were flatter. There is an open issue related to this: https://github.com/npm/npm/issues/19770. The file is a bit ridiculous; I should explore using npm 5.5.1 or yarn at some point. Not sure if yarn behavior is better; avoided yarn in this pull request because I want to minimize our use of third party packages to reduce complexity.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-456642632:443,reduce,reduce,443,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-456642632,1,['reduce'],['reduce']
Energy Efficiency,"Yes, DistributedBackend vs. LocalBackend with the assumption that the generic terms would identify our native implementations. I didn't want to call it SchedulerBackend because hopefully eventually everything (shuffle, etc) will tie in here, right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6304#issuecomment-500933118:152,Schedul,SchedulerBackend,152,https://hail.is,https://github.com/hail-is/hail/pull/6304#issuecomment-500933118,1,['Schedul'],['SchedulerBackend']
Energy Efficiency,You should leave the optional/required classes -- those are easy ways to intern a ptype so it never gets allocated more than once. > by adding the final class modifier to PCanonicalString. You can remove the `final` modifier here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7750#issuecomment-567082926:105,allocate,allocated,105,https://hail.is,https://github.com/hail-is/hail/pull/7750#issuecomment-567082926,1,['allocate'],['allocated']
Energy Efficiency,"\n2022-11-15 20:31:41.362 root: INFO: RegionPool: REPORT_THRESHOLD: 56.2M allocated (192.0K blocks / 56.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.368 root: INFO: RegionPool: REPORT_THRESHOLD: 64.2M allocated (192.0K blocks / 64.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.374 root: INFO: RegionPool: REPORT_THRESHOLD: 72.2M allocated (192.0K blocks / 72.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.414 root: INFO: RegionPool: REPORT_THRESHOLD: 128.2M allocated (192.0K blocks / 128.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.506 root: INFO: RegionPool: REPORT_THRESHOLD: 256.2M allocated (192.0K blocks / 256.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.910 root: INFO: RegionPool: REPORT_THRESHOLD: 512.0M allocated (111.9M blocks / 400.1M chunks), regions.size = 5, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:42.730 root: INFO: RegionPool: REPORT_THRESHOLD: 1.2G allocated (439.1M blocks / 781.5M chunks), regions.size = 5, 0 current java objects, thread 8: pool-1-thread-1""}, 'service_backend_debug_info': {'batch_attributes': {'name': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'driver_cores': None, 'driver_memory': None, ...}} or 'batch.worker.jvm_entryway_protocol.EndOfStream' in {'batch_status': {'attributes': {'name': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'closed': True, 'complete': True, ...}, 'job_status': {'attributes': {'name': 'driver'}, 'batch_id': 6627669, 'billing_project': 'test', 'cost': 0.0015413897092729028, ...}, 'log': {'main': ""2022-11-15 20:30:18.004 Tokens: INFO: tokens found for namespaces {default}\n2022-11-15 20:30:18.004 tls: INFO: ssl config file found at /batch/2bbb233e4e3c4a96bbffb515019daac9/secret",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:64046,allocate,allocated,64046,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,1,['allocate'],['allocated']
Energy Efficiency,"\n2022-11-15 20:31:41.368 root: INFO: RegionPool: REPORT_THRESHOLD: 64.2M allocated (192.0K blocks / 64.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.374 root: INFO: RegionPool: REPORT_THRESHOLD: 72.2M allocated (192.0K blocks / 72.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.414 root: INFO: RegionPool: REPORT_THRESHOLD: 128.2M allocated (192.0K blocks / 128.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.506 root: INFO: RegionPool: REPORT_THRESHOLD: 256.2M allocated (192.0K blocks / 256.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.910 root: INFO: RegionPool: REPORT_THRESHOLD: 512.0M allocated (111.9M blocks / 400.1M chunks), regions.size = 5, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:42.730 root: INFO: RegionPool: REPORT_THRESHOLD: 1.2G allocated (439.1M blocks / 781.5M chunks), regions.size = 5, 0 current java objects, thread 8: pool-1-thread-1""}, 'service_backend_debug_info': {'batch_attributes': {'name': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'driver_cores': None, 'driver_memory': None, ...}} or 'batch.worker.jvm_entryway_protocol.EndOfStream' in {'batch_status': {'attributes': {'name': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'closed': True, 'complete': True, ...}, 'job_status': {'attributes': {'name': 'driver'}, 'batch_id': 6627669, 'billing_project': 'test', 'cost': 0.0015413897092729028, ...}, 'log': {'main': ""2022-11-15 20:30:18.004 Tokens: INFO: tokens found for namespaces {default}\n2022-11-15 20:30:18.004 tls: INFO: ssl config file found at /batch/2bbb233e4e3c4a96bbffb515019daac9/secrets/ssl-config/ssl-config.json\n2022-11-15 20:30:18.006 GoogleStorageFS$: INFO: Initializing google storage client from service account key\n2022-11-15 20:30:18.114 root: INFO: RegionPo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:64229,allocate,allocated,64229,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,1,['allocate'],['allocated']
Energy Efficiency,"__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:14498,schedul,schedulerName,14498,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,2,['schedul'],"['scheduler', 'schedulerName']"
Energy Efficiency,"_message_path': '/dev/termination-log',; 'termination_message_policy': 'File',; 'tty': None,; 'volume_devices': None,; 'volume_mounts': [{'mount_path': '/gsa-key',; 'mount_propagation': None,; 'name': 'gsa-key',; 'read_only': None,; 'sub_path': None},; {'mount_path': '/io',; 'mount_propagation': None,; 'name': 'batch-2554-job-4-8vvgl',; 'read_only': None,; 'sub_path': None},; {'mount_path': '/var/run/secrets/kubernetes.io/serviceaccount',; 'mount_propagation': None,; 'name': 'default-token-8h99c',; 'read_only': True,; 'sub_path': None}],; 'working_dir': None}],; 'dns_config': None,; 'dns_policy': 'ClusterFirst',; 'enable_service_links': True,; 'host_aliases': None,; 'host_ipc': None,; 'host_network': None,; 'host_pid': None,; 'hostname': None,; 'image_pull_secrets': None,; 'init_containers': None,; 'node_name': 'gke-vdc-preemptible-pool-9c7148b2-4gq2',; 'node_selector': None,; 'priority': 500000,; 'priority_class_name': 'user',; 'readiness_gates': None,; 'restart_policy': 'Never',; 'runtime_class_name': None,; 'scheduler_name': 'default-scheduler',; 'security_context': {'fs_group': None,; 'run_as_group': None,; 'run_as_non_root': None,; 'run_as_user': None,; 'se_linux_options': None,; 'supplemental_groups': None,; 'sysctls': None},; 'service_account': 'default',; 'service_account_name': 'default',; 'share_process_namespace': None,; 'subdomain': None,; 'termination_grace_period_seconds': 30,; 'tolerations': [{'effect': None,; 'key': 'preemptible',; 'operator': None,; 'toleration_seconds': None,; 'value': 'true'},; {'effect': 'NoExecute',; 'key': 'node.kubernetes.io/not-ready',; 'operator': 'Exists',; 'toleration_seconds': 300,; 'value': None},; {'effect': 'NoExecute',; 'key': 'node.kubernetes.io/unreachable',; 'operator': 'Exists',; 'toleration_seconds': 300,; 'value': None}],; 'volumes': [{'aws_elastic_block_store': None,; 'azure_disk': None,; 'azure_file': None,; 'cephfs': None,; 'cinder': None,; 'config_map': None,; 'downward_api': None,; 'empty_dir': None,; 'fc':",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:4950,schedul,scheduler,4950,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['schedul'],['scheduler']
Energy Efficiency,"```; + make -k check-services; PYTHONPATH=""hail/python:auth:batch:ci:memory:notebook:monitoring:website:gear:web_common"" python3 -m flake8 --config setup.cfg auth; auth/auth/auth.py:515:86: W291 trailing whitespace; make: *** [Makefile:42: check-auth] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12889#issuecomment-1515270732:85,monitor,monitoring,85,https://hail.is,https://github.com/hail-is/hail/pull/12889#issuecomment-1515270732,1,['monitor'],['monitoring']
Energy Efficiency,`gradle test` runs the scheduler tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6734#issuecomment-514891047:23,schedul,scheduler,23,https://hail.is,https://github.com/hail-is/hail/pull/6734#issuecomment-514891047,1,['schedul'],['scheduler']
Energy Efficiency,`kubectl describe pod POD_NAME` will tell you there reasons the pod could not be scheduled. I often see this issue when we run out of available CPU.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5269#issuecomment-461105202:81,schedul,scheduled,81,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461105202,1,['schedul'],['scheduled']
Energy Efficiency,"`large_range_matrix_table_sum()` failed in the benchmarks, looking into that. When I ran it locally, seemed to be allocating more memory than I would think, so there's probably a leak there. Otherwise, I think this is safe to review while I track this one down (and maybe you'll catch the cause of this). ```; 2020-03-26 12:41:14 root: INFO: RegionPool: REPORT_THRESHOLD: 16.0M allocated (792.0K blocks / 15.3M chunks), thread 70: Executor task launch worker for task 10; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8315#issuecomment-604542228:378,allocate,allocated,378,https://hail.is,https://github.com/hail-is/hail/pull/8315#issuecomment-604542228,1,['allocate'],['allocated']
Energy Efficiency,"`pip install -e .`; Defaulting to user installation because normal site-packages is not writeable; Obtaining file:///home/skr/hail2/hail; Installing build dependencies ... done; Checking if build backend supports build_editable ... done; Getting requirements to build editable ... error; error: subprocess-exited-with-error; ;  Getting requirements to build editable did not run successfully.;  exit code: 1; > [14 lines of output]; error: Multiple top-level packages discovered in a flat-layout: ['tls', 'gear', 'hail', 'auth', 'blog', 'infra', 'batch', 'query', 'docker', 'memory', 'devbin', 'gateway', 'website', 'grafana', 'notebook', 'graphics', 'datasets', 'monitoring', 'web_common', 'prometheus', 'letsencrypt'].; ; To avoid accidental inclusion of unwanted files or directories,; setuptools will not proceed with this build.; ; If you are trying to create a single distribution with multiple packages; on purpose, you should not rely on automatic discovery.; Instead, consider the following options:; ; 1. set up custom discovery (`find` directive with `include` or `exclude`); 2. use a `src-layout`; 3. explicitly set `py_modules` or `packages` with a list of names; ; To find more information, look for ""package discovery"" on setuptools docs.; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; error: subprocess-exited-with-error.  Getting requirements to build editable did not run successfully.;  exit code: 1; > See above for output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12844#issuecomment-1502112290:668,monitor,monitoring,668,https://hail.is,https://github.com/hail-is/hail/issues/12844#issuecomment-1502112290,1,['monitor'],['monitoring']
Energy Efficiency,a.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:222); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 11 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); 	at org.apache.spark.rdd.RDDOperationScope$.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:3610,schedul,scheduler,3610,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['schedul'],['scheduler']
Energy Efficiency,abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1921); at org.apache.spark.rdd.RDD.count(RDD.scala:1125); at org.broadinstitute.hail.driver.Count$.run(Count.scala:37); at org.broadinstitute.hail.driver.Count$.run(Count.scala:9); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Ma,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:7179,schedul,scheduler,7179,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['schedul'],['scheduler']
Energy Efficiency,abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:139); at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1063); at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1127); at is.hail.io.vcf.MatrixVCFReader.coercer$lzycompute(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.coercer(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.apply(LoadVCF.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:6285,schedul,scheduler,6285,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['schedul'],['scheduler']
Energy Efficiency,abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:107); at is.hail.expr.ir.Interpret$.apply(Interpret.scal,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:9540,schedul,scheduler,9540,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['schedul'],['scheduler']
Energy Efficiency,abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:944); at is.hail.expr.ir.functions.MatrixWriteBlockMatrix.execute(MatrixWriteBlockMatrix.scala:47); at is.hail.expr.ir.functions.WrappedMatrixToValueFunction.execute(RelationalFunctions.scala:88); at is.hail.expr.ir.Interpret$.run(Interpret.scala:735); at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); at is.hail.expr.ir.InterpretNonC,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:13417,schedul,scheduler,13417,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['schedul'],['scheduler']
Energy Efficiency,"ache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8949,schedul,scheduler,8949,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['schedul'],['scheduler']
Energy Efficiency,"ache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206888,schedul,scheduler,206888,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,ackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:14); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:167); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:161); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); E 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); E 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.execute$1(EvalRelationalLets.scala:10); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:18); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:32); E 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:157); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scal,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:11101,adapt,adapted,11101,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['adapt'],['adapted']
Energy Efficiency,"action for a current 2.1.0 user:; ```bash; dking@wmb16-359 # gradle -Dspark.verison=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.781 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); 2.1.0; dking@wmb16-359 # gradle -Dspark.version=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 42. * What went wrong:; A problem occurred evaluating root project 'hail'.; > The spark version must now be explicitly specified in the `gradle.properties`; file. Do *not* specify it with `-Dspark.version`. This version *must* match the; version of the spark installed on the machine or cluster that will execute; hail. You can override the setting in `gradle.properties` with a command line; like:. ./gradlew -PsparkVersion=2.1.1 shadowJar. The previous implicit, default spark version was 2.0.2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.778 secs; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total time: 4.418 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020:1577,schedul,scheduled,1577,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020,1,['schedul'],['scheduled']
Energy Efficiency,"ad 10: pool-2-thread-2; 2023-09-27 16:43:11.709 : INFO: RegionPool: REPORT_THRESHOLD: 32.8M allocated (576.0K blocks / 32.3M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:44:22.426 GoogleStorageFS$: INFO: createNoCompression: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd8-4f1b-bcde-fbf29180c347; 2023-09-27 16:44:22.495 GoogleStorageFS$: INFO: close: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd8-4f1b-bcde-fbf29180c347; 2023-09-27 16:44:22.620 GoogleStorageFS$: INFO: closed: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd8-4f1b-bcde-fbf29180c347; 2023-09-27 16:44:22.621 : INFO: TaskReport: stage=0, partition=7028, attempt=0, peakBytes=62266032, peakBytesReadable=59.38 MiB, chunks requested=72126, cache hits=72121; 2023-09-27 16:44:22.622 : INFO: RegionPool: FREE: 59.4M allocated (25.2M blocks / 34.2M chunks), regions.size = 11, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:44:22.623 WorkerTimer$: INFO: executeFunction took 71843.446957 ms.; 2023-09-27 16:44:22.623 GoogleStorageFS$: INFO: createNoCompression: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=/result.7028; 2023-09-27 16:44:22.668 GoogleStorageFS$: INFO: close: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=/result.7028; 2023-09-27 16:44:33.788 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:5777,allocate,allocated,5777,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['allocate'],['allocated']
Energy Efficiency,"ad-1\n2022-11-15 20:31:41.351 root: INFO: RegionPool: REPORT_THRESHOLD: 40.2M allocated (192.0K blocks / 40.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.357 root: INFO: RegionPool: REPORT_THRESHOLD: 48.2M allocated (192.0K blocks / 48.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.362 root: INFO: RegionPool: REPORT_THRESHOLD: 56.2M allocated (192.0K blocks / 56.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.368 root: INFO: RegionPool: REPORT_THRESHOLD: 64.2M allocated (192.0K blocks / 64.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.374 root: INFO: RegionPool: REPORT_THRESHOLD: 72.2M allocated (192.0K blocks / 72.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.414 root: INFO: RegionPool: REPORT_THRESHOLD: 128.2M allocated (192.0K blocks / 128.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.506 root: INFO: RegionPool: REPORT_THRESHOLD: 256.2M allocated (192.0K blocks / 256.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.910 root: INFO: RegionPool: REPORT_THRESHOLD: 512.0M allocated (111.9M blocks / 400.1M chunks), regions.size = 5, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:42.730 root: INFO: RegionPool: REPORT_THRESHOLD: 1.2G allocated (439.1M blocks / 781.5M chunks), regions.size = 5, 0 current java objects, thread 8: pool-1-thread-1""}, 'service_backend_debug_info': {'batch_attributes': {'name': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'driver_cores': None, 'driver_memory': None, ...}} or 'batch.worker.jvm_entryway_protocol.EndOfStream' in {'batch_status': {'attributes': {'name': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'clo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:63676,allocate,allocated,63676,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,1,['allocate'],['allocated']
Energy Efficiency,adcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:222); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 11 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:3529,schedul,scheduler,3529,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['schedul'],['scheduler']
Energy Efficiency,"added something that skips the field parsing, but still allocates an entry array.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3448#issuecomment-385763925:56,allocate,allocates,56,https://hail.is,https://github.com/hail-is/hail/issues/3448#issuecomment-385763925,1,['allocate'],['allocates']
Energy Efficiency,ag mismatch!; 	at sun.security.ssl.Alert.createSSLException(Alert.java:133) ~[?:1.8.0_392]; 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:331) ~[?:1.8.0_392]; 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:274) ~[?:1.8.0_392]; 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:269) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:119) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1404) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1372) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:966) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_392]; 	at sun.net.www.MeteredStream.read(MeteredStream.java:134) ~[?:1.8.0_392]; 	at java.io.FilterInputStream.read(FilterInputStream.java:133) ~[?:1.8.0_392]; 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3460) ~[?:1.8.0_392]; 	at com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:164) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_392]; 	at is.hail.relocated.com.google.cloud.storage.StorageByteChannels$ScatteringByteChannelFacade.read(StorageByteChannels.java:242) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.ApiaryUnbufferedReadableByteChannel.read(ApiaryUnbufferedReadableByteChannel.java:113) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:1071,Meter,MeteredStream,1071,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['Meter'],['MeteredStream']
Energy Efficiency,age$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9.apply(RVD.scala:221); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9.apply(RVD.scala:220); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:220); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:218); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSched,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:2593,schedul,scheduler,2593,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['schedul'],['scheduler']
Energy Efficiency,"age$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9.apply(RVD.scala:221); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9.apply(RVD.scala:220); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:220); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:218); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-e6de08e; Error summary: ClassCastException: is.hail.codegen.generated.C14 cannot be cast to is.hail.asm4s.AsmFunction2; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [3c5f402fed564ccd85257c0919d4bffb] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""pyhail.py"", line 128, in <module>; main(args, pass_through_args); File ""pyhail.py"", line 109, in main; subprocess.check_output(job); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 573, in check_output; raise CalledProcessError(retcode, cmd, output=output); subprocess.CalledProcessError: Command '['gcloud', 'datapr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:7112,schedul,scheduler,7112,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['schedul'],['scheduler']
Energy Efficiency,"ain website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1371,schedul,schedule,1371,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878,1,['schedul'],['schedule']
Energy Efficiency,al.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3$adapted(LocalBackend.scala:223); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:144); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:144); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$2(LocalBackend.scala:223); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:249); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$2(LocalBackend.scala:314); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(LocalBackend.scala:309); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1$adapted(LocalBackend.scala:308); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65); 	at is.hail.backend.local.LocalBackend.$anonfun$withExecuteContext$2(LocalBackend.scala:144); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); 	at is.hail.backend.local.LocalBackend.withExecuteContext(LocalBackend.scala:130); 	at is.hail.backend.local.LocalBackend.execute(LocalBackend.scala:308); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpser,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:2764,adapt,adapted,2764,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,4,['adapt'],['adapted']
Energy Efficiency,"al: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: All done; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. Notice:; 1. The total memory available on the machine is less than 52 GiB (= 53,248 MiB), indeed it is a full 1025 MiB below the advertised amount.; 2. Once all the components of the Dataproc cluster have started (but before any Hail Query jobs are submitted) the total memory available is already depleted to 42760 MiB. Recall that Hail allocates 41 GiB (= 41,984 MiB) to its JVM. This leaves the Python process and all other daemons on the system only 776 MiB of excess RAM. For reference `python3 -c 'import hail'` needs 206 MiB. ---. We must address this situation. It seems safe to assume that the system daemons will use a constant 9.5 GiB of RAM. Moreover the advertised RAM amount is at least 1 GiB larger than reality. I propose:; 1. The driver memory calculation in `hailctl dataproc` should take the advertised RAM amount, subtract 10.5 GiB, and then use 90% of the remaining value. For an n1-highmem-8, that reduces our allocation from 41 GiB to 37 GiB yielding an additional 4GiB to Python and deamon memory fluctuations.; 2. AoU RWB needs to review its memory settings for Spark driver nodes to ensure that the JVM is set to an appropriate maximum heap size. For what it's worth, I think the reason we didn't get an outcry from our local scientific community is that many of them have transitioned to Query-on-Batch where we have exact and total control over the memory available to the driver and the workers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790:3074,reduce,reduces,3074,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790,1,['reduce'],['reduces']
Energy Efficiency,also `org.apache.spark.scheduler.DAGScheduler.submitJob`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4526#issuecomment-431683286:23,schedul,scheduler,23,https://hail.is,https://github.com/hail-is/hail/issues/4526#issuecomment-431683286,1,['schedul'],['scheduler']
Energy Efficiency,"analysis_type=VariantFiltration input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.indels.unfiltered.vcf) mask=(RodBinding name= source=UNBOUND) out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub filterExpression=[FS>200.0, QD<2.0, ReadPosRankSum<-20.0, InbreedingCoeff<-0.8] filterName=[Indel_FS, Indel_QD, Indel_ReadPosRankSum, Indel_InbreedingCoeff] genotypeFilterExpression=[] genotypeFilterName=[] clusterSize=3 clusterWindowSize=0 maskExtension=0 ma",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:20004,monitor,monitorThreadEfficiency,20004,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['monitor'],['monitorThreadEfficiency']
Energy Efficiency,apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:2381,schedul,scheduler,2381,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['schedul'],['scheduler']
Energy Efficiency,apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cance,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203278,schedul,scheduler,203278,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAG,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201153,schedul,scheduler,201153,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204800,schedul,scheduler,204800,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"aproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""workerConfig"": {; ""numInstances"": 2,; ""instanceNames"": [; ""cluster-2-w-0"",; ""cluster-2-w-1""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:5037,reduce,reduce,5037,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,1,['reduce'],['reduce']
Energy Efficiency,arnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199976,schedul,scheduler,199976,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,array_agg with hl.agg.sum is probably as efficient as this?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5361#issuecomment-463885950:41,efficient,efficient,41,https://hail.is,https://github.com/hail-is/hail/pull/5361#issuecomment-463885950,1,['efficient'],['efficient']
Energy Efficiency,"at immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--async`, `--bucket`, etc. And more minor:; - For `hailctl dataproc start` especially, it could seem pretty arbitrary to a user which arguments go before `--` vs after. For example, `--num-worker-local-ssds` is a `hailctl dataproc start` argument, but `--num-secondary-worker-local-ssds` is not. This could cause some confusion/annoyance. This could potentially be reduced by minimizing the number of `gcloud dataproc` arguments that are defined as `hailctl dataproc` arguments. For example, `--num-worker-local-ssds` is just passed through to `gcloud`, so there's no real need for it to be a `hailctl dataproc start` argument. On the other hand, it is nice to have some of those `gcloud` arguments show up in `hailctl dataproc start --help`.; - While making breaking changes, it would be nice if the `--configuration`/`--gcloud-configuration` argument was consistent across `hailctl dataproc` commands. For example, currently `hailctl dataproc start` takes `--configuration` but `hailctl dataproc submit` takes `--gcloud-configuration`. It would also be nice to standardize on kebab case for all arguments. There are some other arguments that use underscores (ex. `--init_timeout` to `hailctl dataproc start`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:2360,reduce,reduced,2360,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034,1,['reduce'],['reduced']
Energy Efficiency,at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); 	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); 	at is.hail.sparkextras.ContextRDD$$anonfun$6$$anonfun$apply$11.apply(ContextRDD.scala:237); 	at is.hail.sparkextras.ContextRDD$$anonfun$6$$anonfun$apply$11.apply(ContextRDD.scala:235); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$6.apply(ContextRDD.scala:235); 	at is.hail.sparkextras.ContextRDD$$anonfun$6.apply(ContextRDD.scala:234); 	at scala.Function1$$anonfun$andThen$1.apply(Function1.scala:52); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:9740,schedul,scheduler,9740,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,2,['schedul'],['scheduler']
Energy Efficiency,at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:2282,schedul,scheduler,2282,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['schedul'],['scheduler']
Energy Efficiency,at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1921); at org.apache.spark.rdd.RDD.count(RDD.scala:1125); at org.broadinstitute.hail.driver.Count$.run,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6588,schedul,scheduler,6588,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['schedul'],['scheduler']
Energy Efficiency,at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RD,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12826,schedul,scheduler,12826,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['schedul'],['scheduler']
Energy Efficiency,"at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205563,schedul,scheduler,205563,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:4082,schedul,scheduler,4082,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,2,['schedul'],['scheduler']
Energy Efficiency,ator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5589,schedul,scheduler,5589,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['schedul'],['scheduler']
Energy Efficiency,ava:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.m,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:11566,schedul,scheduler,11566,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['schedul'],['scheduler']
Energy Efficiency,"be9d88a80695b04a2a9eb5826361e0897d94c042; 2023-09-13 16:37:36.614 Worker$: INFO: running job 38854/47960 at root gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho= with scratch directory '/batch/1c00c7157d4d41bcbf508f12d75329b1'; 2023-09-13 16:37:36.617 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-13 16:37:36.821 services: WARN: A limited retry error has occured. We will automatically retry 4 more times. Do not be alarmed. (next delay: 1938). The most recent error was javax.net.ssl.SSLException: Connection reset.; 2023-09-13 16:37:38.893 WorkerTimer$: INFO: readInputs took 2278.496020 ms.; 2023-09-13 16:37:38.893 : INFO: RegionPool: initialized for thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 : INFO: TaskReport: stage=0, partition=38854, attempt=0, peakBytes=65536, peakBytesReadable=64.00 KiB, chunks requested=0, cache hits=0; 2023-09-13 16:37:38.903 : INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:2194,allocate,allocated,2194,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,1,['allocate'],['allocated']
Energy Efficiency,"ble with sample IDs and PRS scores; mt.cols().export('gs://ukbb_prs/prs/UKB_'+pheno+'_PRS_22.txt'). parser = argparse.ArgumentParser(); parser.add_argument(""--phenotype"", help=""name of the sumstat phenotype""); args = parser.parse_args(). try:; start = time.time(); main(args.phenotype); end = time.time(); message = ""Success! Job was completed in %s"" % time.strftime(""%H:%M:%S"", time.gmtime(end - start)); send_message(message); except Exception as e:; send_message(""Fail.""); ```. ""Failure Reason"":; ```; Job aborted due to stage failure: Task 98 in stage 13.0 failed 20 times, most recent failure: Lost task 98.19 in stage 13.0 (TID 22699, ccarey-sw-xt4j.c.ukbb-robinson.internal, executor 68): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:815); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:804); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:914); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:914); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.nex",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:3307,allocate,allocate,3307,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['allocate'],['allocate']
Energy Efficiency,"broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""workerConfig"": {; ""numInstances"": 2,; ""instanceNames"": [; ""cluster-2-w-0"",; ""cluster-2-w-1""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; ""metrics"": {}; }; ```. # Analysis Thus Far. The job doesn't terminate on its own. After stopping",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:5132,reduce,reduce,5132,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,1,['reduce'],['reduce']
Energy Efficiency,"bs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkConte",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206603,schedul,scheduler,206603,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,bump. This would be nice to have for monitoring batch tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7808#issuecomment-573321397:37,monitor,monitoring,37,https://hail.is,https://github.com/hail-is/hail/pull/7808#issuecomment-573321397,1,['monitor'],['monitoring']
Energy Efficiency,"cala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because Sp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205462,schedul,scheduler,205462,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,cala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200941,schedul,scheduler,200941,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,cala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204588,schedul,scheduler,204588,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,cala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; ```. _All_ of my workers had this error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:815); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:804); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:914); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:914); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.nex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:6902,allocate,allocate,6902,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['allocate'],['allocate']
Energy Efficiency,"cc: @cseed Not sure how you feel about this. We've talked about making tests easy to write a few times. This solution isn't prefect because nested structures get allocated twice. For example:. ```scala; addStruct(region, ""foo"", addStruct(region, ""bar"", 3)); ```. Allocates the struct with field ""bar"" first, then allocates the outer struct, copying in the value of the inner struct. I think there's a better way to do this using a builder pattern (basically an AST for RegionValues, which is basically the IR? dunno, there's more thought needed here), but it would take more work than I was willing to put in for my tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2511#issuecomment-348577509:162,allocate,allocated,162,https://hail.is,https://github.com/hail-is/hail/pull/2511#issuecomment-348577509,3,"['Allocate', 'allocate']","['Allocates', 'allocated', 'allocates']"
Energy Efficiency,cheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6483,schedul,scheduler,6483,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['schedul'],['scheduler']
Energy Efficiency,closing until it is necessary to reduce PR burden,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8203#issuecomment-604461227:33,reduce,reduce,33,https://hail.is,https://github.com/hail-is/hail/pull/8203#issuecomment-604461227,1,['reduce'],['reduce']
Energy Efficiency,"costs begin to dominate for short lived jobs. ---. On the last sentence:. There are two major questions, the first of which is much higher priority. We probably need to do a bit of research, at least on the second question. . 1. How can we allow public Internet egress without risking untracked cost? I suspect we must track bytes and charge some, possibly very high, rate. 2. How can we allow public Internet egress at or near the real cost to us?. The second question is complex because Google's egress pricing is complex. To directly respond to your comment: I don't think we need to disaggregate by destination IP address, but we do need to disaggregate by destination ""type & location"". GeoIP _might_ allow us to do this in iptables, we should figure out what is and isn't possible and how hard it would be. ---. The following is distilled from [Network Pricing](https://cloud.google.com/vpc/network-pricing). There are six types of egress:; 1. VM-to-Internet; 2. VM-to-VM or VM-to-Google-Service (which are charged equally); 3. Spanner-to-VM; 4. VM-to-Spanner; 5. GCS-to-VM; 6. VM-to-GCS. Egress types (3) and (5) do not apply to us because hail-vdc does not have Spanner and user jobs cannot read from hail-vdc buckets. Egress types (4) and (6) are slightly ambiguous. We should create a support ticket to verify, but I believe they're charged just like (2). This means we are concerned with just two types of egress:. 1. VM-to-Internet; 2. VM-to-VM / VM-to-Google-Service. Each type has a different cost table based on the _destination location_. In these tables, the cheapest price applies, so, for example, for traffic form us-central1-a to us-central1-a the within-zone price applies, not the within-region price. 1. VM-to-Internet. Prices decrease with more usage.; 1. Standard Tier Networking. For the first 10 TiB: 0.085 USD per GiB.; 2. Premium Tier Networking (we are using this currently). For the first 1 TiB:. | Destination | Cost (USD per GiB) |; | --- | --- |; | Anywhere except C",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13428#issuecomment-1692168526:1191,charge,charged,1191,https://hail.is,https://github.com/hail-is/hail/issues/13428#issuecomment-1692168526,1,['charge'],['charged']
Energy Efficiency,"create_vm; await self.compute_client.post(f'/zones/{location}/instances', params=params, json=vm_config); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiocloud/common/base_client.py"", line 30, in post; async with await self._session.post(url, **kwargs) as resp:; File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiocloud/common/session.py"", line 21, in post; return await self.request('POST', url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiocloud/common/session.py"", line 103, in request; return await request_retry_transient_errors(self._http_session, method, url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 770, in request_retry_transient_errors; return await retry_transient_errors(session.request, method, url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 729, in retry_transient_errors; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 147, in request_and_raise_for_status; body=body; hailtop.httpx.ClientResponseError: 400, message='Bad Request', url=URL('https://compute.googleapis.com/compute/v1/projects/hail-vdc/zones/us-central1-a/instances?requestId=e2555a38-1583-47e2-ab15-c3d7ad84e700') body='{\n ""error"": {\n ""code"": 400,\n ""message"": ""Invalid value for field \'resource.scheduling.instanceTerminationAction\': \'DELETE\'. You cannot specify a termination action for a VM instance that has the standard provisioning model (default). To use instance termination action, the VM instance must use the Spot provisioning model."",\n ""errors"": [\n {\n ""message"": ""Invalid value for field \'resource.scheduling.instanceTerminationAction\': \'DELETE\'. You cannot specify a termination action for a VM instance that has the standard provisioning model (default). To use instance termination action, the VM instance must use the Spot provisioning model."",\n ""domain"": ""global"",\n ""reason"": ""invalid""\n }\n ]\n }\n}\n'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11878#issuecomment-1144087728:1523,schedul,scheduling,1523,https://hail.is,https://github.com/hail-is/hail/pull/11878#issuecomment-1144087728,2,['schedul'],['scheduling']
Energy Efficiency,ction.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5842,schedul,scheduler,5842,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['schedul'],['scheduler']
Energy Efficiency,cute$1(EvalRelationalLets.scala:10); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:18); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:32); E 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:157); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.apply(LoweringPass.scala:151); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); E 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); E 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:150); E 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:189); E 	at is.hail.backend.local.LocalBackend.$anonfun$executeToEncoded$1(LocalBackend.scala:209); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteCon,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:12504,adapt,adapted,12504,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['adapt'],['adapted']
Energy Efficiency,cuteContext.close(ExecuteContext.scala:148); E 	at is.hail.utils.package$.using(package.scala:660); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$executeOneCommand$1(ServiceBackend.scala:555); E 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); E 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.withExecuteContext$1(ServiceBackend.scala:535); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:602); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$7(ServiceBackend.scala:433); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$7$adapted(ServiceBackend.scala:432); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.scala:432); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:77); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:432); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5$adapted(ServiceBackend.scala:430); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:430); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:77); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:430); E 	at is.hail.backend.service.Main$.main(Main.scala:33); E 	at is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:2670,adapt,adapted,2670,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['adapt'],['adapted']
Energy Efficiency,"d (192.0K blocks / 32.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.351 root: INFO: RegionPool: REPORT_THRESHOLD: 40.2M allocated (192.0K blocks / 40.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.357 root: INFO: RegionPool: REPORT_THRESHOLD: 48.2M allocated (192.0K blocks / 48.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.362 root: INFO: RegionPool: REPORT_THRESHOLD: 56.2M allocated (192.0K blocks / 56.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.368 root: INFO: RegionPool: REPORT_THRESHOLD: 64.2M allocated (192.0K blocks / 64.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.374 root: INFO: RegionPool: REPORT_THRESHOLD: 72.2M allocated (192.0K blocks / 72.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.414 root: INFO: RegionPool: REPORT_THRESHOLD: 128.2M allocated (192.0K blocks / 128.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.506 root: INFO: RegionPool: REPORT_THRESHOLD: 256.2M allocated (192.0K blocks / 256.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.910 root: INFO: RegionPool: REPORT_THRESHOLD: 512.0M allocated (111.9M blocks / 400.1M chunks), regions.size = 5, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:42.730 root: INFO: RegionPool: REPORT_THRESHOLD: 1.2G allocated (439.1M blocks / 781.5M chunks), regions.size = 5, 0 current java objects, thread 8: pool-1-thread-1""}, 'service_backend_debug_info': {'batch_attributes': {'name': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'driver_cores': None, 'driver_memory': None, ...}}). test/hail/backend/test_service_backend.py:12: AssertionError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:127669,allocate,allocated,127669,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,4,['allocate'],['allocated']
Energy Efficiency,"d about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 con",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:2934,power,power,2934,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,1,['power'],['power']
Energy Efficiency,"d for thread 10: pool-2-thread-2; 2023-09-27 16:43:10.787 : INFO: RegionPool: REPORT_THRESHOLD: 2.2M allocated (192.0K blocks / 2.0M chunks), regions.size = 3, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.3M allocated (192.0K blocks / 2.1M chunks), regions.size = 3, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.3M allocated (256.0K blocks / 2.1M chunks), regions.size = 4, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.4M allocated (320.0K blocks / 2.1M chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.286 : INFO: RegionPool: REPORT_THRESHOLD: 28.8M allocated (576.0K blocks / 28.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.657 : INFO: RegionPool: REPORT_THRESHOLD: 30.8M allocated (576.0K blocks / 30.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.683 : INFO: RegionPool: REPORT_THRESHOLD: 32.8M allocated (576.0K blocks / 32.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.709 : INFO: RegionPool: REPORT_THRESHOLD: 32.8M allocated (576.0K blocks / 32.3M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:44:22.426 GoogleStorageFS$: INFO: createNoCompression: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd8-4f1b-bcde-fbf29180c347; 2023-09-27 16:44:22.495 GoogleStorageFS$: INFO: close: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd8-4f1b-bcde-fbf29180c347; 2023-09-27 16:44:22.620 GoogleStorageFS$: INFO: closed: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:4514,allocate,allocated,4514,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['allocate'],['allocated']
Energy Efficiency,d.java:748); Caused by: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:222); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 11 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:3038,schedul,scheduler,3038,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['schedul'],['scheduler']
Energy Efficiency,"d/.local/src/hail/hail/python/hail/expr/expressions/expression_typecheck.py"", line 80, in check; raise TypecheckFailure from e; hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in <module>; cli.main(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main; run(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file; runpy.run_path(target, run_name=""__main__""); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path; return _run_module_code(code, init_globals, run_name,; File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code; _run_code(code, mod_globals, init_globals,; File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 124, in _run_code; exec(code, run_globals); File ""/home/edmund/.local/src/hail/test.py"", line 10, in <module>; expr = hl.any(lambda x:; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/functions.py"", line 3530, in any; collection = arg_check(ar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:3427,adapt,adapter,3427,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['adapt'],['adapter']
Energy Efficiency,e a fresh OOS for each object:; ```. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.ForceCountTable.execute(ForceCount.scala:11); 	at is.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:1049,schedul,scheduler,1049,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307,1,['schedul'],['scheduler']
Energy Efficiency,"e spark partition defaults on our cluster to split data into 8,00 partitions. We had [read](https://cloud.google.com/dataproc/docs/support/spark-job-tuning) that this number could be changed to 3x the number of vCPUs on our cluster. Because we are using autoscaling, the number of vCPUs used is not predetermined. Because of this we started with 1x the maximum number of secondary workers in our cluster. Our maximum is set to 1000 n1-highmem-8 machines. These nodes contain 8 vCPUs each, so 8 x 1,000 = 8,000. After speaking with Google, we verified that we could have used 3x the maximum number of vCPUs to increase parallelism. With a maximum of 10 workers and 1,000 secondary workers, all n1-highmem-8 nodes, we could have increased our partition to 24,240. A sample cluster declaration using autoscaling and default shuffle partitions and parallelism of 8000 is below. 3) The hail team had informed us that ""You might try adding `block_size=2048` to your King invocation. That will reduce the memory requirements on the workers to ~1/4 of the default which should give ample room for the analysis."" Because of this, we changed the block size in king to ```block_size=2048```. After looking through the king [source code](https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html#BlockMatrix.default_block_size), we were able to determine the default block size is 4096. . ```; hailctl dataproc start cluster \; --vep GRCh38 \; --autoscaling-policy=MVP_autoscaling_policy \; --requester-pays-allow-annotation-db \; --packages gnomad \; --requester-pays-allow-buckets gnomad-public-requester-pays \; --secondary-worker-type=non-preemptible \; --master-machine-type=n1-highmem-8 \; --worker-machine-type=n1-highmem-8 \; --worker-boot-disk-size=1000 \; --preemptible-worker-boot-disk-size=1000 \; --properties=dataproc:dataproc.logging.stackdriver.enable=true,dataproc:dataproc.monitoring.stackdriver.enable=true,spark:spark.sql.shuffle.partitions=8000,spark:spark.default.parallelism=8000; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290#issuecomment-1284270117:1215,reduce,reduce,1215,https://hail.is,https://github.com/hail-is/hail/issues/12290#issuecomment-1284270117,2,"['monitor', 'reduce']","['monitoring', 'reduce']"
Energy Efficiency,"e.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206714,schedul,scheduler,206714,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,e=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.bam.list] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/scatter/temp_0001_of_1200/scattered.intervals] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=75 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false genotype_likelihoods_model=BOTH pcr_error_rate=1.0E-4 computeSLOD=false annotateNDA=false pair_hmm_implementation=ORIGINAL min_base_quality_score=17 max_deletion_fraction=0.05 min_indel_count_for_genotyping=5 min_indel_fraction_per_sample=0.25 indel_heterozygosity=1.25E-4 indelGapContinuationPenalty=10 indelGapOpenPenalty=45 indelHaplotypeSize=80 indelDebug=false ignoreSNPAlleles=false allReadsSP=false ignoreLaneInfo=false reference_sample_calls=(RodBinding name= source=UNBOUND) reference_sample_name=null sample_ploidy=2 min_quality_score=1 max_quality_score=40 site_quality_prior=20 min_power_threshold_for_calling=0.95 min_reference_depth=100 exclude_filtered_reference_sites,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:14744,monitor,monitorThreadEfficiency,14744,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['monitor'],['monitorThreadEfficiency']
Energy Efficiency,"eJlNvxmFfJhdDKBCy0eThWN2qFxQJ4p9SvzGlMd2r3nBy95v9f8WgkN8M/HTDwTsFafNT0arvHnmUY6rFHxQE9TgTRlH1/sZ7mMxzmVZ8NKI/wIXTkv53TbylBYbvkEXyVFl3OBj1MUvo17v99LGdQNFcAiWR/pRsDvXY415FzootwShgQpmvuPLP7buTqVcrRnwRr2hZcpaydOyEaErYsuEPiot0RPrvsIXSkSI4NlIbqO2i4gjfF8FwpDzyY0WtAvUbsY8dKxWXzcIWtQzUyYeqJq1R0Yh8p3ijmLgrkpAJTI/Lz8WT2foUFg7gYQwc9xbFN6aQzQwUQ0Y8s0DDvQqnbby12IXXHI+rjuh1TH8lIRPw/UsFInJn3WS1MBp4FRiXwRs9EwVhfeb+b8Z5rnaQ3RrmM8SY0kjg0i05rkMkygEnPuSec6qKYREHW8n4wbYQNhJvDW9RhUIGnzn3IQRJB57bOZ8xwPkZ97PM0WGsCMWwupSOuEk/NsFe69cZwbElYZJeqeA/bKKsmRsJ/tjzyYMLUlj4L++4GQIwPHgtjmQ9kUEeaw== dgoldste@wmce3-cb7\n"",; ""path"": ""/home/batch-worker/.ssh/authorized_keys""; }; ]; }; },; ""requireGuestProvisionSignal"": true,; ""secrets"": [],; ""windowsConfiguration"": null; },; ""plan"": null,; ""platformFaultDomain"": null,; ""priority"": ""Spot"",; ""provisioningState"": ""Succeeded"",; ""proximityPlacementGroup"": null,; ""resourceGroup"": ""dgoldste"",; ""resources"": null,; ""scheduledEventsProfile"": null,; ""securityProfile"": null,; ""storageProfile"": {; ""dataDisks"": [; {; ""caching"": ""None"",; ""createOption"": ""Empty"",; ""deleteOption"": ""Delete"",; ""detachOption"": null,; ""diskIopsReadWrite"": null,; ""diskMBpsReadWrite"": null,; ""diskSizeGb"": 375,; ""image"": null,; ""lun"": 2,; ""managedDisk"": {; ""diskEncryptionSet"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/disks/batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5-data"",; ""resourceGroup"": ""dgoldste"",; ""storageAccountType"": ""Standard_LRS""; },; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5-data"",; ""toBeDetached"": false,; ""vhd"": null,; ""writeAcceleratorEnabled"": null; }; ],; ""imageReference"": {; ""exactVersion"": ""0.0.12"",; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/galleries/dgoldste_batch/images/batch-worker/versions/0.0.12"",; ""offer"": null,; ""publisher"": null,; ""resourceGroup"": ""dgoldste"",; ""sharedGalleryImageId"": null,; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:3529,schedul,scheduledEventsProfile,3529,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,1,['schedul'],['scheduledEventsProfile']
Energy Efficiency,eOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.46-6ef64c08b000; Error summary: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:19348,schedul,scheduler,19348,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,2,['schedul'],['scheduler']
Energy Efficiency,eSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:11637,schedul,scheduler,11637,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['schedul'],['scheduler']
Energy Efficiency,eVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:6,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:5558,schedul,scheduler,5558,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['schedul'],['scheduler']
Energy Efficiency,eachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201477,schedul,scheduler,201477,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"eachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205124,schedul,scheduler,205124,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"ead-1\n2022-11-15 20:31:41.335 root: INFO: RegionPool: REPORT_THRESHOLD: 16.2M allocated (192.0K blocks / 16.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.339 root: INFO: RegionPool: REPORT_THRESHOLD: 24.2M allocated (192.0K blocks / 24.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.345 root: INFO: RegionPool: REPORT_THRESHOLD: 32.2M allocated (192.0K blocks / 32.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.351 root: INFO: RegionPool: REPORT_THRESHOLD: 40.2M allocated (192.0K blocks / 40.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.357 root: INFO: RegionPool: REPORT_THRESHOLD: 48.2M allocated (192.0K blocks / 48.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.362 root: INFO: RegionPool: REPORT_THRESHOLD: 56.2M allocated (192.0K blocks / 56.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.368 root: INFO: RegionPool: REPORT_THRESHOLD: 64.2M allocated (192.0K blocks / 64.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.374 root: INFO: RegionPool: REPORT_THRESHOLD: 72.2M allocated (192.0K blocks / 72.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.414 root: INFO: RegionPool: REPORT_THRESHOLD: 128.2M allocated (192.0K blocks / 128.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.506 root: INFO: RegionPool: REPORT_THRESHOLD: 256.2M allocated (192.0K blocks / 256.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.910 root: INFO: RegionPool: REPORT_THRESHOLD: 512.0M allocated (111.9M blocks / 400.1M chunks), regions.size = 5, 0 current java objects, t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:63126,allocate,allocated,63126,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['allocate'],['allocated']
Energy Efficiency,"ead-1\n2022-11-15 20:31:41.339 root: INFO: RegionPool: REPORT_THRESHOLD: 24.2M allocated (192.0K blocks / 24.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.345 root: INFO: RegionPool: REPORT_THRESHOLD: 32.2M allocated (192.0K blocks / 32.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.351 root: INFO: RegionPool: REPORT_THRESHOLD: 40.2M allocated (192.0K blocks / 40.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.357 root: INFO: RegionPool: REPORT_THRESHOLD: 48.2M allocated (192.0K blocks / 48.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.362 root: INFO: RegionPool: REPORT_THRESHOLD: 56.2M allocated (192.0K blocks / 56.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.368 root: INFO: RegionPool: REPORT_THRESHOLD: 64.2M allocated (192.0K blocks / 64.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.374 root: INFO: RegionPool: REPORT_THRESHOLD: 72.2M allocated (192.0K blocks / 72.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.414 root: INFO: RegionPool: REPORT_THRESHOLD: 128.2M allocated (192.0K blocks / 128.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.506 root: INFO: RegionPool: REPORT_THRESHOLD: 256.2M allocated (192.0K blocks / 256.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.910 root: INFO: RegionPool: REPORT_THRESHOLD: 512.0M allocated (111.9M blocks / 400.1M chunks), regions.size = 5, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:42.730 root: INFO: RegionPool: REPORT_THRESHOLD: 1.2G allocated (439.1M blocks / 781.5M chunks), regions.size = 5, 0 current java objects, t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:63309,allocate,allocated,63309,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['allocate'],['allocated']
Energy Efficiency,"ead-1\n2022-11-15 20:31:41.345 root: INFO: RegionPool: REPORT_THRESHOLD: 32.2M allocated (192.0K blocks / 32.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.351 root: INFO: RegionPool: REPORT_THRESHOLD: 40.2M allocated (192.0K blocks / 40.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.357 root: INFO: RegionPool: REPORT_THRESHOLD: 48.2M allocated (192.0K blocks / 48.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.362 root: INFO: RegionPool: REPORT_THRESHOLD: 56.2M allocated (192.0K blocks / 56.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.368 root: INFO: RegionPool: REPORT_THRESHOLD: 64.2M allocated (192.0K blocks / 64.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.374 root: INFO: RegionPool: REPORT_THRESHOLD: 72.2M allocated (192.0K blocks / 72.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.414 root: INFO: RegionPool: REPORT_THRESHOLD: 128.2M allocated (192.0K blocks / 128.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.506 root: INFO: RegionPool: REPORT_THRESHOLD: 256.2M allocated (192.0K blocks / 256.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.910 root: INFO: RegionPool: REPORT_THRESHOLD: 512.0M allocated (111.9M blocks / 400.1M chunks), regions.size = 5, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:42.730 root: INFO: RegionPool: REPORT_THRESHOLD: 1.2G allocated (439.1M blocks / 781.5M chunks), regions.size = 5, 0 current java objects, thread 8: pool-1-thread-1""}, 'service_backend_debug_info': {'batch_attributes': {'name': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'driver_cores': None, 'driver_m",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:63492,allocate,allocated,63492,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['allocate'],['allocated']
Energy Efficiency,"eadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 03:59:30,730 | server.py | set_state:141 | job 153 changed state: Created -> Complete; INFO | 2018-10-23 03:59:30,730 | server.py | mark_complete:184 | job 153 complete, exit_code 2; INFO | 2018-10-23 03:59:30,737 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,806 | _internal.py | _log:88 | 10.56.142.33 - - [23/Oct/2018 03:59:30] ""GET /jobs HTTP/1.1"" 200 -; INFO | 2018-10-23 03:59:30,815 | _internal.py | _",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:1717,adapt,adapter,1717,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038,1,['adapt'],['adapter']
Energy Efficiency,"ed maximum threshold: limit 2.25 GiB, allocated 3.35 GiB; Report: 3.4G allocated (192.0K blocks / 3.4G chunks), regions.size = 3, 0 current java objects, thread 9: pool-1-thread-2; is.hail.utils.HailException: Hail off-heap memory exceeded maximum threshold: limit 2.25 GiB, allocated 3.35 GiB; Report: 3.4G allocated (192.0K blocks / 3.4G chunks), regions.size = 3, 0 current java objects, thread 9: pool-1-thread-2; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.annotations.RegionPool.closeAndThrow(RegionPool.scala:58); 	at is.hail.annotations.RegionPool.incrementAllocatedBytes(RegionPool.scala:73); 	at is.hail.annotations.ChunkCache.newChunk(ChunkCache.scala:75); 	at is.hail.annotations.ChunkCache.getChunk(ChunkCache.scala:130); 	at is.hail.annotations.RegionPool.getChunk(RegionPool.scala:96); 	at is.hail.annotations.RegionMemory.allocateBigChunk(RegionMemory.scala:62); 	at is.hail.annotations.RegionMemory.allocate(RegionMemory.scala:96); 	at is.hail.annotations.Region.allocate(Region.scala:332); 	at __C35collect_distributed_array.__m61split_ToArray(Unknown Source); 	at __C35collect_distributed_array.__m54split_StreamFor(Unknown Source); 	at __C35collect_distributed_array.__m49begin_group_0(Unknown Source); 	at __C35collect_distributed_array.apply(Unknown Source); 	at __C35collect_distributed_array.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$2(BackendUtils.scala:31); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$1(BackendUtils.scala:30); 	at is.hail.backend.service.Worker$.$anonfun$main$13(Worker.scala:142); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:77); 	at is.h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11777#issuecomment-1110147573:1128,allocate,allocateBigChunk,1128,https://hail.is,https://github.com/hail-is/hail/pull/11777#issuecomment-1110147573,1,['allocate'],['allocateBigChunk']
Energy Efficiency,"ed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; gcloud command:; gcloud beta dataproc clusters create \; ci-test-6boype3d \; --image-version=1.2-deb9 \; --metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \; --initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh \; --master-machine-type=n1-standard-1 \; --master-boot-disk-size=40GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-1 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --bucket=hail-ci-0-1-dataproc-staging-bucket \; --max-idle=10m; Starting cluster 'ci-test-6boype3d'...; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 85, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:8827,monitor,monitoring,8827,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518,1,['monitor'],['monitoring']
Energy Efficiency,"eduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.co",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205811,schedul,scheduler,205811,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"elet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; + kubectl describe pvc -n batch-pods batch-2554-job-4-8vvgl; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Annotations: pv.kubernetes.io/bind-completed: yes; pv.kubernetes.io/bound-by-controller: yes; volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd; Finalizers: [kubernetes.io/pvc-protection]; Capacity: 1Gi; Access Modes: RWO; VolumeMode: Filesystem; Events: <none>; Mounted By: batch-2554-job-4-main-vsk7h; ```; The events; ```; + kubectl get events -n batch-pods --sort-by=.metadata.creationTimestamp; + grep 2554; 12m Warning FailedMount Pod Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; 11m Normal Scheduled Pod Successfully assigned batch-pods/batch-2554-job-4-main-vsk7h to gke-vdc-preemptible-pool-9c7148b2-4gq2; 36s Warning FailedMount Pod Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:20745,Schedul,Scheduled,20745,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['Schedul'],['Scheduled']
Energy Efficiency,"en12GenotypeIterator(0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:; ```; ```; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.app",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:1419,schedul,scheduler,1419,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['schedul'],['scheduler']
Energy Efficiency,eneric.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RD,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5694,schedul,scheduler,5694,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['schedul'],['scheduler']
Energy Efficiency,ent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachE,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199482,schedul,scheduler,199482,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:911); 	at is.hail.utils.richUtils.RichRDD$.writePartitions$extension(RichRDD.scal,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:3257,schedul,scheduler,3257,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['schedul'],['scheduler']
Energy Efficiency,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1134); 	at is.hail.variant.VariantSampleMatrix.countVariants(VariantSampleMatrix.scala:810); 	at is.hail.variant.VariantDatasetFunctions$.count$extension(VariantDataset.scala:504); 	at is.hail.variant.VariantDatasetFunctions.count(VariantDataset.scala:494); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(Nativ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4814,schedul,scheduler,4814,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['schedul'],['scheduler']
Energy Efficiency,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScop,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:1606,schedul,scheduler,1606,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['schedul'],['scheduler']
Energy Efficiency,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:26); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:22); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.H,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:4053,schedul,scheduler,4053,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['schedul'],['scheduler']
Energy Efficiency,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:143); 	at is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:5702,schedul,scheduler,5702,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719,1,['schedul'],['scheduler']
Energy Efficiency,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.table.Table.collect(Table.scala:841); 	at is.hail.table.Table.c,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:5057,schedul,scheduler,5057,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,1,['schedul'],['scheduler']
Energy Efficiency,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1158); 	at is.hail.rvd.RVD$class.count(RVD.scala:183); 	at is.hail.rvd.OrderedRVD.count(OrderedRVD.scala:19); 	at is.hail.methods.LDPrune$$anonfun$9.apply(LDPrune.scala:471); 	at is.hail.methods.LDPrune$$anonfun$9.apply(LDPrune.scala:469); 	at is.hail.utils.package$.time(package.scala:82); 	at is.hail.methods.LDPrune$.apply(LDPrune.scala:469); 	at is.hail.methods.LDPrune.ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:3973,schedul,scheduler,3973,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['schedul'],['scheduler']
Energy Efficiency,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.ForceCountTable.execute(ForceCount.scala:11); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:771); 	at is.hail.expr.ir.Interpret$.apply(Interpret,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:1147,schedul,scheduler,1147,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307,1,['schedul'],['scheduler']
Energy Efficiency,erImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(D,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200990,schedul,scheduler,200990,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,erImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204637,schedul,scheduler,204637,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,ext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3938,schedul,scheduler,3938,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['schedul'],['scheduler']
Energy Efficiency,extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:5957,schedul,scheduler,5957,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['schedul'],['scheduler']
Energy Efficiency,"extra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/Na; tiveCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/FS.o -o lib/linux-x86-64/libhail.so; cp -p -f lib/linux-x86-64/libboot.so lib/linux-x86-64/libhail.so ../../../prebuilt/lib/linux-x86-64/; make[1]: Leaving directory `/mnt/tmp/hail/hail/src/main/c'; ./gradlew shadowJar -Dscala.version=2.12.15 -Dspark.version=3.3.2 -Delasticsearch.major-version=7; Downloading https://services.gradle.org/distributions/gradle-8.3-bin.zip; ............10%............20%.............30%............40%.............50%............60%.............70%............80%.............90%............100%. Welcome to Gradle 8.3!. Here are the highlights of this release:; - Faster Java compilation; - Reduced memory usage; - Support for running on Java 20. For more details see https://docs.gradle.org/8.3/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; WARNING: Hail primarily tested with Spark 3.3.0, use other versions at your own risk. > Task :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:15731,Reduce,Reduced,15731,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Reduce'],['Reduced']
Energy Efficiency,"fd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGSchedul",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:5102,schedul,scheduler,5102,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['schedul'],['scheduler']
Energy Efficiency,flectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$Torrent,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:6504,schedul,scheduler,6504,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['schedul'],['scheduler']
Energy Efficiency,"g google storage client from service account key; 2023-09-27 16:43:10.779 WorkerTimer$: INFO: readInputs took 384.743327 ms.; 2023-09-27 16:43:10.779 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-27 16:43:10.787 : INFO: RegionPool: REPORT_THRESHOLD: 2.2M allocated (192.0K blocks / 2.0M chunks), regions.size = 3, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.3M allocated (192.0K blocks / 2.1M chunks), regions.size = 3, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.3M allocated (256.0K blocks / 2.1M chunks), regions.size = 4, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.4M allocated (320.0K blocks / 2.1M chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.286 : INFO: RegionPool: REPORT_THRESHOLD: 28.8M allocated (576.0K blocks / 28.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.657 : INFO: RegionPool: REPORT_THRESHOLD: 30.8M allocated (576.0K blocks / 30.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.683 : INFO: RegionPool: REPORT_THRESHOLD: 32.8M allocated (576.0K blocks / 32.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.709 : INFO: RegionPool: REPORT_THRESHOLD: 32.8M allocated (576.0K blocks / 32.3M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:44:22.426 GoogleStorageFS$: INFO: createNoCompression: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd8-4f1b-bcde-fbf29180c347; 2023-09-27 16:44:22.495 GoogleStorageFS$: INFO: close: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:4334,allocate,allocated,4334,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['allocate'],['allocated']
Energy Efficiency,g.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:3195,schedul,scheduler,3195,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['schedul'],['scheduler']
Energy Efficiency,"gatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Interrupting monitor thread; 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Shutting down all executors; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asking each executor to shut down; 2019-01-22 13:12:06 SchedulerExtensionServices: INFO: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Stopped; 2019-01-22 13:12:06 MapOutputTrackerMasterEndpoint: INFO: MapOutputTrackerMasterEndpoint stopped!; 2019-01-22 13:12:06 MemoryStore: INFO: MemoryStore cleared; 2019-01-22 13:12:06 BlockManager: INFO: BlockManager stopped; 2019-01-22 13:12:06 BlockManagerMaster: INFO: BlockManagerMaster stopped; 2019-01-22 13:12:06 OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: INFO: OutputCommitCoordinator stopped!; 2019-01-22 13:12:06 TransportResponseHandler: ERROR: Still have 1 requests outstanding when connection from /192.168.18.203:44844 is closed; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 14 at RPC address 192.168.18.189:50356, but got no response. Marking as slave lost.; java.io.IOException: Connection from /192.168.18.203:44844 closed; at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(Transpo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:209408,monitor,monitor,209408,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,3,"['Schedul', 'monitor']","['SchedulerExtensionServices', 'monitor']"
Energy Efficiency,"ge$RVDContextIsPointed$.point(package.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:6); at is.hail.utils.package$.point(package.scala:593); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD.is$hail$sparkextras$ContextRDD$$sparkManagedContext(ContextRDD.scala:129); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:138); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Daniel King: JAR and ZIP are both deleted from google storage. for the record the bad SHA was 5702f5c6b299; ```. A discuss user [is seeing that the JVM crashes when he tries to use hail](http://discuss.hail.is/t/gcp-py4jnetworkerror-answer-from-java-side-is-empty/630):. ```; hl.init(); mt = hl.read_matrix_table('gs://hail-datasets/hail-data/1000_genomes_phase3_autosomes.GRCh37.mt'); mt.describe() ...; mt.rsid.show(5); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:3054,schedul,scheduler,3054,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186,1,['schedul'],['scheduler']
Energy Efficiency,ges(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2244); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2269); 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029); 	at is.hail.backend.spark.SparkBackend.parallelizeAndComputeWithIndex(SparkBack,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:6144,schedul,scheduler,6144,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['schedul'],['scheduler']
Energy Efficiency,"gl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: konradk-gsa-key; Optional: false; batch-2554-job-4-8vvgl:; Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-2554-job-4-8vvgl; ReadOnly: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 11m default-scheduler Successfully assigned batch-pods/batch-2554-job-4-main-vsk7h to gke-vdc-preemptible-pool-9c7148b2-4gq2; Warning FailedMount 36s (x5 over 9m46s) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; + kubectl describe pvc -n batch-pods batch-2554-job-4-8vvgl; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Annotations: pv.kubernetes.io/bind-completed: yes; pv.kubernetes.io/bound-by-controller: yes; volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd; Finalizers: [kubernetes.io/pvc-protection]; Capacity: 1Gi; Access Modes: RWO; VolumeMode: Filesystem; Events: <none>; Mounted By: batch-2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:19067,Schedul,Scheduled,19067,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,2,"['Schedul', 'schedul']","['Scheduled', 'scheduler']"
Energy Efficiency,"h vectors and matrices with primary dimension the number of samples (as in QR), and because BLAS3 matrix multiplication is fast. I also checked that upping to 8 covariates didn't balance things out. It didn't. The fancy approach basically trades X.t * X and generic k-dim solve for a QR on X and triangular k-dim solve...better for larger k and smaller n. ```; Standard. 2 cov; Lin 7s; Score 54.5s; LRT 93s; Wald 90s. 2 cov, QR / TriSolve; Lin 7.42s; Score 53.6s, 53.1s; LRT 2m06s, 1m59s; Wald 1m53s, 1m54s. 8 cov; Lin 7.16s; Score 59.1s; LRT 2m25s, 2m20s, 2m26s; Wald 2m27s, 2m27s, 2m25s. 8 cov, QR / TriSolve; Lin 7.76s; Score 52.7s; LRT 3m30s; Wald 3m26s; ```. For Firth, since I'm using QR anyway, may as will use TriSolve (though the timing is not particularly effected even with 8 covariates):. ```; 2 cov:; Firth 5m 10s, 4m55s, 5m7s. 8 cov:; Firth 10m37s, 10m50s, 10m28s; ```. For reference, here's the core logic of the QR approach. This corresponds to another version of LogisticRegressionFit where I tried to reduce unnecessary computation, see below. ```; while (!converged && !exploded && iter <= maxIter) {; try {; val mu = sigmoid(X * b); val sqrtW = sqrt(mu :* (1d - mu)); val QR = qr.reduced(X(::, *) :* sqrtW). deltaB = TriSolve(QR.r, QR.q.t * ((y - mu) :/ sqrtW)). if (max(abs(deltaB)) < tol) {; converged = true; if (computeScoreR) {; optScore = Some(X.t * (y - mu)); optR = Some(QR.r); }; if (computeSe) {; val invR = inv(QR.r) // could speed up inverting as upper triangular, or avoid altogether as 1 / se(-1) = fit.fisherSqrt(-1, -1); optSe = Some(norm(invR(*, ::))); }; if (computeLogLkld); optLogLkhd = Some(sum(breeze.numerics.log((y :* mu) + ((1d - y) :* (1d - mu))))); } else {; iter += 1; b += deltaB; }; }; ```. ```; case class LogisticRegressionFit(; b: DenseVector[Double],; optScore: Option[DenseVector[Double]],; optR: Option[DenseMatrix[Double]],; optSe: Option[DenseVector[Double]],; optLogLkhd: Option[Double],; nIter: Int,; converged: Boolean,; exploded: Boolean);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1375#issuecomment-279532833:1239,reduce,reduce,1239,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279532833,1,['reduce'],['reduce']
Energy Efficiency,hMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGSchedul,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201308,schedul,scheduler,201308,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,hMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204955,schedul,scheduler,204955,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"ha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.48 response 200\n2022-11-15 20:31:41.071 ServiceBackend$: INFO: result 48 complete - 8157265 bytes\n2022-11-15 20:31:41.071 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.49\n2022-11-15 20:31:41.231 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.49 response 200\n2022-11-15 20:31:41.330 ServiceBackend$: INFO: result 49 complete - 8157265 bytes\n2022-11-15 20:31:41.330 ServiceBackend$: INFO: all results complete\n2022-11-15 20:31:41.331 root: INFO: executed D-Array [table_aggregate_singlestage] in 1m23.1s\n2022-11-15 20:31:41.331 root: INFO: RegionPool: REPORT_THRESHOLD: 8.2M allocated (192.0K blocks / 8.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.335 root: INFO: RegionPool: REPORT_THRESHOLD: 16.2M allocated (192.0K blocks / 16.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.339 root: INFO: RegionPool: REPORT_THRESHOLD: 24.2M allocated (192.0K blocks / 24.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.345 root: INFO: RegionPool: REPORT_THRESHOLD: 32.2M allocated (192.0K blocks / 32.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.351 root: INFO: RegionPool: REPORT_THRESHOLD: 40.2M allocated (192.0K blocks / 40.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.357 root: INFO: RegionPool: REPORT_THRESHOLD: 48.2M allocated (192.0K blocks / 48.0M chunks), regions.size = 3, 0 current java objects, thread 8",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:62029,allocate,allocated,62029,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['allocate'],['allocated']
Energy Efficiency,"hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95""],""grace"":""48h"",""recursive"":true,""tag_filter_all"":""cache-pr-.*""}; ```. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/batch-worker"",""us-docker.pkg.dev/hail-vdc/hail/benchmark"",""us-docker.pkg.dev/hail-vdc/hail/blog_nginx"",""us-docker.pkg.dev/hail-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-docker.pkg.dev/hail-vdc/hail/grafana"",""us-docker.pkg.dev/hail-vdc/hail/hail-base"",""us-docker.pkg.dev/hail-vdc/hail/hail-build"",""us-docker.pkg.dev/hail-vdc/hail/hail-buildkit"",""us-docker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python38"",""us-docker.pkg.dev/hail-vdc/hail/hail-ubuntu"",""us-docker.pkg.dev/hail-vdc/hail/memory"",""us-docker.pkg.dev/hail-vdc/hail/monitoring"",""us-docker.pkg.dev/hail-vdc/hail/notebook"",""us-docker.pkg.dev/hail-vdc/hail/notebook_nginx"",""us-docker.pkg.dev/hail-vdc/hail/prometheus"",""us-docker.pkg.dev/hail-vdc/hail/service-base"",""us-docker.pkg.dev/hail-vdc/hail/service-java-run-base"",""us-docker.pkg.dev/hail-vdc/hail/test-ci"",""us-docker.pkg.dev/hail-vdc/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello""],""grace"":""48h"",""recursive"":true}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545:3191,monitor,monitoring,3191,https://hail.is,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545,2,['monitor'],['monitoring']
Energy Efficiency,he.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1134); 	at is.hail.variant.VariantSampl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4477,schedul,scheduler,4477,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['schedul'],['scheduler']
Energy Efficiency,he.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); 	at org.apache,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:2920,schedul,scheduler,2920,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['schedul'],['scheduler']
Energy Efficiency,he.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:4720,schedul,scheduler,4720,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,2,['schedul'],['scheduler']
Energy Efficiency,he.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1158); 	at is.hail.rvd.RVD$class.count(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:3636,schedul,scheduler,3636,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['schedul'],['scheduler']
Energy Efficiency,hedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201712,schedul,scheduler,201712,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"hl.is_defined(sampleids[mt.s])). # score sample; mt = mt.annotate_cols(prs=hl.agg.sum(mt.beta * mt.dosage)). # write out table with sample IDs and PRS scores; mt.cols().export('gs://ukbb_prs/prs/UKB_'+pheno+'_PRS_22.txt'). parser = argparse.ArgumentParser(); parser.add_argument(""--phenotype"", help=""name of the sumstat phenotype""); args = parser.parse_args(). try:; start = time.time(); main(args.phenotype); end = time.time(); message = ""Success! Job was completed in %s"" % time.strftime(""%H:%M:%S"", time.gmtime(end - start)); send_message(message); except Exception as e:; send_message(""Fail.""); ```. ""Failure Reason"":; ```; Job aborted due to stage failure: Task 98 in stage 13.0 failed 20 times, most recent failure: Lost task 98.19 in stage 13.0 (TID 22699, ccarey-sw-xt4j.c.ukbb-robinson.internal, executor 68): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:815); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:804); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:914); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rv",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:3186,allocate,allocate,3186,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['allocate'],['allocate']
Energy Efficiency,https://github.com/hail-is/hail/pull/2301 is now in. This should be ready for a look. How's your ASHG schedule? Let me know if you want me to give this to someone else.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2302#issuecomment-337420156:102,schedul,schedule,102,https://hail.is,https://github.com/hail-is/hail/pull/2302#issuecomment-337420156,1,['schedul'],['schedule']
Energy Efficiency,https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/resources.md#resource-quantities,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6215#issuecomment-497102468:82,schedul,scheduling,82,https://hail.is,https://github.com/hail-is/hail/pull/6215#issuecomment-497102468,1,['schedul'],['scheduling']
Energy Efficiency,"https://internal.hail.is/monitoring/kibana/app/infra#/logs?_g=()&flyoutOptions=(flyoutId:Cs8K1GsBylIYXqvkSfbm,flyoutVisibility:hidden,surroundingLogsId:!n)&logFilter=(expression:'kubernetes.labels.app%20:ci',kind:kuery)&logPosition=(position:(tiebreaker:23440993,time:1562665935824),streamLive:!f)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6587#issuecomment-509634461:25,monitor,monitoring,25,https://hail.is,https://github.com/hail-is/hail/issues/6587#issuecomment-509634461,1,['monitor'],['monitoring']
Energy Efficiency,"hub/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8513,schedul,scheduler,8513,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['schedul'],['scheduler']
Energy Efficiency,ic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.sp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:3605,schedul,scheduler,3605,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,2,['schedul'],['scheduler']
Energy Efficiency,ignal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2244); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2269); 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414); 	at org.apache.spark.rdd.RDD.collec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:6046,schedul,scheduler,6046,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['schedul'],['scheduler']
Energy Efficiency,il.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:222); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-f69b497; Error summary: SparkException: Failed to get broadcast_4_piece0 of broadcast_4; >>> ; ```; @danking,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:8189,schedul,scheduler,8189,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,2,['schedul'],['scheduler']
Energy Efficiency,"il/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8278,schedul,scheduler,8278,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['schedul'],['scheduler']
Energy Efficiency,iled to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:222); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 11 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:3078,schedul,scheduler,3078,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['schedul'],['scheduler']
Energy Efficiency,in : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cance,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199631,schedul,scheduler,199631,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"ine 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: SparkException: Failed to get broadcast_4_piece0 of broadcast_4. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, com2, executor 1): java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:1660,schedul,scheduler,1660,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['schedul'],['scheduler']
Energy Efficiency,iner exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2244); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2269); 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:5956,schedul,scheduler,5956,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['schedul'],['scheduler']
Energy Efficiency,iner from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2244); 	at org.apache.spark.SparkContext.runJob(Spark,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:5702,schedul,scheduler,5702,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['schedul'],['scheduler']
Energy Efficiency,ion(Utils.scala:1303); 	... 11 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:26); 	at is.hail.utils.richUtils.RichRDD$.foral,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:3955,schedul,scheduler,3955,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['schedul'],['scheduler']
Energy Efficiency,ion: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:7841,schedul,scheduler,7841,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,2,['schedul'],['scheduler']
Energy Efficiency,"is that a reuse? If the region is freed in between, one might expect the value in the second row to be allocated in the same spot.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8952#issuecomment-661894256:103,allocate,allocated,103,https://hail.is,https://github.com/hail-is/hail/pull/8952#issuecomment-661894256,1,['allocate'],['allocated']
Energy Efficiency,"it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I j",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1291,schedul,scheduled,1291,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878,1,['schedul'],['scheduled']
Energy Efficiency,"ithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g= with scratch directory '/batch/83e7aee9e9244f6884b8a84ea81b4c7a'; 2023-09-27 16:43:10.398 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-27 16:43:10.779 WorkerTimer$: INFO: readInputs took 384.743327 ms.; 2023-09-27 16:43:10.779 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-27 16:43:10.787 : INFO: RegionPool: REPORT_THRESHOLD: 2.2M allocated (192.0K blocks / 2.0M chunks), regions.size = 3, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.3M allocated (192.0K blocks / 2.1M chunks), regions.size = 3, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.3M allocated (256.0K blocks / 2.1M chunks), regions.size = 4, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.4M allocated (320.0K blocks / 2.1M chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.286 : INFO: RegionPool: REPORT_THRESHOLD: 28.8M allocated (576.0K blocks / 28.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.657 : INFO: RegionPool: REPORT_THRESHOLD: 30.8M allocated (576.0K blocks / 30.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.683 : INFO: RegionPool: REPORT_THRESHOLD: 32.8M allocated (576.0K blocks / 32.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.709 : INFO: RegionPool: REPORT_THRESHOLD: 32.8M allocated (576.0K blocks / 32.3M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:44:22.426 GoogleStorageFS$: INFO: createNoCompression: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd8-4f",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:4155,allocate,allocated,4155,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['allocate'],['allocated']
Energy Efficiency,"k (most recent call last):; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 584, in arg_check; return checker.check(arg, function_name, arg_name); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_typecheck.py"", line 80, in check; raise TypecheckFailure from e; hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in <module>; cli.main(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main; run(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file; runpy.run_path(target, run_name=""__main__""); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path; return _run_module_code(code, init_globals, run_name,; File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code; _run_code(code, mod_globals, init_globals,; File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 124, in _run_code; exec(code, run_globals); File ""/home/ed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:3233,adapt,adapter,3233,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['adapt'],['adapter']
Energy Efficiency,k has been configured); E 	at reactor.core.Exceptions.propagate(Exceptions.java:392); E 	at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:97); E 	at reactor.core.publisher.Flux.blockLast(Flux.java:2519); E 	at com.azure.core.util.paging.ContinuablePagedByIteratorBase.requestPage(ContinuablePagedByIteratorBase.java:94); E 	at com.azure.core.util.paging.ContinuablePagedByItemIterable$ContinuablePagedByItemIterator.<init>(ContinuablePagedByItemIterable.java:50); E 	at com.azure.core.util.paging.ContinuablePagedByItemIterable.iterator(ContinuablePagedByItemIterable.java:37); E 	at com.azure.core.util.paging.ContinuablePagedIterable.iterator(ContinuablePagedIterable.java:106); E 	at java.lang.Iterable.forEach(Iterable.java:74); E 	at is.hail.io.fs.AzureStorageFS.delete(AzureStorageFS.scala:203); E 	at is.hail.backend.OwningTempFileManager.$anonfun$cleanup$1(ExecuteContext.scala:27); E 	at is.hail.backend.OwningTempFileManager.$anonfun$cleanup$1$adapted(ExecuteContext.scala:26); E 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); E 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); E 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); E 	at is.hail.backend.OwningTempFileManager.cleanup(ExecuteContext.scala:26); E 	at is.hail.backend.ExecuteContext.close(ExecuteContext.scala:148); E 	at is.hail.utils.package$.using(package.scala:660); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$executeOneCommand$1(ServiceBackend.scala:555); E 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); E 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); E 	at i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:1301,adapt,adapted,1301,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['adapt'],['adapted']
Energy Efficiency,k.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; ```. _All_ of my workers had this error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:815); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:804); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:914); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scal,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:6840,allocate,allocate,6840,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['allocate'],['allocate']
Energy Efficiency,kContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6403,schedul,scheduler,6403,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['schedul'],['scheduler']
Energy Efficiency,"kException: Job aborted due to stage failure: Task 1.0 in stage 5.0 (TID 2681) had a not serializable result: is.hail.io.bgen.Bgen12GenotypeIterator; Serialization stack:; 	- object not serializable (class: is.hail.io.bgen.Bgen12GenotypeIterator, value: Bgen12GenotypeIterator(0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:; ```; ```; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.rdd.PairRDDFunctions$$an",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:1163,schedul,scheduler,1163,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['schedul'],['scheduler']
Energy Efficiency,kMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12195,schedul,scheduler,12195,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['schedul'],['scheduler']
Energy Efficiency,"kend$: INFO: result 48 complete - 8157265 bytes\n2022-11-15 20:31:41.071 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.49\n2022-11-15 20:31:41.231 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.49 response 200\n2022-11-15 20:31:41.330 ServiceBackend$: INFO: result 49 complete - 8157265 bytes\n2022-11-15 20:31:41.330 ServiceBackend$: INFO: all results complete\n2022-11-15 20:31:41.331 root: INFO: executed D-Array [table_aggregate_singlestage] in 1m23.1s\n2022-11-15 20:31:41.331 root: INFO: RegionPool: REPORT_THRESHOLD: 8.2M allocated (192.0K blocks / 8.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.335 root: INFO: RegionPool: REPORT_THRESHOLD: 16.2M allocated (192.0K blocks / 16.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.339 root: INFO: RegionPool: REPORT_THRESHOLD: 24.2M allocated (192.0K blocks / 24.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.345 root: INFO: RegionPool: REPORT_THRESHOLD: 32.2M allocated (192.0K blocks / 32.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.351 root: INFO: RegionPool: REPORT_THRESHOLD: 40.2M allocated (192.0K blocks / 40.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.357 root: INFO: RegionPool: REPORT_THRESHOLD: 48.2M allocated (192.0K blocks / 48.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.362 root: INFO: RegionPool: REPORT_THRESHOLD: 56.2M allocated (192.0K blocks / 56.0M chunks), regions.size = 3, 0 current java objects, thread ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:62211,allocate,allocated,62211,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['allocate'],['allocated']
Energy Efficiency,"ker; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 6: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 7: 7028; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 8: 9060; 2023-09-27 16:43:10.390 JVMEntryway: INFO: Yielding control to the QoB Job.; 2023-09-27 16:43:10.393 Worker$: INFO: is.hail.backend.service.Worker 09526a168d57dac1a26f8caa4ab49593931ed2ef; 2023-09-27 16:43:10.394 Worker$: INFO: running job 7028/9060 at root gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g= with scratch directory '/batch/83e7aee9e9244f6884b8a84ea81b4c7a'; 2023-09-27 16:43:10.398 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-27 16:43:10.779 WorkerTimer$: INFO: readInputs took 384.743327 ms.; 2023-09-27 16:43:10.779 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-09-27 16:43:10.787 : INFO: RegionPool: REPORT_THRESHOLD: 2.2M allocated (192.0K blocks / 2.0M chunks), regions.size = 3, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.3M allocated (192.0K blocks / 2.1M chunks), regions.size = 3, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.3M allocated (256.0K blocks / 2.1M chunks), regions.size = 4, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.4M allocated (320.0K blocks / 2.1M chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.286 : INFO: RegionPool: REPORT_THRESHOLD: 28.8M allocated (576.0K blocks / 28.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.657 : INFO: RegionPool: REPORT_THRESHOLD: 30.8M allocated (576.0K blocks / 30.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:3621,allocate,allocated,3621,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['allocate'],['allocated']
Energy Efficiency,l$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201615,schedul,scheduler,201615,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"l$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGSchedule",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205262,schedul,scheduler,205262,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"lInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213966,Schedul,ScheduledThreadPoolExecutor,213966,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['Schedul'],"['ScheduledFutureTask', 'ScheduledThreadPoolExecutor']"
Energy Efficiency,le-cloud-sdk/lib/third_party/containerregistry/client/v1/__pycache__/docker_http_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/client/__pycache__/docker_creds_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/client/__pycache__/docker_name_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_puller_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_pusher_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_appender_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_appender_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_puller_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_pusher_.cpython-39.pyc; /usr/local/share/google/dataproc/npd-config/docker-monitor-counter.json; /usr/local/share/google/dataproc/npd-config/docker-monitor.json; /usr/local/share/google/dataproc/npd-config/health-checker-docker.json; /usr/local/share/google/dataproc/npd-config/docker-monitor-filelog.json; /usr/local/share/google/dataproc/bdutil/fluentd/container_logging/plugin/test/Dockerfile; /usr/local/share/google/dataproc/bdutil/components/initialize/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/post-install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/activate/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh; /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:11730,monitor,monitor,11730,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['monitor'],['monitor']
Energy Efficiency,llection.AbstractIterator.aggregate(Iterator.scala:1336); 	at is.hail.sparkextras.ContextRDD$$anonfun$6$$anonfun$apply$11.apply(ContextRDD.scala:237); 	at is.hail.sparkextras.ContextRDD$$anonfun$6$$anonfun$apply$11.apply(ContextRDD.scala:235); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$6.apply(ContextRDD.scala:235); 	at is.hail.sparkextras.ContextRDD$$anonfun$6.apply(ContextRDD.scala:234); 	at scala.Function1$$anonfun$andThen$1.apply(Function1.scala:52); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; ```. _All_ of my workers had this error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:815); 	at is.hail.expr.MatrixM,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:6217,schedul,scheduler,6217,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['schedul'],['scheduler']
Energy Efficiency,"llib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 03:59:30,730 | server.py | set_state:141 | job 153 changed state: Created -> Complete; INFO | 2018-10-23 03:59:30,730 | server.py | mark_complete:184 | job 153 complete, exit_code 2; INFO | 2018-10-23 03:59:30,737 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,806 | _internal.py | _log:88 | 10.56.142.33 - - [23/Oct/2018 03:59:30] ""GET /jobs HTTP/1.1"" 200 -; INFO | 2018-10-23 03:59:30,815 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:1798,adapt,adapters,1798,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038,1,['adapt'],['adapters']
Energy Efficiency,ls.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:2242,schedul,scheduler,2242,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['schedul'],['scheduler']
Energy Efficiency,"lt: is.hail.io.bgen.Bgen12GenotypeIterator; Serialization stack:; 	- object not serializable (class: is.hail.io.bgen.Bgen12GenotypeIterator, value: Bgen12GenotypeIterator(0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:; ```; ```; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1219); 	at org.apache.spark.rdd.PairRDDFun",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:1269,schedul,scheduler,1269,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['schedul'],['scheduler']
Energy Efficiency,ltTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:3530,schedul,scheduler,3530,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,3,['schedul'],['scheduler']
Energy Efficiency,ly$20.apply(ContextRDD.scala:280); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-76c42fe; Error summary: ClassCastException: is.hail.codegen.generated.C29 cannot be cast to is.hail.asm4s.AsmFunction5; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:9136,schedul,scheduler,9136,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,2,['schedul'],['scheduler']
Energy Efficiency,ly(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:70); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$mapAnnotations$1.apply(VariantSampleMatrix.scala:399); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:285); at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-241800222:2376,schedul,scheduler,2376,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-241800222,2,['schedul'],['scheduler']
Energy Efficiency,moved to asana to be scheduled.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7574#issuecomment-613493348:21,schedul,scheduled,21,https://hail.is,https://github.com/hail-is/hail/issues/7574#issuecomment-613493348,2,['schedul'],['scheduled']
Energy Efficiency,n$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200701,schedul,scheduler,200701,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,n$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204348,schedul,scheduler,204348,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,n$apply$9$$anonfun$apply$10.apply(RVD.scala:221); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9.apply(RVD.scala:221); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9.apply(RVD.scala:220); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:220); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:218); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.sp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:2521,schedul,scheduler,2521,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['schedul'],['scheduler']
Energy Efficiency,"n$apply$9$$anonfun$apply$10.apply(RVD.scala:221); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9.apply(RVD.scala:221); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9.apply(RVD.scala:220); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:220); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:218); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-e6de08e; Error summary: ClassCastException: is.hail.codegen.generated.C14 cannot be cast to is.hail.asm4s.AsmFunction2; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [3c5f402fed564ccd85257c0919d4bffb] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""pyhail.py"", line 128, in <module>; main(args, pass_through_args); File ""pyhail.py"", line 109, in main; subprocess.check_output(job); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 573, in check_output; raise CalledProcessError(retcode, cmd, ou",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:7040,schedul,scheduler,7040,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['schedul'],['scheduler']
Energy Efficiency,nagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199772,schedul,scheduler,199772,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):. > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode. Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission. See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957). [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479640539:624,power,powerful,624,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640539,1,['power'],['powerful']
Energy Efficiency,nd(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1887,schedul,scheduler,1887,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,1,['schedul'],['scheduler']
Energy Efficiency,"nformation. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very worried about that. I think it's fine and it helps simplify the architecture. It avoids entangling the monitor with the pools and the JPIM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:1169,monitor,monitor,1169,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358,3,['monitor'],"['monitor', 'monitoring']"
Energy Efficiency,nfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12721,schedul,scheduler,12721,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['schedul'],['scheduler']
Energy Efficiency,nfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:18); E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:36); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.expr.ir.Compile$.$anonfun$apply$4(Compile.scala:45); E 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction(Backend.scala:126); E 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction$(Backend.scala:122); E 	at is.hail.backend.local.LocalBackend.lookupOrCompileCachedFunction(LocalBackend.scala:73); E 	at is.hail.expr.ir.Compile$.apply(Compile.scala:39); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$5(CompileAndEvaluate.scala:66); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:66); E 	at,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:8341,adapt,adapted,8341,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['adapt'],['adapted']
Energy Efficiency,nfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.def,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201963,schedul,scheduler,201963,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,nfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkConte,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:3449,schedul,scheduler,3449,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,3,['schedul'],['scheduler']
Energy Efficiency,nnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:70); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); I0824 16:44:07.061986 9121 sched.cpp:1771] Asked to stop the driver; I0824 16:44:07.062144 8743 sched.cpp:1040] Stopping framework '0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932'`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:11371,schedul,scheduler,11371,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,2,['schedul'],['scheduler']
Energy Efficiency,non$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3839,schedul,scheduler,3839,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['schedul'],['scheduler']
Energy Efficiency,"not moved to table there. Same needs to be done for VDS, this isn't too hard); - Add the non-core methods to `hail.methods` / `hail.genetics.methods`; - some stuff here is much harder than the rest, like `filter_alleles`; - This is mostly just labor, but some require more thought than others, like moving TDT to use hail2 expr; - Support intervals in the `index_*` methods. It's possible now to join by locus, but not using the `annotateLociTable` fast path.; - Move to Python 3 so argument order is preserved; - Test the hail2 api much more rigorously than we do now (at the very least, call each parameter branch for each method!; - Typecheck the expression language. This isn't super trivial, and making a nice system to integrate our `typecheck` module and expressions will require some thoughtful design work.; - Some more organization around the package: monkey patching with `import hail.genetics` is an idea I like, but want to think about the edge cases first. ## Documentation; - Document the `index_*` methods / joins; - Translate the _Hail Overview_ tutorial; - Make new tutorials to replace the 2 expr ones we have; - Fill in docs on api2 methods (they're not all there yet); - Fill in docs on expression language (things like __mul__ on NumericExpression haven't been documented); - Write ""integrative docs"" that provide how-tos for common types of workflows. Show the power of annotate / select / group_by/aggregate, etc. ## Longer term QoL:; - Move over tests to Python as much as possible. I looked at the linear regression suite and it can be moved entirely into Python without many problems.; - Write a type parser in Python. The nested calls into the JVM for Type._from_java make the library feel extremely sluggish on teensy data.; - Integrate RV with C/C++, so we can transmit data much more efficiently between Python and Java.; - Rethink the expr language function registry, because many functions there can be implemented in terms of others in Python.; - add back in de novo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2588#issuecomment-352190554:1650,power,power,1650,https://hail.is,https://github.com/hail-is/hail/pull/2588#issuecomment-352190554,2,"['efficient', 'power']","['efficiently', 'power']"
Energy Efficiency,"notate_cols(prs=hl.agg.sum(mt.beta * mt.dosage)). # write out table with sample IDs and PRS scores; mt.cols().export('gs://ukbb_prs/prs/UKB_'+pheno+'_PRS_22.txt'). parser = argparse.ArgumentParser(); parser.add_argument(""--phenotype"", help=""name of the sumstat phenotype""); args = parser.parse_args(). try:; start = time.time(); main(args.phenotype); end = time.time(); message = ""Success! Job was completed in %s"" % time.strftime(""%H:%M:%S"", time.gmtime(end - start)); send_message(message); except Exception as e:; send_message(""Fail.""); ```. ""Failure Reason"":; ```; Job aborted due to stage failure: Task 98 in stage 13.0 failed 20 times, most recent failure: Lost task 98.19 in stage 13.0 (TID 22699, ccarey-sw-xt4j.c.ukbb-robinson.internal, executor 68): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:815); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:804); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:914); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scal",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:3245,allocate,allocate,3245,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['allocate'],['allocate']
Energy Efficiency,oOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203419,schedul,scheduler,203419,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"ocated (192.0K blocks / 3.4G chunks), regions.size = 3, 0 current java objects, thread 9: pool-1-thread-2; is.hail.utils.HailException: Hail off-heap memory exceeded maximum threshold: limit 2.25 GiB, allocated 3.35 GiB; Report: 3.4G allocated (192.0K blocks / 3.4G chunks), regions.size = 3, 0 current java objects, thread 9: pool-1-thread-2; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.annotations.RegionPool.closeAndThrow(RegionPool.scala:58); 	at is.hail.annotations.RegionPool.incrementAllocatedBytes(RegionPool.scala:73); 	at is.hail.annotations.ChunkCache.newChunk(ChunkCache.scala:75); 	at is.hail.annotations.ChunkCache.getChunk(ChunkCache.scala:130); 	at is.hail.annotations.RegionPool.getChunk(RegionPool.scala:96); 	at is.hail.annotations.RegionMemory.allocateBigChunk(RegionMemory.scala:62); 	at is.hail.annotations.RegionMemory.allocate(RegionMemory.scala:96); 	at is.hail.annotations.Region.allocate(Region.scala:332); 	at __C35collect_distributed_array.__m61split_ToArray(Unknown Source); 	at __C35collect_distributed_array.__m54split_StreamFor(Unknown Source); 	at __C35collect_distributed_array.__m49begin_group_0(Unknown Source); 	at __C35collect_distributed_array.apply(Unknown Source); 	at __C35collect_distributed_array.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$2(BackendUtils.scala:31); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$1(BackendUtils.scala:30); 	at is.hail.backend.service.Worker$.$anonfun$main$13(Worker.scala:142); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:77); 	at is.hail.backend.service.Worker$.main(Worker.scala:142); 	at is.hail.backend.se",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11777#issuecomment-1110147573:1206,allocate,allocate,1206,https://hail.is,https://github.com/hail-is/hail/pull/11777#issuecomment-1110147573,1,['allocate'],['allocate']
Energy Efficiency,"ogle Monitoring set up to retrieve process-level memory statistics from the driver node that should also work. Just to be clear, I don't anticipate any changes to Hail in the next week that would change the memory use of this pipeline. There could be a memory leak, but I have no clews that lead to it. I realize this is an unsatisfying answer. I'm pretty perplexed as to what could be the issue here. #### technical details. We'll call the second to most recent run Run A and the most recent run Run B. Run A (like all runs before it) only manages two sample groups before failing. Run B made it through 50 groups before failing on 51. Why did they fail? The syslog for Run A is clear: the oomkiller killed Run A. We lack syslogs for Run B, so we cannot be certain but the lack of a JVM stack trace suggests to me that (a) the driver failed and (b) the driver was killed by the system.; Let's focus on the driver machines. In Run A, we used an n1-highmem-8 which is advertised to have 52GiB (53248 MiB). In Run B, we used an n1-highmem-16 which is advertised to have 104GiB (106,496 MiB). hailctl sets the JVM max heap size to 80% of the advertised RAM, so 42598 MiB (see hailctl's --master-memory-fraction). In Run A (the only run for which we have syslogs), based on the driver's syslog, before Spark starts, the system has already allocated 8500 MiB to Linux/Google/Dataproc daemons. Moreover, the actual RAM of the system (as reported by the earlyoom daemon) is 52223 MiB (51 GiB, 1GiB less than Google advertises for n1-highmem-8). Assuming these daemons never release their memory, all our user code must fit in 43723 MiB. Since the JVM's max heap is 42598 MiB, Python (and indeed, anything else on the system) is limited to allocating 1125 MiB. I assume that an n1-highmem-16 uses the same amount of memory for system daemons, so I'd expect just over ten GiB that is used neither by system daemons nor the JVM. Assuming that's right, I can't explain why the oomkiller killed the JVM in Run B.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449:2477,allocate,allocated,2477,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449,1,['allocate'],['allocated']
Energy Efficiency,ollection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:4042,schedul,scheduler,4042,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,2,['schedul'],['scheduler']
Energy Efficiency,onfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5023,schedul,scheduler,5023,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['schedul'],['scheduler']
Energy Efficiency,onfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:4279,schedul,scheduler,4279,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,2,['schedul'],['scheduler']
Energy Efficiency,onfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkConte,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4290,schedul,scheduler,4290,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['schedul'],['scheduler']
Energy Efficiency,ontainerregistry/client/__pycache__/docker_creds_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/client/__pycache__/docker_name_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_puller_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_pusher_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_appender_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_appender_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_puller_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_pusher_.cpython-39.pyc; /usr/local/share/google/dataproc/npd-config/docker-monitor-counter.json; /usr/local/share/google/dataproc/npd-config/docker-monitor.json; /usr/local/share/google/dataproc/npd-config/health-checker-docker.json; /usr/local/share/google/dataproc/npd-config/docker-monitor-filelog.json; /usr/local/share/google/dataproc/bdutil/fluentd/container_logging/plugin/test/Dockerfile; /usr/local/share/google/dataproc/bdutil/components/initialize/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/post-install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/activate/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh; /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/docker-ce.done; /tmp/dataproc/components/pre-uninstall/docker-ce.running; /tmp/dataproc/components/pre-uninstall/docker-ce.done; /etc/apt/pre,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:11867,monitor,monitor-filelog,11867,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['monitor'],['monitor-filelog']
Energy Efficiency,"ook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1504,schedul,scheduled,1504,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878,1,['schedul'],['scheduled']
Energy Efficiency,or$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6055,schedul,scheduler,6055,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['schedul'],['scheduler']
Energy Efficiency,"or: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206982,schedul,scheduler,206982,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"ord. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (What Jupyter hub does.). > I thought it wou",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:987,schedul,schedule,987,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659,1,['schedul'],['schedule']
Energy Efficiency,"ored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:214872,schedul,scheduler,214872,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"ow.get(UnsafeRow.scala:254); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:871); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:860); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:637); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:631); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:631); 	at is.hail.io.RichRDDRegionValue$.writeRowsPartition(RowStore.scala:510); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-df17cef; Error summary: IndexOutOfBoundsException: 3; ```; (NB: a custom VEP/LOFTEE, but that shouldn't matter - ran same thing on `devel-cd48e11` and it worked fine)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:6673,schedul,scheduler,6673,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,2,['schedul'],['scheduler']
Energy Efficiency,pache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:222); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 11 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperation,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:3716,schedul,scheduler,3716,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['schedul'],['scheduler']
Energy Efficiency,"pache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1992,schedul,scheduler,1992,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,1,['schedul'],['scheduler']
Energy Efficiency,ply$27.apply(ContextRDD.scala:359); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-f80a6f10bc84; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:11890,schedul,scheduler,11890,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719,2,['schedul'],['scheduler']
Energy Efficiency,pply(Partitioner.scala:66); 	at org.apache.spark.Partitioner$$anonfun$defaultPartitioner$2.apply(Partitioner.scala:66); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.immutable.List.map(List.scala:285); 	at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:66); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:329); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:329); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:328); 	at is.hail.methods.MendelErrors.nErrorPerNuclearFamily(MendelErrors.scala:145); 	at is.hail.methods.MendelErrors.fMendelKT(MendelErrors.scala:184); 	at is.hail.variant.MatrixTable.mendelErrors(MatrixTable.scala:1913); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3074#issuecomment-370494908:9306,reduce,reduceByKey,9306,https://hail.is,https://github.com/hail-is/hail/issues/3074#issuecomment-370494908,1,['reduce'],['reduceByKey']
Energy Efficiency,pute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12390,schedul,scheduler,12390,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['schedul'],['scheduler']
Energy Efficiency,"r node syslogs as well as the Hail log file. For some reason all logs other than the Hail logs are missing from this file. We separately need to determine why all the Spark logs etc. are missing. Based on the syslog, after system start up and just before the Jupyter notebook starts, the system is already using ~8,500MiB:; ```; Nov 22 14:29:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 43808 of 52223 MiB (83.89%), swap free: 0 of 0 MiB ( 0.00%); ```; So, the effective maximum memory that Hail could possibly use is around 43808MiB. After the Notebook and Spark initialize we're down to 42,700 MiB (about ~1000MiB more in use).; ```; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. `hailctl` sets the VM RAM limit to 80% of the instance type's memory, so 80% * 52GiB = 42598MiB. This means the JVM is permitted to effectively use all the remaining memory. At time of sigkill the total memory allocated by the JVM was about 2000MiB below the max heap size. Note that the heap is contained within all memory allocated by the JVM.; ```; Nov 22 15:31:05 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 43 of 52223 MiB ( 0.08%), swap free: 0 of 0 MiB ( 0.00%); Nov 22 15:31:09 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: low memory! at or below SIGTERM limits: mem 0.12%, swap 1.00%; Nov 22 15:31:09 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM to process 8421 uid 0 ""java"": badness 1852, VmRSS 40578 MiB; ```. Indeed, the VmRSS is the memory in use from the kernel's perspective so it includes any off-heap memory created by Hail. The Hail log indicates the region pools are tiny, ~10s of MiB. Not a concern. After the JVM is killed, memory jumps back up to 40683MiB (which checks out, that's roughly what the killed process was using).; ```; Nov 22 15:31:10 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 40683 of 52223 MiB (77.90%), swap free: 0 of 0 MiB ( 0.00%); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1832531419:1795,allocate,allocated,1795,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832531419,2,['allocate'],['allocated']
Energy Efficiency,rLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:5621,schedul,scheduler,5621,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['schedul'],['scheduler']
Energy Efficiency,"rack bytes and charge some, possibly very high, rate. 2. How can we allow public Internet egress at or near the real cost to us?. The second question is complex because Google's egress pricing is complex. To directly respond to your comment: I don't think we need to disaggregate by destination IP address, but we do need to disaggregate by destination ""type & location"". GeoIP _might_ allow us to do this in iptables, we should figure out what is and isn't possible and how hard it would be. ---. The following is distilled from [Network Pricing](https://cloud.google.com/vpc/network-pricing). There are six types of egress:; 1. VM-to-Internet; 2. VM-to-VM or VM-to-Google-Service (which are charged equally); 3. Spanner-to-VM; 4. VM-to-Spanner; 5. GCS-to-VM; 6. VM-to-GCS. Egress types (3) and (5) do not apply to us because hail-vdc does not have Spanner and user jobs cannot read from hail-vdc buckets. Egress types (4) and (6) are slightly ambiguous. We should create a support ticket to verify, but I believe they're charged just like (2). This means we are concerned with just two types of egress:. 1. VM-to-Internet; 2. VM-to-VM / VM-to-Google-Service. Each type has a different cost table based on the _destination location_. In these tables, the cheapest price applies, so, for example, for traffic form us-central1-a to us-central1-a the within-zone price applies, not the within-region price. 1. VM-to-Internet. Prices decrease with more usage.; 1. Standard Tier Networking. For the first 10 TiB: 0.085 USD per GiB.; 2. Premium Tier Networking (we are using this currently). For the first 1 TiB:. | Destination | Cost (USD per GiB) |; | --- | --- |; | Anywhere except China, except Australia, but including Hong Kong | 0.12 |; | China except Hong Kong | 0.23 |; | Australia | 0.19 |; 5. VM-to-VM or VM-to-Google-Service. | Location Type | Cost (USD per GiB) |; | --- | --- |; |Within-Zone|0.00|; |Within-Region (but different Zones)|0.01|; |Within-US/Canada|0.01|; |Within-Europe|0.02|; |",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13428#issuecomment-1692168526:1521,charge,charged,1521,https://hail.is,https://github.com/hail-is/hail/issues/13428#issuecomment-1692168526,1,['charge'],['charged']
Energy Efficiency,rator.scala:1334); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:655); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:653); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096); 	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157); 	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:6939,schedul,scheduler,6939,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307,2,['schedul'],['scheduler']
Energy Efficiency,"rce (or it may not exist)."",; E ""reason"": ""forbidden""; E }; E ],; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist).""; E }; E ; E Java stack trace:; E java.io.IOException: Error accessing gs://hail-test-requester-pays-fds32/zero-to-nine; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1986); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1882); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystemImpl.getFileInfoInternal(GoogleCloudStorageFileSystemImpl.java:861); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystemImpl.getFileInfo(GoogleCloudStorageFileSystemImpl.java:833); E 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem.getFileStatus(GoogleHadoopFileSystem.java:724); E 	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:115); E 	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:349); E 	at org.apache.hadoop.fs.Globber.glob(Globber.java:202); E 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2142); E 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem.globStatus(GoogleHadoopFileSystem.java:759); E 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem.globStatus(GoogleHadoopFileSystem.java:1277); E 	at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:162); E 	at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:85); E 	at is.hail.io.fs.FS.glob(FS.scala:402); E 	at is.hail.io.fs.FS.glob$(FS.scala:402); E 	at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:85); E 	at is.hail.io.fs.HadoopFS.$anonfun$globAll$1(HadoopFS.scala:154); E 	at is.hail.io.fs.HadoopFS.$anonfun$globAll$1$adapted(HadoopFS.scala:153). ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236:3197,adapt,adapted,3197,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236,1,['adapt'],['adapted']
Energy Efficiency,"re: memory allocation --- passing NDArrays around on the stack seems adequate for now given that the data array will still be region-allocated and I don't think we expect the rest of it to ever get large, but we should probably set up a time to talk about a fuller plan for allocation/memory management sooner rather than later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5627#issuecomment-474470091:133,allocate,allocated,133,https://hail.is,https://github.com/hail-is/hail/pull/5627#issuecomment-474470091,1,['allocate'],['allocated']
Energy Efficiency,"rent java objects, thread 9: pool-1-thread-2; is.hail.utils.HailException: Hail off-heap memory exceeded maximum threshold: limit 2.25 GiB, allocated 3.35 GiB; Report: 3.4G allocated (192.0K blocks / 3.4G chunks), regions.size = 3, 0 current java objects, thread 9: pool-1-thread-2; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.annotations.RegionPool.closeAndThrow(RegionPool.scala:58); 	at is.hail.annotations.RegionPool.incrementAllocatedBytes(RegionPool.scala:73); 	at is.hail.annotations.ChunkCache.newChunk(ChunkCache.scala:75); 	at is.hail.annotations.ChunkCache.getChunk(ChunkCache.scala:130); 	at is.hail.annotations.RegionPool.getChunk(RegionPool.scala:96); 	at is.hail.annotations.RegionMemory.allocateBigChunk(RegionMemory.scala:62); 	at is.hail.annotations.RegionMemory.allocate(RegionMemory.scala:96); 	at is.hail.annotations.Region.allocate(Region.scala:332); 	at __C35collect_distributed_array.__m61split_ToArray(Unknown Source); 	at __C35collect_distributed_array.__m54split_StreamFor(Unknown Source); 	at __C35collect_distributed_array.__m49begin_group_0(Unknown Source); 	at __C35collect_distributed_array.apply(Unknown Source); 	at __C35collect_distributed_array.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$2(BackendUtils.scala:31); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$1(BackendUtils.scala:30); 	at is.hail.backend.service.Worker$.$anonfun$main$13(Worker.scala:142); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:77); 	at is.hail.backend.service.Worker$.main(Worker.scala:142); 	at is.hail.backend.service.Main$.main(Main.scala:32); 	at is.hail.backend.service.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11777#issuecomment-1110147573:1270,allocate,allocate,1270,https://hail.is,https://github.com/hail-is/hail/pull/11777#issuecomment-1110147573,1,['allocate'],['allocate']
Energy Efficiency,rg.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4036,schedul,scheduler,4036,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['schedul'],['scheduler']
Energy Efficiency,rk.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250); 	at org.apache.spark.Partitioner$$anonfun$defaultPartitioner$2.apply(Partitioner.scala:66); 	at org.apache.spark.Partitioner$$anonfun$defaultPartitioner$2.apply(Partitioner.scala:66); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.immutable.List.map(List.scala:285); 	at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:66); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:329); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:329); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:328); 	at is.hail.methods.MendelErrors.nErrorPerNuclearFamily(MendelErrors.scala:145); 	at is.hail.methods.MendelErrors.fMendelKT(MendelErrors.scala:184); 	at is.hail.variant.MatrixTable.mendelErrors(MatrixTable.scala:1913); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3074#issuecomment-370494908:8992,reduce,reduceByKey,8992,https://hail.is,https://github.com/hail-is/hail/issues/3074#issuecomment-370494908,1,['reduce'],['reduceByKey']
Energy Efficiency,roadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:5487,schedul,scheduler,5487,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['schedul'],['scheduler']
Energy Efficiency,"rotating log: https://stackoverflow.com/questions/40636021/how-to-list-kubernetes-recently-deleted-pods . For users, and for investors, we want to have a permanent record of all user interactions. * Some kinds of data may be awkward to store and query within pod labels. For instance, how much user state do we want to store in labels? How do we store operation graphs / history?; ; * aggregation operations across users or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there either is, or should be a way of querying one selector across all resources). . * Performance, even for very basic queries. An initial assumption, may prove to be incorrect, but I think we will find a fast db to be much faster than K8, even if we could directly query etcd. This is based on my experience with Amazon data stores, and general impression/experience with google cs products.; * Open issue on this: even directly accessed, etcd is slow, doesn't index selectors https://github.com/kubernetes/kubernetes/issues/4817. * We may want to differentiate between k8 outages and lack of user requests. I also want to be able to quickly present a potential investor aggregate data, for notebook and all other manner of hail services: for notebook case: # notebooks created, length a notebook was used, how many notebooks were shared with others (I think this could be a useful feature, even if ""sharing"" meant taking a user-opt-in text dump that could be used to re-create a notebook, rather than connecting ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:1958,efficient,efficient,1958,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290,1,['efficient'],['efficient']
Energy Efficiency,rray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2244); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2269); 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029); 	at is.hail.backend.spark.SparkBackend.parallelizeAndComputeWithIndex(SparkBackend.scala:355); 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:43); 	at __C16570Compiled.__m16792split_CollectDistributedArray(Emit.scala); 	at __C16570Compiled.__m16791begin_group_0(Emit.scala); 	at __C16570Compiled.apply(Emit.scala); 	at is,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:6405,schedul,scheduler,6405,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['schedul'],['scheduler']
Energy Efficiency,rrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:222); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 11 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:3177,schedul,scheduler,3177,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['schedul'],['scheduler']
Energy Efficiency,rrentBroadcast.scala:178); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:222); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 11 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:3275,schedul,scheduler,3275,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['schedul'],['scheduler']
Energy Efficiency,rsableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5930,schedul,scheduler,5930,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['schedul'],['scheduler']
Energy Efficiency,ry/client/v1/__pycache__/docker_image_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/client/v1/__pycache__/docker_http_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/client/__pycache__/docker_creds_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/client/__pycache__/docker_name_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_puller_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_pusher_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_appender_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_appender_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_puller_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_pusher_.cpython-39.pyc; /usr/local/share/google/dataproc/npd-config/docker-monitor-counter.json; /usr/local/share/google/dataproc/npd-config/docker-monitor.json; /usr/local/share/google/dataproc/npd-config/health-checker-docker.json; /usr/local/share/google/dataproc/npd-config/docker-monitor-filelog.json; /usr/local/share/google/dataproc/bdutil/fluentd/container_logging/plugin/test/Dockerfile; /usr/local/share/google/dataproc/bdutil/components/initialize/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/post-install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/activate/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh; /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/compon,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:11657,monitor,monitor-counter,11657,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['monitor'],['monitor-counter']
Energy Efficiency,"s=""analysis_type=SelectVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.unfiltered.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] select_expressions=[] excludeNonVariants=false excludeFiltered=false regenotype=false restrictAllelesTo=ALL kee",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:12316,monitor,monitorThreadEfficiency,12316,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['monitor'],['monitorThreadEfficiency']
Energy Efficiency,scala:20); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:58); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:63); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:62); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:462); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:498); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:63); 	at is.hail.backend.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:9486,adapt,adapted,9486,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,1,['adapt'],['adapted']
Energy Efficiency,scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at o,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200594,schedul,scheduler,200594,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['schedul'],['scheduler']
Energy Efficiency,scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:450); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:486); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.ba,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:8876,adapt,adapted,8876,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['adapt'],['adapted']
Energy Efficiency,scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerI,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:202148,schedul,scheduler,202148,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,see `org.apache.spark.scheduler.JobWaiter`. I think this will be fine.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4526#issuecomment-431683150:22,schedul,scheduler,22,https://hail.is,https://github.com/hail-is/hail/issues/4526#issuecomment-431683150,1,['schedul'],['scheduler']
Energy Efficiency,"sh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDKC9kZBCsWb78yJ1zcdhmrYEmjNEOaJN5RGuMBuoszUXGGOCJMFi6jMTgSVjTql90NchA0tWXAuooVLV++f8WIOwpP7TY1YzN1XCREyk8jKOMrIdCc22ueJlNvxmFfJhdDKBCy0eThWN2qFxQJ4p9SvzGlMd2r3nBy95v9f8WgkN8M/HTDwTsFafNT0arvHnmUY6rFHxQE9TgTRlH1/sZ7mMxzmVZ8NKI/wIXTkv53TbylBYbvkEXyVFl3OBj1MUvo17v99LGdQNFcAiWR/pRsDvXY415FzootwShgQpmvuPLP7buTqVcrRnwRr2hZcpaydOyEaErYsuEPiot0RPrvsIXSkSI4NlIbqO2i4gjfF8FwpDzyY0WtAvUbsY8dKxWXzcIWtQzUyYeqJq1R0Yh8p3ijmLgrkpAJTI/Lz8WT2foUFg7gYQwc9xbFN6aQzQwUQ0Y8s0DDvQqnbby12IXXHI+rjuh1TH8lIRPw/UsFInJn3WS1MBp4FRiXwRs9EwVhfeb+b8Z5rnaQ3RrmM8SY0kjg0i05rkMkygEnPuSec6qKYREHW8n4wbYQNhJvDW9RhUIGnzn3IQRJB57bOZ8xwPkZ97PM0WGsCMWwupSOuEk/NsFe69cZwbElYZJeqeA/bKKsmRsJ/tjzyYMLUlj4L++4GQIwPHgtjmQ9kUEeaw== dgoldste@wmce3-cb7\n"",; ""path"": ""/home/batch-worker/.ssh/authorized_keys""; }; ]; }; },; ""requireGuestProvisionSignal"": true,; ""secrets"": [],; ""windowsConfiguration"": null; },; ""plan"": null,; ""platformFaultDomain"": null,; ""priority"": ""Spot"",; ""provisioningState"": ""Succeeded"",; ""proximityPlacementGroup"": null,; ""resourceGroup"": ""dgoldste"",; ""resources"": null,; ""scheduledEventsProfile"": null,; ""securityProfile"": null,; ""storageProfile"": {; ""dataDisks"": [],; ""imageReference"": {; ""exactVersion"": ""0.0.12"",; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/galleries/dgoldste_batch/images/batch-worker/versions/0.0.12"",; ""offer"": null,; ""publisher"": null,; ""resourceGroup"": ""dgoldste"",; ""sharedGalleryImageId"": null,; ""sku"": null,; ""version"": null; },; ""osDisk"": {; ""caching"": ""ReadOnly"",; ""createOption"": ""FromImage"",; ""deleteOption"": ""Delete"",; ""diffDiskSettings"": null,; ""diskSizeGb"": 30,; ""encryptionSettings"": null,; ""image"": null,; ""managedDisk"": {; ""diskEncryptionSet"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/disks/batch-worker-pr-11144-default-nbthv8fduvd6-highmem-13t6m-os"",; ""resourceGroup"": ""dgoldste"",; ""storageAccountType"":",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:8524,schedul,scheduledEventsProfile,8524,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,1,['schedul'],['scheduledEventsProfile']
Energy Efficiency,"sh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDKC9kZBCsWb78yJ1zcdhmrYEmjNEOaJN5RGuMBuoszUXGGOCJMFi6jMTgSVjTql90NchA0tWXAuooVLV++f8WIOwpP7TY1YzN1XCREyk8jKOMrIdCc22ueJlNvxmFfJhdDKBCy0eThWN2qFxQJ4p9SvzGlMd2r3nBy95v9f8WgkN8M/HTDwTsFafNT0arvHnmUY6rFHxQE9TgTRlH1/sZ7mMxzmVZ8NKI/wIXTkv53TbylBYbvkEXyVFl3OBj1MUvo17v99LGdQNFcAiWR/pRsDvXY415FzootwShgQpmvuPLP7buTqVcrRnwRr2hZcpaydOyEaErYsuEPiot0RPrvsIXSkSI4NlIbqO2i4gjfF8FwpDzyY0WtAvUbsY8dKxWXzcIWtQzUyYeqJq1R0Yh8p3ijmLgrkpAJTI/Lz8WT2foUFg7gYQwc9xbFN6aQzQwUQ0Y8s0DDvQqnbby12IXXHI+rjuh1TH8lIRPw/UsFInJn3WS1MBp4FRiXwRs9EwVhfeb+b8Z5rnaQ3RrmM8SY0kjg0i05rkMkygEnPuSec6qKYREHW8n4wbYQNhJvDW9RhUIGnzn3IQRJB57bOZ8xwPkZ97PM0WGsCMWwupSOuEk/NsFe69cZwbElYZJeqeA/bKKsmRsJ/tjzyYMLUlj4L++4GQIwPHgtjmQ9kUEeaw== dgoldste@wmce3-cb7\n"",; ""path"": ""/home/batch-worker/.ssh/authorized_keys""; }; ]; }; },; ""requireGuestProvisionSignal"": true,; ""secrets"": [],; ""windowsConfiguration"": null; },; ""plan"": null,; ""platformFaultDomain"": null,; ""priority"": ""Spot"",; ""provisioningState"": ""Succeeded"",; ""proximityPlacementGroup"": null,; ""resourceGroup"": ""dgoldste"",; ""resources"": null,; ""scheduledEventsProfile"": null,; ""securityProfile"": null,; ""storageProfile"": {; ""dataDisks"": [],; ""imageReference"": {; ""exactVersion"": ""0.0.12"",; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/galleries/dgoldste_batch/images/batch-worker/versions/0.0.12"",; ""offer"": null,; ""publisher"": null,; ""resourceGroup"": ""dgoldste"",; ""sharedGalleryImageId"": null,; ""sku"": null,; ""version"": null; },; ""osDisk"": {; ""caching"": ""ReadOnly"",; ""createOption"": ""FromImage"",; ""deleteOption"": ""Delete"",; ""diffDiskSettings"": null,; ""diskSizeGb"": 30,; ""encryptionSettings"": null,; ""image"": null,; ""managedDisk"": {; ""diskEncryptionSet"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/disks/batch-worker-pr-11144-default-nbthv8fduvd6-standard-i4sun-os"",; ""resourceGroup"": ""dgoldste"",; ""storageAccountType""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:12872,schedul,scheduledEventsProfile,12872,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,1,['schedul'],['scheduledEventsProfile']
Energy Efficiency,shMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201517,schedul,scheduler,201517,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"shMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 f",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205164,schedul,scheduler,205164,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"st-6boype3d'...; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 85, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 210, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-6boype3d', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip', '--properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh', '--master-machine-type=n1-standard-1', '--master-boot-disk-size=40GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-standard-1', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--bucket=hail-ci-0-1-dataproc-staging-bucket', '--max-idle=10m']' returned non-zero exit status 1. real	20m34.381s; user	0m6.329s; sys	0m1.522s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:10766,monitor,monitoring,10766,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518,1,['monitor'],['monitoring']
Energy Efficiency,stics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2244); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2269); 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:5857,adapt,adapted,5857,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['adapt'],['adapted']
Energy Efficiency,"svd symEigD symEig symEigR; 500 .092 .051 .234 .038; 500 .088 .050 .217 .046; 500 .093 .047 .229 .041; 1000 .458 .193 1.659 .191; 1000 .430 .184 1.469 .195; 1000 .441 .207 1.464 .183; 1500 1.399 .7245 4.810 .595; 1500 1.407 .5990 4.777 .601; 1500 1.421 .5835 5.236 .627; 2000 3.272 1.479 10.942 1.386; 2000 3.205 1.337 11.006 1.381; 2000 3.473 1.354 10.933 1.366; 2500 6.180 2.519 21.639 2.750; 2500 6.217 2.718 21.772 2.758; 2500 6.580 2.590 21.176 2.661; 3000 10.169 4.117 51.154 4.716; 3000 10.414 4.131 51.602 4.834; 3000 10.709 4.219 46.711 4.794; 3500 15.451 6.549 72.2 7.365; 3500 15.353 7.058 75.9 7.194; 3500 15.350 6.516 70.9 7.210; 4000 20.584 9.111 112.6 10.725; 4000 22.085 9.476 110.3 10.594; 4000 21.920 9.461 108.2 11.062; 4500 29.075 13.488 143.8 15.440; 4500 30.305 13.402 140.2 15.338; 4500 31.339 13.562 134.3 15.294; 5000 43.908 17.818 196.0 21.286; 5000 40.874 17.821 197.7 21.231; 5000 41.582 18.088 198.9 21.357; 5500 58.772 24.747 271.1 28.879; 5500 60.6 23.844 269.8 28.130; 5500 60.9 24.197 275.3 28.356; ```. Here's the R code for the plot for reference. I gave up on making a proper legend. ```; library(ggplot2); df <- read.table(""/Users/Jon/Desktop/svdEigen.tsv"",header=TRUE); ggplot(df, aes(dim)) + ; geom_point(aes(y = svd), color=""black"") + ; geom_smooth(aes(y = svd), method=lm, formula = y ~ poly(x, 3), color=""orange"", fullrange=TRUE) +; geom_point(aes(y = symEigD), color=""black"") + ; geom_smooth(aes(y = symEigD), method=lm, formula = y ~ poly(x, 3), color=""blue"", fullrange=TRUE) +; geom_point(aes(y = symEig), color=""black"") + ; geom_smooth(aes(y = symEig), method=lm, formula = y ~ poly(x, 3), color=""red"", fullrange=TRUE) +; geom_point(aes(y = symEigR), color=""black"") + ; geom_smooth(aes(y = symEigR), method=lm, formula = y ~ poly(x, 3), color=""green"", fullrange=TRUE) +; xlim(0, 360) +; ylim(0, 6000) +; ggtitle(""symEig (red), svd (orange), symEigR (green), symEigD (blue)\nstandard Wishart matrix, cubic spline"") +; labs(x=""dimension"", y=""seconds""); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/906#issuecomment-251835211:3189,green,green,3189,https://hail.is,https://github.com/hail-is/hail/pull/906#issuecomment-251835211,2,['green'],['green']
Energy Efficiency,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1134); 	at is.hail.variant.VariantSampleMatrix.countVariants(VariantSampleMatrix.scala:810); 	at is.hail.variant.VariantDatasetFunctions$.count$extension(VariantDataset.scala:504); 	at is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4627,schedul,scheduler,4627,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['schedul'],['scheduler']
Energy Efficiency,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:3070,schedul,scheduler,3070,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['schedul'],['scheduler']
Energy Efficiency,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:4870,schedul,scheduler,4870,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,2,['schedul'],['scheduler']
Energy Efficiency,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1158); 	at is.hail.rvd.RVD$class.count(RVD.scala:183); 	at is.hail.rvd.OrderedRVD.count(OrderedRVD.scala:19); 	at is.hail.methods.LDPrune$$anonfun$9.apply(LDPrune.scala:471); 	at is.hail.me,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:3786,schedul,scheduler,3786,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['schedul'],['scheduler']
Energy Efficiency,"t org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGSched",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205727,schedul,scheduler,205727,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,t scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:4181,schedul,scheduler,4181,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,2,['schedul'],['scheduler']
Energy Efficiency,t.scala:285); 	at org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:84); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250); 	at org.apache.spark.Partitioner$$anonfun$defaultPartitioner$2.apply(Partitioner.scala:66); 	at org.apache.spark.Partitioner$$anonfun$defaultPartitioner$2.apply(Partitioner.scala:66); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.immutable.List.map(List.scala:285); 	at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:66); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:329); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:329); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:328); 	at is.hail.methods.MendelErrors.nErrorPerNuclearFamily(MendelErrors.scala:145); 	at is.hail.methods.MendelErrors.fMendelKT(MendelErrors.scala:184); 	at is.hail.variant.MatrixTable.mendelErrors(MatrixTable.scala:1913); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodI,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3074#issuecomment-370494908:8892,reduce,reduceByKey,8892,https://hail.is,https://github.com/hail-is/hail/issues/3074#issuecomment-370494908,1,['reduce'],['reduceByKey']
Energy Efficiency,"tebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-name; secret:; defaultMode: 420; secretName: gsa-key-j7gwm; - name: user-kmpnh-token-hbdd4; secret:; defaultMode: 420; secretName: user-kmpnh-token-hbdd4. hostIP: 10.128.0.32; phase: Running; podIP: 10.32.19.165; qosClass: Burstable; startTime: ""2019-04-02T19:50:21Z""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:2929,schedul,schedulerName,2929,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611,2,['schedul'],"['scheduler', 'schedulerName']"
Energy Efficiency,"ted caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8844,schedul,scheduler,8844,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['schedul'],['scheduler']
Energy Efficiency,"ted, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.Callb",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215097,schedul,scheduler,215097,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"ternal signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.u",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:5337,adapt,adapted,5337,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['adapt'],['adapted']
Energy Efficiency,ternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1509,schedul,scheduler,1509,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,1,['schedul'],['scheduler']
Energy Efficiency,tests are green now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11323#issuecomment-1041946188:10,green,green,10,https://hail.is,https://github.com/hail-is/hail/pull/11323#issuecomment-1041946188,1,['green'],['green']
Energy Efficiency,textRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5063,schedul,scheduler,5063,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['schedul'],['scheduler']
Energy Efficiency,thIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5161,schedul,scheduler,5161,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['schedul'],['scheduler']
Energy Efficiency,thanks for leading the charge to fix this stuff fast!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8806#issuecomment-629406978:23,charge,charge,23,https://hail.is,https://github.com/hail-is/hail/pull/8806#issuecomment-629406978,1,['charge'],['charge']
Energy Efficiency,the two rewrites have made this much more powerful and understandable,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5245#issuecomment-464144563:42,power,powerful,42,https://hail.is,https://github.com/hail-is/hail/pull/5245#issuecomment-464144563,1,['power'],['powerful']
Energy Efficiency,"think helped to reduce the number of places that are aware of the notion of a default_region. It's really now just isolated to the `InstanceCollectionManager`, since that's the piece of code that's making the decision ""use the default region when the cluster is small"". I didn't quite like the pattern of retrieving a default from a `LocationMonitor` just to give it right back to the location monitor in the next line. I think this way the `LocationMonitor` API is much simpler, and we can actually remove its `default_location` method entirely as I believe it is no longer used. I can do that in a follow-up PR if you like this approach. One other thing is I wanted to articulate the distinction between the ""region CI needs its jobs in"" and the ""default region that batch will spin up workers in for small clusters"". While they are in practice the same, I found that treating them both as the ""default_region"" tied the logic around jobs for CI closely with internal Batch decisions and made it more confusing for me to reason about. I tried to separate out these two concepts so that in the future when jobs support region-specific scheduling it will be easier to excise the CI-specific code from the Batch scheduler. Another thing that I realized during this process is that Azure has regions and zones just like GCP, though they may differ slightly since we don't need to specify a zone for a VM and such. I am fine with using the term ""location"" to mean ""where we scheduled the VM, either zone or region depending on the cloud provider"", but I would also like to follow up with a sweep that makes this language more precise where possible. For example, the `possible_cloud_locations` function is really just `possible_cloud_regions`, and we could even go so far as mandating a `region` field in the global config instead of having `azure_location` and `gcp_region`, which are synonymous even though named differently. It also leads me to wonder why we only schedule in a single region in Azure?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12078#issuecomment-1207095240:1160,schedul,scheduling,1160,https://hail.is,https://github.com/hail-is/hail/pull/12078#issuecomment-1207095240,4,['schedul'],"['schedule', 'scheduled', 'scheduler', 'scheduling']"
Energy Efficiency,tor$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:2998,schedul,scheduler,2998,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['schedul'],['scheduler']
Energy Efficiency,"tors and matrices with primary dimension the number of samples (as in QR), and because BLAS3 matrix multiplication is fast. I also checked that upping to 8 covariates didn't balance things out. It didn't. The fancy approach basically trades X.t * X and generic k-dim solve for a QR on X and triangular k-dim solve...better for larger k and smaller n. ```; Standard. 2 cov; Lin 7s; Score 54.5s; LRT 93s; Wald 90s. 2 cov, QR / TriSolve; Lin 7.42s; Score 53.6s, 53.1s; LRT 2m06s, 1m59s; Wald 1m53s, 1m54s. 8 cov; Lin 7.16s; Score 59.1s; LRT 2m25s, 2m20s, 2m26s; Wald 2m27s, 2m27s, 2m25s. 8 cov, QR / TriSolve; Lin 7.76s; Score 52.7s; LRT 3m30s; Wald 3m26s; ```. For Firth, since I'm using QR anyway, may as will use TriSolve (though the timing is not particularly effected even with 8 covariates):. ```; 2 cov:; Firth 5m 10s, 4m55s, 5m7s. 8 cov:; Firth 10m37s, 10m50s, 10m28s; ```. For reference, here's the core logic of the QR approach. This corresponds to another version of LogisticRegressionFit where I tried to reduce unnecessary computation, see below. ```; while (!converged && !exploded && iter <= maxIter) {; try {; val mu = sigmoid(X * b); val sqrtW = sqrt(mu :* (1d - mu)); val QR = qr.reduced(X(::, *) :* sqrtW). deltaB = TriSolve(QR.r, QR.q.t * ((y - mu) :/ sqrtW)). if (max(abs(deltaB)) < tol) {; converged = true; if (computeScoreR) {; optScore = Some(X.t * (y - mu)); optR = Some(QR.r); }; if (computeSe) {; val invR = inv(QR.r) // could speed up inverting as upper triangular, or avoid altogether as 1 / se(-1) = fit.fisherSqrt(-1, -1); optSe = Some(norm(invR(*, ::))); }; if (computeLogLkld); optLogLkhd = Some(sum(breeze.numerics.log((y :* mu) + ((1d - y) :* (1d - mu))))); } else {; iter += 1; b += deltaB; }; }; ```. ```; case class LogisticRegressionFit(; b: DenseVector[Double],; optScore: Option[DenseVector[Double]],; optR: Option[DenseMatrix[Double]],; optSe: Option[DenseVector[Double]],; optLogLkhd: Option[Double],; nIter: Int,; converged: Boolean,; exploded: Boolean); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1375#issuecomment-279532833:1420,reduce,reduced,1420,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279532833,1,['reduce'],['reduced']
Energy Efficiency,trix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12293,schedul,scheduler,12293,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['schedul'],['scheduler']
Energy Efficiency,"ts, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.3M allocated (192.0K blocks / 2.1M chunks), regions.size = 3, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.3M allocated (256.0K blocks / 2.1M chunks), regions.size = 4, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.4M allocated (320.0K blocks / 2.1M chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.286 : INFO: RegionPool: REPORT_THRESHOLD: 28.8M allocated (576.0K blocks / 28.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.657 : INFO: RegionPool: REPORT_THRESHOLD: 30.8M allocated (576.0K blocks / 30.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.683 : INFO: RegionPool: REPORT_THRESHOLD: 32.8M allocated (576.0K blocks / 32.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.709 : INFO: RegionPool: REPORT_THRESHOLD: 32.8M allocated (576.0K blocks / 32.3M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:44:22.426 GoogleStorageFS$: INFO: createNoCompression: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd8-4f1b-bcde-fbf29180c347; 2023-09-27 16:44:22.495 GoogleStorageFS$: INFO: close: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd8-4f1b-bcde-fbf29180c347; 2023-09-27 16:44:22.620 GoogleStorageFS$: INFO: closed: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd8-4f1b-bcde-fbf29180c347; 2023-09-27 16:44:22.621 : INFO: TaskReport: stage=0, partition=7028, attempt=0, peakBytes=62266032, peakBytesReadable=59.38 MiB, chunks requested=7212",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:4694,allocate,allocated,4694,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['allocate'],['allocated']
Energy Efficiency,ultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4371,schedul,scheduler,4371,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['schedul'],['scheduler']
Energy Efficiency,ultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:2814,schedul,scheduler,2814,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['schedul'],['scheduler']
Energy Efficiency,un$apply$8.apply(RVD.scala:218); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:2958,schedul,scheduler,2958,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['schedul'],['scheduler']
Energy Efficiency,un$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5258,schedul,scheduler,5258,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['schedul'],['scheduler']
Energy Efficiency,upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 0; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:297); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:279); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:297); 		at is.hail.io.fs.FSPositionedOutputStream.write(FS.scala:219); 		at java.io.DataOutputStream.write(DataOutputStream.java:107); 		at is.hail.fs.FSSuite.$anonfun$testSeekMoreThanMaxInt$1(FSSuite.scala:329); 		at is.hail.fs.FSSuite.$anonfun$testSeekMoreThanMaxInt$1$adapted(FSSuite.scala:323); 		at is.hail.utils.package$.using(package.scala:635); 		... 26 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:9019,adapt,adapted,9019,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,1,['adapt'],['adapted']
Energy Efficiency,utionTimer$.time(ExecutionTimer.scala:52); E 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.withExecuteContext$1(ServiceBackend.scala:535); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:602); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$7(ServiceBackend.scala:433); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$7$adapted(ServiceBackend.scala:432); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.scala:432); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:77); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:432); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5$adapted(ServiceBackend.scala:430); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:430); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:77); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:430); E 	at is.hail.backend.service.Main$.main(Main.scala:33); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:3180,adapt,adapted,3180,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['adapt'],['adapted']
Energy Efficiency,"v/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/batch-worker"",""us-docker.pkg.dev/hail-vdc/hail/benchmark"",""us-docker.pkg.dev/hail-vdc/hail/blog_nginx"",""us-docker.pkg.dev/hail-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-docker.pkg.dev/hail-vdc/hail/grafana"",""us-docker.pkg.dev/hail-vdc/hail/hail-base"",""us-docker.pkg.dev/hail-vdc/hail/hail-build"",""us-docker.pkg.dev/hail-vdc/hail/hail-buildkit"",""us-docker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python38"",""us-docker.pkg.dev/hail-vdc/hail/hail-ubuntu"",""us-docker.pkg.dev/hail-vdc/hail/memory"",""us-docker.pkg.dev/hail-vdc/hail/monitoring"",""us-docker.pkg.dev/hail-vdc/hail/notebook"",""us-docker.pkg.dev/hail-vdc/hail/notebook_nginx"",""us-docker.pkg.dev/hail-vdc/hail/prometheus"",""us-docker.pkg.dev/hail-vdc/hail/service-base"",""us-docker.pkg.dev/hail-vdc/hail/service-java-run-base"",""us-docker.pkg.dev/hail-vdc/hail/test-ci"",""us-docker.pkg.dev/hail-vdc/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95""],""grace"":""48h"",""recursive"":true,""tag_filter_all"":""cache-pr-.*""}; ```. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/b",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545:1349,monitor,monitoring,1349,https://hail.is,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545,1,['monitor'],['monitoring']
Energy Efficiency,va.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1134); 	at is.hail.variant.VariantSampleMatrix.countVariants(VariantSampleMatrix.scala:810); 	at is.hail.variant.VariantDatasetFunctions$.count$extension(VariantDataset.scala:504); 	at is.hail.variant.VariantDatasetFunctions.count(VariantDataset.scala:494); 	at sun.reflect.Nati,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4716,schedul,scheduler,4716,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['schedul'],['scheduler']
Energy Efficiency,va.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:3159,schedul,scheduler,3159,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['schedul'],['scheduler']
Energy Efficiency,va.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:4959,schedul,scheduler,4959,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,2,['schedul'],['scheduler']
Energy Efficiency,va.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1158); 	at is.hail.rvd.RVD$class.count(RVD.scala:183); 	at is.hail.rvd.OrderedRVD.count(OrderedRVD.scala:19); 	at is.hail.methods.LDPrune$$anonfun$9.apply(LDPrune.scala:471); 	at is.hail.methods.LDPrune$$anonfun$9.apply(LDPrune.scala:469); 	at is.hail.utils.package$.time(packag,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:3875,schedul,scheduler,3875,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['schedul'],['scheduler']
Energy Efficiency,va:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1921); at org.apache.spark.rdd.RDD.count(RDD.scala:1125); at org.broadinstitute.hail.driver.Count$.run(Count.scala:37); at org.broadinstitute.hail.driver.Count$.run(Count.scala:9); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:23,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6736,schedul,scheduler,6736,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['schedul'],['scheduler']
Energy Efficiency,va:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12974,schedul,scheduler,12974,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['schedul'],['scheduler']
Energy Efficiency,"version, deepest)); hail.java.FatalError: SparkException: Failed to get broadcast_4_piece0 of broadcast_4. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, com2, executor 1): java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:1732,schedul,scheduler,1732,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['schedul'],['scheduler']
Energy Efficiency,"which reduced the diff to 0, and GitHub automatically closed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5004#issuecomment-448649659:6,reduce,reduced,6,https://hail.is,https://github.com/hail-is/hail/pull/5004#issuecomment-448649659,1,['reduce'],['reduced']
Energy Efficiency,xt(Iterator.scala:444); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:3097,schedul,scheduler,3097,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['schedul'],['scheduler']
Energy Efficiency,"xt: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:214785,schedul,scheduler,214785,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,y one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:725); at i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:9185,schedul,scheduler,9185,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['schedul'],['scheduler']
Energy Efficiency,y$6(BackendUtils.scala:52) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.utils.package$.using(package.scala:635) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:51) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$12(Worker.scala:167) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$11(Worker.scala:166) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$11$adapted(Worker.scala:164) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.utils.package$.using(package.scala:635) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.main(Worker.scala:164) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Main$.main(Main.scala:14) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	... 11 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:6041,adapt,adapted,6041,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,1,['adapt'],['adapted']
Energy Efficiency,y$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:202043,schedul,scheduler,202043,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"ya I think I wanted something like that in concept but agree with your sentiments about the implementation being wasteful. Having `parent_ids` and `rel_parent_ids` that are both just arrays of numbers, where the latter gets transformed and concatenated to the former seems like an efficient and nicely backwards-compatible implementation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1218087772:281,efficient,efficient,281,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1218087772,1,['efficient'],['efficient']
Energy Efficiency,"~I verified this scales all the way to 50,000 partitions but the batch-driver can't schedule these fast enough to make the test fast. They take less than a second but more than 300ms. We'd need like 64 8-core (512 cores) nodes to bring test time down to a reasonable amount. We should strive to get there but batch would need to schedule at 512 jobs per second for that to make sense.~ Hmm, something went wrong. OK, we'll need to revisit 50k partition tables. But let's get this in, the current code is obviously wrong.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13065#issuecomment-1550142533:84,schedul,schedule,84,https://hail.is,https://github.com/hail-is/hail/pull/13065#issuecomment-1550142533,2,['schedul'],['schedule']
Integrability,	at is.hail.shadedazure.com.azure.storage.common.implementation.StorageImplUtils.blockWithOptionalTimeout(StorageImplUtils.java:133); 		at is.hail.shadedazure.com.azure.storage.blob.specialized.BlobClientBase.getPropertiesWithResponse(BlobClientBase.java:1379); 		at is.hail.shadedazure.com.azure.storage.blob.specialized.BlobClientBase.getProperties(BlobClientBase.java:1348); 		at is.hail.io.fs.AzureStorageFS.$anonfun$openNoCompression$1(AzureStorageFS.scala:223); 		at is.hail.io.fs.AzureStorageFS.$anonfun$handlePublicAccessError$1(AzureStorageFS.scala:175); 		at is.hail.services.package$.retryTransientErrors(package.scala:124); 		at is.hail.io.fs.AzureStorageFS.handlePublicAccessError(AzureStorageFS.scala:174); 		at is.hail.io.fs.AzureStorageFS.openNoCompression(AzureStorageFS.scala:220); 		at is.hail.io.fs.RouterFS.openNoCompression(RouterFS.scala:20); 		at is.hail.io.fs.FS.openNoCompression(FS.scala:322); 		at is.hail.io.fs.FS.openNoCompression$(FS.scala:322); 		at is.hail.io.fs.RouterFS.openNoCompression(RouterFS.scala:3); 		at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); 		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 		at is.hail.services.package$.retryTransientErrors(package.scala:124); 		at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:459); 		at is.hail.backend.service.Main$.main(Main.scala:15); 		at is.hail.backend.service.Main.main(Main.scala); 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 		at java.lang.reflect.Method.invoke(Method.java:498); 		at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at java.util.concurrent.FutureTask.run(FutureTask.java:266); 		at java.util.concurrent,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906430:1293,Rout,RouterFS,1293,https://hail.is,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906430,1,['Rout'],['RouterFS']
Integrability, 	at is.hail.expr.ir.EmitCode$.fromI(Emit.scala:391); 	at is.hail.expr.ir.Emit.$anonfun$emitI$25(Emit.scala:816); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at scala.collection.TraversableLike.map(TraversableLike.scala:286); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279); 	at scala.collection.AbstractTraversable.map(Traversable.scala:108); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:815); 	at is.hail.expr.ir.Emit$.$anonfun$apply$4(Emit.scala:99); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:19); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedCode(EmitCodeBuilder.scala:24); 	at is.hail.expr.ir.EmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1044); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1095); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder$(EmitClassBuilder.scala:1095); 	at is.hail.expr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1192); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:97); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:78); 	at is.hail.TestUtils$.eval(TestUtils.scala:256); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$5(TestUtils.scala:366); 	at scala.collection.immutable.Set$Set4.foreach(Set.scala:289); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$4(TestUtils.scala:348); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$4$adapted(TestUtils.scala:339); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13);,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10330#issuecomment-827119604:1647,Wrap,WrappedEmitMethodBuilder,1647,https://hail.is,https://github.com/hail-is/hail/pull/10330#issuecomment-827119604,1,['Wrap'],['WrappedEmitMethodBuilder']
Integrability," ""/tmp/c38a09976d3c40abbc33f88f4fd39639/pyscripts_Zf07i2.zip/constraint_utils/constraint_basics.py"", line 41, in get_old_mu_data; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 772, in transmute; File ""<decorator-gen-648>"", line 2, in _select; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/typecheck/check.py"", line 546, in wrapper; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 438, in _select; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 447, in _select_scala; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: NullPointerException: null. Java stack trace:; java.lang.NullPointerException: null; 	at scala.collection.convert.Wrappers$JListWrapper.length(Wrappers.scala:86); 	at scala.collection.SeqLike$class.size(SeqLike.scala:106); 	at scala.collection.AbstractSeq.size(Seq.scala:41); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:285); 	at scala.collection.AbstractTraversable.toArray(Traversable.scala:104); 	at is.hail.utils.richUtils.RichIterable.toFastIndexedSeq(RichIterable.scala:83); 	at is.hail.table.Table.select(Table.scala:436); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4314#issuecomment-420405267:1746,Wrap,Wrappers,1746,https://hail.is,https://github.com/hail-is/hail/issues/4314#issuecomment-420405267,1,['Wrap'],['Wrappers']
Integrability," 'cinder': None,; 'config_map': None,; 'downward_api': None,; 'empty_dir': None,; 'fc': None,; 'flex_volume': None,; 'flocker': None,; 'gce_persistent_disk': None,; 'git_repo': None,; 'glusterfs': None,; 'host_path': None,; 'iscsi': None,; 'name': 'default-token-8h99c',; 'nfs': None,; 'persistent_volume_claim': None,; 'photon_persistent_disk': None,; 'portworx_volume': None,; 'projected': None,; 'quobyte': None,; 'rbd': None,; 'scale_io': None,; 'secret': {'default_mode': 420,; 'items': None,; 'optional': None,; 'secret_name': 'default-token-8h99c'},; 'storageos': None,; 'vsphere_volume': None}]},; 'status': {'conditions': [{'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'Initialized'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'Ready'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'ContainersReady'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'PodScheduled'}],; 'container_statuses': [{'container_id': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_id': '',; 'last_state': {'running': None,; 'terminated': None,; 'waiting': None},; 'name': 'main',; 'ready': False,; 'restart_count': 0,; 'state': {'running': None,; 'terminated': {'container_id': None,; 'exit_code': 0,; 'finished_at': None,; 'message': None,; 'reason': None,; 'signal': None,; 'started_at': None},; 'waiting': None}}],; 'host_ip': '10.128.0.8',; 'init_container_statuses': None,; 'message': N",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:8090,message,message,8090,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['message'],['message']
Integrability," (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (What Jupyter hub does.). > I thought it would take less time to get a subdirectory working than figure out how to add a new domain and a cert and deal with DNS. Fair. I added a wildcard *.staging.hail.is for staging, I'll do the same thing for Hail. Then you don't need to change the DNS to add a domain, and I'll write up instructions for adding a new domain to get certs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:1925,depend,depending,1925,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659,1,['depend'],['depending']
Integrability," /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in persist(self, storage_level); 2110 Persisted table.; 2111 """"""; -> 2112 return Env.backend().persist(self); 2113 ; 2114 def unpersist(self) -> 'Table':. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py in persist(self, dataset); 167 from hail.context import TemporaryFilename; 168 tempfile = TemporaryFilename(prefix=f'persist_{type(dataset).__name__}'); --> 169 persisted = dataset.checkpoint(tempfile.__enter__()); 170 self._persisted_locations[persisted] = (tempfile, dataset); 171 return persisted. <decorator-gen-1330> in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals); 1329 ; 1330 if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; -> 1331 self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); 1332 _assert_type = self._type; 1333 _load_refs = False. <decorator-gen-1332> in write(self, output, overwrite, stage_locally, _codec_spec). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/minico",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:4273,wrap,wrapper,4273,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,1,['wrap'],['wrapper']
Integrability," = ir.typ._from_encoding(result); 102 return (value, timings) if timed else value. File ~/mambaforge/lib/python3.9/site-packages/py4j/java_gateway.py:1304, in JavaMember.__call__(self, *args); 1298 command = proto.CALL_COMMAND_NAME +\; 1299 self.command_header +\; 1300 args_command +\; 1301 proto.END_COMMAND_PART; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1307 for temp_arg in temp_args:; 1308 temp_arg._detach(). File ~/mambaforge/lib/python3.9/site-packages/hail/backend/py4j_backend.py:21, in handle_java_exception.<locals>.deco(*args, **kwargs); 19 import pyspark; 20 try:; ---> 21 return f(*args, **kwargs); 22 except py4j.protocol.Py4JJavaError as e:; 23 s = e.java_exception.toString(). File ~/mambaforge/lib/python3.9/site-packages/py4j/protocol.py:330, in get_return_value(answer, gateway_client, target_id, name); 326 raise Py4JJavaError(; 327 ""An error occurred while calling {0}{1}{2}.\n"".; 328 format(target_id, ""."", name), value); 329 else:; --> 330 raise Py4JError(; 331 ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; 332 format(target_id, ""."", name, value)); 333 else:; 334 raise Py4JError(; 335 ""An error occurred while calling {0}{1}{2}"".; 336 format(target_id, ""."", name)). Py4JError: An error occurred while calling o83._1. Trace:; java.lang.NegativeArraySizeException: -1966455376; 	at py4j.Base64.encodeToChar(Base64.java:681); 	at py4j.Base64.encodeToString(Base64.java:734); 	at py4j.Protocol.encodeBytes(Protocol.java:154); 	at py4j.ReturnObject.getPrimitiveReturnObject(ReturnObject.java:150); 	at py4j.Gateway.getReturnObject(Gateway.java:188); 	at py4j.Gateway.invoke(Gateway.java:283); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:829); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12035#issuecomment-1186014691:3623,Protocol,Protocol,3623,https://hail.is,https://github.com/hail-is/hail/issues/12035#issuecomment-1186014691,2,['Protocol'],['Protocol']
Integrability, E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:36); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.expr.ir.Compile$.$anonfun$apply$4(Compile.scala:45); E 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction(Backend.scala:126); E 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction$(Backend.scala:122); E 	at is.hail.backend.local.LocalBackend.lookupOrCompileCachedFunction(LocalBackend.scala:73); E 	at is.hail.expr.ir.Compile$.apply(Compile.scala:39); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$5(CompileAndEvaluate.scala:66); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:66); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); E 	at i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:8573,Wrap,WrappedArray,8573,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Wrap'],['WrappedArray']
Integrability," RV. 4. The hash function on (options, source) is now beefed up to cope with having only a; few distinct values of options; and is modified with the output of ""$(CXX) --version"",; so that when you upgrade your compiler, you won't get hits on modules compiled with the old; compiler. 5. build.gradle has a new target ""nativeLibPrebuilt"", for updating the prebuilt/lib/linux-x86-64; or prebuilt/lib/darwin. 6. The committed prebuilt libraries are built thus:. darwin - On my MacOS laptop, with the default (clang-based) compiler, -march=sandybridge; From my reading, I believe this should be compatible withall MacBook Pro's; released since 2011, and all versions of MacOS since 10.9 (the first to use; libc++ rather than libstdc++ as the default C++ library) - we're now at 10.13,; with 10.14 arriving some time in the fall. linux-x86-64 - Built on my home desktop running Ubuntu-16.04 LTS, and g++-5.0.4, with; -fabi-version=9. In theory this should work with all systems based on g++5.x and; later. I made some effort to move std::string out of the interfaces between prebuilt; and dynamic code, which gives it some chance of working on systems based on; g++-4.x, but haven't tested that. I'm planning to fire up VM's either in cloud or under VirtualBox, to test this against Ubuntu-14.04,; Ubuntu-18.04, and the latest stable RHEL, which should cover most of the bases. In the interest of getting this committed, I have not made changes relating to logging and; error messages. The DLL's are still in the jar, and I think it has to stay that way because; all nodes need to see libhail.so. The header files are also in the jar, and have to be; unpacked in a convoluted way, and that could probably be simplified if/when we change; the approach to packaging. Once this goes in, I can follow it with a PR which adds the NativePackDecoder in RowStore.scala,; controlled by whether environment variable ""HAIL_ENABLE_CPP_CODEGEN"" is defined; (so defaulting to using the JVM bytecode CompiledPackDecoder).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863:1655,interface,interfaces,1655,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863,2,"['interface', 'message']","['interfaces', 'messages']"
Integrability," [required] ;  arguments [ARGUMENTS]... You should use -- if you want to pass option-like arguments through. [default: None] ; ;  Options ;  --files TEXT Files or directories to add to the working directory of the job. [default: None] ;  --name TEXT The name of the batch. ;  --image-name TEXT Name of Docker image for the job (default: hailgenetics/hail) [default: None] ;  --output -o [text|yaml|json] [default: text] ;  --help Show this message and exit. ; . (base) dking@wm28c-761 hail % ; (base) dking@wm28c-761 hail % hailctl hdinsight submit --help ; ; Usage: hailctl hdinsight submit [OPTIONS] NAME STORAGE_ACCOUNT HTTP_PASSWORD ; SCRIPT [ARGUMENTS]... ; ; Submit a job to an HDInsight cluster configured for Hail. ; If you wish to pass option-like arguments you should use ""--"". For example: ; ; $ hailctl hdinsight submit name account password script.py --image-name docker.io/image my_script.py -- some-argument --animal dog ; ;  Arguments ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13447#issuecomment-1681403012:1859,message,message,1859,https://hail.is,https://github.com/hail-is/hail/pull/13447#issuecomment-1681403012,1,['message'],['message']
Integrability," ```; MakeStruct(WrappedArray((elements,Let(__iruid_5,GetField(Literal(struct{rows: array<struct{a: int32, b: str}>, global: struct{x: str}},[ArrayBuffer([0,row0]),[global]]),rows),ToArray(StreamMap(StreamRange(Let(__iruid_6,If(ApplyComparisonOp(LT(int32,int32),ArrayLen(Ref(__iruid_5,array<struct{a: int32, b: str}>)),I32(16)),ArrayLen(Ref(__iruid_5,array<struct{a: int32, b: str}>)),I32(16)),Let(__iruid_7,ApplyBinaryPrimOp(RoundToNegInfDivide(),ArrayLen(Ref(__iruid_5,array<struct{a: int32, b: str}>)),Ref(__iruid_6,int32)),Let(__iruid_8,ApplyBinaryPrimOp(Subtract(),ArrayLen(Ref(__iruid_5,array<struct{a: int32, b: str}>)),ApplyBinaryPrimOp(Multiply(),Ref(__iruid_7,int32),Ref(__iruid_6,int32))),If(ApplyComparisonOp(GTEQ(int32,int32),Ref(__iruid_6,int32),I32(1)),If(ApplyComparisonOp(GT(int32,int32),Ref(__iruid_8,int32),I32(0)),If(ApplyComparisonOp(LT(int32,int32),Ref(__iruid_8,int32),I32(1)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Multiply(),Ref(__iruid_7,int32),I32(1)),Ref(__iruid_8,int32)),ApplyBinaryPrimOp(Multiply(),ApplyBinaryPrimOp(Add(),Ref(__iruid_7,int32),I32(1)),I32(1))),ApplyBinaryPrimOp(Multiply(),Ref(__iruid_7,int32),I32(1))),I32(0))))),ApplyBinaryPrimOp(Add(),Let(__iruid_6,If(ApplyComparisonOp(LT(int32,int32),ArrayLen(Ref(__iruid_5,array<struct{a: int32, b: str}>)),I32(16)),ArrayLen(Ref(__iruid_5,array<struct{a: int32, b: str}>)),I32(16)),Let(__iruid_7,ApplyBinaryPrimOp(RoundToNegInfDivide(),ArrayLen(Ref(__iruid_5,array<struct{a: int32, b: str}>)),Ref(__iruid_6,int32)),Let(__iruid_8,ApplyBinaryPrimOp(Subtract(),ArrayLen(Ref(__iruid_5,array<struct{a: i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8599#issuecomment-617899349:430,Wrap,WrappedArray,430,https://hail.is,https://github.com/hail-is/hail/pull/8599#issuecomment-617899349,1,['Wrap'],['WrappedArray']
Integrability," a 35K cohort. The VCF format of chr1 is 2.4T.; > ; > Heh. So, yes, ""project"" VCFs grow super-linearly in the number of samples. I (and others) are currently pushing very hard for the VCF spec to support two sparse representations: ""local alleles"" ([samtools/hts-specs#434](https://github.com/samtools/hts-specs/pull/434)) and ""reference blocks"" ([samtools/hts-specs#435](https://github.com/samtools/hts-specs/pull/435)). When using these two sparse representations, you should be able to store 35,000 whole genomes in ~10TiB of GZIP-compressed VCF.; > ; > What is your calling pipeline? Do you generate GVCFs? If yes, I strongly recommend you use the [VDS Combiner](https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html#hail.vds.combiner.VariantDatasetCombiner) to produce a [VDS](https://hail.is/docs/0.2/vds/index.html). You can read more details in [this recent preprint we wrote](https://www.biorxiv.org/content/10.1101/2024.01.09.574205v1.full.pdf), but a VDS of 35,000 whole genomes should be a few terabytes. I'd guess 4 TiB, but it depends on your reference block granularity. I strongly recommend using size 10 GQ buckets. Looks like VDS is a better solution than HailMatrix. However, we got the joint call result as vcf alreay. Can VDS Combiner read joint call VCF and then save it as VDS format? I cannot find any example to transfer VCF to VDS. Thanks. > ; > > I don't know the Kryo JAR. I tested on both docker images hailgenetics/hail:0.2.126-py3.11 and hailgenetics/hail:0.2.127-py3.11.; > ; > Those should use Kryo 4.0.2. OK. My conclusion is that Kryo still has a bug preventing the serialization of very large objects. This becomes a limitation in Hail: we cannot support PLINK files with tens of millions of variants. Our community is largely transitioning to GVCFs and VDS, so I doubt we'll improve our PLINK1 importer to support such large PLINK1 files. That said, PRs are always welcome if loading such large PLINK1 files is a hard requirement for you all.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168#issuecomment-1937459344:1073,depend,depends,1073,https://hail.is,https://github.com/hail-is/hail/issues/14168#issuecomment-1937459344,1,['depend'],['depends']
Integrability," a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used in that package. This can be used with Kubernetes. `kubectl create secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_U",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:1661,depend,dependencies,1661,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935,1,['depend'],['dependencies']
Integrability," at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I. Yep. In fact everything request to a Hail service (besides a lets-encrypt path) gets redirected to https. > Or am I misunderstanding how this works?. Nope, you have it correct. $http_x_forwarded_proto should never be absent, and would be fine to use instead of $updated_scheme (but I'd prefer one of those two, rather than https, because otherwise we're not relying on our upstream infrastructure). > * I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? . Yeah that's right. Host refers to the current server (or in the proxied case, what gateway set Host to). X-Forwarded-Host is set by gateway to be the $http_host at the time it proxies the request to router, which is going to be blog.hail.is. > I don't quite follow your comment about our use of `Host` being wrong, in this case; I _think_ I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enough to know what happens if I change it to `proxy_set_header Host $http_host;` instead. I just mean that we're setting Host to the value that X-Forwarded-Host has, when it seems more appropriate for it to be the domain:port of the internal request. So we're relying on X-Forwarded-Host to set that value, but we could, and I think it may be cleaner / more informative, to rely on X-Forwarded-Host all the way through to the internal server that's receiving the request - in this case Ghost's ExpressJS server - instead of rewriting the Host field to be that of the external request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548082569:1688,rout,router,1688,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548082569,1,['rout'],['router']
Integrability," header). /opt/conda/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. /opt/conda/lib/python3.7/site-packages/hail/expr/expressions/base_expression.py in export(self, path, delimiter, missing, header); 1068 **{output_col_name: hl.delimit(column_names, delimiter)}); 1069 file_contents = header_table.union(file_contents); -> 1070 file_contents.export(path, delimiter=delimiter, header=False); 1071 ; 1072 @typecheck_method(n=int, _localize=bool). <decorator-gen-1190> in export(self, output, types_file, header, parallel, delimiter). /opt/conda/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. /opt/conda/lib/python3.7/site-packages/hail/table.py in export(self, output, types_file, header, parallel, delimiter); 1097 parallel = ir.ExportType.default(parallel); 1098 Env.backend().execute(; -> 1099 ir.TableWrite(self._tir, ir.TableTextWriter(output, types_file, header, parallel, delimiter))); 1100 ; 1101 def group_by(self, *exprs, **named_exprs) -> 'GroupedTable':. /opt/conda/lib/python3.7/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 103 return (value, timings) if timed else value; 104 except FatalError as e:; --> 105 raise e.maybe_user_error(ir) from None; 106 ; 107 async def _async_execute(self, ir, timed=False):. /opt/conda/lib/python3.7/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 97 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 98 try:; ---> 99 result_tuple = self._jba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:1718,wrap,wrapper,1718,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,3,['wrap'],['wrapper']
Integrability," in under ten seconds - in; that realm compilation time could become a critical factor as a serial bottleneck. [The place where persistent cacheing of compiled files helps most of all is in testing,; where you really are running the exact same queries over and over again on the same; small datasets, and in many cases after making small changes which only affect a few; of the queries]. b) We may actually have some version of the locking problem even if we don't try to reuse the files -; since we have several workers on a node, and possibly a master as well, all needing to put code; into a file (or wait for someone else to populate the file) so that they can load it. Depending on ; precisely how Spark manages things (which I wouldn't want to depend on too much anyway). In fact it's essential that all the workers share the same DLL file, because otherwise they'd be; trying to load multiple DLL's defining the same symbols. That aspect of it could be handled by; putting the files into a per-process directory and using in-memory (std::mutex) synchronization.; But y'know, given that we have to write the DLL's out, it just seemed natural to let them persist; (and until debugging it on MacOS, I thought I could manage it with nothing but atomic file-create; and atomic-rename, but that didn't quite pan out). As for writing LLVM IR, it can definitely be done, because that's what Endeca/Oracle did. But there; was such a huge learning curve that only 3 people ever did it successfully (I wasn't one of them),; and debugging seemed very unpleasant and slow. [It was also a masterful achievement in ; job-security-through-obscurity, because no-one in management was going to mess with the; two people who wrote it - until the whole project got canned]. ... and in the time I was there, the Endeca/Oracle stuff wasn't distributed, which could be another; place where the generate-LLVM-IR needs some kind of extra glue for distributing compiled code,; whereas the conventional DLL's are trivial",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385:1831,synchroniz,synchronization,1831,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385,1,['synchroniz'],['synchronization']
Integrability," is.hail.keytable.KeyTable.export(KeyTable.scala:537); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). ```. I instead tried to run the same code in two separate jupyter notebooks, with the same code inside but different ways to initialize the hailcontext, one like this (works and exports):. ```; from hail import *; hc = HailContext(); ```; With startup messages looking like this:. ```; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/01/08 13:51:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/01/08 13:51:03 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).; 18/01/08 13:51:03 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 18/01/08 13:51:03 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/home/ludvig/Programs/ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:5665,message,messages,5665,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['message'],['messages']
Integrability," is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: Java heap space; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:10987,protocol,protocol,10987,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,2,['protocol'],['protocol']
Integrability," k,; 103 compute_loadings). <decorator-gen-1780> in pca(entry_expr, k, compute_loadings). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/methods/pca.py in pca(entry_expr, k, compute_loadings); 209 'k': k,; 210 'computeLoadings': compute_loadings; --> 211 })).persist()); 212 ; 213 g = t.index_globals(). <decorator-gen-1340> in persist(self, storage_level). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in persist(self, storage_level); 2110 Persisted table.; 2111 """"""; -> 2112 return Env.backend().persist(self); 2113 ; 2114 def unpersist(self) -> 'Table':. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py in persist(self, dataset); 167 from hail.context import TemporaryFilename; 168 tempfile = TemporaryFilename(prefix=f'persist_{type(dataset).__name__}'); --> 169 persisted = dataset.checkpoint(tempfile.__enter__()); 170 self._persisted_locations[persisted] = (tempfile, dataset); 171 return persisted. <decorator-gen-1330> in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:3173,wrap,wrapper,3173,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,1,['wrap'],['wrapper']
Integrability," overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions: Int, shuffle: Boolean)(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:1942,depend,dependencies,1942,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,1,['depend'],['dependencies']
Integrability," secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=https://hail.auth0.com/api/v2/; ```. Organization of the web app is simple. There is a pages directory. Routes match the folder structure. Pages that don't need to maintain their own state look a lot like HTML wrapped in a function:. ```js; export default function() { <div>Hello World </div> }; ```. or in JS ES6 form:; ```js; export default () => <div>Hello World</div>; ``` . Performance is excellent. SSR should run about as fast as jinja2 (will be getting faster in 2019). Client side interactions are obviously far more performant. Bundle sizes are on a downward trajectory; react + react-dom is about as big as jQuery today, and reducing that is a focus on facebook in 2019. There are alternatives to react-dom that are under 10kb, but in practice 20kb is nothing to worry about, especially when initial load doesn't require it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:3337,Rout,Routes,3337,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935,2,"['Rout', 'wrap']","['Routes', 'wrapped']"
Integrability," six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB); Collecting urllib3==1.26.16; Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB); Collecting uvloop==0.17.0; Using cached uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB); Collecting wrapt==1.15.0; Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB); Collecting xyzservices==2023.7.0; Using cached xyzservices-2023.7.0-py3-none-any.whl (56 kB); Collecting yarl==1.9.2; Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB); Building wheels for collected packages: avro; Building wheel for avro (pyproject.toml): started; Building wheel for avro (pyproject.toml): finished with status 'done'; Created wheel for avro: filename=avro-1.11.2-py2.py3-none-any.whl size=119738 sha256=d7f238f86de270b449b018590930a06270766887328bdb51066eccff2cd696a6; Stored in directory: /home/hadoop/.cache/pip/wheels/e3/a2/1e/5c1be0865f4170a89de34e0a798f32f674a7eaf63a93272c7f; Successfully built avro; Installing collected packages: sortedcontainers, pytz, py4j, commonmark, azure-common, xyzservices, wrapt, uvloop, urllib3, tzdata, typing-extensions, tornado, tenacity, tabulate, six, regex, pyyaml, python-json-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:40869,wrap,wrapt-,40869,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['wrap'],['wrapt-']
Integrability," still isn't quite right?. I also saw some issues with sockets timing out but I don't know what to make of those yet.; ```; _______________________________ test_union_rows1 _______________________________. @test_timeout(local=3 * 60); def test_union_rows1():; vds = hl.vds.read_vds(os.path.join(resource('vds'), '1kg_chr22_5_samples.vds')); ; vds1 = hl.vds.filter_intervals(vds,; [hl.parse_locus_interval('chr22:start-10754094', reference_genome='GRCh38')],; split_reference_blocks=True); vds2 = hl.vds.filter_intervals(vds,; [hl.parse_locus_interval('chr22:10754094-end', reference_genome='GRCh38')],; split_reference_blocks=True); ; ; vds_union = vds1.union_rows(vds2); > assert hl.vds.to_dense_mt(vds)._same(hl.vds.to_dense_mt(vds_union)). test/hail/vds/test_vds.py:597: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; <decorator-gen-1386>:2: in _same; ???; /usr/local/lib/python3.9/dist-packages/hail/typecheck/check.py:587: in wrapper; return __original_func(*args_, **kwargs_); /usr/local/lib/python3.9/dist-packages/hail/matrixtable.py:3762: in _same; return self._localize_entries(entries_name, cols_name)._same(; <decorator-gen-1276>:2: in _same; ???; /usr/local/lib/python3.9/dist-packages/hail/typecheck/check.py:587: in wrapper; return __original_func(*args_, **kwargs_); /usr/local/lib/python3.9/dist-packages/hail/table.py:3658: in _same; mismatched_globals, mismatched_rows = t.aggregate(hl.tuple((; <decorator-gen-1216>:2: in aggregate; ???; /usr/local/lib/python3.9/dist-packages/hail/typecheck/check.py:587: in wrapper; return __original_func(*args_, **kwargs_); /usr/local/lib/python3.9/dist-packages/hail/table.py:1285: in aggregate; return Env.backend().execute(hl.ir.MakeTuple([agg_ir]))[0]; /usr/local/lib/python3.9/dist-packages/hail/backend/py4j_backend.py:86: in execute; raise e.maybe_user_error(ir) from None; /usr/local/lib/python3.9/dist-packages/hail/backend/py4j_backend.py:76: in execute; result_tuple = self._jbackend.executeEncod",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:999,wrap,wrapper,999,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['wrap'],['wrapper']
Integrability," x:; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/functions.py"", line 3531, in any; return collection.any(f); File ""<decorator-gen-510>"", line 2, in any; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 68, in any; return hl.array(self).fold(lambda accum, elt: accum | f(elt), False); File ""<decorator-gen-518>"", line 2, in fold; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 221, in fold; return collection._to_stream().fold(lambda x, y: f(x, y), zero); File ""<decorator-gen-650>"", line 2, in fold; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 4522, in fold; body = to_expr(f(accum_ref, elt_ref)); File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 364, in f; ret = x(*args); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 221, in <lambda>; return collection._to_stream().fold(lambda x, y: f(x, y), zero); File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 364, in f; ret = x(*args); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 68, in <lambda>; return hl.array(self).fold(lambda accum, elt: accum | f(elt), False); File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 364, in f; ret = x(*args); File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 364, in f; ret = x(*args); File ""test.py"", line 11, in <lambda>; (m",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:6243,wrap,wrapper,6243,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['wrap'],['wrapper']
Integrability,"""T"", ""A""])]. expr = hl.any(; lambda x:; (mt.locus.contig == hl.literal(x[0])) & \; (mt.locus.position == hl.literal(int(x[1]))) & \; (mt.alleles == hl.literal(x[2])),; variants; ). hl.eval(expr). ```; Leads to the following error (which looks like the bug!):; ```; Traceback (most recent call last):; File ""test.py"", line 10, in <module>; expr = hl.any(lambda x:; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/functions.py"", line 3531, in any; return collection.any(f); File ""<decorator-gen-510>"", line 2, in any; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 68, in any; return hl.array(self).fold(lambda accum, elt: accum | f(elt), False); File ""<decorator-gen-518>"", line 2, in fold; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 221, in fold; return collection._to_stream().fold(lambda x, y: f(x, y), zero); File ""<decorator-gen-650>"", line 2, in fold; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 4522, in fold; body = to_expr(f(accum_ref, elt_ref)); File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 364, in f; ret = x(*args); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 221, in <lambda>; return collection._to_stream().fold(lambda x, y: f(x, y), zero); File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 364, in f; ret = x(*args); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:5884,wrap,wrapper,5884,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['wrap'],['wrapper']
Integrability,"""contract types""? What's that mean?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1813#issuecomment-317562462:1,contract,contract,1,https://hail.is,https://github.com/hail-is/hail/issues/1813#issuecomment-317562462,1,['contract'],['contract']
Integrability,"# logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApprox(h2)'). ##### Reality check that sigma and sdPost are close; sigma; sdPost; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1222,integrat,integrate,1222,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538,1,['integrat'],['integrate']
Integrability,"#11699, still trying to sort out the shaded azure dependency, this is operating under the assumption that we can use the azure dependency specified in the build.gradle",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11617#issuecomment-1082248758:50,depend,dependency,50,https://hail.is,https://github.com/hail-is/hail/pull/11617#issuecomment-1082248758,2,['depend'],['dependency']
Integrability,"')],; split_reference_blocks=True); ; ; vds_union = vds1.union_rows(vds2); > assert hl.vds.to_dense_mt(vds)._same(hl.vds.to_dense_mt(vds_union)). test/hail/vds/test_vds.py:597: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; <decorator-gen-1386>:2: in _same; ???; /usr/local/lib/python3.9/dist-packages/hail/typecheck/check.py:587: in wrapper; return __original_func(*args_, **kwargs_); /usr/local/lib/python3.9/dist-packages/hail/matrixtable.py:3762: in _same; return self._localize_entries(entries_name, cols_name)._same(; <decorator-gen-1276>:2: in _same; ???; /usr/local/lib/python3.9/dist-packages/hail/typecheck/check.py:587: in wrapper; return __original_func(*args_, **kwargs_); /usr/local/lib/python3.9/dist-packages/hail/table.py:3658: in _same; mismatched_globals, mismatched_rows = t.aggregate(hl.tuple((; <decorator-gen-1216>:2: in aggregate; ???; /usr/local/lib/python3.9/dist-packages/hail/typecheck/check.py:587: in wrapper; return __original_func(*args_, **kwargs_); /usr/local/lib/python3.9/dist-packages/hail/table.py:1285: in aggregate; return Env.backend().execute(hl.ir.MakeTuple([agg_ir]))[0]; /usr/local/lib/python3.9/dist-packages/hail/backend/py4j_backend.py:86: in execute; raise e.maybe_user_error(ir) from None; /usr/local/lib/python3.9/dist-packages/hail/backend/py4j_backend.py:76: in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); /usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1321: in __call__; return_value = get_return_value(; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . args = ('xro552', <py4j.java_gateway.GatewayClient object at 0x7f01e1182160>, 'o1', 'executeEncode'); kwargs = {}; pyspark = <module 'pyspark' from '/usr/local/lib/python3.9/dist-packages/pyspark/__init__.py'>; s = 'java.lang.RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000'; tpl = JavaObject id=o553; deepest = 'RuntimeExce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:1596,wrap,wrapper,1596,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['wrap'],['wrapper']
Integrability,"': 'ContainersReady'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'PodScheduled'}],; 'container_statuses': [{'container_id': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_id': '',; 'last_state': {'running': None,; 'terminated': None,; 'waiting': None},; 'name': 'main',; 'ready': False,; 'restart_count': 0,; 'state': {'running': None,; 'terminated': {'container_id': None,; 'exit_code': 0,; 'finished_at': None,; 'message': None,; 'reason': None,; 'signal': None,; 'started_at': None},; 'waiting': None}}],; 'host_ip': '10.128.0.8',; 'init_container_statuses': None,; 'message': None,; 'nominated_node_name': None,; 'phase': 'Pending',; 'pod_ip': None,; 'qos_class': 'Burstable',; 'reason': None,; 'start_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal())}}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 53, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:9422,wrap,wrapped,9422,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['wrap'],['wrapped']
Integrability,"'reason': None,; 'status': 'True',; 'type': 'Initialized'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'Ready'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'ContainersReady'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'PodScheduled'}],; 'container_statuses': [{'container_id': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_id': '',; 'last_state': {'running': None,; 'terminated': None,; 'waiting': None},; 'name': 'main',; 'ready': False,; 'restart_count': 0,; 'state': {'running': None,; 'terminated': {'container_id': None,; 'exit_code': 0,; 'finished_at': None,; 'message': None,; 'reason': None,; 'signal': None,; 'started_at': None},; 'waiting': None}}],; 'host_ip': '10.128.0.8',; 'init_container_statuses': None,; 'message': None,; 'nominated_node_name': None,; 'phase': 'Pending',; 'pod_ip': None,; 'qos_class': 'Burstable',; 'reason': None,; 'start_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal())}}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 53, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/co",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:8991,message,message,8991,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['message'],['message']
Integrability,"(If you want to take a look at a live instance, I've deployed to my namespace and you can look at the batch logs here: https://ci.hail.is/batches/634 ). ignore all the log messages that have to do with `/wang/blog/healthcheck/`---since it's logging to persistent storage, it's also printing all the old logs from a few days ago to the batch logs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548103287:172,message,messages,172,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103287,1,['message'],['messages']
Integrability,"(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). After lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). is.hail.utils.HailException: not a streamable IR: (GetTupleElement 0; (In PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]] 0)); ...; at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:850). ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TContainer]) {; ToStream(node); } else {; node; }; }; }; }; ````. with . ```scala; private def toStream(node: ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:5629,Wrap,WrappedArray,5629,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113,2,['Wrap'],['WrappedArray']
Integrability,"(This definitely feels like a problem. Took three minutes to serialize an array with 10 million ints and with 40 million ints, this happens:). ```; >>> def serialize_test(n):; ... t = [i for i in range(n)]; ... t1 = datetime.now(); ... print(len(t)); ... print(hl.eval(hl.len(hl.array(t)))); ... t2 = datetime.now(); ... print(t2 - t1); ... ; >>> serialize_test(40000000); 40000000; Exception in thread ""Thread-2"" java.lang.OutOfMemoryError: Java heap space; 	at java.util.Arrays.copyOf(Arrays.java:3332); 	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); 	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:649); 	at java.lang.StringBuilder.append(StringBuilder.java:202); 	at py4j.StringUtil.unescape(StringUtil.java:57); 	at py4j.Protocol.getString(Protocol.java:475); 	at py4j.Protocol.getObject(Protocol.java:302); 	at py4j.commands.AbstractCommand.getArguments(AbstractCommand.java:82); 	at py4j.commands.CallCommand.execute(CallCommand.java:77); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:793,Protocol,Protocol,793,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184,4,['Protocol'],['Protocol']
Integrability,(TransportContext.java:331) ~[?:1.8.0_392]; 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:274) ~[?:1.8.0_392]; 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:269) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:119) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1404) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1372) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:966) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_392]; 	at sun.net.www.MeteredStream.read(MeteredStream.java:134) ~[?:1.8.0_392]; 	at java.io.FilterInputStream.read(FilterInputStream.java:133) ~[?:1.8.0_392]; 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3460) ~[?:1.8.0_392]; 	at com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:164) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_392]; 	at is.hail.relocated.com.google.cloud.storage.StorageByteChannels$ScatteringByteChannelFacade.read(StorageByteChannels.java:242) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.ApiaryUnbufferedReadableByteChannel.read(ApiaryUnbufferedReadableByteChannel.java:113) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.UnbufferedReadableByteChannelSession$Unbuffered,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:1206,protocol,protocol,1206,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['protocol'],['protocol']
Integrability,"(We've added `if messages: ` to the code in question to prevent this, but it would be good to fix the issue from the Hail end as well.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14700#issuecomment-2372549348:17,message,messages,17,https://hail.is,https://github.com/hail-is/hail/pull/14700#issuecomment-2372549348,1,['message'],['messages']
Integrability,"(i) -> O(i, i)` makes a square matrix whose diagonal is T. * `T(i) -> O(i, j)` and `T(i) -> O(j, i)` broadcast T over a matrix in the two possible directions. Now let T be a 2-tensor. * `T(i, j) -> O(i)` is the vector of row-sums of T. * `T(i, i) -> O(i)` is the diagonal of T. * `T(i, i) -> O()` is the trace of T. * `T(i, j) -> O(j, i)` is transposition. How do we represent matrix multiplication? Let T1 and T2 be 2-tensors. Then letting `T = Out(T1, T2, ""and"").map((x, y) => x * y)`, the matrix product is given by. * `T(i, j, j, k) -> O(i, k)`. In general, an index operation on T requires specifying an output tensor O (including its shape, though you can deduce that in non-broadcasting cases), a set of index variables (eg. ""i, j, k""), and an assignment of a variable to each dimension of T and O. . More abstractly, let DT and DO be the sets of dimensions of T and O. An index operation consists of a set I and two functions DT -> I <- DO, a ""cospan"". These operations compose by cospan composition, which involves a pushout (a disjoint union and a quotient). Let i: DT -> I and o: DO -> I be the two maps assigning index variables. It helps to consider some special cases (compare to the examples above):. * If i is surjective, and o is identity, this is extracting a diagonal from T. * If i is injective, and o is identity, this is a broadcast. * If i is identity, and o is surjective, this embeds T as a diagonal of a higher-dimensional output tensor. * If i is identity, and o is injective, this is a pure aggregation, summing out some dimensions of T. To make composition easy to compute, we could represent an index operation using a union-find structure for I. In other words, an index operation consists of a union-find structure I, and two arrays of points of I, encoding the two functions above. Then the composition of (T, I1, M) and (M, I2, O) is (T, I', O), where I' is computed by taking the union I1+I2, then for each dimension of M, unioning the assigned points of I1 and I2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772:3583,inject,injective,3583,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772,2,['inject'],['injective']
Integrability,"(that's not to say we shouldn't do dependency audits every few months, bumping the pinned version to latest)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542205189:35,depend,dependency,35,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542205189,1,['depend'],['dependency']
Integrability,"); File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/pyscripts_Zf07i2.zip/gnomad_hail/utils/slack.py"", line 112, in try_slack; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/pyscripts_Zf07i2.zip/gnomad_hail/utils/slack.py"", line 95, in try_slack; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/constraint.py"", line 36, in main; recalculate_all_possible_summary=True, remove_common_downsampled=False, remove_common_ordinary=True); File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/pyscripts_Zf07i2.zip/constraint_utils/constraint_basics.py"", line 263, in calculate_mu_by_downsampling; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/pyscripts_Zf07i2.zip/constraint_utils/constraint_basics.py"", line 41, in get_old_mu_data; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 772, in transmute; File ""<decorator-gen-648>"", line 2, in _select; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/typecheck/check.py"", line 546, in wrapper; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 438, in _select; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 447, in _select_scala; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: NullPointerException: null. Java stack trace:; java.lang.NullPointerException: null; 	at scala.collection.convert.Wrappers$JListWrapper.length(Wrappers.scala:86); 	at scala.collection.SeqLike$class.size(SeqLike.scala:106); 	at scala.collection.AbstractSeq.size(Seq.scala:41); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:285); 	at scala.collection.AbstractTraversable.toArray(Traversable.scala:104); 	at is.hail.utils.richUtils.RichIterable.toFastIndexedSeq(RichIterable.scala:83); 	at is.hail.table.Table.select(Table.scala:436",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4314#issuecomment-420405267:1158,wrap,wrapper,1158,https://hail.is,https://github.com/hail-is/hail/issues/4314#issuecomment-420405267,1,['wrap'],['wrapper']
Integrability,"* **#14451** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14451?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14451#issuecomment-2045755749:654,depend,dependencies,654,https://hail.is,https://github.com/hail-is/hail/pull/14451#issuecomment-2045755749,1,['depend'],['dependencies']
Integrability,"* **#14455** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14455?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14455#issuecomment-2046054784:654,depend,dependencies,654,https://hail.is,https://github.com/hail-is/hail/pull/14455#issuecomment-2046054784,1,['depend'],['dependencies']
Integrability,"* **#14496** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14496?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14509](https://github.com/hail-is/hail/pull/14509) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14509?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14514](https://github.com/hail-is/hail/pull/14514) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14514?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>) ; * **#14495** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14495?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14475** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14475?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14496#issuecomment-2070873404:238,depend,dependent,238,https://hail.is,https://github.com/hail-is/hail/pull/14496#issuecomment-2070873404,2,['depend'],"['dependencies', 'dependent']"
Integrability,"* **#14496** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14496?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14509](https://github.com/hail-is/hail/pull/14509) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14509?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14514](https://github.com/hail-is/hail/pull/14514) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14514?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14495** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14495?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * **#14475** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14475?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14495#issuecomment-2070873392:238,depend,dependent,238,https://hail.is,https://github.com/hail-is/hail/pull/14495#issuecomment-2070873392,2,['depend'],"['dependencies', 'dependent']"
Integrability,"* **#14496** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14496?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14509](https://github.com/hail-is/hail/pull/14509) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14509?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14514](https://github.com/hail-is/hail/pull/14514) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14514?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14495** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14495?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14475** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14475?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14475#issuecomment-2059793128:238,depend,dependent,238,https://hail.is,https://github.com/hail-is/hail/pull/14475#issuecomment-2059793128,2,['depend'],"['dependencies', 'dependent']"
Integrability,"* **#14514** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14514?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14509](https://github.com/hail-is/hail/pull/14509) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14509?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14554](https://github.com/hail-is/hail/pull/14554) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14554?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>) ; * **#14517** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14517?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14514#issuecomment-2086536884:238,depend,dependent,238,https://hail.is,https://github.com/hail-is/hail/pull/14514#issuecomment-2086536884,2,['depend'],"['dependencies', 'dependent']"
Integrability,"* **#14514** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14514?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14509](https://github.com/hail-is/hail/pull/14509) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14509?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14554](https://github.com/hail-is/hail/pull/14554) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14554?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14517** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14517?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14517#issuecomment-2091504683:238,depend,dependent,238,https://hail.is,https://github.com/hail-is/hail/pull/14517#issuecomment-2091504683,2,['depend'],"['dependencies', 'dependent']"
Integrability,"* **#14533** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14533?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14509** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14509?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * **#14514** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14514?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 1 other dependent PR ([#14554](https://github.com/hail-is/hail/pull/14554) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14554?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14517** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14517?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14509#issuecomment-2080036916:718,depend,dependent,718,https://hail.is,https://github.com/hail-is/hail/pull/14509#issuecomment-2080036916,2,['depend'],"['dependencies', 'dependent']"
Integrability,"* **#14547** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14547?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14547#issuecomment-2105153451:654,depend,dependencies,654,https://hail.is,https://github.com/hail-is/hail/pull/14547#issuecomment-2105153451,1,['depend'],['dependencies']
Integrability,"* **#14551** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14551?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * **#14550** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14550?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14551#issuecomment-2108604092:890,depend,dependencies,890,https://hail.is,https://github.com/hail-is/hail/pull/14551#issuecomment-2108604092,1,['depend'],['dependencies']
Integrability,"* **#14551** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14551?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14550** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14550?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14550#issuecomment-2108442266:890,depend,dependencies,890,https://hail.is,https://github.com/hail-is/hail/pull/14550#issuecomment-2108442266,1,['depend'],['dependencies']
Integrability,"* **#14553** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14553?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14553#issuecomment-2110596841:654,depend,dependencies,654,https://hail.is,https://github.com/hail-is/hail/pull/14553#issuecomment-2110596841,1,['depend'],['dependencies']
Integrability,"* **#14554** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14554?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * **#14514** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14514?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 1 other dependent PR ([#14509](https://github.com/hail-is/hail/pull/14509) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14509?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14517** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14517?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14554#issuecomment-2110766283:482,depend,dependent,482,https://hail.is,https://github.com/hail-is/hail/pull/14554#issuecomment-2110766283,2,['depend'],"['dependencies', 'dependent']"
Integrability,"* **#14578** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14578?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14578#issuecomment-2163445582:654,depend,dependencies,654,https://hail.is,https://github.com/hail-is/hail/pull/14578#issuecomment-2163445582,1,['depend'],['dependencies']
Integrability,"* **#14582** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14582?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14582#issuecomment-2166604877:654,depend,dependencies,654,https://hail.is,https://github.com/hail-is/hail/pull/14582#issuecomment-2166604877,1,['depend'],['dependencies']
Integrability,"* **#14638** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14638?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14638#issuecomment-2252777195:654,depend,dependencies,654,https://hail.is,https://github.com/hail-is/hail/pull/14638#issuecomment-2252777195,1,['depend'],['dependencies']
Integrability,"* **#14663** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14663?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14662** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14662?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14662#issuecomment-2302524285:890,depend,dependencies,890,https://hail.is,https://github.com/hail-is/hail/pull/14662#issuecomment-2302524285,1,['depend'],['dependencies']
Integrability,"* **#14673** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14673?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14673#issuecomment-2334696866:654,depend,dependencies,654,https://hail.is,https://github.com/hail-is/hail/pull/14673#issuecomment-2334696866,1,['depend'],['dependencies']
Integrability,"* **#14684** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14684?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14686](https://github.com/hail-is/hail/pull/14686) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14686?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14747](https://github.com/hail-is/hail/pull/14747) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14747?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>) ; * **#14683** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14683?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @ehigham and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14684#issuecomment-2349934247:238,depend,dependent,238,https://hail.is,https://github.com/hail-is/hail/pull/14684#issuecomment-2349934247,2,['depend'],"['dependencies', 'dependent']"
Integrability,"* **#14684** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14684?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14686](https://github.com/hail-is/hail/pull/14686) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14686?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14747](https://github.com/hail-is/hail/pull/14747) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14747?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14683** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14683?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @ehigham and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14683#issuecomment-2349934211:238,depend,dependent,238,https://hail.is,https://github.com/hail-is/hail/pull/14683#issuecomment-2349934211,2,['depend'],"['dependencies', 'dependent']"
Integrability,"* **#14745** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14745?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14745#issuecomment-2435880729:654,depend,dependencies,654,https://hail.is,https://github.com/hail-is/hail/pull/14745#issuecomment-2435880729,1,['depend'],['dependencies']
Integrability,"**c++ -v**; Using built-in specs.; COLLECT_GCC=c++; COLLECT_LTO_WRAPPER=/opt/local/libexec/gcc/x86_64-apple-darwin15/4.9.3/lto-wrapper; Target: x86_64-apple-darwin15; Configured with: /opt/local/var/macports/build/_opt_mports_dports_lang_gcc49/gcc49/work/gcc-4.9.3/configure --prefix=/opt/local --build=x86_64-apple-darwin15 --enable-languages=c,c++,objc,obj-c++,lto,fortran,java --libdir=/opt/local/lib/gcc49 --includedir=/opt/local/include/gcc49 --infodir=/opt/local/share/info --mandir=/opt/local/share/man --datarootdir=/opt/local/share/gcc-4.9 --with-local-prefix=/opt/local --with-system-zlib --disable-nls --program-suffix=-mp-4.9 --with-gxx-include-dir=/opt/local/include/gcc49/c++/ --with-gmp=/opt/local --with-mpfr=/opt/local --with-mpc=/opt/local --with-isl=/opt/local --disable-isl-version-check --with-cloog=/opt/local --disable-cloog-version-check --enable-stage1-checking --disable-multilib --enable-lto --enable-libstdcxx-time --with-as=/opt/local/bin/as --with-ld=/opt/local/bin/ld --with-ar=/opt/local/bin/ar --with-bugurl=https://trac.macports.org/newticket --with-pkgversion='MacPorts gcc49 4.9.3_0' --with-build-config=bootstrap-debug; Thread model: posix; gcc version 4.9.3 (MacPorts gcc49 4.9.3_0) . **sysctl -a | grep machdep.cpu**; machdep.cpu.tsc_ccc.denominator: 0; machdep.cpu.tsc_ccc.numerator: 0; machdep.cpu.thread_count: 8; machdep.cpu.core_count: 4; machdep.cpu.address_bits.virtual: 48; machdep.cpu.address_bits.physical: 36; machdep.cpu.tlb.shared: 512; machdep.cpu.tlb.data.large: 32; machdep.cpu.tlb.data.small: 64; machdep.cpu.tlb.inst.large: 8; machdep.cpu.tlb.inst.small: 64; machdep.cpu.cache.size: 256; machdep.cpu.cache.L2_associativity: 8; machdep.cpu.cache.linesize: 64; machdep.cpu.arch_perf.fixed_width: 48; machdep.cpu.arch_perf.fixed_number: 3; machdep.cpu.arch_perf.events: 0; machdep.cpu.arch_perf.events_number: 7; machdep.cpu.arch_perf.width: 48; machdep.cpu.arch_perf.number: 4; machdep.cpu.arch_perf.version: 3; machdep.cpu.xsave.extended_state1:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543:127,wrap,wrapper,127,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543,1,['wrap'],['wrapper']
Integrability,*sigh*. ```; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); ```. Azure's `StorageOutputStream.close` method is not idempotent in the version that we use. It has been made idempotent in `12.18.0`. I would be surprised if spark let us upgrade to a version that recent,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12968#issuecomment-1532328901:572,Rout,RouterFS,572,https://hail.is,https://github.com/hail-is/hail/pull/12968#issuecomment-1532328901,2,['Rout'],['RouterFS']
Integrability,",ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). After lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). is.hail.utils.HailException: not a streamable IR: (GetTupleElement 0; (In PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]] 0))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:8432,Wrap,WrappedArray,8432,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113,2,['Wrap'],['WrappedArray']
Integrability,"-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API:; >; > https://hail.is/pyhail/getting_started.html; >; > Please give it a spin and let us know if you run into any problems. The; > documentation for the python API is nearly complete, but the Tutorial and; > General Reference section are still being ported to python and will need; > another week or so. Thanks for your patience!; >; > ; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/1218#issuecomment-272357689>, or mute; > the thread; > <http",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:1549,protocol,protocol,1549,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799,1,['protocol'],['protocol']
Integrability,". debian8), and systems; based on g++-5.x and later with new-ABI std::string by default (e.g. debian9). That; incompatibility is the problem. >Not having access to the standard library seems problematic. You absolutely have access to the full C++11 standard library in dynamically-generated code -; you're compiling with the master node's default compiler, and header files, and using; the default libstdc++.o (libc++.dylib), and it's all fine. And you'll be using whichever flavor; of std::string is the default for that system, which will presumably also be interoperable with any; third-party library packages on that system. The issues we're getting round are:. a) If libhail.so is prebuilt on a new-ABI system *and* uses std::string, then it can't run against; the libstdc++ on an old-ABI system. b) If libhail.so is prebuilt on an old-ABI system, then it can run against either old-ABI or new-ABI; libstdc++, but if it then gets linked against third-party libraries compiled against new-ABI; headers, you'll have two different flavors of std::string floating around in the same program,; which causes confusion at any interfaceswhich pass std::string around. Now if you want to go further in shipping more of the system, then the question of whether to; ship your own libstdc++ (or libc++) is independent of the choice of compiler version. *If* you ; ship your own libstdc++, then you potentially introduce the problems of interoperability with; other libraries on the system). And there's a slight question about whether your libstdc++ will; work against the other systems libc.so, though I think the ABI at that level has been stable enough for long enough that it probably doesn't cause trouble.; ; In short, it's a can of worms. Avoiding std::string in libhail.so keeps the can closed for now.; And I believe dataproc will move to using debian9 images as the default in November, so at; some point the need to support old-ABI systems (debian8) will diminish and possibly go away completely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941:1743,interoperab,interoperability,1743,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941,1,['interoperab'],['interoperability']
Integrability,".. and I hand-deployed the router and it looks good. Most of our existing services can't handle being located at internal.hail.is/ns/svc, so I'm going to make a series of changes to fix that, possibly folded into my auth changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6928#issuecomment-524334117:27,rout,router,27,https://hail.is,https://github.com/hail-is/hail/pull/6928#issuecomment-524334117,1,['rout'],['router']
Integrability,".; It's just one step short of using containers - but since it doesn't require; a containerized OS, I think it works; for laptops etc. I believe the package could have all the stuff we currently manage my; manual install, viz JDK, Spark, Python-3.6,; R, R packages, as well as Hail and a friendly-C++17-capable compiler. All; without perturbing anything else; on the system. See https://bitnami.com. I took a similar approach at PhysicsSpeed, though without using any bitnami; tools because we had less than; zero dollars :-(. I don't know if this adds any value in the containerized/cloud environment,; where custom machine images; are presumably the way to go. But it makes setup easy for standalone use. Regards; Richard. On Thu, Aug 2, 2018 at 10:44 PM Richard Cownie <rcownie@broadinstitute.org>; wrote:. > We have a difference of opinion about the risks involved in using whatever; > compiler happens to show up as $(CXX); > to try to compile arbitrarily large auto-generated C++ files, and maybe; > about what happens when that fails; > and gives an error message about something in the middle of 12000 lines of; > code that bears no obvious relationship; > to what the user is doing. Or when that compiler takes 15 minutes to; > compile it. It's the C++ equivalent of; > the JVM ""no, that's just too much bytecode"". Or worst of all, it compiles; > it but the code gives the wrong answers; > because that particular compiler has a bug, and we never tested the; > combination of our codegen with *that*; > compiler/version.; >; > A couple of years ago I was seeing g++ take 40-60 seconds to compile; > something that clang did in 2 seconds; > (fairly heavily templated code generated for an SQL query, so very much in; > the same ballpark as parts of Hail),; > which contributes to my concern about this, especially on linux where g++; > is the default.; >; > So in the long run I expect we'll ship a compiler, or specify a compiler.; > But that becomes a problem in itself; > if we want the sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410235287:1386,message,message,1386,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410235287,1,['message'],['message']
Integrability,".py:76: in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); ../../.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321: in __call__; return_value = get_return_value(; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . args = ('xro549', <py4j.clientserver.JavaClient object at 0x7fd0d58f6fb0>, 'o1', 'executeEncode'); kwargs = {}; pyspark = <module 'pyspark' from '/home/edmund/.local/src/hail/.venv/lib/python3.10/site-packages/pyspark/__init__.py'>; s = 'java.lang.AssertionError: assertion failed', tpl = JavaObject id=o550; deepest = 'AssertionError: assertion failed'; full = 'java.lang.AssertionError: assertion failed\n\tat scala.Predef$.assert(Predef.scala:208)\n\tat is.hail.expr.ir.BlockMa...lientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\n\n'; error_id = -1. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); > raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; E hail.utils.java.FatalError: AssertionError: assertion failed; E ; E Java stack trace:; E java.lang.AssertionError: assertion failed; E 	at scala.Predef$.assert(Predef.scala:208); E 	at is.hail.expr.ir.BlockMatrixMap.execute(BlockMatrixIR.scala:269); E 	at is.hail.expr.ir.BlockMatrixMap2.execute(BlockMatrixIR.scala:393); E 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:875); E 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:59); E 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:20); E 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(Low",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229:2313,protocol,protocol,2313,https://hail.is,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229,1,['protocol'],['protocol']
Integrability,".ss.nt1) &; (mt.alleles[1] == mt.ss.nt2)) |; ((flip_text(mt.alleles[0]) == mt.ss.nt1) &; (flip_text(mt.alleles[1]) == mt.ss.nt2)),; (-1*mt.ss.ldpred_inf_beta)); .when(((mt.alleles[0] == mt.ss.nt2) &; (mt.alleles[1] == mt.ss.nt1)) |; ((flip_text(mt.alleles[0]) == mt.ss.nt2) &; (flip_text(mt.alleles[1]) == mt.ss.nt1)),; mt.ss.ldpred_inf_beta); .or_missing()). # filter bgen matrixtable down to only SNPs with betas; mt = mt.filter_rows(hl.is_defined(mt.beta)). # filter bgen matrixtable to only include people in scoring sample; mt = mt.filter_cols(hl.is_defined(sampleids[mt.s])). # score sample; mt = mt.annotate_cols(prs=hl.agg.sum(mt.beta * mt.dosage)). # write out table with sample IDs and PRS scores; mt.cols().export('gs://ukbb_prs/prs/UKB_'+pheno+'_PRS_22.txt'). parser = argparse.ArgumentParser(); parser.add_argument(""--phenotype"", help=""name of the sumstat phenotype""); args = parser.parse_args(). try:; start = time.time(); main(args.phenotype); end = time.time(); message = ""Success! Job was completed in %s"" % time.strftime(""%H:%M:%S"", time.gmtime(end - start)); send_message(message); except Exception as e:; send_message(""Fail.""); ```. ""Failure Reason"":; ```; Job aborted due to stage failure: Task 98 in stage 13.0 failed 20 times, most recent failure: Lost task 98.19 in stage 13.0 (TID 22699, ccarey-sw-xt4j.c.ukbb-robinson.internal, executor 68): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:815); 	at is.hail.expr.MatrixMapRows$$anonfun$25",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:2623,message,message,2623,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['message'],['message']
Integrability,"/opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals); 1329 ; 1330 if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; -> 1331 self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); 1332 _assert_type = self._type; 1333 _load_refs = False. <decorator-gen-1332> in write(self, output, overwrite, stage_locally, _codec_spec). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in write(self, output, overwrite, stage_locally, _codec_spec); 1375 hl.current_backend().validate_file_scheme(output); 1376 ; -> 1377 Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); 1378 ; 1379 @typecheck_method(output=str,. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 80 return (value, timings) if timed else value; 81 except FatalError as e:; ---> 82 raise e.maybe_user_error(ir) from None; 83 ; 84 async def _async_execute(self, ir, timed=False):. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/backend/py4j_backend.py in execute(self, ir,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:4875,wrap,wrapper,4875,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,2,['wrap'],['wrapper']
Integrability,"0 azure-mgmt-storage-20.1.0 azure-storage-blob-12.17.0 bokeh-3.2.2 boto3-1.28.41 botocore-1.31.; 41 cachetools-5.3.1 certifi-2023.7.22 cffi-1.15.1 charset-normalizer-3.2.0 commonmark-0.9.1 contourpy-1.1.0 cryptography-41.0.3 decorator-4.4.2 deprecated-1.2.14 dill-0.3.7 frozenlist-1.4.0 google-api-core-2.11.1 google-auth-2.22.0 google-auth-oauthlib-0.8.0 google-cloud-core-2.3.3 google-cloud-storag; e-2.10.0 google-crc32c-1.5.0 google-resumable-media-2.5.0 googleapis-common-protos-1.60.0 humanize-1.1.0 idna-3.4 isodate-0.6.1 janus-1.0.0 jinja2-3.1.2 jmespath-1.0.1 jproperties-2.1.1 markupsafe-2.1.3 msal-1.23.0 msal-extensions-1.0.0 msrest-0.7.1 multidict-6.0.4 nest-asyncio-1.5.7 numpy-1.25.2 oaut; hlib-3.2.2 orjson-3.9.5 packaging-23.1 pandas-2.1.0 parsimonious-0.10.0 pillow-10.0.0 plotly-5.16.1 portalocker-2.7.0 protobuf-3.20.2 py4j-0.10.9.5 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycares-4.3.0 pycparser-2.21 pygments-2.16.1 pyjwt-2.8.0 python-dateutil-2.8.2 python-json-logger-2.0.7 pytz-2023.3.post; 1 pyyaml-6.0.1 regex-2023.8.8 requests-2.31.0 requests-oauthlib-1.3.1 rich-12.6.0 rsa-4.9 s3transfer-0.6.2 scipy-1.11.2 six-1.16.0 sortedcontainers-2.4.0 tabulate-0.9.0 tenacity-8.2.3 tornado-6.3.3 typer-0.9.0 typing-extensions-4.7.1 tzdata-2023.3 urllib3-1.26.16 uvloop-0.17.0 wrapt-1.15.0 xyzservices; -2023.7.0 yarl-1.9.2. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; python3 -m pip uninstall -y hail; WARNING: Skipping hail as it is not installed.; python3 -m pip install build/deploy/dist/hail-0.2.124-py3-none-any.whl --no-deps; Defaulting to user installation because normal site-packages is not writeable; Processing ./build/deploy/dist/hail-0.2.124-py3-none-any.whl; Installing collected packages: hail; Successfully installed hail-0.2.124. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; hailctl config set query/backend spark; </p>; </details>",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:44347,wrap,wrapt-,44347,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['wrap'],['wrapt-']
Integrability,"1. Because of a bug. 2. Yes. 3. The message in the exception should use BaseType.parseableType instead of pretty/toString, since that prints the missingness details.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4527#issuecomment-429178603:36,message,message,36,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429178603,1,['message'],['message']
Integrability,"112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.sch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:2355,wrap,wrapException,2355,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['wrap'],['wrapException']
Integrability,"2-debian11` (see [here](https://github.com/hail-is/hail/blob/main/hail/python/hailtop/hailctl/dataproc/start.py#L147)) which [was released in January 2023](https://cloud.google.com/dataproc/docs/release-notes#January_23_2023). The latest available version of [Dataproc's Debian images](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1) is 2.1.25-debian11 which depends on GoogleCloudDataproc hadoop connector version [2.2.15](https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/tag/2.2.15) which relies on Google Cloud Storage client library version [2.22.3](https://github.com/GoogleCloudDataproc/hadoop-connectors/commit/8b79f025ef5e8231de827f4c620cd23e230c3489). I have [a PR](https://github.com/hail-is/hail/pull/13732) to upgrade us to 2.27.1 because the library broke retries in versions [2.25.0, 2.27.0). AFAICT, Google's image version page only shows the most recent five. There's no way to go back further in time. Luckily, the way back machine has [a March 2023 capture](https://web.archive.org/web/20230307225815/https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1) which includes our version. 2.1.2-debian11 used Google Cloud Dataproc hadoop connector version [2.2.9](https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/tag/v2.2.9) This version of the hadoop connector was [using some alpha version of a gRPC version of the cloud storage library](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/18f6e9f1c745e1854d76bea9362e2332898d8895/pom.xml#L96C1-L97C1). I'm not sure what's up with that. OK, here's my proposal: let's change that IMAGE_VERSION to the latest one and see if that fixes things. If that works, let's just merge and forget this happened. If that *doesn't* work, we gotta wade into the Lovecraftian horror of JARs. Most likely we're not fully relocating the dependencies pulled in by the Google Cloud Storage client libraries and they conflict with what Dataproc produces.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13690#issuecomment-1738196645:2208,depend,dependencies,2208,https://hail.is,https://github.com/hail-is/hail/issues/13690#issuecomment-1738196645,1,['depend'],['dependencies']
Integrability,2.1.0 test was removed from CI (and as a merge dependency),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2639#issuecomment-356088106:47,depend,dependency,47,https://hail.is,https://github.com/hail-is/hail/pull/2639#issuecomment-356088106,1,['depend'],['dependency']
Integrability,"40abbc33f88f4fd39639/pyscripts_Zf07i2.zip/constraint_utils/constraint_basics.py"", line 41, in get_old_mu_data; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 772, in transmute; File ""<decorator-gen-648>"", line 2, in _select; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/typecheck/check.py"", line 546, in wrapper; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 438, in _select; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/table.py"", line 447, in _select_scala; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/c38a09976d3c40abbc33f88f4fd39639/hail-devel-17a988f2a628.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: NullPointerException: null. Java stack trace:; java.lang.NullPointerException: null; 	at scala.collection.convert.Wrappers$JListWrapper.length(Wrappers.scala:86); 	at scala.collection.SeqLike$class.size(SeqLike.scala:106); 	at scala.collection.AbstractSeq.size(Seq.scala:41); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:285); 	at scala.collection.AbstractTraversable.toArray(Traversable.scala:104); 	at is.hail.utils.richUtils.RichIterable.toFastIndexedSeq(RichIterable.scala:83); 	at is.hail.table.Table.select(Table.scala:436); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4314#issuecomment-420405267:1775,Wrap,Wrappers,1775,https://hail.is,https://github.com/hail-is/hail/issues/4314#issuecomment-420405267,1,['Wrap'],['Wrappers']
Integrability,62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.sch,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:6833,wrap,wrapException,6833,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['wrap'],['wrapException']
Integrability,"8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:15343,message,message,15343,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,2,['message'],['message']
Integrability,": 'Ready'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'ContainersReady'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'PodScheduled'}],; 'container_statuses': [{'container_id': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_id': '',; 'last_state': {'running': None,; 'terminated': None,; 'waiting': None},; 'name': 'main',; 'ready': False,; 'restart_count': 0,; 'state': {'running': None,; 'terminated': {'container_id': None,; 'exit_code': 0,; 'finished_at': None,; 'message': None,; 'reason': None,; 'signal': None,; 'started_at': None},; 'waiting': None}}],; 'host_ip': '10.128.0.8',; 'init_container_statuses': None,; 'message': None,; 'nominated_node_name': None,; 'phase': 'Pending',; 'pod_ip': None,; 'qos_class': 'Burstable',; 'reason': None,; 'start_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal())}}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 53, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collecti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:9146,message,message,9146,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['message'],['message']
Integrability,": ; 2.49.0; Measurement time: ; 2023-06-05 03:25:16 PM ; Running on GCE: ; True; GCE Instance:; 	; Bucket location: ; US-CENTRAL1; Bucket storage class: ; REGIONAL; Google Server: ; ; Google Server IP Addresses: ; 142.250.128.128; 142.251.6.128; 108.177.112.128; 74.125.124.128; 172.217.212.128; 172.217.214.128; 172.253.119.128; 108.177.111.128; 142.250.1.128; 108.177.121.128; 142.250.103.128; 108.177.120.128; 142.250.159.128; 142.251.120.128; 142.251.161.128; 74.125.126.128; Google Server Hostnames: ; ib-in-f128.1e100.net; ic-in-f128.1e100.net; jo-in-f128.1e100.net; jp-in-f128.1e100.net; jq-in-f128.1e100.net; jr-in-f128.1e100.net; jt-in-f128.1e100.net; jv-in-f128.1e100.net; jw-in-f128.1e100.net; jx-in-f128.1e100.net; jy-in-f128.1e100.net; jz-in-f128.1e100.net; ie-in-f128.1e100.net; if-in-f128.1e100.net; ig-in-f128.1e100.net; ik-in-f128.1e100.net; Google DNS thinks your IP is: ; ; CPU Count: ; 16; CPU Load Average: ; [32.39, 33.2, 19.0]; Total Memory: ; 57.5 GiB; Free Memory: ; 38.41 GiB; TCP segment counts not available because ""netstat"" was not found during test runs; Disk Counter Deltas:; disk reads writes rbytes wbytes rtime wtime ; loop0 0 0 0 0 0 0 ; loop1 0 0 0 0 0 0 ; loop3 0 0 0 0 0 0 ; loop4 0 0 0 0 0 0 ; loop5 0 0 0 0 0 0 ; nvme0n1 4385 4694 581857280 1743810560 6453 527129 ; sda1 0 544 0 3731456 0 429 ; sda14 0 0 0 0 0 0 ; sda15 0 0 0 0 0 0 ; TCP /proc values:; tcp_timestamps = 1; tcp_sack = 1; tcp_window_scaling = 1; Boto HTTPS Enabled: ; True; Requests routed through proxy: ; False; Latency of the DNS lookup for Google Storage server (ms): ; 1.5; Latencies connecting to Google Storage server IPs (ms):; 74.125.126.128 = 1.1. ------------------------------------------------------------------------------; In-Process HTTP Statistics ; ------------------------------------------------------------------------------; Total HTTP requests made: 149; HTTP 5xx errors: 0; HTTP connections broken: 0; Availability: 100%. Output file written to '/tmp/output.json'.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:4153,rout,routed,4153,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['rout'],['routed']
Integrability,": i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## 2I installed the atlas-devel , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https://github.com/broadinstitute/hail/files/417544/gradle_check_info1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1718,Depend,Dependency,1718,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,2,['Depend'],['Dependency']
Integrability,": sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs:  1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-ebs: Downloading aiohttp_session-2.7.0-py3-none-any.whl (14 kB); 899 | amazon-ebs: Collecting asyncinit<0.3,>=0.2.4; 900 | amazon-ebs: Downloading asyncinit-0.2.4-py3-none-any.whl (2.8 kB); 901 | amazon-ebs: Collecting avro<1.12,>=1.10; 902 | amazon-ebs: Downloading avro-1.11.1.tar.gz (84 kB); 903 | amazon-ebs:  84.2/84.2 kB 22.0 MB/s eta 0:00:00; 904 | amazon-ebs: Installing build dependencies: started; 905 | amazon-ebs: Installing build dependencies: finished with status 'done'; 906 | amazon-ebs: Getting requirements to build wheel: started; 907 | amazon-ebs: Getting requirements to build wheel: finished with status 'done'; 908 | amazon-ebs: Preparing metadata (pyproject.toml): started; 909 | amazon-ebs: Preparing metadata (pyproject.toml): finished with status 'done'; 910 | amazon-ebs: Collecting azure-identity==1.6.0; 911 | amazon-ebs: Downloading azure_identity-1.6.0-py2.py3-none-any.whl (108 kB); 912 | amazon-ebs:  108.5/108.5 kB 28.5 MB/s eta 0:00:00; 913 | amazon-ebs: Collecting azure-storage-blob==12.11.0; 914 | amazon-ebs: Downloading azure_storage_blob-12.11.0-py3-none-any.whl (346 kB); 915 | amazon-ebs:  346.4/346.4 kB 41.0 MB/s eta 0:00:00; 916 | amazon-ebs: Collecting bokeh<2.0,>1.3; 917 | amazon-ebs: Downloading bokeh-1.4.0.tar.gz (32.4 MB); 918 | amazon-ebs:  32.4/32.4 MB 48.4 MB/s eta 0:00:00; 919 | amazon-ebs: Preparing metadata (setup.py): started; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:2424,depend,dependencies,2424,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,2,['depend'],['dependencies']
Integrability,":+PInt32})),None)),Some(TableStageDependency(WrappedArray()))),Begin(ArrayBuffer(WriteMetadata(MakeArray(ArrayBuffer(GetField(WritePartition(MakeStream(ArrayBuffer(Literal(struct{},[])),stream<struct{}>,false),Str(""part-0""),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/globals/parts/,None,None)),filePath)),array<str>),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/globals,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(),JArray(List(JObject(List((start,JObject(List())), (end,JObject(List())), (includeStart,JBool(true)), (includeEnd,JBool(true)))))),null,Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_369,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_376,GetField(Ref(__iruid_376,struct{filePath: str, partitionCounts: int64}),filePath))),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/rows,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(idx),JArray(List(JObject(List((start,JObject(List((idx,JInt(0))))), (end,JObject(List((idx,JInt(10))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(10))))), (end,JObject(List((idx,JInt(20))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:5287,Wrap,WrappedArray,5287,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601,1,['Wrap'],['WrappedArray']
Integrability,":+PInt32})),None)),Some(TableStageDependency(WrappedArray()))),Begin(ArrayBuffer(WriteMetadata(MakeArray(ArrayBuffer(GetField(WritePartition(MakeStream(ArrayBuffer(Literal(struct{},[])),stream<struct{}>,false),Str(""part-0""),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/globals/parts/,None,None)),filePath)),array<str>),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/globals,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(),JArray(List(JObject(List((start,JObject(List())), (end,JObject(List())), (includeStart,JBool(true)), (includeEnd,JBool(true)))))),null,Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_465,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_472,GetField(Ref(__iruid_472,struct{filePath: str, partitionCounts: int64}),filePath))),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/rows,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(idx),JArray(List(JObject(List((start,JObject(List((idx,JInt(0))))), (end,JObject(List((idx,JInt(10))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(10))))), (end,JObject(List((idx,JInt(20))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:5007,Wrap,WrappedArray,5007,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693,1,['Wrap'],['WrappedArray']
Integrability,"; ; 18/01/08 13:51:03 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 13:51:03 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 13:51:03 WARN Utils: Your hostname, <my computer name> resolves to a loopback address: <my local IP>; using <my IP> instead (on interface enp3s0); 18/01/08 13:51:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; ```. And the other initialize hail like this (crashes with the stack trace/error in the issue):; ```; from pyspark import *; from hail import *; conf = SparkConf(); conf.set('spark.sql.files.maxPartitionBytes','60000000000') ; conf.set('spark.sql.files.openCostInBytes','60000000000') ; conf.set('spark.driver.cores','1') #test with 1 core; sc = SparkContext(conf=conf); hc = HailContext(sc); ```. With startup messages looking like this:; ```; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/01/08 15:16:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/01/08 15:16:23 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).; 18/01/08 15:16:23 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 18/01/08 15:16:23 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 15:16:2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:7497,message,messages,7497,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['message'],['messages']
Integrability,"; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-nam",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:2294,protocol,protocol,2294,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611,1,['protocol'],['protocol']
Integrability,"===================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend); hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:99: in __init__; min_block_size, branching_factor, tmp_dir, optimizer_iterations); /miniconda3/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py:1257: in __call__; answer, self.gateway_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . answer = 'xspy4j.Py4JException: Method apply([null, class java.lang.String, class scala.Some, class java.lang.String, class jav...java:79)\\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\tat java.lang.Thread.run(Thread.java:748)\\n'; gateway_client = <py4j.java_gateway.GatewayClient object at 0x11098bc50>, target_id = 'z:is.hail.HailContext', name = 'apply'. def get_return_value(answer, gateway_client, target_id=None, name=None):; """"""Converts an answer received from the Java gateway into a Python object.; ; F",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:2035,wrap,wrapper,2035,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928,1,['wrap'],['wrapper']
Integrability,"==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(j",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:5894,wrap,wrapper,5894,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['wrap'],['wrapper']
Integrability,"> > Do you mean `estimated-current.sql` rather than `estimated-current.yaml` above?; > ; > Yes, sorry for the confusion; > ; > > Also another question is how does the schema update enforce certain order of operations.; > > The rename-job-groups-cancelled-column sql should run before other sqls that depend on the modified column name in job_groups_cancelled table, correct?; > ; > Migrations are applied successively. You cannot edit a previous migration or the order in which they're applied as they've already been applied to the production database. That's why I said this:; > ; > > > I fear you'll have to take inspiration from rename-job-groups-tables.sql by applying one ALTER TABLE command then drop and recreate EVERYTHING that references that name (constraints, triggers, procedures etc). This will likely involve copy+paste and rename.; > ; > I think you need to find any trigger or stored procedure that references that column, drop it and recreate it with the field renamed. It's a little scary. @ehigham Thanks for your comments above. Ive added the triggers and stored procedures referencing the `job_groups_cancelled` table in `rename-job-groups-cancelled-column.sql`. I was initially confused by `estimate-current.sql`; I thought it was a system-generated file to track the latest batch DDLs after a schema update, rather than a file that is manually updated. After reading this [thread](https://hail.zulipchat.com/#narrow/stream/300487-Hail-Batch-Dev/topic/mysqldump), I completely agree with your point. In other organizations I've worked with, we maintained schema changes in a separate folder, identified by release versions (e.g., semver) and the DLLs are ordered by sequence number. This way, we had a clear history of DDLs and the order they were applied, eliminating the need for files like `estimate-current.sql`. I just have one question: Do we need to manually update `estimate-current.sql` with the schema changes from `rename-job-groups-cancelled-column.sql`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2340993612:300,depend,depend,300,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2340993612,1,['depend'],['depend']
Integrability,"> > Must have been resolved or caused by a separate issue.; > ; > Can you reproduce it? I looked over the code path and this is very hard to understand for two reasons: nothing treats spaces specifically, so it probably doesn't have anything to do with spaces, and I don't see how you can get a success message but the deletion didn't happen. Nope, just tried, works great.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7162#issuecomment-536323352:303,message,message,303,https://hail.is,https://github.com/hail-is/hail/pull/7162#issuecomment-536323352,1,['message'],['message']
Integrability,"> > New config should be correct.; > > As an example of this slash issue, the following config (deployed right now) doesn't work.; > ; > I'm confused, is it correct, or does it not work? If there's an issue upstream, it needs to get fixed, too. The new config addresses the requested changes on the config. When this pr was created there was a claim that the redirection to service.internal was not found in gateway. That appears to be not the case (or at least the internal router doesn't seem to be receiving those requests) and still needs to be investigated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-541102715:475,rout,router,475,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541102715,1,['rout'],['router']
Integrability,"> > Tim could you help me understand why we do not need the IR node to be supported in Python?; > ; > the In node is unused in Python, and that's good -- python doesn't know about physical types. Ah, I thought the In node was used as then to represent data inflow (from some outside source), and thought the Python interface may want to generale that node. Ok, so what you said makes sense. In that case, why do we also have a `In(_, Type)` constructor (`def apply(i: Int, typ: Type): In = In(i, PType.canonical(typ))`)?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7868#issuecomment-574487828:315,interface,interface,315,https://hail.is,https://github.com/hail-is/hail/pull/7868#issuecomment-574487828,1,['interface'],['interface']
Integrability,"> @ehigham the only reason i represented it this way is because that's how it is in the database, like we have job attributes and then we have the always run flag for a job separately from those, but on the frontend i don't think there would be a problem with representing it as an attribute. i could change the new table column on the Batch page to be Attributes and put ""Always Run"" in there only for jobs that are always run, and i could also move ""Always Run"" to be under the Attributes header on the Job page, instead of the Properties header. happy to make either or both changes, just lmk!. That sounds not too dissimilar to how google adds 'labels' to tabular data. Personally I prefer that style, especially as more information gets added and tables get wider.; That said, I don't know how much extra data we'd want to add to this UI so that route may be slightly crystal-balling! Let's keep it simple for now as you've done already and consider something like that in the future if we have more data to add.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14425#issuecomment-2030060565:851,rout,route,851,https://hail.is,https://github.com/hail-is/hail/pull/14425#issuecomment-2030060565,1,['rout'],['route']
Integrability,"> @lgruen Can this change wait until January when we make breaking interface changes?. Yes, no problem, especially if you want to get rid of `default_image` altogether. I'll close this PR for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9753#issuecomment-740252438:67,interface,interface,67,https://hail.is,https://github.com/hail-is/hail/pull/9753#issuecomment-740252438,1,['interface'],['interface']
Integrability,"> @patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure?. In general, I don't know how to wrap pieces of a stream in methods, since streams need to be able to jump between labels defined by different stream nodes. Maybe to handle large `MakeStream`s, and decomposing other streams into multiple methods, we could compile some streams into iterators. That shouldn't be hard to do, and would let us experiment with the performance effects. (I've been nervous about stream code never getting jit compiled, and never considered this option. I think I like it.). > Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. Yeah, that seems unavoidable. If MakeArray(...) generates better code than ToArray(MakeStream(...)), is it worth keeping MakeArray, with a ToArray(MakeStream(...)) -> MakeArray(...) simplify rule?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391:49,wrap,wrap,49,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391,2,['wrap'],['wrap']
Integrability,"> @pwc2 , I'm gonna close this because PR has trouble with old PRs. Can we revisit this when the PCA project is wrapped up?. Yeah, sounds good",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10874#issuecomment-1050064333:112,wrap,wrapped,112,https://hail.is,https://github.com/hail-is/hail/pull/10874#issuecomment-1050064333,1,['wrap'],['wrapped']
Integrability,"> A few more thoughts.; > ; > ; > ; > I'm apprehensive to make our build process depend on building some third-party software. I think we should assume a REGENIE binary/image exists. It's fine if we produce it by hand and upload it. We should ask the REGENIE folks to start publishing binaries or images. As we discussed, I will ask them to produce it. In the meantime I can include the example folder from the regenie repo alone, as before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-668904836:81,depend,depend,81,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668904836,1,['depend'],['depend']
Integrability,"> A user reported this error concurrent.futures._base.TimeoutError with no stack trace while copying files in a batch job. . This. No stack trace. If you look at this output, the previous stack trace is part of the WARNING message. ```; INFO:deploy_config:deploy config file not found: None; INFO:hailtop.aiocloud.aiogoogle.credentials:using credentials file /gsa-key/key.json: GoogleServiceAccountCredentials for XXXXX@PROJECT.iam.gserviceaccount.com; WARNING:hailtop.utils:Encountered 2 errors (current delay: 0.2). My stack trace is File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/copy.py"", line 128, in <module>; asyncio.run(main()); File ""/usr/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 735, in retry_transient_errors; st = ''.join(traceback.format_stack()); . Most recent error was; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 729, in retry_transient_errors; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 134, in request_and_raise_for_status; resp = await self.client_session._request(method, url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 634, in _request; break; File ""/usr/local/lib/python3.7/dist-packages/aiohttp/helpers.py"", line 721, in __exit__; raise asyncio.TimeoutError from None; concurrent.futures._base.TimeoutError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817#issuecomment-1117642330:223,message,message,223,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117642330,1,['message'],['message']
Integrability,"> Addressed all comments, should be good to go.; > ; > One question I did have is that I have to specify a region size in this aggregator, and I picked `TINY`. Does it matter? `TINY` does seem small compared to the kinds of operations that we'll do on dndarray and block matrix. That specifies the granularity of allocations. Each time the region needs more space than it has allocated, it allocates a new block of size determined by the size parameter. Unless it's trying to make a single allocation larger than the block size, in which case it allocates a block of exactly the desired size. The ideal thing here would be to make a single block with exactly the size needed for the state, but the current Region interface doesn't support that. The closest we can get is using the smallest block size, which is `TINIER`. Then the missing bit and pointer to the data would go in the initial tiny block, and the data would go there too if it's small enough, otherwise it would go in an allocation of exactly the right size.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9209#issuecomment-669255320:713,interface,interface,713,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-669255320,1,['interface'],['interface']
Integrability,"> Ah sorry, I forgot to update the original commit message as that's not actually correct. The optimization is that we don't need to iterate through all blobs until we find the exact blob matching our path name. The list operation returns all blobs that start with the prefix of that path. If we see a blob with a different name that is a child of our path f'{path}/foo, then we know it's a directory and don't need to iterate anymore (although it could be a file as well, but in Scala we don't currently throw errors on paths that are both files and directories, so we just choose the first we see). If we see a blob that matches the path exactly, then we know it's a file and stop iterating. The only reason we need to iterate through more than one blob is if there's blobs that are like '{path}zzzzz/foo or '{path}szzzzz. We need to ignore these as they don't provide any information on whether {path} is a file or directory. This is where isChildOf is needed because we need to make sure the blob is actually a child of the path such as '{path}/file and not {path}zzzzz/file. Ah thanks, this makes more sense to me now. > The other option is to do 2 queries. One to check if it's a file and the next to check if there are any child blobs. This does sound simpler conceptually. I'm not sure off the top of my head what is better performance-wise: the two network requests for a directory or loading a whole page of results in a single request when we only need max 2 results. What do you prefer?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13390#issuecomment-1677489464:51,message,message,51,https://hail.is,https://github.com/hail-is/hail/pull/13390#issuecomment-1677489464,1,['message'],['message']
Integrability,"> Ah, I commented on both PRs by accident. My bad, sorry! In the future, keeping each PR to a single commit can help the reviewer identify the right changes to review for stacked PRs. Please clarify. Unless I misunderstand, 1 commit per PR isnt something that we currently hold ourselves to. For instance the PR you issued for site deploy has something like 10 commits. Maybe we can specify this in a design doc, much like John and you did with the PR message? This would be a useful place to specify related issues, like settling on rebasing vs merging. . My understanding of our use of the stacked label is that the dependent PR isnt meant to be reviewed until the child is merged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8920#issuecomment-638964132:453,message,message,453,https://hail.is,https://github.com/hail-is/hail/pull/8920#issuecomment-638964132,2,"['depend', 'message']","['dependent', 'message']"
Integrability,"> CI's tags are annotated, i.e., they have their own datestamp and message. The recent 0.2.128 tag is a lightweight tag, so for example `git describe` of current HEAD still says 0.2.127-87-ge002b4b23; you have to add `--tags` to get 0.2.128-8-ge002b4b23. Not a biggie, but worth adding to a checklist.; > ; > But yes, eventually, signing them would probably be best practice. Ah, I see, thanks for pushing on this. I was not aware there were two kinds of tags. I'll fix the 0.2.128 tag.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14322#issuecomment-1967423802:67,message,message,67,https://hail.is,https://github.com/hail-is/hail/pull/14322#issuecomment-1967423802,1,['message'],['message']
Integrability,"> Can we dummy proof this a bit more to have a nice error message if this is not the case?. Happy to, but I'm not exactly sure what you mean by ""if this is not the case"". What would you like to add?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13180#issuecomment-1593184582:58,message,message,58,https://hail.is,https://github.com/hail-is/hail/pull/13180#issuecomment-1593184582,1,['message'],['message']
Integrability,"> Could you help me understand what the benefit of gidgethub is over our previous strategy? Since we generally prefer minimal deps. This isn't more deps. I switched gidgethub for PyGithub. gidgethub is async, PyGithub is not. Now that we're using aiohttp over Flask, if we don't use an async library to query Github handling web requests will hang while the synchronous library blocks waiting for requests. Actually, this is fewer deps. We already use gidgethub in CI, so overall this removes the PyGithub dependency. Since both CI and scorecard depend on gidgethub, I moved it from the CI-specific installation to the service base image.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-531942870:506,depend,dependency,506,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-531942870,2,['depend'],"['depend', 'dependency']"
Integrability,"> Depending on desired PL behavior, there's one last mismatch between the new combiner and the old one. > The new combiner doesn't compute RGQ at all for reference alleles, I think this is fine. I think this is OK. Ship it!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10777#issuecomment-921767047:2,Depend,Depending,2,https://hail.is,https://github.com/hail-is/hail/pull/10777#issuecomment-921767047,1,['Depend'],['Depending']
Integrability,"> Do you mean `estimated-current.sql` rather than `estimated-current.yaml` above?. Yes, sorry for the confusion. > Also another question is how does the schema update enforce certain order of operations.; > ; > The rename-job-groups-cancelled-column sql should run before other sqls that depend on the modified column name in job_groups_cancelled table, correct?. Migrations are applied successively. You cannot edit a previous migration or the order in which they're applied as they've already been applied to the production database.; That's why I said this:. >> I fear you'll have to take inspiration from rename-job-groups-tables.sql by applying one ALTER TABLE command then drop and recreate EVERYTHING that references that name (constraints, triggers, procedures etc). This will likely involve copy+paste and rename. I think you need to find any trigger or stored procedure that references that column, drop it and recreate it with the field renamed. It's a little scary.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2339054214:288,depend,depend,288,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2339054214,1,['depend'],['depend']
Integrability,"> Do you think we should add 1-2 more states which are 'any_failure', 'all_failed' and change the others to 'any_success' and 'all_succeeded'? I was thinking it might be nice to have onError jobs with that dependency explicitly in the DAG. I'm fine with this, though I think `any_succeeded` and `all_succeeded` is more consistent (instead of `any_success`)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12365#issuecomment-1289349099:206,depend,dependency,206,https://hail.is,https://github.com/hail-is/hail/pull/12365#issuecomment-1289349099,1,['depend'],['dependency']
Integrability,"> Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an allocateAndStoreString method, which takes care of both steps, potentially more efficiently, but also more ergonomically. PString.allocate probably shouldn't exist, then! The public interfaces to PString probably need to include the ability to copy value=>value (address) and to put a string in a region.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7904#issuecomment-576358719:644,interface,interfaces,644,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358719,1,['interface'],['interfaces']
Integrability,"> For ArrayMap2, should it depend l || r requiredeness?. yep!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7959#issuecomment-580073608:27,depend,depend,27,https://hail.is,https://github.com/hail-is/hail/pull/7959#issuecomment-580073608,1,['depend'],['depend']
Integrability,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:877,rout,routinely,877,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659,1,['rout'],['routinely']
Integrability,"> Github doesn't have a way to say ""stacked on this PR"", so removal of the stacked tag is a manual gating. I'm surprised it doesn't have a way to do this, since when a PR is mentioned by a link or hash, that dependency is updated with its dependent. . Looks like this may be coming: https://twitter.com/natfriedman/status/1170804894241972224",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7417#issuecomment-548479314:208,depend,dependency,208,https://hail.is,https://github.com/hail-is/hail/issues/7417#issuecomment-548479314,2,['depend'],"['dependency', 'dependent']"
Integrability,"> Hey Alex, this is great. The PR protocol is for you to assign the PR to a random developer by using the random user generated by scorecard. Thanks Cotton. I also need to make sure desktop notifications are enabled, across GitHub and Zulip. Apologies for not noticing this earlier.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4947#issuecomment-449080254:34,protocol,protocol,34,https://hail.is,https://github.com/hail-is/hail/pull/4947#issuecomment-449080254,1,['protocol'],['protocol']
Integrability,"> High level question: is the state stored in MySQL the same as the k8s notebook pod state? If so, should we just query k8s rather than duplicating the notebook state in MySQL?. Not entirely the same I think, but I also prefer tools that are well-designed for the purpose, and have limited k8 / etcd experience / survivor bias. One interesting fact (spoke with Dan), is that we don't have direct access to etcd, so are limited to k8 client operations. * We had talked about, in batch context, of having our own master state, against which k8 is reconciled, to protect against k8 operational errors. Obviously that means our record of state is a point of failure, but db failure modes and uptime solutions are well defined across cloud/db provider vendors (we can minimize lock-in as well). The idea seemed to be that we don't particularly care how k8 works, or how much state it persists; the contract is with our users, and we should satisfy . * K8 does not store all state indefinitely. It's more like a rotating log: https://stackoverflow.com/questions/40636021/how-to-list-kubernetes-recently-deleted-pods . For users, and for investors, we want to have a permanent record of all user interactions. * Some kinds of data may be awkward to store and query within pod labels. For instance, how much user state do we want to store in labels? How do we store operation graphs / history?; ; * aggregation operations across users or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there eith",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:893,contract,contract,893,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290,1,['contract'],['contract']
Integrability,"> How do I use a single non-optional allowedOverlap to express this versus a TMP function that doesn't care about keying/sorting at all?. Ah, I see. I think the answer is: you can't. You would need another integer parameter to say ""I actually depend on this prefix of the key being sorted"". It seems like allowedOverlap and requiredSortedPrefix are completely independent. In the single key case (for simplicity), you may or may not care if keys are localized in one partition, and you may or may not care if they're sorted. I don't see any connection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12587#issuecomment-1407126299:243,depend,depend,243,https://hail.is,https://github.com/hail-is/hail/pull/12587#issuecomment-1407126299,1,['depend'],['depend']
Integrability,"> I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. The point isn't to save the few untars, the point is to make the general facilities performant for everyone. Basically, I think directory outputs are untenable in the current design unless they are very small, which is why you're doing all this for your use case, and the tar is still probably as good (if not better) in that case. Fix it for everyone's use case so nobody has to go through this in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560459474:26,rout,route,26,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560459474,1,['rout'],['route']
Integrability,> I cannot figure out why the revision isn't getting generated. The build_hail step seems to completely ignore that I've put that dependency into the Makefile. I see there is a `hail/hail_revision` file in the zip created by `build_hail`. I wonder why it's not found later,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11645#issuecomment-1076382123:130,depend,dependency,130,https://hail.is,https://github.com/hail-is/hail/pull/11645#issuecomment-1076382123,1,['depend'],['dependency']
Integrability,"> I do not understand why, but, by default, no messages are printed. Hmm, it looks to me like the default is to print (for WARN and above) the message with no further format:. ```; $ cat foo.py; import logging. log = logging.getLogger('test'). log.info('info'); log.warning('warning'); log.error('error'); $ python3 foo.py; warning; error; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077:47,message,messages,47,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077,2,['message'],"['message', 'messages']"
Integrability,"> I don't know if this adds any value in the containerized/cloud environment, where custom machine images are presumably the way to go. Dan already dockerized our dependencies for the CI setup. I don't quite know what value it would add there, either. It seems good for letting other people install your application in the cloud, but isn't really a thing we do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410368037:163,depend,dependencies,163,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410368037,1,['depend'],['dependencies']
Integrability,"> I don't understand your first sentence. What didn't you understand? I'd like to clarify if that would be helpful. > In the case of a hybrid query and batch script, using Hail Query is totally kosher! What I don't want is dependence on a JVM and Hail Query just to check if a file exists in Google Cloud Storage for a script that is otherwise independent of Hail Query. So are you ok with keeping the build.yml changes (which would allow hailtop to have something within in that called hail query without being called from batch as a docker image + command line arguments).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-671553680:223,depend,dependence,223,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671553680,1,['depend'],['dependence']
Integrability,"> I have an image built from Dockerfile that I cannot modify, and which has an ENTRYPOINT that is not /bin/bash or /bin/sh. How do I get this to run in batch without this PR? I don't understand the proposal. The proposal is to have the backend run `docker run --entrypoint <shell> ...` unconditionally. This involves no change to the user interface. I believe this would implement the intended Batch behavior on any docker image.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9219#issuecomment-670693135:339,interface,interface,339,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670693135,1,['interface'],['interface']
Integrability,"> I haven't checked if new bokeh supports old pandas. Nor do I know if we have old pandas usage lurking in the codebase. Can we make our pinned-requirements.txt use pandas 2.0, fix whatever issues arise, but leave requirements.txt flexible for folks?. I think there are compromises either way, but I would be surprised if this just worked. It seems very easy to accidentally adopt new functionality so at that point why even have a lower-bound? I think that, while it's very hard to make sure that all our dependencies are compatible at all possible version combinations, and these things will happen, it just feels like an easily-avoidable lie to say we support 1.x and 2.x but then use functionality exclusive to 2.x. So I'm ok keeping the bounds more relaxed if we can guarantee that *our* usage of pandas is compatible with both. FWIW, I think that our primary dependencies release major versions infrequently enough that it is reasonable to only support the most recent major version, in much the same way that we don't support python versions indefinitely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1520500573:506,depend,dependencies,506,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1520500573,2,['depend'],['dependencies']
Integrability,"> I realize this is a flip-flop and probably a bit frustrating, but I think leaving the ComplexPTypes on the abstract PLocus, PCall, PInterval seems preferable to needing to define a `representation` on those classes. We can move the ComplexPType inheritance to the concrete implementations of those interfaces when we get rid of the usages of `representation` outside the ptypes package.; > ; > This is a small refactor, right?. The issue is that without declaring the representation type on those classes we will need more casts (for instance line 17 of BinarySearch now needs one), and this is inherently the same thing as assuming PCanonical* rather than the abstract class (so we should not cast, either use the abstract class and define representation's type, or match on the specific implementation).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7712#issuecomment-566681403:300,interface,interfaces,300,https://hail.is,https://github.com/hail-is/hail/pull/7712#issuecomment-566681403,1,['interface'],['interfaces']
Integrability,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:451,contract,contract,451,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940,2,['contract'],['contract']
Integrability,> I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job. This is a good idea. My current thinking for the autoscaler is to utilize multiple input parameters such as the ones you list to determine the optimal number of instances to create.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8149#issuecomment-592059899:153,message,messages,153,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-592059899,1,['message'],['messages']
Integrability,"> I thought in these cases the code would be evaluated (inner -> outer). Sorry, don't know what you mean here. Can you explain?. > ... added to the function stack. What actually happens?. There's no function stack - just because our codegen routine calls a new method doesn't mean the generated code has a method. We'd have to explicitly put a method boundary in. In the linked example, you're right that there is no generated loop, just a bunch of flat code. You're passing `loadElement(...)` as the `srcAddress` to a field's copyFromType method, which may use it multiple times, inlining load element (and doing a bunch of unnecessary pointer math / dereferences) each time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8099#issuecomment-586417359:241,rout,routine,241,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586417359,1,['rout'],['routine']
Integrability,"> I understand the value of conservative updates before public demonstrations. I don't see this as conservative, and I don't think it has to do with the demo. It is to make incremental changes and disentangle unrelated changes. You should think of a PR as being lightweight. There's no reason not to create lots of them. In fact, by creating lots of them, you increase your velocity by removing false dependencies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5244#issuecomment-460386902:401,depend,dependencies,401,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460386902,1,['depend'],['dependencies']
Integrability,"> I'd propose to do an implicit dependency audit every time you push a new commit. > You can still pin versions on published packages, but use unpinned dependencies for CI testing. I think our only option here would be to test twice -- once with the major versions we explicitly depend on, and once with unbounded versions. I don't really like this model, since it basically couples breaking changes in other tools to Hail commits, which isn't how it actually works. I'd much prefer a weekly cron job that tries to update dependencies, I think, but that thing isn't trivial to build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542222446:32,depend,dependency,32,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542222446,4,['depend'],"['depend', 'dependencies', 'dependency']"
Integrability,"> I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. My apologies, @danking was wrong. I'm currently planning to use Dan's version because yours isn't ready. If yours is ready, I will use it. What does ready mean? From our recent email:. - it needs to get in master,; - and and integrated with our CI/CD (we can't be fixing issues and doing manual deployments leading up to or during a tutorial),; - it need to be beaten on by the team to look for issues (including scale issues), and; - it needs to be scale tested (@danking has a script for the old one that fires up N notebooks and reports any failures and summarizes the latency to notebook available),; - @tpoterba and I need to be comfortable enough with it we have confidence we can fix issues that arise during the tutorial. I probably also need time to review the workshop auth flow since I think that changed. If we're requiring login, we'll need to support more social login providers and/or email/password.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358:527,integrat,integrated,527,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358,1,['integrat'],['integrated']
Integrability,"> I've been working on an R interface to Hail through the sparklyr package. this also sounds awesome. To the issue -- this usually means something really bad happened on an executor, like a segfault. Spark usually is reluctant to provide the real error message, but in this case I think it's our fault. . Did you already do our job of bisecting to that commit for us? I'm not sure where exactly the generated C++ is being used (just the decoder, right, team?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-428377258:28,interface,interface,28,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-428377258,2,"['interface', 'message']","['interface', 'message']"
Integrability,"> Ideally, I'd release the references to the regions of my producer once I finished constructing the new RegionValue. I think references from a region to other regions should be treated the same as data directly contained in the region. In particular, in the current model it all gets freed at the same time. If we move to a stacked/checkpointed model like Cotton suggested, a checkpoint would remember both where in the region to move back to, and where in the list of region dependencies to move back to, releasing those regions to the right of that point.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7952#issuecomment-577853956:477,depend,dependencies,477,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-577853956,1,['depend'],['dependencies']
Integrability,"> Introducing an image with the wheel already installed isn't worthwhile, it adds 2.5 min latency. Agreed. I think our best path for speed is keeping these images totally cacheable so basically dependencies (nothing that will have to change on every commit, e.g. the wheel). Installing the wheels in the image is just adding more latency and work of localization. > The large number of splits often requires default Hail to scale up adding a 2min delay (It would be great to get that down).; I'm gonna revert the change that added images and maybe try to reduce service backend parallelism a bit. 36 minutes is an improvement. We should probably focus on making Hail faster rather than trying to squeeze lower latency out of parallelism. Totally agree.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1561101182:194,depend,dependencies,194,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1561101182,1,['depend'],['dependencies']
Integrability,"> Is this something where you need to add a new docker image to the gcr registry with the package installed? I think for pipeline I had to add new lines to `Dockerfile.pr-builder` and then run `make hail-ci-build-image` and `make push-hail-ci-build-image`. This is just a replacement for notebook, and yeah, basically what you described. This PR adds a dependency (lib sass, corresponding to the `import sass` line in notebook/notebook.py), which will fix the crashloopbackoff currently seen on our cluster (a PR was merged but this was missed).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5451#issuecomment-467611477:353,depend,dependency,353,https://hail.is,https://github.com/hail-is/hail/pull/5451#issuecomment-467611477,1,['depend'],['dependency']
Integrability,"> Just so I understand correctly (and sorry if this is obvious), the current job logs interface is still the same. But if you want a container's logs, then you'll get bytes which the user will have to decode themselves. How does that affect the file download button in the UI and the hailctl batch logs functionality you have? Will you see text or a random byte string?. Good question. For the download button, the file you download is still a normal text file and I've confirmed that I can download and view a log file as I would expect. For the `hailctl batch logs` functionality, I added logic to the CLI in this PR where I download the bytes of the log and if I can decode it as UTF-8 I do, so it prints exactly as before, and if I can't I just print the bytes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12666#issuecomment-1426373373:86,interface,interface,86,https://hail.is,https://github.com/hail-is/hail/pull/12666#issuecomment-1426373373,1,['interface'],['interface']
Integrability,"> Just starting to explore this PR. I compared this in-progress CI run to 7126798 (from #12737). The time to service backend starting is ~7 minutes. In the other PR, its ~8 minutes. I suppose that's because this PR isn't hitting any caches, right?; > ; > Hmm, it also seems like the critical path to the service backend test is through `build_hail_jar_and_wheel_only`. I wonder if we double the cores, would the time halve? On my laptop a fresh build is like 3m.; > . Ya I really only focus here on docker image steps, not on the overall critical path. But I like the idea of adding more cores I'll try that. The docker images are hitting cache though since every PR caches from its own previous runs in addition to main. The reason some images still take ~1 minute is because they introduce a new layer (normally the hail_version has changed since the SHA has changed) and they spend most of the time localizing the layer that has the dependencies installed (why I want lazy pulling). A retry of this PR should build the images very quickly, though I haven't tried that recently.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578#issuecomment-1458611223:936,depend,dependencies,936,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1458611223,1,['depend'],['dependencies']
Integrability,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256:458,message,messages,458,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256,1,['message'],['messages']
Integrability,"> Looks like we need to set the severity correctly in the worker logs. I'm also seeing a lot of this; WARNING: Published ports are discarded when using host network mode; Also looks like we incorrectly log a ContainerTimeoutError as an error log even though that's a user error: https://cloudlogging.app.goo.gl/TUGWNxnFiBiEdsDo9. For what it's worth, this was showing up as an info log because this is a docker log message not from our code, so it's not going through our logging filters. The reason this showed up in the Google Logging query was because the query included this line; ```; severity=ERROR OR WARNING; ```; which means ""logs whose severity is ERROR or whose log entry contains ""WARNING"""", it is *not* equivalent to `severity=ERROR OR severity=WARNING` which does not show that log entry. Either way, #14252 gets rid of that log message entirely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14240#issuecomment-1930672702:415,message,message,415,https://hail.is,https://github.com/hail-is/hail/issues/14240#issuecomment-1930672702,2,['message'],['message']
Integrability,> Looks like you need to [update the Google Artifact Registry cleanup policies](https://batch.hail.is/batches/8076011/jobs/210) to account for your new image. Instructions to do so are in the error message. Thanks!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13936#issuecomment-1785779141:198,message,message,198,https://hail.is,https://github.com/hail-is/hail/pull/13936#issuecomment-1785779141,1,['message'],['message']
Integrability,> Merging in #14233 causes the failure in `test_union_rows1`. Some strangeness with these new dependencies - running without this commit and everything works fine. Doesn't seem to be an issue anymore...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14231#issuecomment-1934834223:94,depend,dependencies,94,https://hail.is,https://github.com/hail-is/hail/pull/14231#issuecomment-1934834223,1,['depend'],['dependencies']
Integrability,"> Must have been resolved or caused by a separate issue. Can you reproduce it? I looked over the code path and this is very hard to understand for two reasons: nothing treats spaces specifically, so it probably doesn't have anything to do with spaces, and I don't see how you can get a success message but the deletion didn't happen.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7162#issuecomment-536320928:294,message,message,294,https://hail.is,https://github.com/hail-is/hail/pull/7162#issuecomment-536320928,1,['message'],['message']
Integrability,"> My main motivation is that I can use up-to-date versions of NumPy, SciPy and Pandas. Yeah, I'm sympathetic to this. Our current pinned-version approach is partially a [over-]reaction to builds breaking without any internal change, especially because many new Hail users are also new Python users and not capable of debugging Python dependency hell. > Are you using some continuous integration?. We do use CI, but I'm not exactly sure what you're proposing. An automatic find-the-latest-version-that-works step? It's true that if we were to use unpinned versions, our CI would start failing if a package introduces a breaking change, and we can fix it. However, I think that's too late -- now all old Hail versions will break from fresh installs! I feel that reproducibility demands we pin versions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542204913:334,depend,dependency,334,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542204913,2,"['depend', 'integrat']","['dependency', 'integration']"
Integrability,"> Not sure what to do about wait for and ghost. rather silly of ghost to refuse connections on http when the base url is set to https. > The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the X-Forwarded-Proto header isn't set. I'm not sure what the right fix is in this case. I still don't quite understand why the wait command doesn't hit gateway. Isn't it just issuing an http request to `f'{base_url}/{endpoint)`? Why doesn't that hit gateway?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548104962:292,rout,router,292,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548104962,1,['rout'],['router']
Integrability,"> OK, but I'm not sure it's the right change to make. Now some jobs will fail silently. I don't think this lets anything fail silently -- the failure from run() is still tracked. Agree that the right solution is to additionally track information about dead JVMs to try to provide a better error message, but that juice isn't worth the squeeze at the moment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12838#issuecomment-1527612619:295,message,message,295,https://hail.is,https://github.com/hail-is/hail/pull/12838#issuecomment-1527612619,1,['message'],['message']
Integrability,"> Only external requests go to the gateway. Why doesn't this do that (where baseurl is `http://{service}.namespace`. ```; base_url = internal_base_url(namespace, name, port); async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=5.0)) as session:; while True:; try:; async with session.get(f'{base_url}/healthcheck') as resp:; ```. edit: Ah it's just routing through Kubernetes. I thought there would need to be something that specified the dns to use... Ok.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548105738:366,rout,routing,366,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548105738,1,['rout'],['routing']
Integrability,"> Oops, sorry, your PR was for the monitoring namespace. I see the problem, CI doesn't have a route for `''`. I'll push a fix. Yeah, monitoring, thought you meant it was a similar issue (exact route not matching). Sounds good, I wasn't looking for the """", such a weird route.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536246245:94,rout,route,94,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536246245,3,['rout'],['route']
Integrability,"> Options: 1) Fix this rule, 2) (Seems not as good) In Emit have ArrayAgg needs to pass its child through Emit.emit a second time to match on ToArray, 3) make unstreamify more specific, such that ArrayAgg is allowed to take streams directly. ToArray definitely needs to wrap StreamRange in some cases (for instance MakeTuple(ToArray(StreamRange)), else get issues with the stream passed to SRVB. it would be helpful to have the intended (but currently applicable) design of stream/array semantics written down for all nodes (maybe it exists, I'll dig through design docs). Besides this 2 more failures. Option 4: rebase on master, where ArrayAgg is not an emittable node (only RunAgg / RunAggScan) :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-583803556:270,wrap,wrap,270,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583803556,1,['wrap'],['wrap']
Integrability,"> Relatedly, the auth system and the front end are not in this pull request (and AFACIT aren't in master yet?), which makes it harder to reason about the overall system. The auth system make sense as an independent PR (is that what #5162 is?). The changes that expose / use this new API (i.e. the UI component) should be a part of this PR so we can reason about the entire proposed change. I'm not sure how to really avoid this, some of it is the nature of our pull request goal (small, single-principle), and the other is the tradeoff of decoupling. This is also why I spend more time writing comments about the intended consumption of the notebook updates. Use those comments to reason about the overall system, and if that doesn't help, ask me to write more helpful comments.; ; The auth system is part of the Greenfield web pull request. That will be split up into something like 10-20 pull requests once the system is fully working, as mentioned in that repo. The auth-gateway will be in 2 of those (one for package-lock, one for the business logic). I've added the gateway changes to this particular pull request; that effectively shows the interface for authorization. I have mixed feelings about mixing that with the rest of this PR, happy to remove and issue separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641:1147,interface,interface,1147,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641,1,['interface'],['interface']
Integrability,"> So I just put back the dependencies on `native-lib-prebuilt`. Since that just calls make recursively, it would probably be better to let mill invoke make, but I didn't want to deal with that, and this is a pretty uncommon use case (I think). That's sane to me, and avoids needing to deal with the make jobserver at the mill level. Feel free to un-WIP this whenever you're ready to merge.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147#issuecomment-1930537252:25,depend,dependencies,25,https://hail.is,https://github.com/hail-is/hail/pull/14147#issuecomment-1930537252,1,['depend'],['dependencies']
Integrability,"> So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I agree that the tail-recursion interface seems like the right primitive to expose in python, on top of which we could implement convenience methods for building while/for loops if we decide it's worth it. > Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Also agree. This will require either adding another context of loops/continuations in the environment (valid places to jump to, and their argument types), or keeping them in the normal value context by adding a new continuation type. > Is Patrick's proposal for extra types written up anywhere?. My proposal has two main differences. In; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), x),; 0, 0); ```; the point that jumps back to the top of the loop is explicit, but the point that jumps out of the loop is not. I suggested making this something like; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:47,interface,interface,47,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407,4,"['interface', 'wrap']","['interface', 'interfaces', 'wrapping']"
Integrability,"> Sorry - one more thing I need help with. There's a cyclical import with the RouterFS in `variables.py`. Should I just pylint ignore it?. So it looks like that circular import is because of `config_variables`, but `config_variables` is only ever used in `hailctl` not elsewhere in `hailtop`:. ```; hailctl/config/cli.py; 10:from hailtop.config.variables import ConfigVariable, config_variables; 46: for var, var_info in config_variables().items():; 56: if parameter not in config_variables():; 62: config_variable_info = config_variables()[parameter]; 86: from hailtop.config import config_variables, get_user_config # pylint: disable=import-outside-toplevel; 99: config_items = {var.name: var_info.help_msg for var, var_info in config_variables().items()}. config/__init__.py; 4:from .variables import ConfigVariable, config_variables; 14: 'config_variables',. config/variables.py; 6:_config_variables = None; 30:def config_variables():; 34: global _config_variables; 36: if _config_variables is None:; 37: _config_variables = {; 112: return _config_variables; ```. Can you move `config_variables` into a file in `hailtop/hailctl/config` and keep the `ConfigVariable` enum in `hailtop/config/variables.py`? Then you can't have a circular reference because `hailtop` can't depend on `hailctl`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13224#issuecomment-1677917371:78,Rout,RouterFS,78,https://hail.is,https://github.com/hail-is/hail/pull/13224#issuecomment-1677917371,2,"['Rout', 'depend']","['RouterFS', 'depend']"
Integrability,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432:419,depend,dependent,419,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432,5,"['depend', 'integrat', 'interface', 'wrap']","['dependent', 'integration', 'interface', 'wrap']"
Integrability,"> Sure, so the proposal is that loadLength takes some region-containing object, and this object has a loadAddress method on it?. No, I think the interfaces you've created in your remove-region-from-load PR are correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826#issuecomment-575733539:145,interface,interfaces,145,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575733539,1,['interface'],['interfaces']
Integrability,"> Thanks for sharing this detailed plan!; > ; > So, I don't currently have any plan for, I suppose, ""applications"" that use Batch. Since we plan to support, maintain, and test the Hail Batch regenie implementation, I think it doesn't belong in a ""contrib"" directory. The hail python package has a `genetics` module for genetics-specific Hail Query functionality, let's mirror that structure. Let's move REGENIE and any non-Python dependencies of it into `hailtop/batch/genetics/regenie`. Sure. > How is the Dockerfile meant to be used? As written it doesn't appear that it would work because there doesn't exist any regenie source code to COPY in. It's meant to create a Regenie docker image that we could use. It's a copy of the regenie c++ repo's dockerfile, with the removal of the ENTRYPOINT /usr/local/bin/regenie, so that I could issue a command that included an executable, which is convenient to give me the ability to check that intermediate files are actually created (wc, ls) by batch, and because that seems more idiomatic for batch. I don't think there is a published regenie image, but docker hub is down so can't double check. . You're right, I shouldn't have deleted the bulk of the repo, kept as is. Didn't want to deal with submodules. > I've made some other in-line comments in the python file. It's not clear to me how all those other files are related to the python files and I'm a bit uncomfortable adding a whole directory with a LICENSE file, especially when not all the files in the directory fall under that license (e.g. the regenie py file) and moreover the license makes claims about things linking to BGEN, which none of our code here does. The license is only contained within the folder with the licensed files. In a previous conversation with Nate/Cotton, if we use any open-source software, best to keep those files segregated, alongside their license (license must be kept alongside the code, easier to see the demarcation if in a separate folder). > We have some BG",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987:430,depend,dependencies,430,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987,1,['depend'],['dependencies']
Integrability,"> Thanks for the explanation! I'm happy to make the change, I was just trying to understand the difference between Host and X-Forwarded-Host a little better before first.; > ; > So if I understand correctly, for the different headers:; > ; > * X-Forwarded-Proto gets passed to the router through the base https server in gateway, which sets X-Forwarded-Proto to `$scheme`, which is always going to be https since that's always going to be the protocol you're using for that server? And so when we use `$updated_scheme` for the blog server in the router's config, it's going to look at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I. Yep. In fact everything request to a Hail service (besides a lets-encrypt path) gets redirected to https. > Or am I misunderstanding how this works?. Nope, you have it correct. $http_x_forwarded_proto should never be absent, and would be fine to use instead of $updated_scheme (but I'd prefer one of those two, rather than https, because otherwise we're not relying on our upstream infrastructure). > * I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? . Yeah that's right. Host refers to the current server (or in the proxied case, what gateway set Host to). X-Forwarded-Host is set by gateway to be the $http_host at the time it proxies the request to router, which is going to be blog.hail.is. > I don't quite follow your comment about our use of `Host` being wrong, in this case; I _think_ I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enoug",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548082569:281,rout,router,281,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548082569,3,"['protocol', 'rout']","['protocol', 'router']"
Integrability,"> That should be a simple fix, though perhaps at this point not worth it as this is not a fruitful optimization for this query. Agreed, although depending on the time line for a good optimization for this query I may circle back on this, as there is currently a bug in this deduplication so if the actual optimizaion won;t be available for a while it might be worth fixing in the meantime",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1839244525:145,depend,depending,145,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1839244525,1,['depend'],['depending']
Integrability,"> The approach in this PR doubles down on the functional Code[T] structure. I don't see anything in the design that prevents us from moving away from `Code[T]`, but it does have to support it for now. > I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. This is also the interface I would like to see for CodeBuilder. And you're right, making that change would allow methods taking a `CodeBuilder` to not need a line number argument. I agree that's better. I may have gotten a bit of tunnel vision in the middle of the giant mechanical refactoring :) I will make this change. > I think part of my concern is that Im not entirely sold by the need to have a whole stack of IR printouts and associated line numbers  right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. I think most of this PR is necessary for debug information with the fully lowered IR line numbers. It doesn't do anything to propagate line numbers through IR lowerings, which is what would be needed to support line numbers at earlier compiler stages. > Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration. My main reason for that approach was to manage the number of changes required in this PR. We could follow up on this with making line number arguments explicit in manageable chunks. But personally this seems like the ideal use case for implicit arguments. And as `Code` goes away, the number of places with implicit line number arguments should go down significantly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9770#issuecomment-742042975:232,interface,interface,232,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742042975,3,"['inject', 'interface']","['injecting', 'interface']"
Integrability,"> The interface is fully async, so we'll need to build some wrappers if you want a synchronous interface. The async interface will get you concurrency within operations (copy, rmtree), the sync interface only gets you currency within operations. +1 for this. Synchronous wrappers would make replacing existing uses of `hl.hadoop_*` much easier.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10043#issuecomment-778393144:6,interface,interface,6,https://hail.is,https://github.com/hail-is/hail/pull/10043#issuecomment-778393144,6,"['interface', 'wrap']","['interface', 'wrappers']"
Integrability,"> The one thing I'd say is just that when we make a user facing change, we try to make one of the commits look like:; >; > CHANGELOG: Added or_error method to SwitchBuilder; >; > This ends up being helpful when I have to go through and generate the change log for a new version release. Good to know, updated the commit message. It would be nice to document conventions like this in the [docs for software developers](https://hail.is/docs/0.2/getting_started_developing.html#contributing) and/or a [contributing file](https://docs.github.com/en/free-pro-team@latest/github/building-a-strong-community/setting-guidelines-for-repository-contributors).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9749#issuecomment-735921468:320,message,message,320,https://hail.is,https://github.com/hail-is/hail/pull/9749#issuecomment-735921468,1,['message'],['message']
Integrability,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:89,interface,interface,89,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805,2,['interface'],['interface']
Integrability,"> There should be no notebook2 links. I just grepped through the entire codebase, I didn't find anything. We're not asking for notebook2 certs anymore. Notebook2 is dead, long live notebook. Nobody is using it besides us, so I don't see any need for maintaining backward compatibility. In particular, if/when we make this more widely available, there shouldn't be anything2. I just mean we should have a gateway/router redirect, or have 404's issued. Right now some kind of routing happens, resulting in a certificate error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536197945:412,rout,router,412,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536197945,2,['rout'],"['router', 'routing']"
Integrability,"> This change achieves the effect you desire. What I additionally need from this PR is a mental model of why this change works. I think I communicated this above: as far as I understand it, the effective resolution of the graphic is dependent on both the viewport resolution and the pixel ratio. The observed resolution is too low with pixelRatio 1, gives thick lines. . reference: https://github.com/mrdoob/three.js/issues/16747. > You observe that pixelRatio affects the visual behavior, at least in Safari, when the lineWidth is set to 0.25. In particular, when the pixel ratio is set to 2 and the lineWidth is set to 0.25, the lines appear thinner. No, as you see above, regardless of pixel ratio, Safari's minimum line width is somewhere below one. Setting a linewidth below 1 has no drawbacks; it will just cause the browser to use the min. > If the intention is to make the lines less striking, can we apply an alpha filter instead? Is there another simple & consistent-across-platforms solution?. Not that I know of. Some people use cylinder geometry to get around line thickness issues. Yes, there may be an alpha-based solution, which I brought up above. This will be more complicated, and further increase the development cycle. > It seems to me that relying on this interpolation behavior will lead to code that is more difficult to understand and manipulate. I don't understand. Is there another fix that seems easier?; ; > If the intention is to make the lines less striking, can we apply an alpha filter instead? Is there another simple & consistent-across-platforms solution?. Potentially, but this will obviously be a significantly more complicated fix. The proposed fix is effectively 1 line, and takes care of the observed issue in a wide range of browsers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8964#issuecomment-645572011:233,depend,dependent,233,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645572011,1,['depend'],['dependent']
Integrability,> This design is very similar to what you're looking for. It has the wrong input type (table). That's why I'm confused about how it is going to used. It can't be a bridge to lowered stuff because the input and output are both non-lowered. Table collect (and friends) gets you from TableIR to (value)IR and bridges in one direction. We need a node that goes in the other direction and this isn't it. I'm not debating the need for the iterator stuff.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8028#issuecomment-589017978:164,bridg,bridge,164,https://hail.is,https://github.com/hail-is/hail/pull/8028#issuecomment-589017978,2,['bridg'],"['bridge', 'bridges']"
Integrability,"> This seems fine, but it seems like you really want parallelism to match bandwidth. When you're on a gigabit ethernet connection (like me at my desk) you're leaving a lot on the table. No. More concurrency isn't going to increase the bandwidth from a single process if the network stack is full. Increasing the value of parallelism here has two benefits that I can see: (1) amortizing request overhead, which is already very small for 1MB requests. 2-way parallelism would probably be sufficient for that. (2) Increasing the parallelism on the server if the batch or database processes are single-process limited. For the latter, we can increase the parallelism but should correspondingly decrease the message size. My bandwidth numbers were the minimum required for things to work without back-off. There's no relationship between that and the maximum bandwidth this code can support.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7971#issuecomment-578807275:703,message,message,703,https://hail.is,https://github.com/hail-is/hail/pull/7971#issuecomment-578807275,1,['message'],['message']
Integrability,"> ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. Without a concrete example, I'm having some trouble understanding what this problem is. What IR nodes can return a container type that aren't included in the streamify match? Isn't it just ToSet/ToDict/GroupByKey? Calling `boundary` on a ToSet is no problem at all, I think. > ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). Can you write such an IR, and what it should return after streamify?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586582159:634,wrap,wrap,634,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586582159,4,['wrap'],"['wrap', 'wrap-in-ToStream', 'wrapping']"
Integrability,"> We have a 35K cohort. The VCF format of chr1 is 2.4T. Heh. So, yes, ""project"" VCFs grow super-linearly in the number of samples. I (and others) are currently pushing very hard for the VCF spec to support two sparse representations: ""local alleles"" (samtools/hts-specs#434) and ""reference blocks"" (samtools/hts-specs#435). When using these two sparse representations, you should be able to store 35,000 whole genomes in ~10TiB of GZIP-compressed VCF. What is your calling pipeline? Do you generate GVCFs? If yes, I strongly recommend you use the [VDS Combiner](https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html#hail.vds.combiner.VariantDatasetCombiner) to produce a [VDS](https://hail.is/docs/0.2/vds/index.html). You can read more details in [this recent preprint we wrote](https://www.biorxiv.org/content/10.1101/2024.01.09.574205v1.full.pdf), but a VDS of 35,000 whole genomes should be a few terabytes. I'd guess 4 TiB, but it depends on your reference block granularity. I strongly recommend using size 10 GQ buckets. ---. > I don't know the Kryo JAR. I tested on both docker images hailgenetics/hail:0.2.126-py3.11 and hailgenetics/hail:0.2.127-py3.11. Those should use Kryo 4.0.2. OK. My conclusion is that Kryo still has a bug preventing the serialization of very large objects. This becomes a limitation in Hail: we cannot support PLINK files with tens of millions of variants. Our community is largely transitioning to GVCFs and VDS, so I doubt we'll improve our PLINK1 importer to support such large PLINK1 files. That said, PRs are always welcome if loading such large PLINK1 files is a hard requirement for you all.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168#issuecomment-1922048526:957,depend,depends,957,https://hail.is,https://github.com/hail-is/hail/issues/14168#issuecomment-1922048526,1,['depend'],['depends']
Integrability,"> What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace.; > ; > The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which _should_ do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing.; > ; > The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.; ```; [2019-10-30 20:03:15] [36mINFO[39m Ghost boot 5.169s; [2019-10-30 20:03:16] [31mERROR[39m ""GET /pr-7381-default-sx9ail9zkm77/blog/"" [31m503[39m 40ms; [31m; [31mSite is starting up, please wait a moment then retry.[39m. [1m[37mError ID:[39m[22m; [90m4f8b8590-fb50-11e9-ab7c-9dd7e9eff310[39m. [90m----------------------------------------[39m. [90mMaintenanceError: Site is starting up, please wait a moment then retry.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548102772:697,rout,router,697,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102772,1,['rout'],['router']
Integrability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14663?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14663** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14663?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * **#14662** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14662?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14663#issuecomment-2302524299:1266,depend,dependencies,1266,https://hail.is,https://github.com/hail-is/hail/pull/14663#issuecomment-2302524299,1,['depend'],['dependencies']
Integrability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14686?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" targ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14686#issuecomment-2354274670:614,depend,dependent,614,https://hail.is,https://github.com/hail-is/hail/pull/14686#issuecomment-2354274670,1,['depend'],['dependent']
Integrability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14690?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" targ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14690#issuecomment-2356990835:614,depend,dependent,614,https://hail.is,https://github.com/hail-is/hail/pull/14690#issuecomment-2356990835,1,['depend'],['dependent']
Integrability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14691?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" targ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14691#issuecomment-2357221524:614,depend,dependent,614,https://hail.is,https://github.com/hail-is/hail/pull/14691#issuecomment-2357221524,1,['depend'],['dependent']
Integrability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" targ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14692#issuecomment-2358958754:614,depend,dependent,614,https://hail.is,https://github.com/hail-is/hail/pull/14692#issuecomment-2358958754,1,['depend'],['dependent']
Integrability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" ta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14693#issuecomment-2362149504:614,depend,dependent,614,https://hail.is,https://github.com/hail-is/hail/pull/14693#issuecomment-2362149504,1,['depend'],['dependent']
Integrability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" ta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14696#issuecomment-2362438406:614,depend,dependent,614,https://hail.is,https://github.com/hail-is/hail/pull/14696#issuecomment-2362438406,1,['depend'],['dependent']
Integrability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" ta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14698#issuecomment-2364681873:614,depend,dependent,614,https://hail.is,https://github.com/hail-is/hail/pull/14698#issuecomment-2364681873,1,['depend'],['dependent']
Integrability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>) ; * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" ta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417750658:614,depend,dependent,614,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417750658,1,['depend'],['dependent']
Integrability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14732** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 1 other dependent PR ([#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://sta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14732#issuecomment-2419668108:858,depend,dependent,858,https://hail.is,https://github.com/hail-is/hail/pull/14732#issuecomment-2419668108,1,['depend'],['dependent']
Integrability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14748** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 1 other dependent PR ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://sta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14748#issuecomment-2444958196:858,depend,dependent,858,https://hail.is,https://github.com/hail-is/hail/pull/14748#issuecomment-2444958196,1,['depend'],['dependent']
Integrability,"> ah---that's just a log message from the readinessProbe sending requests before the server is fully up, I believe. So partly it looks like that, especially with Ghost timing itself as >5s to actually listen on the port after the container starts up (or really after Ghost starts running which may be different). The strange this is the timing. I thought the timestamps were supposed to correspond to the time the request was received/log message was generated (the log messages themselves can be printed out of order, but I think the timestamps should be accurate).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548103795:25,message,message,25,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103795,3,['message'],"['message', 'messages']"
Integrability,"> all options that are also gcloud options (such as --project)? [That] could be difficult. Yes, this is what I was thinking. `hailctl` could parse (as much as is needed) the `gcloud` options to find options (like `--project`) and modify others (like `--initialization-actions`). The latter is somewhat surprising since one expects everything after the `--` to pass through unchanged. OK, summarizing our options so far:. - hailctl has no options that are also gcloud options. gcloud options go after the `--`, and get modified as needed by hailctl (with a message).; - hailctl has no gcloud options that are simply pass through. gcloud options that are needed by hailctl commands are hailctl options (like `--project`). When a gcloud option is needed by some hailctl command, all hailctl commands take that option (when it makes sense), even if in some cases that makes them simply pass through. This fixes the inconsistency issues, but the user still needs to keep track of which gcloud options needs to be passed to hailctl and which are passed to gcloud directly. If you specify an option twice, once to hailctl and once to gcloud, we invoke gcloud with the option duplicated. Pros and cons:; - The first option has the most consistent interface.; - The first option modifies options after the --, which is surprising.; - The first option involves replication (some of) the gcloud option parsing semantics, which is annoying.; - The second option requires the user to know which gcloud options need to be passed to hailctl instead (but globally, not per-command).; - With the second option, if we want to warn (or error) on duplicate options, we're back to duplicating the gcloud option parsing logic. I think I'm coming around to the second option. > so that hailctl dataproc submit cluster -- --script-options would work. I see, so if there is only one `--` it refers to script options, and if there are two, the first one corresponds to gcloud options? I think that should be doable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-758128554:556,message,message,556,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758128554,2,"['interface', 'message']","['interface', 'message']"
Integrability,"> but not the thunk of thunking. You mean the dependent aggregators? The issue is the copy code needs access to the type that was matched in the TVariable. The TVariable binding is cleaned on every function match, which is long before the code is executed on a worker. Therefore, we need to run some code to capture the TVariable binding just after the match happens. That's what the extra level of thunking is about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2360#issuecomment-340047873:46,depend,dependent,46,https://hail.is,https://github.com/hail-is/hail/pull/2360#issuecomment-340047873,1,['depend'],['dependent']
Integrability,"> danking pushed 1 commit to branch master. Commits by cseed (1).; > duh (#7899) (6b7f8f6). using the intended PR title as the commit message requires no more typing and saves us from ""duh""!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7899#issuecomment-575399003:134,message,message,134,https://hail.is,https://github.com/hail-is/hail/pull/7899#issuecomment-575399003,1,['message'],['message']
Integrability,> gidgethub. Will remove dependencies in separate PR so I don't want to wait for all the images to rebuild. Can't quite get rid of Flask yet as it is used in callback tests. Could you help me understand what the benefit of gidgethub is over our previous strategy? Since we generally prefer minimal deps,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-531938132:25,depend,dependencies,25,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-531938132,1,['depend'],['dependencies']
Integrability,"> http://blog.wang/wang/blog/, for instance, sends a request to the `blog` service in the `wang` namespace inside our k8s cluster, which is all handled by kubernetes. So the wait basically sends a GET request on the `/wang/blog/` endpoint directly to the `blog` service in that namespace, which never passes through either gateway or router. Yeah, I got it, thanks. I think we may need to set the X-Forwarded-Proto in the wait_for.py http command (session.get). https://docs.aiohttp.org/en/stable/client_advanced.html. And provide a `headers: `field within build.yml's wait property.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548107052:334,rout,router,334,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548107052,1,['rout'],['router']
Integrability,"> if there's a problem with the expression, I don't want to get a crash from a requirementError from the Variant constructor without any context. You need to do validation in the expr code. User could isn't allowed to fail with a requirement error. I'd solve the error message problem by carrying it along with the annotation. Line can be generalized to carry line information about any type. > My CNV work involves parallelizing file parsing, and this interface wouldn't be compatible with that use case. I don't understand, can you elaborate on this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233005833:269,message,message,269,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233005833,2,"['interface', 'message']","['interface', 'message']"
Integrability,"> nginx should need the services (i.e. domain names) to exist, not the deployments. Ah, that makes sense. I suppose that's why the service definition of `router` used to exist along with gateway's k8s config.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10736#issuecomment-891312778:154,rout,router,154,https://hail.is,https://github.com/hail-is/hail/pull/10736#issuecomment-891312778,1,['rout'],['router']
Integrability,"> old world a MethodBuilder had a FunctionBuilder as a field, and now instead a FunctionBuilder has a ClassBuilder and also a way to make a MethodBuilder. Uh, so there are three pictures, the old way, this PR, and where I'm headed (the PR comment); - the old way, method builder had a function builder, because function builder was really also the class builder (for an AsmFunctionN); - now I added class builder and function builder (which was really building a class) has one; - in the final picture, function builder will be a simpler wrapper to class and method builders. Method builders will point back to their class.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8179#issuecomment-592278110:538,wrap,wrapper,538,https://hail.is,https://github.com/hail-is/hail/pull/8179#issuecomment-592278110,1,['wrap'],['wrapper']
Integrability,"> print out a message and don't label but still create. That's what I meant, yeah. But I'll do the munging.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6276#issuecomment-499694445:14,message,message,14,https://hail.is,https://github.com/hail-is/hail/pull/6276#issuecomment-499694445,1,['message'],['message']
Integrability,"> printing an error. Error sounds like you're going to exit and not do anything. But you mean print out a message and don't label but still create the cluster? That seems fine, although munging seems more informational, that is, it's an error to us/Sam, not the user, esp. since they can't do anything about it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6276#issuecomment-499694220:106,message,message,106,https://hail.is,https://github.com/hail-is/hail/pull/6276#issuecomment-499694220,1,['message'],['message']
Integrability,"> runtime in the container status is a little confusing. It's fine for now, but for user interface, runtime should be a top level state, and starting and running should be substates. But again, this is fine for now. I'm a little confused by this. runtime isn't a state, it is rough the runtime of the container. Note, isn't the full runtime: it doesn't include image pull or log upload, for example. I included it in timing because it was something I timed. I included it because (1) it is comparable to `docker run` for comparison, and (2) I think it is what we should change the user for the runtime of the container ... I think? pull is also non trivial, as well as the storage of the image while it is running. Billing is going to be a challenge.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7354#issuecomment-545167038:89,interface,interface,89,https://hail.is,https://github.com/hail-is/hail/pull/7354#issuecomment-545167038,1,['interface'],['interface']
Integrability,"> should the `haas/packages` folder be checked in?. I don't quite understand. haas is a monorepo containing all the bits that will make up the web/mobile/desktop/auth api and any other services that allow users to interact with Hail without installing it themselves. . https://danluu.com/monorepo/. Everything in packages is a separate package (web, api1), separately publishable to npm, but which allows dependencies to be shared. Does that help?. Edit: wrong link",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-446604474:405,depend,dependencies,405,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-446604474,1,['depend'],['dependencies']
Integrability,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233041581:92,message,messages,92,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581,5,"['interface', 'message', 'wrap']","['interface', 'messages', 'wrapper']"
Integrability,">> should be explicitly representable in the IR; >; > How would that work with MakeArray?. I have some thoughts on that, but I think it would require a bigger change to how we represent streams in the IR. One idea is to put stream producers and stream consumers on equal footing. Right now, a stream consumer is a node with a stream child that produces a non-stream value, e.g. `ArrayFor(stream, name, body)`. We could let stream consumers be values themselves, `ArrayFor(name, body)`, with other nodes for connecting producers with consumers, e.g. `RunStream(stream, ArrayFor(name, body))`. Then I think we would need two `MakeStream`s, one that can be unrolled, and one that can be zipped. The former just directly takes a consumer and produces a value (and can choose to duplicate the consumer code or wrap it in a method). The later constructs a stream producer, which can be freely used in zips, etc. It would definitely require some thought how to optimize such a representation (is there a normal form?). > I'm not sure I see that. If you don't duplicate the consumer, the push code is the same. The question is, do you use a variable + switch to track where you are in the MakeStream, or do you use the program counter via labels, where you get the latter from the former by code duplication. I think the easiest way to see it is to think about how you would zip two `MakeStream`s, both of which represent their state using the program counter. You could do it by statically fusing them into a single `MakeStream`, but what if it was something more complicated like `Zip(Map(MakeStream(...), ...), Filter(MakeStream(...), ...))`? You can't represent a compound state with a single program counter. At a higher level, you can only do the optimization of unrolling the `MakeStream` loop if that is the outermost loop in the generated code. A producer wanting to create the outermost loop is what I mean by a push stream. By contrast, to be able to zip arbitrary streams, you want the consumer to",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8148#issuecomment-591962972:805,wrap,wrap,805,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-591962972,1,['wrap'],['wrap']
Integrability,">Can you explain why std::string is special here vs the rest of the standard library? What assumption >are you working under?. Yes. The C++11 final standard introduced some new constraints on the definition of std::string; and std::list (essentially outlawing a copy-on-write implementation of std::string, and requiring; that std::list::size() must be O(1)). Consequently libstdc++ had to be redesigned with incompatible implementations of ; std::string and std::list to become C++11-compliant. Systems using the earlier libstdc++; (which also typically have g++-4.8.x/4.9x) have the old-ABI not-fully-compliant std::string; (and std::list, but I've gone 20 years without ever using that). Later versions of libstdc++ have *both* flavors of std::string, but use namespaces to allow them; to coexist (but not to be interchangeable, so interfaces between old-ABI and new-ABI are; problematic). https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html. ""In the GCC 5.1 release libstdc++ introduced a new library ABI that includes new implementations of std::string and std::list. These changes were necessary to conform to the 2011 C++ standard which forbids Copy-On-Write strings and requires lists to keep track of their size. In order to maintain backwards compatibility for existing code linked to libstdc++ the library's soname has not changed and the old implementations are still supported in parallel with the new ones. This is achieved by defining the new implementations in an inline namespace so they have different names for linkage purposes, e.g. the new version of std::list<int> is actually defined as std::__cxx11::list<int>. Because the symbols for the new implementations have different names the definitions for both versions can be present in the same library."". OSX doesn't have this ABI-compatibility issue because for several years it has been using; libc++ as the default library, and libc++ is a post-C++11 rewrite-from-scratch implementation; of the required standar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424797612:835,interface,interfaces,835,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424797612,1,['interface'],['interfaces']
Integrability,">We only have to support two platforms: recent Linux and OSX. OSX is not a problem, all recent versions are based on libc++ (rather than libstdc++) which has; had a more stable ABI. The problem is precisely that ""recent Linux"" encompasses both; systems based on g++-4.8.x/4.9.x with only old-ABI std::string's (e.g. debian8), and systems; based on g++-5.x and later with new-ABI std::string by default (e.g. debian9). That; incompatibility is the problem. >Not having access to the standard library seems problematic. You absolutely have access to the full C++11 standard library in dynamically-generated code -; you're compiling with the master node's default compiler, and header files, and using; the default libstdc++.o (libc++.dylib), and it's all fine. And you'll be using whichever flavor; of std::string is the default for that system, which will presumably also be interoperable with any; third-party library packages on that system. The issues we're getting round are:. a) If libhail.so is prebuilt on a new-ABI system *and* uses std::string, then it can't run against; the libstdc++ on an old-ABI system. b) If libhail.so is prebuilt on an old-ABI system, then it can run against either old-ABI or new-ABI; libstdc++, but if it then gets linked against third-party libraries compiled against new-ABI; headers, you'll have two different flavors of std::string floating around in the same program,; which causes confusion at any interfaceswhich pass std::string around. Now if you want to go further in shipping more of the system, then the question of whether to; ship your own libstdc++ (or libc++) is independent of the choice of compiler version. *If* you ; ship your own libstdc++, then you potentially introduce the problems of interoperability with; other libraries on the system). And there's a slight question about whether your libstdc++ will; work against the other systems libc.so, though I think the ABI at that level has been stable enough for long enough that it probably doesn",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941:874,interoperab,interoperable,874,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941,1,['interoperab'],['interoperable']
Integrability,"@Sun-shan According to the error message you posted, Spark itself cannot find `/hail/test/BRCA1.raw_indel.vcf`:; ```; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.; : org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/hail/test/BRCA1.raw_indel.vcf; ```. Looking at that error message, it looks like Spark is interpreting your path as a local file system path, _not_ a hadoop path. Moreover, earlier in your posted output this line:; ```; 17/08/15 08:58:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; ```; suggests that you're not actually connecting to a Spark cluster with a properly configured Hadoop installation. ---. Your Spark cluster appears improperly configured. I'm not sure if `pyspark` is even connecting to your cluster. You might try looking at [this StackOverflow post](https://stackoverflow.com/questions/34642292/cant-connect-pyspark-to-master) about connecting `pyspark` to a Spark cluster. I strongly recommend running `pyspark` again and executing:; ```; spark.sparkContext.master; ```; This should print the URL of your Spark master node. If this prints a String starting with `local`, then you're definitely not connecting to a Spark cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-322539635:33,message,message,33,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-322539635,3,"['message', 'protocol']","['message', 'protocol']"
Integrability,@akotlar re: `package-lock.json` is there an explanation somewhere of why `package-lock.json` recursively includes the description of every dependency? More specifically why isn't the contents of `devDependencies` plus the resolved address and the SHA256 sufficient?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-456601345:140,depend,dependency,140,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-456601345,1,['depend'],['dependency']
Integrability,"@bw2 that package name is a lie, sadly. The [maven repository page](https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-20_2.11/5.5.1) for `org.elasticsearch:elasticsearch-spark-20_2.11:5.5.1` lists `org.apache.spark:spark-core_2.11:2.1.0` as a dependency, which is decidedly not 2.0. We'll have to use elasticsearch-spark 5.1.2. It's a bit annoying. You'll have to extend the [spark version-specific logic](https://github.com/hail-is/hail/pull/2049/files#diff-c197962302397baf3a4cc36463dce5eaR44) in `build.gradle`. You'll want to bind a new name, something like `elasticsearchSparkVersion`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2049#issuecomment-320335957:266,depend,dependency,266,https://hail.is,https://github.com/hail-is/hail/pull/2049#issuecomment-320335957,1,['depend'],['dependency']
Integrability,"@catoverdrive FYI, I had to switch two newLocal to newField in StagedRegionValueBuilder. This is because if you try to allocate in a new method builder (inside wrapToMethod, say), the local will go into the method builder the SRVB was originally constructed with which is the wrong one. Probably SRVB should not have a method builder but a class builder and allocate should take the MethodBuilder we're emitting the allocation into.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3669#issuecomment-392413962:160,wrap,wrapToMethod,160,https://hail.is,https://github.com/hail-is/hail/pull/3669#issuecomment-392413962,1,['wrap'],['wrapToMethod']
Integrability,"@catoverdrive I dismissed your review because I added more changes to address your comment on the FIXME. I think addRegionValue now does a minimal amount of work. In particular, if you write add a region value at the top level to the same region (rvb.start(t); rvb.addRegionValue(rv); rvb.end), it doesn't modify the region but simply sets start = rv.offset. This means that rvb.start can't actually do anything, and some other add routines need to check if they need to allocate. This adds some overhead that should get compiled away in the staged version. I also improved the tests to test adding to the same as well as a different region.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2299#issuecomment-336900902:432,rout,routines,432,https://hail.is,https://github.com/hail-is/hail/pull/2299#issuecomment-336900902,1,['rout'],['routines']
Integrability,"@catoverdrive and I discussed 0.2 interfaces and agreed that the single_key logic will be stripped from group_rows_by and group_cols_by, and we'll add explode_rows and explode_cols. This will happen in a future PR",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2497#issuecomment-348280201:34,interface,interfaces,34,https://hail.is,https://github.com/hail-is/hail/pull/2497#issuecomment-348280201,1,['interface'],['interfaces']
Integrability,"@catoverdrive here's the replacement for #2441, I had to create a couple shim classes to get JSON parsing to work  because you can't `jsonValue.extract[T]` an interface T (see `JSONAnnotationImpex`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2451#issuecomment-345801639:160,interface,interface,160,https://hail.is,https://github.com/hail-is/hail/pull/2451#issuecomment-345801639,1,['interface'],['interface']
Integrability,"@chrisvittal I took the plunge and also created a `MatrixHybridReader`. Changes (including above):. - FunctionBuilder now accepts `Code[Unit]` to be added to the `init` method of the function object; - SRVB now has an `init` method that should be called in the `init` method of a function object if many methods will share the SRVB; - `CodeChar` now exists; - `TextMatrixReader` exists which mimics `TextTableReader`; these should get unified at some point; - minor documentation fixes to `import_matrix_table`; - better error messages wrt using the name `row_id`, which is reserved for use by `import_matrix_table` when there are no keys specified; - several new tests, including:; - extensive testing of the product space of `header`, `delimiter`, `header`, and `entry_type` (including such weird things as using `9` as the missing value); - a pathological file: `9`-separated values with `8` representing the missing value; - several tests that trigger the pruner; - rename `LoadMatrix.scala` to `TextMatrixReader.scala`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6987#issuecomment-529001365:527,message,messages,527,https://hail.is,https://github.com/hail-is/hail/pull/6987#issuecomment-529001365,1,['message'],['messages']
Integrability,"@chrisvittal Just to draw your attention: I removed my wrapper for FsDataInputStream (HailInputStream).... it is used prominently in BGzipInputStream, which relies on Hadoop compression libraries. I found it more complicated to avoid FsDataInputStream, with I think little obvious benefit to doing so: the [FSDataInputStream constructor just takes an InputStream](https://hadoop.apache.org/docs/r3.0.3/api/org/apache/hadoop/fs/FSDataInputStream.html), which is Java stdlib, which I assume will be returned by most future FS implementations. Happy to explore a different solution if you think there's a better abstraction here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-492742054:55,wrap,wrapper,55,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492742054,1,['wrap'],['wrapper']
Integrability,"@cseed ; >Second, I feel like for the tiebreaker we should just have a node weight (Float64) and just take the node with the larger weight to break ties. @danking Would that handle the cases you're aware of? (e.g. LD pruning but keeping the variants with the largest p-values.). This is how I originally wanted to implement this. I tried to pitch this to a couple analysts (I remember talking to Raymond in particular) and they were not having it. The tools they were using (or building themselves) did a somewhat ad-hoc tie breaking (I'm not sure their tie-breaking always induced a total ordering on nodes). I don't think providing a weight is sufficient for their purposes (or at least, it's not how they want to write it). I do think having weights would be cool too, but I think we have to preserve the current interface. This obviously creates a bit of a snag because currently we collect to `Annotation` and the IR compiler only operates on `RegionValue`. To eliminate AST from this method, we need to encode, collect, and decode on master such that `T`s in the BinaryHeap are `RegionValue`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3425#issuecomment-384077392:816,interface,interface,816,https://hail.is,https://github.com/hail-is/hail/pull/3425#issuecomment-384077392,1,['interface'],['interface']
Integrability,"@cseed @patrick-schultz @jbloom22 This now depends only on #5463, so I've gone ahead and assigned it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5469#issuecomment-469877688:43,depend,depends,43,https://hail.is,https://github.com/hail-is/hail/pull/5469#issuecomment-469877688,1,['depend'],['depends']
Integrability,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:178,rout,routing,178,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996,2,['rout'],['routing']
Integrability,@cseed I added inter-project dependencies to `projects.yaml` does that address the docker-batch dependence?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5655#issuecomment-475301184:29,depend,dependencies,29,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475301184,2,['depend'],"['dependence', 'dependencies']"
Integrability,@cseed I also switched this PR to use the existing interfaces.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2073#issuecomment-322529797:51,interface,interfaces,51,https://hail.is,https://github.com/hail-is/hail/pull/2073#issuecomment-322529797,1,['interface'],['interfaces']
Integrability,"@cseed I don't really like how this looks with lists as the inputs to the commands. To me, it's much harder to read and modify. I like the commands looking as much like writing a shell script as possible. Maybe others feel differently though... Also, you can't do something like this ` ' '.join([task.ofile for task in shapeit_tasks])` because you'll lose information about the dependencies before `command` sees the original resource inputs. What I really want is a version of f-string interpolation where I parse and detect the variables (known and unknown), handle them properly by either creating new resources or adding dependencies to the Task, and then execute the Python formatting code inside the curly braces. I'm not sure if it is possible to do this. If it is, it's probably complicated and we'll have to use the Python `ast` and `parser` modules and call `eval` ourselves. ; ```python; from pyapi import Pipeline; p = Pipeline(). bfile_root = 'gs://jigold/input'; bed = bfile_root + '.bed'; bim = bfile_root + '.bim'; fam = bfile_root + '.fam'. p.write_input(bed=bed, bim=bim, fam=fam). subset = p.new_task(); subset = (subset; .label('subset'); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--make-bed', '--out', subset.tmp1]); .command(['awk', ""'{ print $1, $2}'"", subset.tmp1 + '.fam', ""| sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }'"",; '>', subset.tmp2]); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--remove', subset.tmp2,; '--make-bed', '--out', subset.tmp2])). shapeit_tasks = []; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .command(['shapeit', '--bed-file', subset.ofile, '--chr ', contig, '--out', shapeit.ofile])); shapeit_tasks.append(shapeit). merger = p.new_task(); merger = (merger; .label('merge'); .command(['cat', ' '.join([task.ofile for task in shapeit_tasks]), '>>', merger.ofile)). p.write_output(merger.ofile + "".haps"", ""gs://jigold/final_output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-447469039:378,depend,dependencies,378,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-447469039,2,['depend'],['dependencies']
Integrability,@cseed I switched to using the existing interfaces. Can this be approved now?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2071#issuecomment-322527815:40,interface,interfaces,40,https://hail.is,https://github.com/hail-is/hail/pull/2071#issuecomment-322527815,1,['interface'],['interfaces']
Integrability,"@cseed I think my point is that I *want* an interface that assumes the structure of the loop. I'm happy to implement other interfaces as well--the one that I've currently got in this PR is basically your second suggestion--but I think it would also be nice to have a while loop construct that looks syntactically more similar to what you'd expect a while loop to look like in python, if that makes sense?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614#issuecomment-558330236:44,interface,interface,44,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558330236,2,['interface'],"['interface', 'interfaces']"
Integrability,"@cseed I went ahead and made a PR with the design I think is best. Basically, it's the same thing I proposed before `group_rows_by().aggregate_rows().aggregate_entries().result()`, but I kept `aggregate()` as an alias for `aggregate_entries.result`. I think this design is ideal so the common use case is shorter and we maintain backwards compatibility. . If you are ok with the interface, I'll go ahead and assign someone my PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4022#issuecomment-413280515:379,interface,interface,379,https://hail.is,https://github.com/hail-is/hail/issues/4022#issuecomment-413280515,1,['interface'],['interface']
Integrability,@cseed I'll close this and just ask for feedback on the branch diff once I have a pipeline working end-to-end and thoughts on proper integration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2890#issuecomment-366455460:133,integrat,integration,133,https://hail.is,https://github.com/hail-is/hail/pull/2890#issuecomment-366455460,1,['integrat'],['integration']
Integrability,"@cseed I'm really happy with the interface now! Could you please look over this again and let me know if there are any suggestions you have before I write some tests and give this to someone to code review. I also called this `pyapi` for lack of a better name and it's currently in the batch module... ```python3; from pyapi import Pipeline, resource_group_builder. p = Pipeline() # initialize a pipeline. # Define resource group builders (used with `declare_resource_group`); rgb_bfile = resource_group_builder(bed=""{root}.bed"",; bim=""{root}.bim"",; fam=""{root}.fam""). rgb_shapeit = resource_group_builder(haps=""{root}.haps"",; log=""{root}.log""). # Import a file as a resource; file = p.write_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.write_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .declare_resource_group(tmp1=rgb_bfile, ofile=rgb_bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile=rgb_shapeit); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741:33,interface,interface,33,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741,1,['interface'],['interface']
Integrability,"@cseed I've fixed string slicing to do the thing you describe (converting to utf16, slicing, converting back). The Java default is to replace invalid codepoints (e.g. if only one codepoint from a two-codepoint character is kept) with the replacement character \ufffd, but when converted back to UTF8 the replacement character is just `?`. I've written a test to reflect this, but this seems pretty encoding dependent and maybe like something we should just keep in mind and pick a consistent solution for.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4844#issuecomment-473490897:407,depend,dependent,407,https://hail.is,https://github.com/hail-is/hail/issues/4844#issuecomment-473490897,1,['depend'],['dependent']
Integrability,@cseed Should be done now. `assembly:single` will build both jars into `target/hail-0.2-sha-jar-with-dependencies.jar` and `target/hail-0.2-sha-test-jar-with-dependencies.jar`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5906#issuecomment-484685725:101,depend,dependencies,101,https://hail.is,https://github.com/hail-is/hail/pull/5906#issuecomment-484685725,2,['depend'],['dependencies']
Integrability,"@cseed Sorry if this doesn't make sense -- we can discuss in person. Here's my attempt to fix the problems outlined above. The tradeoff made is that we have to refer to the object (ex: `subset`). So instead of thinking of writing the command as a template where `inputs` specifies how to substitute into the template, writing commands in this interface is the same as using Python to generate the correct string to output. I'm not sure that I like this better. One of the advantages of writing commands as templates is they are reusable. In the latter case, the strings are reusable if they are written as `.format()` templates instead of `f-strings`. So maybe it's approximately the same, but there's an extra step to define the inputs to `format`. I tried hacking the Python AST to not have to refer to the object, but I think it's going to be difficult to get the AST parsing exactly right and not have too many implicit rules within our language. I also considered writing a DSL, but found that it's hard to specify the part with the `shapeit_output` in a DSL. ```python3; from pyapi import Pipeline, resource_group; p = Pipeline(). input_bfile = p.new_resource_group(bed=""gs://jigold/input_root.bed"",; bim=""gs://jigold/input_root.bim"",; fam=""gs://jigold/input_root.fam""). def bfile(root):; return resource_group(root, lambda x: {""bed"": x + "".bed"", ""bim"": x + "".bim"", ""fam"": x + "".fam""}). subset = (p.new_task(); .label('subset')); subset = subset; .command(f'plink --bfile {input_bfile} --make-bed {bfile(subset.tmp1)}'); .command(""awk '{print $1, $2}'"" +; subset.tmp1.fam +; "" | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > "" +; subset.tmp2); .command(f""plink --bfile {input_bfile} --remove {subset.tmp2} --make-bed {bfile(subset.ofile)}"")). def shapeit_output(root):; return resource_group(root, lambda x: {""haps"": x + "".haps"", ""log"": x + "".log""}). for contig in [str(x) for x in range(1, 4)]:; shapeit = (p.new_task(); 		.label('shapeit')); shapeit = (shapeit; 		.command(f'shapeit -",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-451295662:343,interface,interface,343,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-451295662,1,['interface'],['interface']
Integrability,"@cseed This should now be sufficient for Wed. I will probably add 2 features before then: reconnecting web sockets (though connection should infrequently drop, shouldn't affect workshop), and an admin panel to blow out users / delete notebooks. And maybe SSL. Edit: my commit https://github.com/hail-is/hail/pull/5162/commits/bac155c9713d99c68cd7d0605eb59585656c14ea makes reference to csrf. This may not be necessary: inspecting the login request I notice a nonce, generated by the auth0-js lib, sent with login and silent token rotation / refresh, which is nice. Assuming it's used to prevent replay attacks, in this case it has the same purpose as a CSRF token. Yay auth0. Edit2: Regarding performance. Scorecard page, before server-side caching takes 100ms on refresh, and 50ms on navigation from another page. This effectively means no overhead from my web implementation. Why? It takes ~50ms to return *anything* (including favicon.ico of 0 bytes), i.e 50ms is the time it takes from my computer to kubernetes and back, with no additional work done. We have 2 such requests currently when visiting app.hail.is/scorecard: one to to the web app server, one to scorecard/json. After caching (which is invalidated every 3 minutes), takes 50ms. So, if current scorecard.hail.is needed to hit a json endpoint to get data for its template, we would expect it to be no faster. Alternatively if we placed the json-generating function in the web-app's nodejs server it's response time would drop by ~50ms.; * I also am trying to use the internal DNS (SSR phase routes to http://scorecard/json, so should use kubernetes DNS; still take ~50ms to get the json... before caching).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-460071524:1557,rout,routes,1557,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-460071524,1,['rout'],['routes']
Integrability,@cseed You can take this for a spin at https://internal.hail.is/dking/website/ . It's a bit sluggish. the end-to-end timing at gateway for `api.html` (the API reference) hovers around 125ms. About 35ms is website / jinja2 rendering. Router adds anywhere from 5ms to 20ms. It seems like hitting the router-resolver and whatever else `gateway` does makes up the rest. [Service logs here](https://console.cloud.google.com/logs/query;query=labels.%22k8s-pod%2Fapp%22%3D%22gateway%22%20OR%20labels.%22k8s-pod%2Fapp%22%3D%22router%22%20OR%20labels.%22k8s-pod%2Fapp%22%3D%22website%22%0AjsonPayload.message:%22%2Fdking%2Fwebsite%2Fdocs%2F0.2%2Fapi.html%22;summaryFields=labels%252F%2522k8s-pod%252Fapp%2522:false:32:beginning?project=hail-vdc&query=%0A). I think in production it will be fine because there's a fair bit less indirection.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10056#issuecomment-780248628:233,Rout,Router,233,https://hail.is,https://github.com/hail-is/hail/pull/10056#issuecomment-780248628,3,"['Rout', 'message', 'rout']","['Router', 'message', 'router-resolver']"
Integrability,"@cseed looking at the code it appears this aggregator doesn't support primitive types. This is intentional, right? Should we just add a little wrapper to Python to wrap the thing in a tuple or something?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5345#issuecomment-466819238:143,wrap,wrapper,143,https://hail.is,https://github.com/hail-is/hail/issues/5345#issuecomment-466819238,2,['wrap'],"['wrap', 'wrapper']"
Integrability,"@cseed ripping out the references to Report (DuplicateReport) objects screws up the Python interface because it supports the method deduplicate() in VariantSampleMatrix, which initializes a Report object",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2024#issuecomment-318098685:91,interface,interface,91,https://hail.is,https://github.com/hail-is/hail/pull/2024#issuecomment-318098685,1,['interface'],['interface']
Integrability,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588:74,interface,interfaces,74,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588,2,"['interface', 'message']","['interfaces', 'message']"
Integrability,"@cseed, digesting from this and our in-person conversation:. I added a steps about job dependencies and batch closure. Regarding ""a way to refer to individual inputs/outputs"", in the original post, I gave the example of:. ```; - type: exec; name: build-jar; image: hail-pr-builder; namespace: ns; command: [""./gradlew"", ""test"", ""shadowJar""]; outputs:; - ""build/libs/hail-all-spark.jar""; - type: exec; name: pytests; image: hail-pr-builder; dependsOn:; - build-jar; command: [""./run-python-tests-using-input-jar.sh""]; ```. The outputs of a job are stated explicitly by the definition. For the `pytests` job in the example above, the input from `build-jar` is placed at `/inputs/build-jar`. Regarding ""specify a series of stacked containers to execute"", I don't see a straightforward way to implement this. It's tricky enough to have a ""anti-init""/""finalizer"" container. Inter-job I/O will be handled by batch. The user controls the image and the command and the environment variables of the build step, so they can arrange for permission to copy results to a bucket they own. Are we worried about the setting of user's wanting to run untrustworthy software? They already run arbitrary software on cloud instances that have plenty of latent credentials. I think we can at least punt on this until other functionality is in. Local disk sounds like a nice thing to add eventually. Agreed, that sounds like a nice model. I'll consider it as I envision a persistent batch system.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-459537502:87,depend,dependencies,87,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-459537502,2,['depend'],"['dependencies', 'dependsOn']"
Integrability,@daniel-goldstein I added an exit 1 after argument validation and removed the test-dataproc and wheel dependencies in the Makefile to demonstrate the functionality in these examples:. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \ ; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE= \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=o,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:102,depend,dependencies,102,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['depend'],['dependencies']
Integrability,"@daniel-goldstein The logs show this:. ```; insertId: ""88db8ey9nom69366""; jsonPayload: {; asctime: ""2022-02-16 23:07:40,794""; exc_info: ""Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 65, in notify_batch_job_complete; await request(session); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 56, in request; await session.post(callback, json=batch_record_to_dict(record)); File ""/usr/local/lib/python3.7/dist-packages/batch/batch.py"", line 18, in batch_record_to_dict; elif record['n_failed'] > 0:; KeyError: 'n_failed'""; filename: ""job.py""; funcNameAndLine: ""notify_batch_job_complete:69""; hail_log: 1; levelname: ""ERROR""; message: ""callback for batch 1731 failed, will not retry.""; ```. `notify_batch_job_complete` just needs to join against the volatile table.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1049107348:707,message,message,707,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1049107348,1,['message'],['message']
Integrability,"@daniel-goldstein, since the `release` step decides whether or not to publish a new release, I decided not to add `is_release` to CI. Instead I've introduced a new file called `release-hail-flag`, writted to by the release step, and used by its dependents to decide if subesquent release actions should be run. What do you think? Is this reasonable?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14398#issuecomment-2026073794:245,depend,dependents,245,https://hail.is,https://github.com/hail-is/hail/pull/14398#issuecomment-2026073794,1,['depend'],['dependents']
Integrability,"@danking , I'm a bit stuck on how to proceed with the credential refreshing. Here's the layout of the problem:. 1. In normal Azure, we accept user-provided SAS tokens. Since they are user-provided, we have no way of obtaining new ones and the onus is on the user to obtain a SAS token for however long they expect to need to use it.; 2. This current design in Terra is to not make the user have to do that, because that seems annoying, and for terra-controlled ABS containers we have an endpoint we can hit to get a SAS token. Ok, but now we need to update our Azure FS infrastructure to refresh a credential if it expires. But, we use the azure client lib and don't control all http requests. For example, for `AzureStorageFS.open`, we call `downloader.readall()` if we want to load the whole file into memory. I went spelunking through their source and looks like `readall` mostly wraps a sequence of range reads, but regardless if we were to use that method we would have to catch credential expiration errors, reset credentials on the blob client and retry hoping that we didn't break any invariants -- I don't want to do that as I wouldn't trust a stream that encountered a non-transient error like that. It could be that getting rid of `downloader.readall` is the only thing we have to worry about, but it makes me uneasy not having control of the http requests we're making to ABS. Do you see a solution other than raking through our `aioazure.fs` and making sure that we only use ""quick"" methods and possibly retrying 401s? It just seems to me like we're going against the grain and even though it feels user-hostile the intention of SAS tokens are to have users own credential expiration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13944#issuecomment-1930482715:883,wrap,wraps,883,https://hail.is,https://github.com/hail-is/hail/pull/13944#issuecomment-1930482715,1,['wrap'],['wraps']
Integrability,"@danking . This should be ready to look at. Stacks on #5452. Once that PR is merged, changes are:. 1) notebook.html: organize into form (notebook-form.html) and notebook state (notebook-state.html) components.; 2) add modified versions of notebook-api. Namely I don't use the marshaling procedure you didn't like, and refactor as much of the JS stuff as I can into synchronous http requests.; * modifies /notebook routes , adds `marshall_notebook`, `get_live_user_notebooks`, `wait_websocket`, and replace any calls to `session['pod_name']` and `session['svc_name`] with equivalent versions based on `session['notebook']`, which contains the notebook object of the existing session. Changes mainly contained within commit: https://github.com/hail-is/hail/pull/5476/commits/2f180ed0bfb3b0dfb7224df1ef6afba0e1a9cbfc (the following pr only renames notebook-obj.html to notebook-state.html). Basically feels like a synchronous / refresh-based version of what we had on app.hail.is, with less state insight (uses only the websocket-based reachability check). Upcoming PR will restore fine-grained state updates via JS/websocket. cc @cseed. Images:; <img width=""1302"" alt=""screen shot 2019-02-28 at 3 06 46 pm"" src=""https://user-images.githubusercontent.com/5543229/53595163-88d8ea00-3b6a-11e9-841b-7dbf6981c990.png"">. <img width=""1301"" alt=""screen shot 2019-02-28 at 3 06 51 pm"" src=""https://user-images.githubusercontent.com/5543229/53595148-7ced2800-3b6a-11e9-9428-5290b5ee1dc7.png"">. <img width=""1301"" alt=""screen shot 2019-02-28 at 3 09 17 pm"" src=""https://user-images.githubusercontent.com/5543229/53595276-d35a6680-3b6a-11e9-930e-5ef0757e181e.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5476#issuecomment-468419208:414,rout,routes,414,https://hail.is,https://github.com/hail-is/hail/pull/5476#issuecomment-468419208,1,['rout'],['routes']
Integrability,"@danking @cseed should be ready for testing soon. pruned portions we're not using atm, wrote docker files, tested. if you want to run both packages on your local machine, you could use the top-level docker file (or each package's). To start a local instance of the web app, simply run:; `npm install && npm bootstrap`. To get a hot-reloading version of the web app (links to your browser, refreshes all changes): `cd packages/public && npm run dev`. To start the gateway: `cd packages/hail-api-gateway && nodemon index.js`. Dev mode routing is slow. To see a production, minified build: `cd packages/public && npm run build && npm run prod-test`.; * Build is a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:533,rout,routing,533,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935,2,"['depend', 'rout']","['dependencies', 'routing']"
Integrability,@danking Can you look over the synchronized/volatile code here? You thought through this stuff originally. I reviewed everything else (mainly comments on the previous commit).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1789#issuecomment-302176856:31,synchroniz,synchronized,31,https://hail.is,https://github.com/hail-is/hail/pull/1789#issuecomment-302176856,1,['synchroniz'],['synchronized']
Integrability,"@danking Do you know what's up with this test? https://ci.hail.is/batches/5921822/jobs/68 Is the test wrong? I didn't change anything about the test in this PR so I feel like I must have somehow broken a dependency, but I'm unsure how because we don't use the google client library and I doubt it's a problem with aiohttp.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11974#issuecomment-1276770148:204,depend,dependency,204,https://hail.is,https://github.com/hail-is/hail/pull/11974#issuecomment-1276770148,1,['depend'],['dependency']
Integrability,"@danking I agree we shouldn't do this whole interleaving. > If we had (2), we could write an async version of TemporaryDirectory and TemporaryFilename and use those in async methods (in particular, in hail.backend.ServiceBackend). Do you mean that `FS` should have async variants for its methods? I could write `__aenter__` and `__aexit__` methods for `TemporaryDirectory` or `TemporaryFilename`. I think that would require restricting their `FS` field to a `RouterFS` and using that to grab the `RouterAsyncFS` to get at async implementations for the methods. Kinda gross.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13677#issuecomment-1743370832:459,Rout,RouterFS,459,https://hail.is,https://github.com/hail-is/hail/pull/13677#issuecomment-1743370832,2,['Rout'],"['RouterAsyncFS', 'RouterFS']"
Integrability,"@danking I have unit + integration tests for 2 methods in this package. Do you want me to add them here, or in a separate PR?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5369#issuecomment-465392000:23,integrat,integration,23,https://hail.is,https://github.com/hail-is/hail/pull/5369#issuecomment-465392000,1,['integrat'],['integration']
Integrability,"@danking I think I addressed most of your comments. Everything is passing. Can you look if you like the interface for `sync_check_call` etc.? If so, then can I translate your TLS commands in lists into a single string? Is there ever a reason to want to pass the list explicitly in this case?. Once you're happy with the changes, then I'll do another test round with dev deploy to make sure the cache is actually doing its thing and the garbage collection loop is working.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9095#issuecomment-665855101:104,interface,interface,104,https://hail.is,https://github.com/hail-is/hail/pull/9095#issuecomment-665855101,1,['interface'],['interface']
Integrability,@danking I think you have to add `rich` to the `hail/python` dependencies. Docker gets it by accident through `twine`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12346#issuecomment-1281505193:61,depend,dependencies,61,https://hail.is,https://github.com/hail-is/hail/pull/12346#issuecomment-1281505193,1,['depend'],['dependencies']
Integrability,"@danking I'd apply this patch. Not sure why we install those dependencies in the auth dockerfile, they're already in the docker requirements:. ```diff; diff --git a/auth/Dockerfile b/auth/Dockerfile; index 0c2bfa4dad..a94928c697 100644; --- a/auth/Dockerfile; +++ b/auth/Dockerfile; @@ -1,9 +1,5 @@; FROM {{ service_base_image.image }}; ; -RUN hail-pip-install \; - google-auth-oauthlib==0.4.6 \; - google-auth==1.25.0; -; COPY auth/setup.py auth/MANIFEST.in /auth/; COPY auth/auth /auth/auth/; RUN hail-pip-install /auth && rm -rf /auth; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12544#issuecomment-1349545278:61,depend,dependencies,61,https://hail.is,https://github.com/hail-is/hail/pull/12544#issuecomment-1349545278,1,['depend'],['dependencies']
Integrability,"@danking I'm pretty sure using UnsafeRows doesn't work in this case. When I changed from UnsafeRow to SafeRow, the tests were passing on a saved random dataset that wasn't previously working. This depends on #3724 so you can ignore changes in Relational.scala and the python tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3720#issuecomment-395741181:197,depend,depends,197,https://hail.is,https://github.com/hail-is/hail/pull/3720#issuecomment-395741181,1,['depend'],['depends']
Integrability,"@danking I've made all of the changes except the one about the interface for `af_dist`. (I agree with the point you're making about it being confusing that both functions are seeded separately and need to be kept the same for the same dataset to be produced, but making it take a function might just add a lot of visual noise. Honestly, I'm really tempted to take all of the `seed` stuff out of `balding_nichols_model` and put a note in to the effect of ""for reproducible results, use `hl.set_global_seed()` just before creating a dataset."" How would you feel about that?). re: how seeding functions happens more generally---I don't think anything I've said in the docs is actually incorrect (I would consider array transformations a context, kind of like the axes of a table or matrix table, so the table should end up with `x = y = z` but different values for the elements in the array, and different values between rows), but I'll work on making the language clearer so that they actually say what I mean. w.r.t. `[z, z]` being two draws---I have sometimes waffled on this but in general I think I tend to disagree, in part because I have struggled to come up with a consistent definition of what that would mean. Happy to talk more about this, but I don't know if it's super relevant to balding nichols since all of the distributions are once-per-element draws.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4166#issuecomment-415848091:63,interface,interface,63,https://hail.is,https://github.com/hail-is/hail/pull/4166#issuecomment-415848091,1,['interface'],['interface']
Integrability,"@danking IIUC the TeamCity build is now working with spark-2.1.0 but not spark-2.0.2; (even though running `./gradlew shadowJar archiveZip` on my laptop with spark-2.0.2 works fine.). From looking at the Maven repo; https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-20_2.11; and the elasticsearch-spark connector docs; https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html; there's no indication that some versions only support v2.1, though it does say; ```; elasticsearch-hadoop allows Elasticsearch to be used in Spark in two ways: through the dedicated support available since 2.1 or through the Map/Reduce bridge since 2.0. Spark 2.0 is supported in elasticsearch-hadoop since version 5.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2049#issuecomment-320274234:650,bridg,bridge,650,https://hail.is,https://github.com/hail-is/hail/pull/2049#issuecomment-320274234,1,['bridg'],['bridge']
Integrability,"@danking If Konrad is fine with this for now and Python 3 will fix the issue, I think we should keep this PR as is and avoid adding additional dependencies. I'll add checking the time zone to the 0.2 to-do list and we can revisit once we've switched to Python 3.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2243#issuecomment-331923128:143,depend,dependencies,143,https://hail.is,https://github.com/hail-is/hail/pull/2243#issuecomment-331923128,1,['depend'],['dependencies']
Integrability,"@danking Pushed a version that should work on local, now focusing on deployment changes. This is a clean fork; I rolled back all notebook changes to master. notebook-api/notebook/notebook.py is the file to review. Corresponding client pr commit: https://github.com/hail-is/hail/pull/5162/commits/7afc4a5b599a233a4e4b40bb9c7a260b062dd925; - This also includes all CORS bits. With the caveat that this is my first attempt at Kubernetes events, I think this moves things in the right direction. We now have an authenticated, push-notification system for an arbitrary number of notebooks. There are a few issues with it currently, mostly in handling closed web socket connections in gevent, which I will move away from in the iteration after Wednesday, but I handle dead socket errors and they don't *seem* to accumulate over time. I also need to implement a reconnection system on the client. The neat thing about this synchronizes sessions between refresh. So if you have N collaborators all on the same window (or more likely, you have 2 windows open), they will all get consistent state as quickly as Kubernetes knows it. This may not seem useful atm, but it allows us to get really fine-grained view into svc/pod uptime. This also should be much faster, provided we don't overburden the server with watchers (can be solved using server implementation as well), say by using an interval of a second, because we query kubernetes directly, rather than hitting the liveness endpoint by traveling over public internet and then being proxied at the boundary by nginx. We know within ms of the true state. Remaining q is whether this completely replicates the liveness endpoint. I also tried to make the serializing the kubernetes object the domain of the caller; I like this because the called can stop thinking about whether something implements __getitem__, and can specify pretty arbitrary transformations on that data (see lines 172-210, 331, 360, and all other calls to marshall_json), and allows us t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459839071:916,synchroniz,synchronizes,916,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459839071,1,['synchroniz'],['synchronizes']
Integrability,"@danking Set auth0 callback based on window (this turns out to be cleanest), cleaned up styles, further reduced bundle size a bit by removing use of a state management tool (that wraps any arbitrary object in an observable that can be watched), and added a basic header menu to allow logout. I think the most challenging part of using this web architecture will be managing actions on server vs browser. That is probably the only piece that isn't obvious. I added a few comments that may help; namely _app.js 's constructor runs before everything else (as it wraps all other components), but lifecycle functions (componentDidMount) run from the inner child out to the parent. Constructor runs both on server and client (since the class contains the needed functions that are translated into HTML). getInitialProps is the only lifecycle event that runs on both server and client. All other events are client only. To require something to only run on the server or browser, from getInitialProps, or constructor, check for (typeof window === 'undefined'). Not incredibly elegant, but not terribly problematic either.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454606562:179,wrap,wraps,179,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454606562,2,['wrap'],['wraps']
Integrability,@danking Should I review this now? Or is this dependent on #2270?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2280#issuecomment-335470740:46,depend,dependent,46,https://hail.is,https://github.com/hail-is/hail/pull/2280#issuecomment-335470740,1,['depend'],['dependent']
Integrability,"@danking Sorry to keep making you break things out, but it is really helpful for me and the changes will go in faster. Can you make a separate PR with the following changes that don't relate to passing the indices and the new index code? Specifically, the following items from your list:. ```; added row_fields which prevents reading and allocation of LID and RSID (also improved python-type-checking for row_fields and entry_fields). I changed several asserts to if's with fatals, so as not to allocate strings. We no longer copy the genotype data into a buffer in the block reader. This was forcing the fastKeys to do an unnecessary data copy. I changed the contract on BgenRecord to require that getValue is called to ""consume"" the record before the next record is taken. getValue(null) just skips bytes (no copy, no decompression). I added RegionValueBuilder.unsafeAdvance which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work. I use RegionValueBuilder.unsafeAdvance to make loading a BGEN without entry fields very fast. I fixed Table.index to not trigger a partition key info gathering; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3727#issuecomment-397281018:660,contract,contract,660,https://hail.is,https://github.com/hail-is/hail/pull/3727#issuecomment-397281018,1,['contract'],['contract']
Integrability,"@danking Thanks for the great comments! I've responded to all of them, and have begun addressing them. The rest I will either need to do a bit more research on, or potentially speak with you in person about. I think we can have this wrapped up tomorrow though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-463477132:233,wrap,wrapped,233,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-463477132,1,['wrap'],['wrapped']
Integrability,@danking back to you. restarted the integration test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1748#issuecomment-298957026:36,integrat,integration,36,https://hail.is,https://github.com/hail-is/hail/pull/1748#issuecomment-298957026,1,['integrat'],['integration']
Integrability,@danking can you edit the title / commit message to something more patch-notes-y? ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5421#issuecomment-467106563:41,message,message,41,https://hail.is,https://github.com/hail-is/hail/pull/5421#issuecomment-467106563,1,['message'],['message']
Integrability,"@danking i'm also not really certain why this works haha, i pretty much just did a bunch of trial and error. it would probably be worth taking a deeper look at how we manage our jvm dependencies (especially spark) at some point, but i figured it made sense to just revert a couple lines to fix the bug in the short term",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13759#issuecomment-1745640178:182,depend,dependencies,182,https://hail.is,https://github.com/hail-is/hail/pull/13759#issuecomment-1745640178,1,['depend'],['dependencies']
Integrability,"@danking should internal_base_url in wait-for.py be https? Do we have anything that should route through http?. As an alternative, I think it would be reasonable to always rewrite port: 443 to https:// in that function. I can PR if you're ok with that. edit: The comment states that the protocol should match hailtop.config. Not sure why that is, besides sharing gateway probably? In any case, this function doesn't match that. We're always in the `self._location == 'external'` space I think, which means https according to hailtop.config",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548098055:91,rout,route,91,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548098055,2,"['protocol', 'rout']","['protocol', 'route']"
Integrability,"@danking this is just the first step, but it provides a good interface to the function you need for PCrelate",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1839#issuecomment-302194372:61,interface,interface,61,https://hail.is,https://github.com/hail-is/hail/pull/1839#issuecomment-302194372,1,['interface'],['interface']
Integrability,"@danking this is ready for a look now!. There's some slight changes to interface, etc to take advantage of the new key design--mostly now it takes row fields instead of an expr. There's also a small fix to some of the partitioner/interval stuff. The python interface is gone now, and I'll put it back in a separate PR, with real docs this time. And I pulled out all the parsing logic into its own thing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2804#issuecomment-366001940:71,interface,interface,71,https://hail.is,https://github.com/hail-is/hail/pull/2804#issuecomment-366001940,2,['interface'],['interface']
Integrability,"@danking, @cseed An attempt to use --notebook-dir failed (Didn't understand the path). Will make another attempt to set this as a config, but if not, I think we should defer folder creation as an improvement to jgscm, I'll open an issue. Have forked jgscm, and have identified what appears a likely path to the fix (they don't specify the full blob path, gs://bucket/blob). As an aside, jgscm is effectively unmaintained. 2 of the problems I've encountered have issues dating to May & August (last accepted PR was April 2018). After we patch in the fixes needed (dependencies, folder creation), I think we should consider publishing a separate package from our fork (say jgscm2), unless we want to maintain jgscm in our repo, which may be less desirable from a licensing perspective based on our earlier convos (jgscm is MIT, but I believe you still may prefer to not mix codebases?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480335780:563,depend,dependencies,563,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480335780,1,['depend'],['dependencies']
Integrability,@dependabot ignore this dependency,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11596#issuecomment-1069575022:1,depend,dependabot,1,https://hail.is,https://github.com/hail-is/hail/pull/11596#issuecomment-1069575022,2,['depend'],"['dependabot', 'dependency']"
Integrability,@dependabot ignore this major version,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12477#issuecomment-1319090961:1,depend,dependabot,1,https://hail.is,https://github.com/hail-is/hail/pull/12477#issuecomment-1319090961,1,['depend'],['dependabot']
Integrability,"@ehigham Also another question is how does the schema update enforce certain order of operations. . The `rename-job-groups-cancelled-column` sql should run before other sqls that depend on the modified column name in `job_groups_cancelled` table, correct?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2334796201:179,depend,depend,179,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2334796201,1,['depend'],['depend']
Integrability,@jbloom22 @johnc1231 I can't approve as long as the interface is broken.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1984#issuecomment-321887358:52,interface,interface,52,https://hail.is,https://github.com/hail-is/hail/pull/1984#issuecomment-321887358,1,['interface'],['interface']
Integrability,"@jbloom22 @tpoterba this makes me uncomfortable, we have no canary if PCRelate starts failing or if any of the infrastructure on which it depends starts failing. What is the long term plan?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3274#issuecomment-377931559:138,depend,depends,138,https://hail.is,https://github.com/hail-is/hail/pull/3274#issuecomment-377931559,1,['depend'],['depends']
Integrability,"@jbloom22 I cannot have two parameters without breaking current interface. In order to have two parameters, I'd have to give kinshipMatrix a default value of None, but I can't do that since it is not permissible to have default valued arguments listed before arguments that don't have defaults. Going to have to just have the name be ""kinshipMatrix"" but it can be either LD or Kinship until we change name in next version.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1984#issuecomment-315443392:64,interface,interface,64,https://hail.is,https://github.com/hail-is/hail/pull/1984#issuecomment-315443392,1,['interface'],['interface']
Integrability,"@jbloom22 This line is wrong:; https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/utils/Graph.scala#L80; `l` and `r` should be tuples (i.e. rows). When the IR route was added, it was modified to take tuples, but the AST route was not updated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3704#issuecomment-394407542:175,rout,route,175,https://hail.is,https://github.com/hail-is/hail/pull/3704#issuecomment-394407542,2,['rout'],['route']
Integrability,"@jbloom22 can you review this? You know most about Nirvana interop. Also, how do we get it integrated into cloudtools?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3266#issuecomment-377697212:91,integrat,integrated,91,https://hail.is,https://github.com/hail-is/hail/pull/3266#issuecomment-377697212,1,['integrat'],['integrated']
Integrability,"@jigold Doesn't this suggest that the error's message is `'job_id'`? ; ```; > assert data['check_resource_aggregation_error'] is None, data; E AssertionError: {'check_incremental_error': None, 'check_resource_aggregation_error': ""'job_id'""}; E assert ""'job_id'"" is None; ```; The select statement for `attempt_by_job_group_resources` doesn't include a `job_id` column.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14282#issuecomment-1940328506:46,message,message,46,https://hail.is,https://github.com/hail-is/hail/pull/14282#issuecomment-1940328506,1,['message'],['message']
Integrability,"@jigold I stood up batch/ci in my own project from `hail-is/hail:main` and then deployed this branch, taking notes of any changes I needed to make and all seemed to work out OK. I think that's about as much as I can properly test this without trying things out in haildev/hail-vdc. The steps were as follows:. 1. Generate the configmaps used by gateway/internal-gateway. These will have the routing configuration for production services (I've edited the bootstrap instructions to match); `make -C gateway envoy-xds-config && make -C internal-gateway envoy-xds-config`; 2.  wait a few seconds for CI to quietly update these configmaps with information about testing namespaces  (can manually verify changes with `download-configmap gateway-xds-config`); 3. Deploy the new versions of gateway/internal-gateway; `make -C gateway deploy NAMESPACE=default && make -C internal-gateway NAMESPACE=default`. This worked for me in my project with no downtime, but either way I would probably do the same thing as with the previous PR where I test it in azure before making changes to hail-vdc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095#issuecomment-1293703346:391,rout,routing,391,https://hail.is,https://github.com/hail-is/hail/pull/12095#issuecomment-1293703346,1,['rout'],['routing']
Integrability,"@jigold If the concern is number of third party dependencies, I dont really have an answer (number of third party dependencies last which productivity drops over some timespan). This library is something like 20 lines long, they just wrap __new__ and __init__ to make those awaitable. . I also think your approach is fine!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5572#issuecomment-471733373:48,depend,dependencies,48,https://hail.is,https://github.com/hail-is/hail/pull/5572#issuecomment-471733373,3,"['depend', 'wrap']","['dependencies', 'wrap']"
Integrability,"@jigold This PR allows us to create fully-pinned, deterministic, requirements files for our pip dependencies. This uses those fully-pinned versions in our docker images so that breaking releases of transitive dependencies don't spontaneously break our CI pipelines. If we want to add or upgrade a dependency, someone needs to:. 1. Update the relevant `requirements.txt` file just as we would do currently.; 2. Run `pip-compile` to generate new lock `pinned-requirements.txt` files and check in the changes to a PR. The added check step verifies that you can't do 1 and forget to do 2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11842#issuecomment-1145288523:96,depend,dependencies,96,https://hail.is,https://github.com/hail-is/hail/pull/11842#issuecomment-1145288523,3,['depend'],"['dependencies', 'dependency']"
Integrability,"@jigold Yes, since the CompileWithAggregators interface changed. Whichever goes in first, the other can rebase and update?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3422#issuecomment-383611333:46,interface,interface,46,https://hail.is,https://github.com/hail-is/hail/pull/3422#issuecomment-383611333,1,['interface'],['interface']
Integrability,"@jigold https://azure.github.io/Storage/docs/application-and-user-data/code-samples/concurrent-uploads-with-versioning. Looks like this is the expected behavior from Azure Blob Storage when concurrently uploading blobs. It seems like *all* blocks are purged when any single upload succeeds. Unfortunately, it doesn't seem possible to distinguish between a truly invalid block list and this transiently invalid block list. I dislike this interface, but it is what we have. It seems to me that the right fix is to deduplicate the file list before uploading. Multiple files uploading to the same target should be an error if they're different source files.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13812#issuecomment-1882088862:437,interface,interface,437,https://hail.is,https://github.com/hail-is/hail/pull/13812#issuecomment-1882088862,1,['interface'],['interface']
Integrability,"@jigold this jogged a memory, is there, perhaps, a really bad error message for `g.ad.sum()`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/846#issuecomment-249638782:68,message,message,68,https://hail.is,https://github.com/hail-is/hail/issues/846#issuecomment-249638782,1,['message'],['message']
Integrability,"@jigold this should pass now. I also learned that Mypy checks each command line argument as an independent module. Instead of specifying individual files, we have to tell it to check `batch`. I also added `google_storage.py` in an ill-fated attempt to let batch use that. I think we can only type check modules that depend on one another by installing them first. We'll leave that for future work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8968#issuecomment-662143859:316,depend,depend,316,https://hail.is,https://github.com/hail-is/hail/pull/8968#issuecomment-662143859,1,['depend'],['depend']
Integrability,"@jigold upon review of the flags PR, I don't see why this would depend on that; do you recall the issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1426081475:64,depend,depend,64,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1426081475,1,['depend'],['depend']
Integrability,@jigold we should probably have the same interface in terms of units. Which do you think is best?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1768#issuecomment-299443875:41,interface,interface,41,https://hail.is,https://github.com/hail-is/hail/pull/1768#issuecomment-299443875,1,['interface'],['interface']
Integrability,"@jmarshall another way to do this might be to ignore/do nothing during the initial call of `job.command('')` when the argument is empty. Assuming we never want to do anything for empty/None commands we might as well eliminate them at source. That way you should only have to handle the situation in one place, and not have to handle the invalid data structure in multiple downstream places. I do like the idea of fixing this in the hail library, but you could also consider altering your upstream code so that it never makes an invalid ""`job.command('')`"" call in the first place. Eg by doing something like ; ```py; job.command('touch before'); for msg in messages:; job.command(f'echo {shlex.quote(msg)}); job.command('touch after'); ```. (assuming it doesn't matter to you whether the messages are handled in the same section of the resulting command or not). What do you think?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14700#issuecomment-2374458692:657,message,messages,657,https://hail.is,https://github.com/hail-is/hail/pull/14700#issuecomment-2374458692,2,['message'],['messages']
Integrability,@johnc1231 Could you also time this routine on the european subset of 1000KG? with PI_HAT=0.2? I'd like to have that information in the PR comments in case we need to know that in the future.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1457#issuecomment-286238444:36,rout,routine,36,https://hail.is,https://github.com/hail-is/hail/pull/1457#issuecomment-286238444,1,['rout'],['routine']
Integrability,@johnc1231 If your *commit message* (not the PR) contains `fixes #863` you should be good.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/863#issuecomment-279758262:27,message,message,27,https://hail.is,https://github.com/hail-is/hail/issues/863#issuecomment-279758262,1,['message'],['message']
Integrability,@konradjk This isn't a breaking change. We're keeping the current interface as an alias to `aggregate_entries().result()`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4154#issuecomment-416591508:66,interface,interface,66,https://hail.is,https://github.com/hail-is/hail/pull/4154#issuecomment-416591508,1,['interface'],['interface']
Integrability,@konradjk This should be good enough for now. I think I need a better interface for resource groups with regards to file extensions for both `read_input_group` and `write_output`. I made an issue for it #5661,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5645#issuecomment-475346629:70,interface,interface,70,https://hail.is,https://github.com/hail-is/hail/pull/5645#issuecomment-475346629,1,['interface'],['interface']
Integrability,"@konradjk group_by/aggregate is doing something pretty different, so the interface there is totally what you want. I'd be open to renaming these things `query` like they were, but I think Cotton has a strong preference for aggregate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2892#issuecomment-365430172:73,interface,interface,73,https://hail.is,https://github.com/hail-is/hail/pull/2892#issuecomment-365430172,1,['interface'],['interface']
Integrability,"@lfrancioli `Slope` was introduced to bokeh in a later version. I'll go through and update some of our python dependencies when I get a chance this week, which should mean this can just go in at that point.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6162#issuecomment-497035653:110,depend,dependencies,110,https://hail.is,https://github.com/hail-is/hail/pull/6162#issuecomment-497035653,1,['depend'],['dependencies']
Integrability,"@lfrancioli see the docs, `parallel` takes a string. The PR adds a typechecker to give a better error message to future you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4033#issuecomment-408956343:102,message,message,102,https://hail.is,https://github.com/hail-is/hail/issues/4033#issuecomment-408956343,1,['message'],['message']
Integrability,@lgruen Can this change wait until January when we make breaking interface changes?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9753#issuecomment-740224418:65,interface,interface,65,https://hail.is,https://github.com/hail-is/hail/pull/9753#issuecomment-740224418,1,['interface'],['interface']
Integrability,@lgruen This breaks the `test_hailctl_batch` test. You need to make sure this message doesn't get printed when running `hailctl batch submit` with the `-o json` option.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12612#issuecomment-1400479921:78,message,message,78,https://hail.is,https://github.com/hail-is/hail/pull/12612#issuecomment-1400479921,1,['message'],['message']
Integrability,"@mhebrard The only way I can imagine that we would mutate your environment is if we are accidentally installing `pyspark`. `install-on-cluster` takes pains to avoid that:; ```; install-on-cluster: $(WHEEL) check-pip-lockfile; 	sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$$' | tr '\n' '\0' | xargs -0 $(PIP) install -U; 	-$(PIP) uninstall -y hail; 	$(PIP) install $(WHEEL) --no-deps; 	hailctl config set query/backend spark; ```. But that is broken somehow? When you ran `install-on-cluster` did you see a `pip` output indicating that pyspark got installed?. Can you check if pyspark is installed via pip now? `pip show pyspark` (should say its not installed). If it is installed, try uninstalling it `pip uninstall pyspark`. You might also try removing the first line of `install-on-cluster` entirely. That will leave you without Hail's dependencies installed, but if `pyspark-shell` is still the right version of Scala, then I suspect the issue is that line.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1769053906:878,depend,dependencies,878,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1769053906,1,['depend'],['dependencies']
Integrability,@patrick-schultz @ehigham I'm abdicating responsibility for this. I've too much to wrap up in the next five work days. It looks like there's a mill issue currently. Otherwise I think it should be ready to merge.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1959922243:83,wrap,wrap,83,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1959922243,1,['wrap'],['wrap']
Integrability,"@patrick-schultz @tpoterba I removed the dependence on #6534 and wrote a test to exercise the IR nodes and all the aggregators we had written. This can now be reviewed/go in independently of the other one, which was failing on a match error in Emit. (I'll request changes to block the other one from going in until we've discussed a plan for using it, since that's now the one that will send everything through the new path.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6535#issuecomment-509383664:41,depend,dependence,41,https://hail.is,https://github.com/hail-is/hail/pull/6535#issuecomment-509383664,1,['depend'],['dependence']
Integrability,"@patrick-schultz Do I understand the error message correctly to mean I need to implement an Interpret step for this? Is that like an hour of work, a day, a week?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13336#issuecomment-1664627297:43,message,message,43,https://hail.is,https://github.com/hail-is/hail/pull/13336#issuecomment-1664627297,1,['message'],['message']
Integrability,"@patrick-schultz I only looked at the Python interface, so I would much appreciate your thoughts on the Scala stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9063#issuecomment-661191810:45,interface,interface,45,https://hail.is,https://github.com/hail-is/hail/pull/9063#issuecomment-661191810,1,['interface'],['interface']
Integrability,@patrick-schultz I resolved the conflict by keeping main which seems to have encompassed your change but with a more descriptive error message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11003#issuecomment-1476992958:135,message,message,135,https://hail.is,https://github.com/hail-is/hail/pull/11003#issuecomment-1476992958,1,['message'],['message']
Integrability,"@patrick-schultz I rewrote the `ReadIterator` stuff as a wrapper for a `Reader` object that actually manages the state. I've left the Region management out of this for now; it's currently pulling the region from the downstream ContextRDD, but the next step is definitely to move the region management into c++.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4729#issuecomment-437161860:57,wrap,wrapper,57,https://hail.is,https://github.com/hail-is/hail/pull/4729#issuecomment-437161860,1,['wrap'],['wrapper']
Integrability,"@patrick-schultz What should the behavior here be like?; During the execution of the above, we are getting the following message in the log:; ```; 2020-03-04 18:34:38 root: INFO: invalid partitioner: !lteqWithOverlap(-1)([[]-[]].right, [[]-[]].left); ```; These are empty range bounds, I feel like in this particular case, `lteqWithOverlap` should be true.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8138#issuecomment-594938133:121,message,message,121,https://hail.is,https://github.com/hail-is/hail/issues/8138#issuecomment-594938133,1,['message'],['message']
Integrability,"@patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure? This PR gets ride of MakeArray but until MakeStream, I don't think this is viable. Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. When the stream consumer is smaller, we might consider inlining it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8148#issuecomment-590660567:47,wrap,wrap,47,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590660567,1,['wrap'],['wrap']
Integrability,"@pwc2 , I'm gonna close this because PR has trouble with old PRs. Can we revisit this when the PCA project is wrapped up?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10874#issuecomment-1050054480:110,wrap,wrapped,110,https://hail.is,https://github.com/hail-is/hail/pull/10874#issuecomment-1050054480,1,['wrap'],['wrapped']
Integrability,"@rcownie I think this design is a reasonable compromise on interface, at least until linear algebra is in the compiler and we can ""cancel"" conversion from ndarray to BlockMatrix and back. The optimal `complexity_bound` will be hugely dependent on cluster setup. To aid user intuition, I made the unit in terms of a single dimension rather than dimension-cubed. I've found the divide-and-conquer eigh method (with memory proportional to elements) to be 2.5-3x faster than the RRR eigh method (with memory proportional to dimension) when run on laptop and GCP; it takes greater advantage of vectorized BLAS3 ops. Since we're CPU rather than RAM limited on a high-core GCP machine, I've set this up to use divide-and-conquer whenever it won't result in an overflow on `lwork` which is still an int32 in the Python stack (boo). @cseed please let me know if you have any high-level feedback",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3977#issuecomment-407268618:59,interface,interface,59,https://hail.is,https://github.com/hail-is/hail/pull/3977#issuecomment-407268618,2,"['depend', 'interface']","['dependent', 'interface']"
Integrability,@sjparsa I invited you to the Hail repo so that the CodeQL pipelines will run. Your GitHub inbox should contain an invite message that you need to accept.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13258#issuecomment-1644149775:122,message,message,122,https://hail.is,https://github.com/hail-is/hail/pull/13258#issuecomment-1644149775,1,['message'],['message']
Integrability,"@tmwong2003 OK, some progress on the CI front. Thanks for your patience. A team member needs to kick off the CI job. The assigned reviewer will be responsible for that. Right now, the tests involve some sensitive tokens which need more work to be protected, so the CI logs aren't public yet. Again, the assigned reviewer should be able to share the relevant part of the logs for failures, etc. @tpoterba I kicked off the build. Can you take another look, it looks like the comments were addressed. Finally, it looks like the PR history is a bit tangled. Is it possible to clean it up so we can a reasonable commit message? Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6262#issuecomment-501887788:614,message,message,614,https://hail.is,https://github.com/hail-is/hail/pull/6262#issuecomment-501887788,1,['message'],['message']
Integrability,"@tomwhite I rebased and modified the code to use the recently added `AnnotationImpex` (import/export) interface. I ran the tests again the quickstart and it looks good. If you're happy with the changes, I'll merge it in. Is there plan for arrays in Kudu? We might consider serializing as JSON, say, rather than the fixed array size.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/450#issuecomment-232474916:102,interface,interface,102,https://hail.is,https://github.com/hail-is/hail/pull/450#issuecomment-232474916,1,['interface'],['interface']
Integrability,"@tpoterba @danking We at Databricks are still interested in this. Although Hail's frontend is in Python, it's still useful to publish to maven central. First, it makes the dependency information available. I've seen people write pipelines that are partly in Hail and partly in PySpark and can include Java libraries for things like data sources. There's a lot of tooling for resolving dependency conflicts between different libraries, but they're not very accessible unless all your dependencies are published to maven repos and have dependency poms available. It's also easier to update pipelines to the latest Hail version if the artifacts published to a standard location.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1963#issuecomment-481354319:172,depend,dependency,172,https://hail.is,https://github.com/hail-is/hail/issues/1963#issuecomment-481354319,4,['depend'],"['dependencies', 'dependency']"
Integrability,"@tpoterba I have two concrete questions for you here:; 1. What do you think of this (very minimal) amount of information about the driver batch?; 2. What should we do about boolean fields going forward? Here I only accept the strings ""True"" and ""true"" but I think we've thus far accepted any non-empty string as a true value. I suppose we should probably continue that process, but I'd like to get your thoughts before I go that route.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11746#issuecomment-1104095231:429,rout,route,429,https://hail.is,https://github.com/hail-is/hail/pull/11746#issuecomment-1104095231,1,['rout'],['route']
Integrability,@tpoterba I think your error message is much better.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1406#issuecomment-280845674:29,message,message,29,https://hail.is,https://github.com/hail-is/hail/pull/1406#issuecomment-280845674,1,['message'],['message']
Integrability,@tpoterba I updated the main message with a better explainer on the changes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130#issuecomment-454592988:29,message,message,29,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454592988,1,['message'],['message']
Integrability,"@tpoterba Rather than review, I made a few changes, mostly around using a right-biased [`Either`](http://typelevel.org/cats/datatypes/either.html) from the [`cats`](http://typelevel.org/cats/) library. I think there's still a bit more work to do (see the commented out messages). I suspect we might actually want a `LookupError` sealed trait with an alternate for ""ambiguous"" and ""not found"" so that the consumer (i.e. `AST.scala`) can produce custom messages if desired.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1056#issuecomment-258482849:269,message,messages,269,https://hail.is,https://github.com/hail-is/hail/pull/1056#issuecomment-258482849,2,['message'],['messages']
Integrability,"@tpoterba Thanks for the clue! Does the code that you linked to have a Scala counterpart (that I could execute from R via Spark)? It would be awesome to have a fully-functional Scala/JVM interface to Hail... :-). EDIT: I mean, I understand what you said :-) My point is, is there an alternative way to achieve the same without using the Python interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10882#issuecomment-930122866:187,interface,interface,187,https://hail.is,https://github.com/hail-is/hail/issues/10882#issuecomment-930122866,2,['interface'],['interface']
Integrability,"@tpoterba We should just make a gradle target for end-users, like: ""releaseJar"" that just depends on the right targets.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6132#issuecomment-494049017:90,depend,depends,90,https://hail.is,https://github.com/hail-is/hail/issues/6132#issuecomment-494049017,1,['depend'],['depends']
Integrability,"@tpoterba and I are discussing changes to interface, possibly to internals.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1650#issuecomment-292925773:42,interface,interface,42,https://hail.is,https://github.com/hail-is/hail/pull/1650#issuecomment-292925773,1,['interface'],['interface']
Integrability,"@tpoterba sorry, may bad, I amended the commit message and re-pushed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6272#issuecomment-499532296:47,message,message,47,https://hail.is,https://github.com/hail-is/hail/pull/6272#issuecomment-499532296,1,['message'],['message']
Integrability,"@tpoterba this one is not dependent on the other two. When this one AND `notebook-server` lands, I need to add a fourth PR that hooks up the notebook server's image to this image-fetcher. What I really want is to be able to have a DAG in git, like:. ```; notebook-gateway <-- notebook-server <-- notebook-worker-image-fetch; |; notebook-image-fetcher <-----------------------+; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4647#issuecomment-433222303:26,depend,dependent,26,https://hail.is,https://github.com/hail-is/hail/pull/4647#issuecomment-433222303,1,['depend'],['dependent']
Integrability,"@zyd14 There are no releases of Hail that support Spark 3.2.x. We very closely follow Google Dataproc's release cycle. If you want Spark 3.2.x support, you'll need to fork, edit the code to handle changes to the APIs of dependencies, and recompile.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11707#issuecomment-1598879560:220,depend,dependencies,220,https://hail.is,https://github.com/hail-is/hail/issues/11707#issuecomment-1598879560,1,['depend'],['dependencies']
Integrability,@zyd14 that's just a warning message printed during compilation. The requirements.txt file [accepts any 3.5.x version](https://github.com/hail-is/hail/pull/14158/files#diff-c4e24762fa3ba8e53670cc9aba74121ade8ac055942738bde9c951fc0514561aR14).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1922339476:29,message,message,29,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1922339476,1,['message'],['message']
Integrability,"A newer version of protobuf exists, but since this PR has been edited by someone other than Dependabot I haven't updated it. You'll get a PR for the updated version as normal once this PR is merged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12518#issuecomment-1334218563:92,Depend,Dependabot,92,https://hail.is,https://github.com/hail-is/hail/pull/12518#issuecomment-1334218563,1,['Depend'],['Dependabot']
Integrability,"A newer version of pylint exists, but since this PR has been edited by someone other than Dependabot I haven't updated it. You'll get a PR for the updated version as normal once this PR is merged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11980#issuecomment-1188158089:90,Depend,Dependabot,90,https://hail.is,https://github.com/hail-is/hail/pull/11980#issuecomment-1188158089,1,['Depend'],['Dependabot']
Integrability,"A reminder for next time, due to weird GitHub behavior, if you have a one commit PR, then the commit's title and message become the commit in master. As reviewers, we should enforce that one commit PRs have meaningful titles and messages in said commit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9116#issuecomment-662792307:113,message,message,113,https://hail.is,https://github.com/hail-is/hail/pull/9116#issuecomment-662792307,2,['message'],"['message', 'messages']"
Integrability,"AFAIK, the ClientResponseError only contains information from the original request or the headers. The RequestInfo contains information from our request to Google, like the URL. You're right, incorporating this into is_transient_error is a bit complex. We need to somehow communicate the body, if any, to is_transient_error. I started to go down this route with httpx.py. I wanted to wrap ClientSession with a new HailClientSession whose `get`, `post` etc. methods would check for non-successful statuses themselves, read the body, and raise HailHTTPException which included the status code *and the body*. Then in is_transient_error we can look for HailHTTPException and use the body to determine if we should retry. For now, we can just fix the compute client.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10432#issuecomment-840652602:351,rout,route,351,https://hail.is,https://github.com/hail-is/hail/pull/10432#issuecomment-840652602,2,"['rout', 'wrap']","['route', 'wrap']"
Integrability,"According to @johnc1231 in https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Troubles.20getting.20started.20(Python.203.2E8):. > It's intentionally limited to 3.7 because of precisely that error; >; > John Compitello: This is because of our dependence on spark 2. Spark 3 won't have that restriction, we are in the process of upgrading, hopefully should be fixed next week. I'll close this and wait for the release with Spark 3.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197#issuecomment-800706947:267,depend,dependence,267,https://hail.is,https://github.com/hail-is/hail/issues/10197#issuecomment-800706947,1,['depend'],['dependence']
Integrability,Actually looks like just a bad error message. One of the Tables had 0 rows.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4216#issuecomment-416051659:37,message,message,37,https://hail.is,https://github.com/hail-is/hail/issues/4216#issuecomment-416051659,1,['message'],['message']
Integrability,"Actually the endpoint does seem to be an issue. https://internal.hail.is/pr-7381-default-sx9ail9zkm77/blog/ works great. edit: Endpoint is / (`+ python3 wait-for.py 60 pr-7381-default-sx9ail9zkm77 Service -p 80 blog --endpoint /`), when I think it should be /blog? We could try setting the endpoint to /blog/ or /default/blog/. The failure also has a line about not being able to connect to hostname blog.pr-7381-default-sx9ail9zkm77. I don't know enough about CI to determine whether this is a problem, but my guess is that is normal. edit2: The wait command's port is 80, not 443. Do we need to force X-Forward-Proto to https to fix it? Although if this is going through gateway, I think the protocol should be https after the redirect from 80/http. edit3: Actually, wait-for.py allows a port to be set, so it seems appropriate to set `port: 443` in the wait command. edit4: Nevermind, 443 will not set protocol to https. It shouldn't matter, I don't think, as long as gateway is redirecting to https, but you could try setting X-Forwarded-Proto to https. I suspect the issue is in the url or domain.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548089835:694,protocol,protocol,694,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548089835,2,['protocol'],['protocol']
Integrability,"Actually, I think the right fix is for CI to grab the PR message and set it as the merge message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9594#issuecomment-725031817:57,message,message,57,https://hail.is,https://github.com/hail-is/hail/pull/9594#issuecomment-725031817,2,['message'],['message']
Integrability,"Actually, it's even more broken. it expects there to be fields called `i` and `j`.; ```; import hail as hl; t = hl.utils.range_table(10); t = t.annotate(foo= t.idx // 2); hl.methods.maximal_independent_set(t.idx, t.foo, tie_breaker = lambda i, j: hl.signum(i - j)) ; ```; ```; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-11-de5acf23a36f> in <module>(); ----> 1 hl.methods.maximal_independent_set(t.idx, t.foo, tie_breaker = lambda i, j: hl.signum(i - j)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 139 .select()); 140 ; --> 141 edges = t.key_by(None).select('i', 'j'); 142 nodes_in_set = Env.hail().utils.Graph.maximalIndependentSet(edges._jt.collect(), node_t._jtype, joption(tie_breaker_hql)); 143 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 863 row = get_select_exprs('Table.select',; 864 exprs, named_exprs, self._row_indices,; --> 865 protect_keys=True); 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 . ~/projects/hail/python/hail/utils/misc.py in get_select_exprs(caller, exprs, named_exprs, indices, protect_keys); 314 def get_select_exprs(caller, exprs, named_exprs, indices, protect_keys=True):; 315 from hail.expr.expressions import to_expr, ExpressionException, TopLevelReference, Select; --> 316 exprs = [to_expr(e) if not isinstance(e, str) ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3706#issuecomment-394395696:601,wrap,wrapper,601,https://hail.is,https://github.com/hail-is/hail/issues/3706#issuecomment-394395696,3,['wrap'],['wrapper']
Integrability,"Actually, the change log doesn't make sense as the version of the service isn't related to the documentation/hail pip version. Thoughts on removing the change log here and just having client interface changes?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8844#issuecomment-631683697:191,interface,interface,191,https://hail.is,https://github.com/hail-is/hail/pull/8844#issuecomment-631683697,1,['interface'],['interface']
Integrability,"Added a region argument, and mandated that users have either configured a region or are using `--region`. I believe older versions of gcloud assume the region is the default region for your project if you don't have anything specified, which causes users to have random errors when they update gcloud that they ask about on Zulip. This way, everyone gets a good error message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8281#issuecomment-597307671:368,message,message,368,https://hail.is,https://github.com/hail-is/hail/pull/8281#issuecomment-597307671,1,['message'],['message']
Integrability,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:510,rout,router,510,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516,4,['rout'],"['router', 'router-', 'routes']"
Integrability,Addressed comments. ; - Refactored to a separate module and added module-level tests. ; - Cleaned up TypeChecker interface to call recursively down,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1727#issuecomment-299293790:113,interface,interface,113,https://hail.is,https://github.com/hail-is/hail/pull/1727#issuecomment-299293790,1,['interface'],['interface']
Integrability,"After looking more at the build logs here, as I was curious as to how the tests passed, when `make` was run in compilation, it ran the target for `simd/simd.h`, which ran the `$(LIBSIMDPP)` target, which ran `tar xzf $(LIBSIMDPP).tar.gz`. Thus accidentally creating the necessary dependencies before the `test` target is run later in the CI build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5331#issuecomment-463907719:280,depend,dependencies,280,https://hail.is,https://github.com/hail-is/hail/pull/5331#issuecomment-463907719,1,['depend'],['dependencies']
Integrability,"After merge, addition of Let, RunAggScan, and ArrayFold2 all tests pass besides testMakeArrayWithDifferentRequiredeness (result mismatch). Will fix tomorrow. org.scalatest.exceptions.TestFailedException: t.valuesSimilar(res, expected, t.valuesSimilar$default$3, t.valuesSimilar$default$4) was false; result=WrappedArray(null, [0,null]); expect=WrappedArray(null, [2,WrappedArray(1)]); strategy=JvmCompile). However, I'm still concerned about the complexity of the lowering step, for instance, the Let should not be needed, but in practice was. Also need to understand why/whether the other IR, besides the child, of ArrayFold, and ArrayFold2 need/do not need to be streamified. Followed Streamify.ArrayFold here, and investigated some (for instance in ArrayFold2 streamifying the seq IR breaks typecheck), but should be more principled. edit: One difference from master run of this test is that we're generating MakeStream instead of MakeArray. for:. MakeTuple(ArrayBuffer((0,MakeArray(ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCStruct{a:PInt32,b:PCArray[PInt32]},1:+PCStruct{a:+PInt32,b:+PCArray[PInt32]}]),0), GetTupleElement(In(0,PCTuple[0:PCStruct{a:PInt32,b:PCArray[PInt32]},1:+PCStruct{a:+PInt32,b:+PCArray[PInt32]}]),1)),array<struct{a: int32, b: array<int32>}>))))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-583812669:307,Wrap,WrappedArray,307,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583812669,3,['Wrap'],['WrappedArray']
Integrability,"After sleeping on how I want the interface to be in general, I think this is ready for review. Thanks for your patience.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9485#issuecomment-699127443:33,interface,interface,33,https://hail.is,https://github.com/hail-is/hail/pull/9485#issuecomment-699127443,1,['interface'],['interface']
Integrability,"Ah ok, I certainly agree then, that `sparsify_blocks` is a more appropriate name, and this should be documented explicitly. It makes me a little bit uncomfortable that the output depends on the block_size parameter that's not set in this function (I understand why that is, it's just a little odd).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5511#issuecomment-469133387:179,depend,depends,179,https://hail.is,https://github.com/hail-is/hail/pull/5511#issuecomment-469133387,1,['depend'],['depends']
Integrability,"Ah right, the dockerfile isn't actually used until after the inputs are brought in, and the Dockerfile _rendering_ doesn't depend on the inputs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10354#issuecomment-824304334:123,depend,depend,123,https://hail.is,https://github.com/hail-is/hail/pull/10354#issuecomment-824304334,1,['depend'],['depend']
Integrability,"Ah sorry, I forgot to update the original commit message as that's not actually correct. The optimization is that we don't need to iterate through all blobs until we find the exact blob matching our path name. The list operation returns all blobs that start with the prefix of that path. If we see a blob with a different name that is a child of our path `f'{path}/foo`, then we know it's a directory and don't need to iterate anymore (although it could be a file as well, but in Scala we don't currently throw errors on paths that are both files and directories, so we just choose the first we see). If we see a blob that matches the path exactly, then we know it's a file and stop iterating. The only reason we need to iterate through more than one blob is if there's blobs that are like `'{path}zzzzz/foo` or `'{path}szzzzz`. We need to ignore these as they don't provide any information on whether `{path}` is a file or directory. This is where `isChildOf` is needed because we need to make sure the blob is actually a child of the path such as `'{path}/file` and not `{path}zzzzz/file`. As for `getValues` versus `iterateAll`, I just used the one that was in the Java documentation for using the `list` method.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13390#issuecomment-1673853274:49,message,message,49,https://hail.is,https://github.com/hail-is/hail/pull/13390#issuecomment-1673853274,1,['message'],['message']
Integrability,"Ah, and this isn't a bug in `main` because `test_batch_invariants` depends on the QoB tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13353#issuecomment-1660807887:67,depend,depends,67,https://hail.is,https://github.com/hail-is/hail/pull/13353#issuecomment-1660807887,1,['depend'],['depends']
Integrability,"Ah, done. Now delete azure and delete GCP depend on the exact same steps (except the image step).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13353#issuecomment-1660806782:42,depend,depend,42,https://hail.is,https://github.com/hail-is/hail/pull/13353#issuecomment-1660806782,1,['depend'],['depend']
Integrability,"Ah, figured out what's going on:; ```; ERROR	2020-01-15 18:17:49,022	batch.py	schedule_job:385	error while scheduling job (11, 3) on instance batch-worker-pr-7886-default-npqddriu0gh7-z20pv	Traceback (most recent call last):\n File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 375, in schedule_job\n raise e\n File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 366, in schedule_job\n await session.post(url, json=body)\n File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 589, in _request\n resp.raise_for_status()\n File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 947, in raise_for_status\n headers=self.headers)\naiohttp.client_exceptions.ClientResponseError: 413, message='Request Entity Too Large', url='http://10.128.0.25:5000/api/v1alpha/batches/jobs/create; ```. This is causing an instance to be marked unhealthy. Somehow that's causing an always_run job to not run before a batch is considered finished.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7886#issuecomment-574813815:748,message,message,748,https://hail.is,https://github.com/hail-is/hail/pull/7886#issuecomment-574813815,1,['message'],['message']
Integrability,"Ah, no, sorry, I just haven't written the other ones (which can be significantly more efficient) while working on the joint caller. Wrapping primitive types is an easy temporary fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5345#issuecomment-466821036:132,Wrap,Wrapping,132,https://hail.is,https://github.com/hail-is/hail/issues/5345#issuecomment-466821036,1,['Wrap'],['Wrapping']
Integrability,"Ah, service backend tests need to depend on deploy_memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059462873:34,depend,depend,34,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059462873,1,['depend'],['depend']
Integrability,"Alex, can you use the PR naming convention for the first line of your commit messages too? Github doesn't use the PR text as the squashed commit message for single-commit PRs, so the untagged/non-descriptive single commit message went into main history:. `we setlled on a name (#9233)`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9233#issuecomment-670542937:77,message,messages,77,https://hail.is,https://github.com/hail-is/hail/pull/9233#issuecomment-670542937,3,['message'],"['message', 'messages']"
Integrability,"All that messy state twiddling is because Scala's `Iterator` is the wrong model for most things we use it for, which is why I made `FlipbookIterator`. Using that, what you have would become; ```scala; private class BgenRecordStateMachine(; ctx: RVDContext,; p: BgenPartition,; settings: BgenSettings; ) extends StateMachine[RegionValue] {; private[this] val bfis = p.makeInputStream; private[this] val rv = RegionValue(ctx.region); private[this] val rvb = ctx.rvb; ; def isValid: Boolean = p.isValid; def value: RegionValue = rv; def advance() { p.advance(); findNextVariant() }; private def findNextVariant() {; // same as existing advance(), but without advancing p; }. findNextVariant() // make sure iterator is initialized in first valid state; }; ```; giving `BgenPartition` a `FlipbookIterator` style interface, with `isValid`, `value`, and `advance()` instead of `hasNext()` and `next()`. Then to create a new iterator `FlipbookIterator(new BgenRecordStateMachine(...))`. But honestly, what you had was clear enough, so if you benchmarked and the allocation isn't an issue, you should do whatever you find most readable. I've been conditioned to avoid `Option` in low-level code, but I don't have a good intuition for when it is or isn't actually a problem.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3893#issuecomment-404156507:807,interface,interface,807,https://hail.is,https://github.com/hail-is/hail/pull/3893#issuecomment-404156507,1,['interface'],['interface']
Integrability,"Also @Dania-Abuhijleh, make sure `deploy_benchmark` depends on this new build step.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9668#issuecomment-721304697:52,depend,depends,52,https://hail.is,https://github.com/hail-is/hail/pull/9668#issuecomment-721304697,1,['depend'],['depends']
Integrability,Also add test case for unterminated string with nice error message (use `interceptFatal`).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/493#issuecomment-235044203:59,message,message,59,https://hail.is,https://github.com/hail-is/hail/issues/493#issuecomment-235044203,1,['message'],['message']
Integrability,Also added Arcturus's method wrapping PR to changes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7738#issuecomment-566591869:29,wrap,wrapping,29,https://hail.is,https://github.com/hail-is/hail/pull/7738#issuecomment-566591869,1,['wrap'],['wrapping']
Integrability,"Also, I forgot making sure all of the Docker images are valid and integrated into CI like the vep images.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13588#issuecomment-1710521152:66,integrat,integrated,66,https://hail.is,https://github.com/hail-is/hail/pull/13588#issuecomment-1710521152,1,['integrat'],['integrated']
Integrability,"Also, I'm gonna add parameters to `hl.init` and set the flags in there. That punts the interface decision down the road by slightly restricting users (you can't change user project mid-pipeline).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12133#issuecomment-1230490168:87,interface,interface,87,https://hail.is,https://github.com/hail-is/hail/pull/12133#issuecomment-1230490168,1,['interface'],['interface']
Integrability,"Also, depends on #2548",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2551#issuecomment-350557759:6,depend,depends,6,https://hail.is,https://github.com/hail-is/hail/pull/2551#issuecomment-350557759,1,['depend'],['depends']
Integrability,"Also, the RVDSpec IS versioned: the version is the name of the concrete spec implementing the RVDSpec interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4734#issuecomment-436684642:102,interface,interface,102,https://hail.is,https://github.com/hail-is/hail/pull/4734#issuecomment-436684642,1,['interface'],['interface']
Integrability,"Although it's possible for entries to be a scalar type like double right now, that won't be possible in 0.2's interface -- they'll always be structs, which follows from lifting all fields out to be top-level. How about something like this:. ```python; vds = hwe_normalize(vds.GT) # adds field gt_norm or something; loadings, pcs, eigenvalues = pca(vds.gt_norm); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2454#issuecomment-348534152:110,interface,interface,110,https://hail.is,https://github.com/hail-is/hail/pull/2454#issuecomment-348534152,1,['interface'],['interface']
Integrability,Although let me try again now that you changed the router code.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6928#issuecomment-524357498:51,rout,router,51,https://hail.is,https://github.com/hail-is/hail/pull/6928#issuecomment-524357498,1,['rout'],['router']
Integrability,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869:1460,rout,routes,1460,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869,1,['rout'],['routes']
Integrability,"And I should note that I've only implemented summation aggregators right now. The non-lambda-taking-ones should follow easily if we're happy with this interface. The lambda-taking-ones shouldn't be too bad either, just a bit more work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2492#issuecomment-347998000:151,interface,interface,151,https://hail.is,https://github.com/hail-is/hail/pull/2492#issuecomment-347998000,1,['interface'],['interface']
Integrability,And I'd eventually like a (synchronous) version of this interface to replace the hadoop_* functions.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9426#issuecomment-689316416:56,interface,interface,56,https://hail.is,https://github.com/hail-is/hail/pull/9426#issuecomment-689316416,1,['interface'],['interface']
Integrability,And here's a squashed commit w/ message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/536#issuecomment-238687799:32,message,message,32,https://hail.is,https://github.com/hail-is/hail/pull/536#issuecomment-238687799,1,['message'],['message']
Integrability,Another option we might consider is specifying job dependencies with UUIDs instead of numerical IDs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1215958702:51,depend,dependencies,51,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1215958702,1,['depend'],['dependencies']
Integrability,"As I mentioned, I think this, plus KinshipMatrix and LDMatrix are getting lost in the domain-specific details. I suggest the following structure:; - an abstract Python `Matrix` class for numeric matrices. This should have (at least) three implementations: local, indexed-row and block. It should have read/write methods. It should support at least basic operations: *, +, -. They might not all be supported on all combination of implementations. There should be operations for converting between them. @danking is working on freeing us from Spark matrices and building on Breeze. You might coordinate here.; - a `Vector` class; - a `KeyedMatrix` which has row and column keys with schemas, or possibly a SymmetricKeyedMatrix to start if that is all we need (e.g. for Kinship and LD). This should again have read/write.; - then Eigen is just a KeyedMatrix with a Vector; - I'd nuke Kinship and LD, or if it is necessary to keep n{Samples, Variants}Used, it should be a simple wrapper class with the integer value and the underlying keyed matrix. Get the structure in place to start, don't worry so much about documentation. The user-facing part should be pretty thin/lightweight.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2160#issuecomment-326643889:975,wrap,wrapper,975,https://hail.is,https://github.com/hail-is/hail/pull/2160#issuecomment-326643889,1,['wrap'],['wrapper']
Integrability,As a separate follow up PR: we should inject the cloud location into the job's environment so that users' can make choices based on that information.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1269124272:38,inject,inject,38,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1269124272,1,['inject'],['inject']
Integrability,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:230,Rout,Routing,230,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336,5,"['Rout', 'rout']","['Routing', 'router', 'router-']"
Integrability,"Assigned John, I think @cseed is busy. John, this PR has 2 features:; 1) Remove catch-all server block, for service-specific blocks, in better keeping with router.nginx.conf. 2) Allow prefix matches only on the exact, slash-less url. Meaning /prometheusss$haxor doesn't work, but /prometheus does. This is most easily accomplished with an exact match location block, because by Nginx semantics, regex-containing locations cannot be elided with a root proxy_pass (one with a trailing /), because nginx wants a static prefix to remove, and regex prevents that. So to accomplish this with regex would require more LOC, namely a URL rewrite rule inside the location block.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-554498117:156,rout,router,156,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-554498117,1,['rout'],['router']
Integrability,"Assigned to Patrick, but anyone should feel free to approve, small change to add better error messages on `MakeNDArray`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10095#issuecomment-784448517:94,message,messages,94,https://hail.is,https://github.com/hail-is/hail/pull/10095#issuecomment-784448517,1,['message'],['messages']
Integrability,"At first glance, the tests look fine to me. More thorough tests could be done using the data files here; https://github.com/broadinstitute/picard/blob/e0bb690d57f73fd2495fc5a77b497e9696f51f81/src/test/java/picard/util/LiftoverVcfTest.java#L65-L99. The interface also looks fine in terms of a non-breaking way to add strand info.; Is the plan to use this to implement a hl.liftover(mt) function that includes the checks from https://github.com/broadinstitute/picard/blob/master/src/main/java/picard/vcf/LiftoverVcf.java#L350-L397 ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4895#issuecomment-444648897:252,interface,interface,252,https://hail.is,https://github.com/hail-is/hail/pull/4895#issuecomment-444648897,1,['interface'],['interface']
Integrability,"At the least, I didn't want the Dockerfile to COPY a huge tgz in. This seems like a slightly nicer interface than something that just decompresses a whole build step input.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-559211542:99,interface,interface,99,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-559211542,1,['interface'],['interface']
Integrability,Avoided by removing this dependency in #4659,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4653#issuecomment-434014181:25,depend,dependency,25,https://hail.is,https://github.com/hail-is/hail/issues/4653#issuecomment-434014181,1,['depend'],['dependency']
Integrability,Awesome! I propose leaving this PR unapproved for now. Please look over the parent_ids PR but don't approve. Let me get a working version of my third PR which is to change the client interface to support atomic batch creation. And then finally the fourth PR will atomically create the batch in the database. That way I'll have 4 working PRs -- easier to debug and review. And we can make sure the final product is what we want. Sound good?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6238#issuecomment-498350998:183,interface,interface,183,https://hail.is,https://github.com/hail-is/hail/pull/6238#issuecomment-498350998,1,['interface'],['interface']
Integrability,"Awesome! I will take a closer look, but here's my quick reaction to the interface. Was I was hoping to write was:. ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), x),; 0, 0); ```. This is basically modeled after letrec in lisp/scheme/ml which would look something like:. ```; (letrec (f i x); (if (< i 10); (f (+ i 1) (+ x i)); x); (f 0 0)); ```. Another option is to get rid of `recur` and make the binding of the loop more explicit with something like:. ```; hl.loop(; lambda f, i, x:; hl.cond(i < 10, f(i + 1, x + i), x),; 0, 0); ```. where the first argument to the lambda is the loop itself. I think main problem with your proposals (except maybe 3) is that it assumes too much about the structure of the loop: namely, it embeds the exit condition into the structure of the loop. The loop may have several backwards calls or exit points (e.g. in a case or if tree) and there maybe shared and conditional work that happens inside that tree (and even nested loops!)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614#issuecomment-558327378:72,interface,interface,72,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558327378,1,['interface'],['interface']
Integrability,"Awesome. God, how long was this in coming. No for this PR, but I observe this seems a bit error prone:. > --vep; > actual = hl.vep(expected.select_rows(), 'gs://hail-common/vep/vep/vep85-loftee-gcloud.json', csq=csq). We should probably think a bit more about the vep/cloudtools interface after this. I'm thinking `cloudtools --vep --vep-version=85 --vep-assembly=GRCh37 --loffee-version=beta ...` and then just `hl.vep(foo, csq=csq)` where the properties file defaults to something set up by cloudtools. @konradjk?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4347#issuecomment-422170789:279,interface,interface,279,https://hail.is,https://github.com/hail-is/hail/pull/4347#issuecomment-422170789,1,['interface'],['interface']
Integrability,"Bah, sorry, CI does all merges and there's a bug introduced in a python dependency of ours. I'll get that fixed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9875#issuecomment-759878149:72,depend,dependency,72,https://hail.is,https://github.com/hail-is/hail/pull/9875#issuecomment-759878149,1,['depend'],['dependency']
Integrability,"Basically done, I just have to go through and rewrite the LAPACK calls to use the new interface (so far I've only done so with NDArrayInverse). If you have any thoughts on the actual reference counting @tpoterba, that can be reviewed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10001#issuecomment-775541037:86,interface,interface,86,https://hail.is,https://github.com/hail-is/hail/pull/10001#issuecomment-775541037,1,['interface'],['interface']
Integrability,"Because of the unpredictable way that git clone might realize the requirements files, I removed the pinned-requirements file as a dependency of the changed targets. In both cases, regenerating that file (either in CI as part of the deploy.yaml target or on a cluster for `install-on-cluster`) could cause a dataproc cluster running with untested dependency versions even if the requirements.txt files are unchanged. I do, however, require that the pinned-requirements files be compatible using the same check we do in CI. I performed the following manual testing:; 1. Creating a dataproc cluster through `hailctl dataproc start`; 2. ssh'ing into said cluster, cloning this branch and running `make -C hail install-on-cluster` to completion; 3. Updating the requirements.txt file to something incompatible and successfully installing on cluster again with updated pinned requirements. However, I'm not sure I'm actually doing this right. I checked that in step 2 I was *not* regenerating any pinned-requirments files, but in step 3 make updated the pinned requirements without me telling it to, I'm guessing because of the wheel's dependence on `PY_FILES` and I changed the source under hail/python. So I don't entirely understand why I have this desirable result.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12510#issuecomment-1338482256:130,depend,dependency,130,https://hail.is,https://github.com/hail-is/hail/pull/12510#issuecomment-1338482256,3,['depend'],"['dependence', 'dependency']"
Integrability,"Besides the question to Jackie, I'd still like a bit of clarity on https://github.com/hail-is/hail/pull/9219#discussion_r469015358 (if you want me to add an entrypoint-override image and test). The question there: is the appropriate place a buildImage step (and dependency for test_hailtop_batch), or in Makefile as you stated (I couldn't see how batch/Makefile was being used by CI, may have missed it)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9219#issuecomment-673080598:262,depend,dependency,262,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-673080598,1,['depend'],['dependency']
Integrability,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682:205,depend,dependency,205,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682,4,"['Rout', 'depend', 'rout']","['Router', 'dependency', 'router']"
Integrability,ByteChannel.java:114) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.ApiaryUnbufferedWritableByteChannel.writeAndClose(ApiaryUnbufferedWritableByteChannel.java:65) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.UnbufferedWritableByteChannelSession$UnbufferedWritableByteChannel.writeAndClose(UnbufferedWritableByteChannelSession.java:40) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.DefaultBufferedWritableByteChannel.close(DefaultBufferedWritableByteChannel.java:166) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.StorageByteChannels$SynchronizedBufferedWritableByteChannel.close(StorageByteChannels.java:119) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.StorageException.wrapIOException(StorageException.java:179) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.BaseStorageWriteChannel.close(BaseStorageWriteChannel.java:84) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$close$2(GoogleStorageFS.scala:310) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:280) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2e,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:11914,Synchroniz,SynchronizedBufferedWritableByteChannel,11914,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['Synchroniz'],['SynchronizedBufferedWritableByteChannel']
Integrability,"CI's tags are annotated, i.e., they have their own datestamp and message. The recent 0.2.128 tag is a lightweight tag, so for example `git describe` of current HEAD still says 0.2.127-87-ge002b4b23; you have to add `--tags` to get 0.2.128-8-ge002b4b23. Not a biggie, but worth adding to a checklist. But yes, eventually, signing them would probably be best practice.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14322#issuecomment-1960288592:65,message,message,65,https://hail.is,https://github.com/hail-is/hail/pull/14322#issuecomment-1960288592,1,['message'],['message']
Integrability,Can this move forward now? The dependency went in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8431#issuecomment-617756822:31,depend,dependency,31,https://hail.is,https://github.com/hail-is/hail/pull/8431#issuecomment-617756822,1,['depend'],['dependency']
Integrability,Can you add a test that drops rsid and lid fields? See message above.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3779#issuecomment-398382290:55,message,message,55,https://hail.is,https://github.com/hail-is/hail/pull/3779#issuecomment-398382290,1,['message'],['message']
Integrability,Can you add the method wrapping and run benchmarks?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10510#issuecomment-867194564:23,wrap,wrapping,23,https://hail.is,https://github.com/hail-is/hail/pull/10510#issuecomment-867194564,1,['wrap'],['wrapping']
Integrability,Can you force push an edit to the commit message that includes a `CHANGELOG:` line indicating that pca and hwe_normalized_pca are now supported in Query-on-Batch?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12457#issuecomment-1314247012:41,message,message,41,https://hail.is,https://github.com/hail-is/hail/pull/12457#issuecomment-1314247012,1,['message'],['message']
Integrability,Can you respond to my concerns in the initial commit message? Specifically some math questions when computing resources and the AsyncWorkerPool usage and the temp variable in the SQL code.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9832#issuecomment-758019254:53,message,message,53,https://hail.is,https://github.com/hail-is/hail/pull/9832#issuecomment-758019254,1,['message'],['message']
Integrability,"Can you take another look, Cotton? This PR:; - sets up the conf as we did in Main (fixes the issues Konrad has had in the last days); - sets up logging properly; - handles exceptions correctly. All new methods that call into scala VDS operations need to wrap in try/except",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1085#issuecomment-261416007:254,wrap,wrap,254,https://hail.is,https://github.com/hail-is/hail/pull/1085#issuecomment-261416007,1,['wrap'],['wrap']
Integrability,"Can you try running this before hail context creation:; ```python; old_popen = subprocess.Popen. def wrapped(*args, **kwargs):; print('args are: ' + str(args)); print('kwargs are: ' + str(kwargs)); return old_popen(*args, **kwargs). subprocess.Popen = wrapped; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319714443:101,wrap,wrapped,101,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319714443,2,['wrap'],['wrapped']
Integrability,"Changed as requested. - files go in per-session directories with names of the form /tmp/hail_XXXXXX. - this eliminates interprocess synchronization; inter-thread synchronization is; by a std::map<std::string, std::mutex> giving a separate mutex coresponding; to each generated module. - it's now ""libhail.so"" and ""libhail.dylib"", without any abi qualification. The prebuilt; libhail.dylib should work on machines compatible with -march=sandybridge and; MacOS back to 10.9. - if you build from source, you can do ""./gradlew nativeLibPrebuilt"" to get; the libraries copied over for packaging into the jar(or copy them by hand, as; previously).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-415067865:132,synchroniz,synchronization,132,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-415067865,2,['synchroniz'],['synchronization']
Integrability,Citation for log4j1 programmatic configuration breaking log4j2: https://logging.apache.org/log4j/2.x/manual/migration.html#limitations-of-the-log4j-1-x-bridge,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941#issuecomment-1524328047:152,bridg,bridge,152,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1524328047,1,['bridg'],['bridge']
Integrability,Closing in favor of PR with new interface.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1650#issuecomment-293427251:32,interface,interface,32,https://hail.is,https://github.com/hail-is/hail/pull/1650#issuecomment-293427251,1,['interface'],['interface']
Integrability,Closing this for now. Didn't realize the bootstrap dependence of not having a letsencrypt config. I don't think we can entirely delete this just yet (but could trivially move it to envoy),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12096#issuecomment-1226092951:51,depend,dependence,51,https://hail.is,https://github.com/hail-is/hail/pull/12096#issuecomment-1226092951,1,['depend'],['dependence']
Integrability,Closing this for now. Want to rethink this interface.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2225#issuecomment-331990417:43,interface,interface,43,https://hail.is,https://github.com/hail-is/hail/pull/2225#issuecomment-331990417,1,['interface'],['interface']
Integrability,"Comments addressed. You asked about scalars so I've rounded that out with support for any combination of scalar and block matrix, as well as unary + and -, testing in notebook along the way. I've marked the class with experimental.rst until I've stabilized the interface with robust testing of all operations in subsequent broadcasting PR. I fixed the process_joins bug as noted, but stopped there in this PR since just switching to select_entries will end up calling the expression machinery twice. The right solution requires simultaneous changes on the Scala side. I'll make a PR to check if `entry_expr` is a field, if not to use `select_entries` to make it one, and then change `MatrixTable.writeBlockMatrix` to take a field rather than an expression. Sound good?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3072#issuecomment-370569833:261,interface,interface,261,https://hail.is,https://github.com/hail-is/hail/pull/3072#issuecomment-370569833,1,['interface'],['interface']
Integrability,Commit message: ; ```; Fixed bug in TextTableReader caused by unsafe ArrayBuilder use. ; Bug occurred for text tables with a number of columns equal to a power of 2; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1127#issuecomment-263676448:7,message,message,7,https://hail.is,https://github.com/hail-is/hail/pull/1127#issuecomment-263676448,1,['message'],['message']
Integrability,"Conceptually, if we're not just interested in missingness, then unboxedGT is useful only in route to nNonRefAlleles (and equal to it if we've split). E.g., the simplest way to extend regression to multi-allelic is to use nNonRefAlleles...though thinking about this further, it's upsettingly asymmetric in the case where the ref allele is the minor allele, so perhaps we should just force deliberate choice of splitting, esp if we're moving toward implementing that less painfully under the hood. Or do regression per alternate allele while maintaining multi-allelic form. (edited). We currently compute nNonRefAlleles from unboxedGT through GTPair, which allocates per genotype. For example, PCA currently requires splitting, but uses nNonRefAlleles. And IBD currently requires splitting but allocates per genotype via:; ```; def countRefs(gtIdx: Int): Int = {; val gt = Genotype.gtPair(gtIdx); indicator(gt.j == 0) + indicator(gt.k == 0); ```. So it may make sense to add `unboxedNNonRefAlleles` that avoids allocation, but doesn't require splitting for these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468:92,rout,route,92,https://hail.is,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468,1,['rout'],['route']
Integrability,Could you please fix the error messages? See above.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3234#issuecomment-376664624:31,message,messages,31,https://hail.is,https://github.com/hail-is/hail/pull/3234#issuecomment-376664624,1,['message'],['messages']
Integrability,"Couple of things:. You left in a bunch of commented out blocks that need to get cleaned up. You need to squash the history. That might be hard. The solution is to merge the current master, create a diff between the resulting version and master, and apply that to a fresh copy of master and then PR against that (or force this branch to the resulting commit). Finally, I'd like you to break out the gradle dependencies as a separate PR first. I need that for the seqr stuff ASAP. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/422#issuecomment-235642374:405,depend,dependencies,405,https://hail.is,https://github.com/hail-is/hail/pull/422#issuecomment-235642374,1,['depend'],['dependencies']
Integrability,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812:18,depend,dependencies,18,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812,1,['depend'],['dependencies']
Integrability,Currently the linear SKAT routine is implemented to be optimal for the case of (genetic variants) k < n (genetic samples). Implementation will process sets of variants associated to the same gene in such a way that there is no redundant computation in the algorithm. . cases handled:; hard call genetic data; dosage genetic data; k << n - (Cannot explicitly form a matrix containing all the genotype data) . ran on chromosome 22 1kgDataset with approximately 100 intervals and the program runs in about 3-4 minutes with 2 workers and 12 pre-emptibles with 8 cores each.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1973#issuecomment-320358482:26,rout,routine,26,https://hail.is,https://github.com/hail-is/hail/pull/1973#issuecomment-320358482,1,['rout'],['routine']
Integrability,"Dan thanks for the comments, some great suggestions. I've addressed some, will get to the rest by Monday. I owe you at least one unit test. You can check the app out at app.hail.is (no SSL yet). Let me know if you have a problem logging in. Currently no one knows the workshop password but me (we can set this to whatever needed), but all team members, besides maybe Dan Goldstein should have access through the normal login. . Login will appear a bit slow because we've decided to not go the popup route, so there's an extra 2 apparent redirects. Also, safari causes some issues if ""Cross-site tracking"" protection is on. A satisfactory solution will be made in time, until then, either another browser, or disable that protection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-460040036:499,rout,route,499,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460040036,1,['rout'],['route']
Integrability,"Dan, I ended up revisiting your output suggestion, after realizing that I could eventually solve the output issue by changing how write_output concatenates identifiers with the prefix. This would be a smaller change than the original thought, which was to allow write_output to treat the output path as a directory and write the identifier unchanged. I still think this would be more what users would expect, but going this route is a bigger deviation so from a project management standpoint less desirable. I think you will now be happy with the code. Should be ready to go modulo the submodule decision. edit: Asana issue made for the output munging issue. If we add a user-definable separator my issue will be solved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-670642110:424,rout,route,424,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-670642110,1,['rout'],['route']
Integrability,"DefaultExecutor.execute(ServerImpl.java:159); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:442); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:408); 	at java.base/java.lang.Thread.run(Thread.java:834). Hail version: 0.2.128-ce3ca9c77507; Error summary: SocketTimeoutException: connect timed out; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/py4j_backend.py"", line 223, in _rpc; raise fatal_error_from_java_error_triplet(; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/backend.py"", line 190, in execute; raise e.maybe_user_error(ir) from None; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/backend.py"", line 190, in execute; raise e.maybe_user_error(ir) from None; File ""/home/edmund/.local/src/hail/hail/python/hail/table.py"", line 2002, in write; Env.backend().execute(; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/test.py"", line 6, in <module>; ht.write('gs://ehigham-hail-tmp/test_hail_in_notebook.ht'); hail.utils.java.FatalError: SocketTimeoutException: connect timed out. Java stack trace:; java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:254); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:11360,wrap,wrapper,11360,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,1,['wrap'],['wrapper']
Integrability,"Dependabot tried to update this pull request, but something went wrong. We're looking into it, but in the meantime you can retry the update by commenting `@dependabot rebase`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12476#issuecomment-1319054705:0,Depend,Dependabot,0,https://hail.is,https://github.com/hail-is/hail/pull/12476#issuecomment-1319054705,10,"['Depend', 'depend']","['Dependabot', 'dependabot']"
Integrability,"Depending on desired PL behavior, there's one last mismatch between the new combiner and the old one. The new combiner doesn't compute `RGQ` at all for reference alleles, I think this is fine.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10777#issuecomment-921329857:0,Depend,Depending,0,https://hail.is,https://github.com/hail-is/hail/pull/10777#issuecomment-921329857,1,['Depend'],['Depending']
Integrability,"Depending on how long the benchmark takes, you might want to make it work harder by setting k to something bigger, like 500 to 1000.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9888#issuecomment-761060616:0,Depend,Depending,0,https://hail.is,https://github.com/hail-is/hail/pull/9888#issuecomment-761060616,1,['Depend'],['Depending']
Integrability,"Depending on which PR of #8844 goes in first, I'll need to update the docs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8850#issuecomment-635432823:0,Depend,Depending,0,https://hail.is,https://github.com/hail-is/hail/pull/8850#issuecomment-635432823,1,['Depend'],['Depending']
Integrability,Depends -- we can extend the HWE aggregator to incorporate sample phenotype information or we can leave it to the user to filter out male samples.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/200#issuecomment-279521097:0,Depend,Depends,0,https://hail.is,https://github.com/hail-is/hail/issues/200#issuecomment-279521097,1,['Depend'],['Depends']
Integrability,Depends on #2063,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2084#issuecomment-321332562:0,Depend,Depends,0,https://hail.is,https://github.com/hail-is/hail/pull/2084#issuecomment-321332562,1,['Depend'],['Depends']
Integrability,Depends on #2111,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2116#issuecomment-322951262:0,Depend,Depends,0,https://hail.is,https://github.com/hail-is/hail/pull/2116#issuecomment-322951262,1,['Depend'],['Depends']
Integrability,Depends on #2551,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2552#issuecomment-350601600:0,Depend,Depends,0,https://hail.is,https://github.com/hail-is/hail/pull/2552#issuecomment-350601600,1,['Depend'],['Depends']
Integrability,"Depends on #2619 (already approved, will go in soon)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2622#issuecomment-353471832:0,Depend,Depends,0,https://hail.is,https://github.com/hail-is/hail/pull/2622#issuecomment-353471832,1,['Depend'],['Depends']
Integrability,Depends on #2657,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2659#issuecomment-355762347:0,Depend,Depends,0,https://hail.is,https://github.com/hail-is/hail/pull/2659#issuecomment-355762347,1,['Depend'],['Depends']
Integrability,Depends on #3245,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3240#issuecomment-376892673:0,Depend,Depends,0,https://hail.is,https://github.com/hail-is/hail/pull/3240#issuecomment-376892673,1,['Depend'],['Depends']
Integrability,Depends on #406,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/439#issuecomment-227984589:0,Depend,Depends,0,https://hail.is,https://github.com/hail-is/hail/issues/439#issuecomment-227984589,1,['Depend'],['Depends']
Integrability,Depends on #5563,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5572#issuecomment-471690561:0,Depend,Depends,0,https://hail.is,https://github.com/hail-is/hail/pull/5572#issuecomment-471690561,1,['Depend'],['Depends']
Integrability,Depends on #5679,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5615#issuecomment-475804671:0,Depend,Depends,0,https://hail.is,https://github.com/hail-is/hail/pull/5615#issuecomment-475804671,1,['Depend'],['Depends']
Integrability,Depends on bug fix in #2620,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2591#issuecomment-353443048:0,Depend,Depends,0,https://hail.is,https://github.com/hail-is/hail/pull/2591#issuecomment-353443048,1,['Depend'],['Depends']
Integrability,"Depends on the partitioning, of course. Currently, I think Chris said ~1m at 20K partitions. I think we should have ~4K, in which case this overhead of this step would be 6%. There's a problem that we want courser partitioning earlier in the merge process, and finer later, but we don't have the ability to repartition dynamically. Note, this isn't 100% overhead, because there was a previous step to compute and collect the partitioning information which is now gone. If that's problematic there are additional ways to speed things up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5596#issuecomment-473367050:0,Depend,Depends,0,https://hail.is,https://github.com/hail-is/hail/pull/5596#issuecomment-473367050,1,['Depend'],['Depends']
Integrability,"Did my best, the behavior of this lowering functionality is complex, and it's hard to come up with a universal solution. The current issue I'm struggling with is the failure it testArrayAggContexts, which finds a ToArray(StreamRange()) being passed to EmitStream, instead of a StreamRange. TLDR: my ArrayAgg rule is stupid and fucked. Options: 1) Fix this rule, 2) (Seems not as good) In Emit have ArrayAgg needs to pass its child through Emit.emit a second time to match on ToArray, 3) make unstreamify more specific, such that ArrayAgg is allowed to take streams directly. ToArray definitely needs to wrap StreamRange in some cases (for instance MakeTuple(ToArray(StreamRange)), else get issues with the stream passed to SRVB. it would be helpful to have the intended (but currently applicable) design of stream/array semantics written down for all nodes (maybe it exists, I'll dig through design docs). Besides this 2 more failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-583795904:603,wrap,wrap,603,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583795904,1,['wrap'],['wrap']
Integrability,"Discussion in a different forum sounds good. This came up with the GPU branch as there was a question on whether we can trust what we are parsing as a resource or did we need to hardcode the SKUs explicitly. This was my proposed solution for us at least knowing that something has changed with lots of driver error messages and to not try and do any updates if the invariants of the ""SKU"" don't hold.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13607#issuecomment-1736209579:315,message,messages,315,https://hail.is,https://github.com/hail-is/hail/pull/13607#issuecomment-1736209579,1,['message'],['messages']
Integrability,"Disregard the message above. The auto increment would not work. Now the critical check to make sure no duplicates are added is this line:. ```python3; job_id = parameters.get('job_id'); has_record = await db.jobs.has_record(batch_id, job_id); if has_record:; log.info(f""database has record for ({batch_id}, {job_id})""); abort(400, f'invalid request: batch {batch_id} already has a job_id={job_id}'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6238#issuecomment-498043583:14,message,message,14,https://hail.is,https://github.com/hail-is/hail/pull/6238#issuecomment-498043583,1,['message'],['message']
Integrability,"Disregard, makes sense because two of our dependencies might specify a shared dependency which is not directly our dependency. That dependency will be resolved in some manner and we want to track exactly what it was resolved to. This actually seems like the right thing to do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-456603534:42,depend,dependencies,42,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-456603534,4,['depend'],"['dependencies', 'dependency']"
Integrability,"Do you have an error message other than that failed post? I'm not seeing why that would stop a deploy. In #13115, the PR healing code is after the `_heal_deploy` code, so I don't know why an exception when healing a PR would stop a deploy from occurring. This POST error is also occurring on GCP. Definitely something to fix but I'm not sure why it's related. > This caused problems because the next merge candidates CI was selecting was causing bad GitHub rate limit requests for exceeding the number of statuses. So it kept retrying that same merge candidate. Unfortunately I'm not sure if this is relevant in Azure. Azure CI thinks about merge candidate when it comes to testing PRs, but it doesn't merge any PRs and whether or not it does a deploy just depends if there's a new commit on `main`, it shouldn't have to do with PRs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1561903065:21,message,message,21,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1561903065,2,"['depend', 'message']","['depends', 'message']"
Integrability,"Do you think we should add 1-2 more states which are 'any_failure', 'all_failed' and change the others to 'any_success' and 'all_succeeded'? I was thinking it might be nice to have onError jobs with that dependency explicitly in the DAG.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12365#issuecomment-1289341106:204,depend,dependency,204,https://hail.is,https://github.com/hail-is/hail/pull/12365#issuecomment-1289341106,1,['depend'],['dependency']
Integrability,Do you want me to adopt this and make sure it doesn't break anything / see if the error messages disappear?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13546#issuecomment-1710276555:88,message,messages,88,https://hail.is,https://github.com/hail-is/hail/pull/13546#issuecomment-1710276555,1,['message'],['messages']
Integrability,Docs and interface looks good to me!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2701#issuecomment-357036272:9,interface,interface,9,https://hail.is,https://github.com/hail-is/hail/pull/2701#issuecomment-357036272,1,['interface'],['interface']
Integrability,Does it have to be in `Gi` not `G`? Otherwise interface looks good (don't know much about batch internals so can't comment on those),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6215#issuecomment-497099235:46,interface,interface,46,https://hail.is,https://github.com/hail-is/hail/pull/6215#issuecomment-497099235,1,['interface'],['interface']
Integrability,Does this need to have the service with the router like all the other services?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9681#issuecomment-722651702:44,rout,router,44,https://hail.is,https://github.com/hail-is/hail/pull/9681#issuecomment-722651702,1,['rout'],['router']
Integrability,"Done!. Thanks Tim!. On Wed, Feb 1, 2017 at 8:24 AM, Tim Poterba <notifications@github.com>; wrote:. > *@tpoterba* commented on this pull request.; >; > Need just one tiny change to the py/j connector. Looks great!; > ------------------------------; >; > In python/hail/dataset.py; > <https://github.com/hail-is/hail/pull/1324#pullrequestreview-19546823>:; >; > > @@ -2336,6 +2336,22 @@ def mendel_errors(self, output, fam):; > pargs = ['mendelerrors', '-o', output, '-f', fam]; > self.hc._run_command(self, pargs); >; > + def min_rep(self):; > + """"""; > + Gives minimal, left-aligned representation of alleles. Note that this can change the variant position.; > +; > + ** Examples **; > + 1) Simple trimming of a multi-allelic site, no change in variant position; > + `1:10000:TAA:TAA,AA` => `1:10000:TA:T,A`; > +; > + 2) Trimming of a bi-allelic site leading to a change in position; > + `1:10000:AATAA,AAGAA` => `1:10002:T:G`; > +; > + """"""; > + jvds = self._jvds.minrep(); >; > add in the try: / except: here, following the other methods in dataset.py.; >; > The default py4j errors look horrible, so calling our wrapper method helps; > a lot.; >; > ; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/1324#pullrequestreview-19546823>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ADVxgaQoXMxYMPE_V-RMRgYp5mvNSf-Pks5rYIePgaJpZM4LzbBv>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1324#issuecomment-276663883:1114,wrap,wrapper,1114,https://hail.is,https://github.com/hail-is/hail/pull/1324#issuecomment-276663883,1,['wrap'],['wrapper']
Integrability,"EDIT: The problem in this message is still valid, but the fix described here was replaced following the conversation with Tim. Ok, second pruning bug fixed. This commit illustrates the change, probably worth looking at in isolation: https://github.com/hail-is/hail/pull/9578/commits/f4ce01d59a56bef7496490145e66d274f51139e6. Essentially, the problem was that we were rebuilding an `InsertFields(child, newFields, _)` node with a requested type of empty struct. The old rebuilding strategy handled this by recursively rebuilding the `child` node, then filtering `newFields` to include only those in the requested type. However, if the `child` node can't be filtered (say it's a `Ref` of a certain type), and one of the fields being inserted is overwriting a field in the child, you need to make sure you do the overwriting so the typing works out. Happy to explain more if this is unclear. I also added an optimization to just drop the whole `InsertFields` if the requested type is the empty struct, not sure if that's good pruning manners though. . CC @tpoterba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9578#issuecomment-708568970:26,message,message,26,https://hail.is,https://github.com/hail-is/hail/pull/9578#issuecomment-708568970,1,['message'],['message']
Integrability,"Erm. There's an interface issue here. I need an output stream that can tell me the file position, but I don't know of any standard library interface that exposes that, so I just hardcoded the `FSDataOutputStream`. Is this going to be a problem? Are all `FS` implementations Hadoop file systems? cc: @akotlar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6333#issuecomment-501732176:16,interface,interface,16,https://hail.is,https://github.com/hail-is/hail/pull/6333#issuecomment-501732176,2,['interface'],['interface']
Integrability,"Err, not sure what happened, but I noticed we removed refresh_k8s_state route, so I think we need to get rid of the call in the api (since the route doesn't exist). ```py; def refresh_k8s_state(self, url):; self.post(f'{url}/refresh_k8s_state', json_response=False); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-483833068:72,rout,route,72,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483833068,2,['rout'],['route']
Integrability,"Error message starting up Batch:. ```; raise ApiException(http_resp=r); kubernetes.client.rest.ApiException: (403); Reason: Forbidden; HTTP response headers: HTTPHeaderDict({'Audit-Id': 'fc886821-4fc7-4697-b8d2-a4bc656b45f6', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'Date': 'Thu, 25 Apr 2019 22:18:26 GMT', 'Content-Length': '252'}); HTTP response body: b'{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""pods is forbidden: User \\""system:serviceaccount:batch-pods:default\\"" cannot watch pods in the namespace \\""test\\"""",""reason"":""Forbidden"",""details"":{""kind"":""pods""},""code"":403}\n'; ```. Going to retest, but it seems like this was a one time thing...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5956#issuecomment-487075273:6,message,message,6,https://hail.is,https://github.com/hail-is/hail/pull/5956#issuecomment-487075273,2,['message'],['message']
Integrability,"Error:; ```; Hail version: 0.2.26-5e79257ea31c; Error summary: GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Bucket is requester pays bucket but no user project provided."",; ""reason"" : ""required""; } ],; ""message"" : ""Bucket is requester pays bucket but no user project provided.""; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7433#issuecomment-548909738:166,message,message,166,https://hail.is,https://github.com/hail-is/hail/issues/7433#issuecomment-548909738,2,['message'],['message']
Integrability,Even better: append a d (or multiple ds to make it unique) to duplicate IDs and generate a message that it happened.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/862#issuecomment-319509326:91,message,message,91,https://hail.is,https://github.com/hail-is/hail/issues/862#issuecomment-319509326,1,['message'],['message']
Integrability,Ew8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$' | tr '\n' '\0' | xargs -0 python3 -m pip install -U; Defaulting to user installation because normal site-packages is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:32590,depend,dependencies,32590,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,2,['depend'],['dependencies']
Integrability,"Exciting, first code-related pull request review! It seems correct. I was wondering how you're testing table.py, backend.py, java.py, and would it be worthwhile to write unit or integration tests for these sections? I'd be happy to work on that if desired.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5019#issuecomment-449156617:178,integrat,integration,178,https://hail.is,https://github.com/hail-is/hail/pull/5019#issuecomment-449156617,1,['integrat'],['integration']
Integrability,"FWIW it does go into the container logs which is how I've always pulled out the true error, but I'm not sure how to get that on every system. For future reference, there's an even more pernicious issue, which is that when running VEP with `-o STDOUT` it actually suppresses certain error messages too - and there's not much you can do about that unless you actually go in and run VEP manually without that, in the environment that hail uses.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8146#issuecomment-591009197:288,message,messages,288,https://hail.is,https://github.com/hail-is/hail/issues/8146#issuecomment-591009197,1,['message'],['messages']
Integrability,"FYI, includes missing dependencies for batch_deploy and ci_deploy on create_accounts that can cause race condition failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6193#issuecomment-496358460:22,depend,dependencies,22,https://hail.is,https://github.com/hail-is/hail/pull/6193#issuecomment-496358460,1,['depend'],['dependencies']
Integrability,"FYI, there's a kubernetes_async that I've used twice now, copies the interface of the official library and works like a charm: https://github.com/tomplus/kubernetes_asyncio.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-483852673:69,interface,interface,69,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483852673,1,['interface'],['interface']
Integrability,Feel free to modify the terminology to something that makes sense to you. I agree I had a hard time figuring out what the correct interface should be.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12078#issuecomment-1206732116:130,interface,interface,130,https://hail.is,https://github.com/hail-is/hail/pull/12078#issuecomment-1206732116,1,['interface'],['interface']
Integrability,"Finally working! Ugh, that was painful. Changes I made since I closed:; - You can't broadcast an object which has a reference to its own broadcast (e.g. ReferenceGenome => locusType => rgBc). I made locusType transient and recompute after serialization.; - Removed BroadcastSerializable. I can't figure out how to check ReferenceGenome/RVDPartitioner are only serialized during partitioning. This is basically a failure of the Kryo interface. I might try again sometime when I'm feeling beat down by serialization.; - Removed removeReference. This just isn't something we can support (except in isolated situations like tests, and I fixed those.) Now, if you add a reference, it only throws an error if an existing reference exists by that name and is incompatible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5512#issuecomment-473088362:432,interface,interface,432,https://hail.is,https://github.com/hail-is/hail/pull/5512#issuecomment-473088362,1,['interface'],['interface']
Integrability,"First, @cristinaluengoagullo, thank you for your contribution! This is awesome. Second, I think this PR will need a little work before it can go in. Let me describe the situation:. We have now stopped work on Hail 0.1 and are now making only critical bug fixes. I think we can accept small feature additions, but we're optimizing for stability over features now. All new development has moved to master/0.2 beta. If you do make changes to 0.1, they should be forward ported to 0.2 if you want them to be carried forward. In addition, there are two problems with your PR:. 1. It is quite large. We prefer contributions to be single conceptual units. For example, a change to VEP should be separate from additions to the function registry. 2. The diff is somewhat confusing and I'm not 100% sure what is going on. It appears to include a large number of our own changes, it looks like from this commit: https://github.com/hail-is/hail/pull/3172/commits/e6f0b7f3a854f0fd64857876ab04375e570ba09f. However, given that the commit is under your name with a new commit message, I can't tell where those changes originated. Also, at least some (all?) of those changes already appear in 0.1, so I'm not sure why Github is displaying them, for example: https://github.com/hail-is/hail/pull/3172/files#diff-f11d07953ac5cd8bd8d4d3fd135a3efbR11. I think squashing your changes (and just your changes) and rebasing them onto the current 0.1 HEAD will fix the problem. Then we can take a closer look at the changes. Finally, it looks like your changed the VEP schema because you're invoking it with extra plugins. I think that's a no-go for us, but you could modify the VEP command (through an argument or in the properties file) to specify an alternate schema.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3172#issuecomment-377559347:1061,message,message,1061,https://hail.is,https://github.com/hail-is/hail/pull/3172#issuecomment-377559347,1,['message'],['message']
Integrability,"Fixed error message in 4d38cca, new issue supercedes in #376",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/361#issuecomment-218033798:12,message,message,12,https://hail.is,https://github.com/hail-is/hail/issues/361#issuecomment-218033798,1,['message'],['message']
Integrability,"For intervals, a lot of the usage is wrapped up in ordering. The lift of ordering to CodeBuilder/PCode is another large project all on its own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8365#issuecomment-605101201:37,wrap,wrapped,37,https://hail.is,https://github.com/hail-is/hail/pull/8365#issuecomment-605101201,1,['wrap'],['wrapped']
Integrability,"For now, I'm going to add them by hand. Later there will be an admin interface for creating/deleting/editing billing projects.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7596#issuecomment-560198758:69,interface,interface,69,https://hail.is,https://github.com/hail-is/hail/pull/7596#issuecomment-560198758,1,['interface'],['interface']
Integrability,"For your consideration We often have a `pathlib.Path` or `cloudpathlib.CloudPath` that we've built up by parts, which is then the path to be used as an input resource:. ```python; res = mybatch.read_input(str(mycloudpath)); ```. Periodically we accidentally omit the `str()`, which leads to a semi-obscure error message and an extra editing round-trip. There is a point of view that `read_input()` and `read_input_group()` could also accept `os.PathLike` objects directly, and have Hail convert them to `str` itself, e.g. in `_new_input_resource_file()` which underlies both methods, as per this PR. The difficulty is how to do that conversion: `str()` does the trick for [`pathlib.Path`](https://docs.python.org/3.12/library/pathlib.html#operators) and [`cloudpathlib.CloudPath`](https://cloudpathlib.drivendata.org/stable/api-reference/cloudpath/), returning the path and URL, respectively, as a string. But it looks like in theory there might be [`os.PathLike`](https://docs.python.org/3/library/os.html#os.PathLike) subclasses that don't define `__str__()` to produce a usable path/URL. The official conversion method appears to be [`os.fspath()`](https://docs.python.org/3/library/os.html#os.fspath), but that does not do the right thing for `cloudpath.CloudPath`  there it downloads the remote file and returns a local path  which is not at all what Hail needs. However probably this is a theoretical concern and `str()` will be fine",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14544#issuecomment-2105616965:314,message,message,314,https://hail.is,https://github.com/hail-is/hail/pull/14544#issuecomment-2105616965,1,['message'],['message']
Integrability,"From a fresh clone, the above (modified with `rm -f`) fails with: ; ```bash; $ rm -f hail/upload-remote-test-resources && make -C hail upload-remote-test-resources; make: Entering directory '/home/edmund/.local/src/hail/hail'; # # If hailtop.aiotools.copy gives you trouble:; # gcloud storage cp -r src/test/resources/\* gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/; # gcloud storage cp -r python/hail/docs/data/\* gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/; python3 -m hailtop.aiotools.copy -vvv 'null' '[\; {""from"":""src/test/resources"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/""},\; {""from"":""python/hail/docs/data"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/""}\; ]' --timeout 600; /home/edmund/.local/src/hail/.venv/bin/python3: Error while finding module specification for 'hailtop.aiotools.copy' (ModuleNotFoundError: No module named 'hailtop'); make: *** [Makefile:355: upload-remote-test-resources] Error 1; make: Leaving directory '/home/edmund/.local/src/hail/hail'; ```. I'll try again with `hailtop` installed - just wanted to point out the dependency failure in `Makefile`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1887719777:1141,depend,dependency,1141,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1887719777,1,['depend'],['dependency']
Integrability,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:750,integrat,integration,750,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665,1,['integrat'],['integration']
Integrability,"Gah, OK, I think I have it now, but there was one more detail:. The gradle configuration `testCompileOnly` [1] *does not* inherit from the `shadow` configuration (as evidence see [this search](https://github.com/search?q=repo%3Ajohnrengelman%2Fshadow%20extendsFrom&type=code) of the shadow repo). We must explicitly request that `shadow` dependencies are included in the compile-time class path of the tests. This is as it should be: the things in `shadow` are things which are provided to us by our runtime environment. That's true of both the *test* runtime environment and the normal runtime environment. The Gradle Shadow plugin takes a different perspective by default, it suggests that `shadow` dependencies shouldn't be used in the tests at all. [1] NB: `testCompile` does not exist but you don't get an error if you try to use it, thanks for nothing gradle.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1710414563:338,depend,dependencies,338,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1710414563,2,['depend'],['dependencies']
Integrability,Ghost does not respect the `X-Forwarded` headers. It should not have a `url` parameter but a `pathPrefix` and the protocol should be set from `X-Forwarded-Proto`. Thanks to this design bug we cannot test connectivity to the blog in PRs until the internal gateway is configured to use TLS. I'll revisit this PR when that happens.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8117#issuecomment-589741905:114,protocol,protocol,114,https://hail.is,https://github.com/hail-is/hail/pull/8117#issuecomment-589741905,1,['protocol'],['protocol']
Integrability,Git has special support for coauthors in the commit message https://docs.github.com/en/pull-requests/committing-changes-to-your-project/creating-and-editing-commits/creating-a-commit-with-multiple-authors,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11750#issuecomment-1104096542:52,message,message,52,https://hail.is,https://github.com/hail-is/hail/pull/11750#issuecomment-1104096542,1,['message'],['message']
Integrability,"Given the breaking changes to interface, I'll make a discuss post and alert in various channels when merging. @liameabbott this may significantly improve performance when you pass a field as `x` in regression. I'll follow up directly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3289#issuecomment-378452971:30,interface,interface,30,https://hail.is,https://github.com/hail-is/hail/pull/3289#issuecomment-378452971,1,['interface'],['interface']
Integrability,Going the small PRs route.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3301#issuecomment-379276463:20,rout,route,20,https://hail.is,https://github.com/hail-is/hail/pull/3301#issuecomment-379276463,1,['rout'],['route']
Integrability,"Going to close this since we no longer have Command Line interface and Hail has changed considerably. If you still have problems @lv-xy11 , please chat with us on Gitter here https://gitter.im/hail-is/hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/683#issuecomment-296671361:57,interface,interface,57,https://hail.is,https://github.com/hail-is/hail/issues/683#issuecomment-296671361,1,['interface'],['interface']
Integrability,"Good catch on the dependency issue, that should be fixed now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1889786055:18,depend,dependency,18,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1889786055,1,['depend'],['dependency']
Integrability,"Good point @johnc1231, it would be nice to have the same interface for `head`/`tail` on tables and arrays. Instead of adding `tail`, should we add `first`/`last` and deprecate `head`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9474#issuecomment-694826372:57,interface,interface,57,https://hail.is,https://github.com/hail-is/hail/pull/9474#issuecomment-694826372,1,['interface'],['interface']
Integrability,Good point. In my branch I've written wrappers for the LAPACK methods that take SNDArrays and determine the leading dimension arguments. I can pull those into this PR and replace all the LAPACK calls with the wrappers.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10614#issuecomment-867766626:38,wrap,wrappers,38,https://hail.is,https://github.com/hail-is/hail/pull/10614#issuecomment-867766626,2,['wrap'],['wrappers']
Integrability,Good work hunting this down; these messages are always a pain to find.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10713#issuecomment-891277817:35,message,messages,35,https://hail.is,https://github.com/hail-is/hail/pull/10713#issuecomment-891277817,1,['message'],['messages']
Integrability,Got it. +1 for a more specific error message if possible.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760#issuecomment-397472784:37,message,message,37,https://hail.is,https://github.com/hail-is/hail/issues/3760#issuecomment-397472784,1,['message'],['message']
Integrability,"Got rid of @handle_py4j, as well, which removes our dependence on decorator ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190661:52,depend,dependence,52,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190661,1,['depend'],['dependence']
Integrability,"Got rid of copyStreamable, and set getNestedElementPTypesOfSameType to use concrete constructors as we discussed at lunch. The child ptypes (elementType, fields) are always cast to the interface, not the concrete class, since this is the correct behavior in our restriction hierarchy (upcast to canonical). This is obviously still not quite right (need to check whether anything canonical, not just head), but that's outside the scope of this pr.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8126#issuecomment-589390319:185,interface,interface,185,https://hail.is,https://github.com/hail-is/hail/pull/8126#issuecomment-589390319,1,['interface'],['interface']
Integrability,"Gotcha, thanks, the trouble with reading your reply on the go. I had assumed you contributed to both from your reply. Glad to hear you're taking aiohttp performance seriously. Regarding flow-control, yep, that pr was merged https://github.com/huge-success/sanic/pull/1179. I haven't seen any further conversations from you or Sanic devs on the issue.; * It's not just on master, it's in 0.8. Part of my concerns over Sanic came from reading your post @ https://www.reddit.com/r/Python/comments/876msl/sanic_python_web_server_thats_written_to_die_fast/ ; this now seems outdated, and it would be interesting to hear the reply of a Sanic contributor. Re: ""The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :)""; - Surely simple (typical-use, i.e /path or /path/<param>) pattern matching isn't a large performance constraint. I would be surprised if this were a bottleneck in either Sanic or aiohttp.; - If aiohttp is bottlenecked by this, and isn't caching matches, why not? The most common route is say /. Edit: To be clear, the bench mentioned above used 1 worker for Sanic and aiohttp, both using uvloop. Bench attached. [bench.zip](https://github.com/hail-is/hail/files/2841473/bench.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461455096:696,rout,router,696,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461455096,2,['rout'],"['route', 'router']"
Integrability,"Great question. The need for two clients is we have an asynchronous one that ci uses and a synchronous one that pipeline uses. The client code in `aioclient.py` is the asynchronous one and the code in `client.py` is for the synchronous one. Rather than duplicating the code as was done before, I either had to make the synchronous client use the asynchronous code or vice versa. It was easier to make the asynchronous code synchronous by using the `run_until_complete` function and wrapping calls to the asynchronous classes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6208#issuecomment-497540673:482,wrap,wrapping,482,https://hail.is,https://github.com/hail-is/hail/pull/6208#issuecomment-497540673,1,['wrap'],['wrapping']
Integrability,"Great! I think we are good on the client. I'll modify it later today to use the new scheme. Before we get into the nitty gritty of the actual implementation, can we move on to the UI components and the semantics of using the client in `test_batch.py`? There's also a change to how the batch fields `time_closed` and `time_created` are used. I also added `time_updated`. I think the new semantics are:. - time_created -- time the batch was created; - time_updated -- time the last update was committed; - time_completed -- time the last time n_completed == n_jobs regardless of whether there are outstanding updates that haven't been committed. For old batches:; - time_created => same; - time_closed => time_updated; - time_completed => same. I also changed what the batch state means:; > There are only two batch states in the database: running and complete. A batch starts out as complete until an update is committed at which point if the n_jobs > 0, it will change to running. There are no longer ""open"" batches. . It's possible I didn't actually implement exactly what I described above as I was having a hard time figuring out whether time_updated should be equal to time_completed if the batch has no outstanding jobs to run. That's why I'd like to take a step back and make sure we agree on the interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1218125677:1303,interface,interface,1303,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1218125677,1,['interface'],['interface']
Integrability,Great! I think we should run the integration tests as part of `testAll` and kill the integration tests in the CI.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1827#issuecomment-302116530:33,integrat,integration,33,https://hail.is,https://github.com/hail-is/hail/pull/1827#issuecomment-302116530,2,['integrat'],['integration']
Integrability,"Great! I will test it out on our cluster. First, I have question on the; Spark version that is recommended. At the very top of the *Getting Started; with Python API*, the document indicates the latest version of Spark 2; should be used. But later on under the *Running on a Spark cluster and in; the cloud section,* it indicates only Spark 1.5 and 1.6 are supported.; Which version would be the best to use? Or does it really depend on whether; it is run locally or on a cluster?. On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API:; >; > https://hail.is/pyhail/getting_started.html; >; > Please give it a spin and let us know if you run into any problems. The; > documentation for the python API is nearly complete, but the Tutorial and; > General Reference section are still being ported to python and will need; > another week or so. Thanks for your patience!; >; > ; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/1218#issuecomment-272357689>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AB3rDZjYATb82CmTNeP61RpKxMCFMhInks5rRvvYgaJpZM4La8Pf>; > .; >. -- ; John Farrell, Ph.D.; Biomedical Genetics-Evans 218; Boston University Medical School; 72 East Concord Street; Boston, MA. ph: 617-638-5491",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1218#issuecomment-272494749:426,depend,depend,426,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272494749,1,['depend'],['depend']
Integrability,"Grr. I realized that our build.yaml `build_hail` step did not explicitly depend on setting up the python package correctly, it just relied on `make jars` doing that as a side effect. I've added an explicit rule for setting up the version files and I call it in build.yaml. The better answer is to change build.yaml to ship around a wheel file. I don't want to do this right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6923#issuecomment-524017142:73,depend,depend,73,https://hail.is,https://github.com/hail-is/hail/pull/6923#issuecomment-524017142,1,['depend'],['depend']
Integrability,"Ha, no, was just curious as to where the interface settled. Not a show-stopper for me at the moment (though I'd probably use it later this week)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4004#issuecomment-408987889:41,interface,interface,41,https://hail.is,https://github.com/hail-is/hail/pull/4004#issuecomment-408987889,1,['interface'],['interface']
Integrability,"Ha, you THINK this would work dependabot you've never sparred with spark",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12450#issuecomment-1310869486:30,depend,dependabot,30,https://hail.is,https://github.com/hail-is/hail/pull/12450#issuecomment-1310869486,1,['depend'],['dependabot']
Integrability,"Had an idea about an easier way to fix this: don't support open terms in interpret, compile, etc., and then wrap the open term in an ArrayAgg to make it closed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5772#issuecomment-480308468:108,wrap,wrap,108,https://hail.is,https://github.com/hail-is/hail/pull/5772#issuecomment-480308468,1,['wrap'],['wrap']
Integrability,"Hail depends on the `decorator` module, and in this case it looks like you've got it but it's out of date. The following should fix it:. ```bash; pip install -U decorator; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1818#issuecomment-302064968:5,depend,depends,5,https://hail.is,https://github.com/hail-is/hail/issues/1818#issuecomment-302064968,1,['depend'],['depends']
Integrability,"Hail does not support heterogeneous arrays: found list with elements of types [dtype('int32'), dtype('str')] . The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 584, in arg_check; return checker.check(arg, function_name, arg_name); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_typecheck.py"", line 80, in check; raise TypecheckFailure from e; hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in <module>; cli.main(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main; run(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file; runpy.run_path(target, run_name=""__main__""); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path; return _run_module_code(code, init_globals, run_name,; File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code; _run_code(code, mod_globals, init_globals,; File ""/home/edmund/.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:3044,adapter,adapter,3044,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['adapter'],['adapter']
Integrability,"Hail tries to do a lot of data integrity checks and warn the user about problems. We've found a number of bugs in upstream tools and workflows that were arguably incorrect. But generating warnings when importing a 2TB file is a challenge in Spark. Right now we use Spark's Accumulators to accumulate classes of error messages and write them out at the end of the pipeline run (see the VCFReport object). However, we use them in non-actions and get incorrect reports (due to job restarts or reused stages in the pipeline). I have an idea about how to fix this by accumulating only at the end of a successful mapPartitions operation and recording the stageId and taskAttemptId from the TaskContext. The accumulator should only accumulate one of the reports from a successful mapPartitions. Using this, I wanted to build an abstraction for reporting warnings and other messages reliably on large import steps. If this works, we plan to float it up to the Spark mailing list to see if it can be of use, or at least write a nice blog post explaining how to get reliable accumulators in Spark. See the discussion here for the current situation:. http://stackoverflow.com/questions/29494452/when-are-accumulators-truly-reliable. Closed as won't fix:. https://issues.apache.org/jira/browse/SPARK-732. Of course, I might be missing something obvious and this won't work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/371#issuecomment-240550289:317,message,messages,317,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-240550289,2,['message'],['messages']
Integrability,Hail2 python interface makes this a lot more consistent,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1509#issuecomment-349717645:13,interface,interface,13,https://hail.is,https://github.com/hail-is/hail/issues/1509#issuecomment-349717645,1,['interface'],['interface']
Integrability,"Hail2 python interface makes this impossible, so no longer worrying about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/634#issuecomment-349719025:13,interface,interface,13,https://hail.is,https://github.com/hail-is/hail/issues/634#issuecomment-349719025,1,['interface'],['interface']
Integrability,"Hello, I am having similar problems. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements ; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; ` hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz')`; `py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1`; Any help? Best, Zillur",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6299#issuecomment-515502374:337,protocol,protocol,337,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-515502374,1,['protocol'],['protocol']
Integrability,"Here's a link with an absolute time window: https://cloudlogging.app.goo.gl/gXAWZpZtUiV8jphXA. This is the assertion's stack trace:; ```; at scala.Predef$.assert(Predef.scala:208); at is.hail.QoBOutputStreamManager.createOutputStream(QoBAppender.scala:38); at org.apache.logging.log4j.core.appender.OutputStreamManager.getOutputStream(OutputStreamManager.java:165); at org.apache.logging.log4j.core.appender.OutputStreamManager.writeToDestination(OutputStreamManager.java:250); at org.apache.logging.log4j.core.appender.OutputStreamManager.flushBuffer(OutputStreamManager.java:283); at org.apache.logging.log4j.core.appender.OutputStreamManager.flush(OutputStreamManager.java:294); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.directEncodeEvent(AbstractOutputStreamAppender.java:217); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.tryAppend(AbstractOutputStreamAppender.java:208); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:199); at org.apache.logging.log4j.core.config.AppenderControl.tryCallAppender(AppenderControl.java:161); ```. And the line of our code that triggers the logger appender:; ```; is.hail.JVMEntryway$2.run(JVMEntryway.java:139); ```. On that line, we should have already evaluated line 97:; ```; QoBOutputStreamManager.changeFileInAllAppenders(logFile);; ```; Which updates the filename for all `QoBOutputStreamManager`s. We should be the only ones allocating `QoBOutputStreamManager` (it has no magic annotations, we don't pass its constructor anywhere). We should only allocate `QoBOutputStreamManager` in its associated object. We always put it into the map in `getInstance`. We don't synchronize the other methods though, so that could be the issue? If we have a stale version of that map?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13242#issuecomment-1703383030:1728,synchroniz,synchronize,1728,https://hail.is,https://github.com/hail-is/hail/issues/13242#issuecomment-1703383030,1,['synchroniz'],['synchronize']
Integrability,"Here's my proposed interface (names to be changed, I'm terrible at those). ```; case class WithSource[T](value: T, source: InputSource) {; def map[U](f: T => U): WithSource[U] = {; try {; copy[U](value = f(value)); } catch {; case e: Exception => source.wrapError(e); }; }; }. abstract class InputSource {; def wrapError(e: Exception): Nothing; }. case class TextSource(line: String, file: String, position: Option[Int]) extends InputSource {; def wrapError(e: Exception): Nothing = {; val msg = e match {; case _: FatalException => e.getMessage; case _ => s""caught $e""; }; val lineToPrint =; if (line.length > 62); line.take(59) + ""...""; else; line. log.error(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $line"""""".stripMargin); fatal(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $lineToPrint"""""".stripMargin); }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233012302:19,interface,interface,19,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233012302,4,"['interface', 'wrap']","['interface', 'wrapError']"
Integrability,"Hey @alanmejiamaza, could you reply to my message above? I can't replicate the issue without more information.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12717#issuecomment-1458661804:42,message,message,42,https://hail.is,https://github.com/hail-is/hail/issues/12717#issuecomment-1458661804,1,['message'],['message']
Integrability,"Hey @anh151 !. I'm sorry you're having trouble with Hail. The message ""Container killed on request. Exit code is 137"" comes from Apache Spark, our underlying distributed compute framework. It indicates that the worker machines have insufficient RAM. In general, using worker machines with higher RAM-to-core ratios will help. If you're on GCP, try the n1-highmem family. Some other suggestions:. 1. Hail is a ""lazy"" system. Your entire pipeline is executed, from the beginning, when you run ""export"" or ""write"". That means that Hail has to do all of that work at once. You can ease the memory pressure by performing less operations at once, by writing an intermediate file (and reading back in and proceeding with it).; 2. We recommend against directly exporting from a complex operation (like group-by-aggregate). Instead, grab the cols table and write it to Hail's fast, binary, parallel format: `.cols().select('field_of_interest').write('my-cols.ht')`. Then read that table and export that: `hl.read_table('my-cols.ht').field_of_interest.export(...)`. Exporting to a text file requires more memory because we have to construct ASCII strings.; 3. Always use a compressed export: `.export('foo.tsv.bgz')` or `hl.export_vcf(..., 'foo.vcf.bgz')`. This won't help your memory problem, but you can avoid parsing strings to create loci by constructing an `hl.Locus` which is the Python-side representation of loci (`hl.locus` is the inside-Hail representation):; ```python3; def create_intervals(data):; return [; hl.Locus(chromosome, start, reference_genome=""GRCh38""); for i, (chromosome, start) in data[[""CHROM"", ""POS""]].iterrows(); ]; ```. Please reply here if you're still having problems after incorporating the above suggestions as it may indicate a more fundamental issue with Hail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1674035485:62,message,message,62,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1674035485,1,['message'],['message']
Integrability,"Hey @cosi1!. We are actively moving away from Scala and the JVM. We don't plan to support a JVM interface to Hail. As a result, I think. the answer to this issue is that this is a known limitation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10882#issuecomment-1145240358:96,interface,interface,96,https://hail.is,https://github.com/hail-is/hail/issues/10882#issuecomment-1145240358,1,['interface'],['interface']
Integrability,"Hey @illusional ! Sorry for the massive latency. OK, so, big apologies are in order, I totally lead you astray by mentioning the makefile. The Makefile *is* the source of hail version truth, but invoking the makefile inside an image build step feels wrong to me. Each step creates a layer which inflates the image sizes. Hail's images are already too big!. I took your commits and added one of my own that snags the version from the `copy_files` build step. Because I wanted `service_base_image` to depend on `copy_files`, I had to move the whole step after `copy_files`. This is a limitation in build.yaml: a step must appear *after* steps on which it depends. I also had to move `check_services` for the same reason: it depends on `service_base_image`. File dependencies in build.yaml work like this:; 1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401:499,depend,depend,499,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401,4,['depend'],"['depend', 'dependencies', 'depends']"
Integrability,"Hey @seanjosephjurgens !. Sorry you're running into trouble. This error message is bad. See https://github.com/hail-is/hail/issues/13346 for that bug. The real issue here is VCF INFO fields like:; ```; AS_BaseQRankSum=0.000,.,0.100,0.500; ```; The VCF spec doesn't explicitly permit missing values as elements of INFO or FORMAT fields. It does permit the whole field to be missing a la `FIELD=.` but `FIELD=1,.,1` or `FIELD=.,.,.` are not explicitly permitted. In particular, `FIELD=.` could mean ""this field is missing"" or ""this field is not-missing, it is a one-element array containing one missing value"". The fix is to use `hl.import_vcf(..., array_elements_required=False)`. When that is true, Hail will parse `1,.,1` as `[1, NA, 1]`. Be forewarned: Hail treats `FIELD=.` as a missing field, not an array with one missing element.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102#issuecomment-1860989511:72,message,message,72,https://hail.is,https://github.com/hail-is/hail/issues/14102#issuecomment-1860989511,1,['message'],['message']
Integrability,"Hey @tomwhite, sorry for the massive delay. There was some concern about not having instructions generic to any cluster in the docs, so I've restructured your PR a bit more to capture the generic Spark cluster instructions and then have a separate section on getting started with a Cloudera cluster. I also opted for ""Cloudera"" instead of ""CDH"" because I don't think our users will recognize the acronym. Does that seem OK to you?. I made my changes as [a PR into your branch](https://github.com/tomwhite/hail/pull/1/files). Also, don't worry about the failing integration test, that's a CI issue on our end. It should resolve it self after the next new commit to your branch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1452#issuecomment-290546429:561,integrat,integration,561,https://hail.is,https://github.com/hail-is/hail/pull/1452#issuecomment-290546429,1,['integrat'],['integration']
Integrability,"Hey Alex, this is great. The PR protocol is for you to assign the PR to a random developer by using the random user generated by scorecard.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4947#issuecomment-446738202:32,protocol,protocol,32,https://hail.is,https://github.com/hail-is/hail/pull/4947#issuecomment-446738202,1,['protocol'],['protocol']
Integrability,"Hey Nick, wanted to loop you in on an offline discussion I had with Cotton about this. First, thank you for picking up review responsibilities! I'll just do a brief review focusing on interaction of this change with intended directions for hailctl. Here are the conclusions from our discussion:. 1. This is a breaking change to the hailctl interface. We're OK with that.; 2. Although we are OK making breaking changes, we should get Grace's team on board and update their scripts/repos before merging/releasing. For that reason this will sit for a few weeks until their current urgent analysis push is done.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767171298:340,interface,interface,340,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767171298,1,['interface'],['interface']
Integrability,"Hi @Sun-shan,. First, I should note that we do not currently test hail against Spark version 2.2.0, I recommend using Spark 2.1.1 or 2.0.2. Spark versions aside, the error you encountered is unrelated to Spark, as far as I know. What version of the `decorator` package is installed on your machine? `decorator` version 4.0.10 should work correctly. Unfortunately, we are still looking for a python dependency management solution. My apologies that you've run into this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-336903534:398,depend,dependency,398,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-336903534,1,['depend'],['dependency']
Integrability,"Hi @alanmejiamaza ,. Just to be clear, you did `pip install hail` and then you opened a notebook and ran something like:; ```; import hail as hl; hl.init(); from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(); ht = hl.utils.range_table(1000); ht = mt.annotate(DP = hl.rand_unif(0, 100)); p = hl.plot.histogram(ht.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); show(p); ```; And the plot didn't appear? Did you get a message saying ""BokehJS 1.4.0 successfully loaded.""? What version of Jupyter are you using? What web browser are you using?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12717#issuecomment-1452599951:461,message,message,461,https://hail.is,https://github.com/hail-is/hail/issues/12717#issuecomment-1452599951,1,['message'],['message']
Integrability,"Hi all,. Here's the error message that I get when I go to install all of my python packages (scipy/uvloop/etc). ```; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; --; 872 | amazon-ebs: rm -rf build/deploy; 873 | amazon-ebs: mkdir -p build/deploy; 874 | amazon-ebs: mkdir -p build/deploy/src; 875 | amazon-ebs: cp ../README.md build/deploy/; 876 | amazon-ebs: rsync -r \; 877 | amazon-ebs: --exclude '.eggs/' \; 878 | amazon-ebs: --exclude '.pytest_cache/' \; 879 | amazon-ebs: --exclude '__pycache__/' \; 880 | amazon-ebs: --exclude 'benchmark_hail/' \; 881 | amazon-ebs: --exclude '.mypy_cache/' \; 882 | amazon-ebs: --exclude 'docs/' \; 883 | amazon-ebs: --exclude 'dist/' \; 884 | amazon-ebs: --exclude 'test/' \; 885 | amazon-ebs: --exclude '*.log' \; 886 | amazon-ebs: python/ build/deploy/; 887 | amazon-ebs: # Clear the bdist build cache before building the wheel; 888 | amazon-ebs: cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; 889 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs:  1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-eb",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:26,message,message,26,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['message'],['message']
Integrability,"Hi! Are you trying to compile hail from source? You can get `lz4` on OS X with Homebrew by doing something like `brew install lz4`, or `apt-get install liblz4-dev` on a Debian-flavored Linux, but we don't (currently) ship with that dependency because the C++ code isn't enabled yet. If you don't need to build from source, but just want a local version to play around with, you can either use `pip` or download the prebuilt distribution following the instructions here: https://hail.is/docs/stable/getting_started.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4651#issuecomment-433220724:232,depend,dependency,232,https://hail.is,https://github.com/hail-is/hail/issues/4651#issuecomment-433220724,1,['depend'],['dependency']
Integrability,"Hi!; This is an odd error message to get -- is your repository updated to the current master? There was an update to the `importannotations table` module a few weeks ago, before which the `-e` option didn't exist. . We are in the midst of a documentation reorganization, so I apologize if it's difficult to find things at the moment. From the cloned repository, all test files are at `src/test/resources/*`. . This command worked for me just now:. ```; hail importannotations table src/test/resources/variantAnnotations.alternateformat.tsv --impute -e '`Chromosome:Position:Ref:Alt`' write -o tmp.vds; ```. The `-e` argument uses an expression to specify how to construct a `Variant`, which in this case is just the column name since the type of that column is `Variant`. If we don't use the `--impute` argument, we can construct it with . ```; -e 'Variant(`Chromosome:Position:Ref:Alt`)'; ```. More info on that [here](https://github.com/broadinstitute/hail/blob/master/docs/commands/ImportAnnotations.md)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/561#issuecomment-238502640:26,message,message,26,https://hail.is,https://github.com/hail-is/hail/issues/561#issuecomment-238502640,1,['message'],['message']
Integrability,"Hi, danking, @danking I tried two log file pathes ,all had access permission, but the error still appeared. 1HDFS file path /user/hail/hail.log have access permission; -rwxrwxrwx 3 hdfs supergroup 0 2016-10-08 10:54 /user/hail/hail.log; 2log filelocal PATH hava access permission; -rwxrwxrwx 1 root root 48523 Oct 8 11:42 hail.log. The error message was attached as follows ; [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/517467/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-252404979:350,message,message,350,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252404979,1,['message'],['message']
Integrability,"Hicseed @cseed , I configured the java related to the Spark cluster, as follows. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-250697347:1431,message,message,1431,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347,1,['message'],['message']
Integrability,"Hmm, I realized one problem with this - the error message will return that of coalesce rather than or_else and might be a little confusing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7088#issuecomment-532938355:50,message,message,50,https://hail.is,https://github.com/hail-is/hail/pull/7088#issuecomment-532938355,1,['message'],['message']
Integrability,"Hmm, there's another issue with our dependency management. If batch changes and docker doesn't, docker won't get rebuilt locally and batch will fail. I made docker touch a build stamp file (so we don't end up building docker multiple times) and make scorecard and batch build it before building themselves.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5655#issuecomment-475060566:36,depend,dependency,36,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475060566,1,['depend'],['dependency']
Integrability,Hmm. OrderedRDD and OrderedPartitioner are being phased out in master. OrderedRDD2 and OrderedPartitioner2 are in. We should probably have an offline discussion about how the linear algebra routines are going to interact with the new RegionValue-based stuff. There seem to be two competing goals here: getting something working for UKB and building something that will integrate with the new 0.2 stuff. We should probably have a chat about how to navigate this.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2171#issuecomment-326647606:190,rout,routines,190,https://hail.is,https://github.com/hail-is/hail/pull/2171#issuecomment-326647606,2,"['integrat', 'rout']","['integrate', 'routines']"
Integrability,"Hmm... shadowJar is building at about 2 minutes on my computer, 1 min 20s of which is compileScala. Master is building at about 1 min 40s (not sure how much scala compile is taking, forgot to check), and I managed to get it down to like 1 min 45s by not bundling some of the dependencies that we weren't bundling before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6248#issuecomment-498742903:275,depend,dependencies,275,https://hail.is,https://github.com/hail-is/hail/pull/6248#issuecomment-498742903,1,['depend'],['dependencies']
Integrability,"Hmph, ya this seems annoyingly complicated, and I'd prefer to make one command with opinionated but configurable defaults than have different commands. One thing that feels inconsistent here is what we do in the Batch interface. We don't have the equivalent of `HAIL_QUERY_BACKEND` and a user specifically has to create a `ServiceBackend` as opposed to relying on the environment dictating which model to use. I feel like it would be OK if we documented `hailctl batch submit` as ""distribute everything on batch by default""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12471#issuecomment-1324131496:218,interface,interface,218,https://hail.is,https://github.com/hail-is/hail/pull/12471#issuecomment-1324131496,1,['interface'],['interface']
Integrability,How does the history slow it down? It's just a wrapper.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2209#issuecomment-328142662:47,wrap,wrapper,47,https://hail.is,https://github.com/hail-is/hail/pull/2209#issuecomment-328142662,1,['wrap'],['wrapper']
Integrability,"Huh, you can't request changes on your own PR. So, right now the hail/apiserver dependency is cyclic. I'll need to fix that to get testing and deploying working right.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5624#issuecomment-473961308:80,depend,dependency,80,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-473961308,1,['depend'],['dependency']
Integrability,"Huh. Well, this is a terrible error message, but the short answer is that Hail doesn't support reading directly from an HTTP(S) server. You can either download that file or use a dataset that is available in a cloud storage bucket. In general, you'll want to convert to Hail's native MatrixTable format before you do further analysis anyway. I'll fix this to give a more reasonable error message, but, in general, not all HTTP(S) servers support the Range header which means Hail can't efficiently read from all HTTP(S) servers. If you're looking for public datasets to experiment with, I strongly recommend using the Dense Hail MatrixTable of the HGDP+1KG dataset hosted for free by the three major clouds https://gnomad.broadinstitute.org/downloads#v3-hgdp-1kg.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12280#issuecomment-1270722614:36,message,message,36,https://hail.is,https://github.com/hail-is/hail/issues/12280#issuecomment-1270722614,2,['message'],['message']
Integrability,"I added 64KB buffer, but I'm unsure of the right interface (should the caller pass in the byte buffer as well?) and location (RichDenseMatrixDouble is quite specific) for these functions. In compute on WriteBlocksRDD, I create a byte buffer of the same size in bytes as the entire IndexedRow, and re-use it for every row. This limits the number of cols to 268 million or so (~2^28) which I don't see us exceeding anytime soon. I could instead use the writeDoubles function but then I'd want to change the interface to pass in the byte buffer so it's not recreated for every row. What do you think is most reasonable?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2531#issuecomment-350128393:49,interface,interface,49,https://hail.is,https://github.com/hail-is/hail/pull/2531#issuecomment-350128393,2,['interface'],['interface']
Integrability,I added an asana task for CI to use the PR message for single commit PRs,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8552#issuecomment-613688867:43,message,message,43,https://hail.is,https://github.com/hail-is/hail/pull/8552#issuecomment-613688867,1,['message'],['message']
Integrability,I added an separate test of correctness of toKeyGsWeightRdd since this function is used in both the Hail and R routes in the end-to-end comparison,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2248#issuecomment-332873098:111,rout,routes,111,https://hail.is,https://github.com/hail-is/hail/pull/2248#issuecomment-332873098,1,['rout'],['routes']
Integrability,"I added the cli. I kind of winged it looking at how `hailctl dev deploy` was done. It seems to work though:. ```; (base) wmecc-475:hail jigold$ hailctl batch billing; usage: hailctl batch billing [-h] {list,get} ... Manage billing on the service managed by the Hail team. positional arguments:; {list,get}; list List billing projects; get Get a particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession o",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:427,message,message,427,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006,1,['message'],['message']
Integrability,"I agree about conda and having a set of docker images for the whole repo. I ran into other problems with conda and inter-project dependencies last week. The solution was to not rely on conda to specify the environment and just use `setup.py`. I can see this approach failing if different projects require different versions of the same package. Also, I think ci is using Python 3.7 while other projects are 3.6. We should probably pick a version for the entire project.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5623#issuecomment-473970604:129,depend,dependencies,129,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-473970604,1,['depend'],['dependencies']
Integrability,"I agree cancel_after_n_failures should be on the group. That lets us match Spark semantics for QoB. 1. I agree, callback per group seems valuable.; 2. I agree attributes seem useful on groups.; 3. I agree, not much value in updates being at the job-group level. . Depends what you mean by prefix search, if that means `LIKE ""X%""`, I think that'll be quite fast on a normal index because you can jump directly to the first record whose prefix is X. I don't see how a fulltext index could do any better in that case. On the other hand, if you mean `LIKE ""%X""` then I agree, a normal index is useless and MySQL will do a table scan. In that case, I expect a fulltext index to be a substantial improvement. > I believe my plan is basically already doing this. It might not be clear because I didn't put the migrations in. But basically all of the current batches tables are now indexed by batch_id, job_group_id where the current ""batch"" has job_group_id = 1. Ah, that sounds good. So the plan would be to drop, for example, `aggregated_batch_resources_v2` and the other tables which are now replaced with the job group ones? That's exactly what I had in mind.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12697#issuecomment-1450945048:264,Depend,Depends,264,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1450945048,1,['Depend'],['Depends']
Integrability,"I agree with your criticism, although my feeling is that rows and r in your proposal are noisy and unnecessary. Two thoughts:. I think this is best resolved in the context of embedding the expression language in Python. I think understanding pandas and what's involved in building a pandas-like interface for VariantDatasets is a good way to start. If we do address it in the current setup, what do we want to write? How about `kt.aggregate('filter(col1 < col2).count()` or, assuming we're doing a summing col1, `filter(col1 < col2).sum(col1)`. Then all the lambdas go away. We clearly need the scope in aggregators. Why not make that explicit, and throw out the single implicit value? Then `filter(col1 < col2).with(col3 = col1 * col2).mean(col3)`. I'm not sure about flatMap. `flatWith(col3 = <array expr>)`? I guess that's the same as `with(col3 = <array expr>).explode(col3)`. Then Aggregables look like Structs:. ```; Aggregable {; col1: Int,; col2: Int, ...; }; ```. Then there's nothing funny going on.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1410#issuecomment-282784540:295,interface,interface,295,https://hail.is,https://github.com/hail-is/hail/issues/1410#issuecomment-282784540,1,['interface'],['interface']
Integrability,"I also broke out routines to create examples of values (tables, matrix tables) with all types, and added a test case that runs expand_types on the table of values of all types.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5021#issuecomment-451248463:17,rout,routines,17,https://hail.is,https://github.com/hail-is/hail/pull/5021#issuecomment-451248463,1,['rout'],['routines']
Integrability,"I also compared `variant_and_sample_qc_nested_with_filters_2` (33% worse on batch) between the two branches on my laptop, and could not detect a difference. I do think the make_ndarray range speedup is real -- there are a few benchmarks that indicate improvement that all are heavily dependent on the performance of the `StreamRange` implementation, which I think slightly improved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10229#issuecomment-814325646:284,depend,dependent,284,https://hail.is,https://github.com/hail-is/hail/pull/10229#issuecomment-814325646,1,['depend'],['dependent']
Integrability,"I also created a Starlette branch; which may be preferable, as Sanic brings with it a bit of controversy and a bunch of errors generate on Techempower benchmarks. I took a brief look at the bench source didn't see an immediate issue, so worry a bit about. Sanic. Starlette is a light layer on top of Uvicorn, one of the leading ASGI web servers. Similar to Sanic/Flaks interface:. https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune&l=zijzen-1. Branch here, can issue a separate pr and close this one: https://github.com/akotlar/hail/tree/scorecard-starlette",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461188115:369,interface,interface,369,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461188115,1,['interface'],['interface']
Integrability,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418:289,rout,router,289,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418,1,['rout'],['router']
Integrability,I also switched the `api` argument to `BatchClient` to `default_api` rather than `None`. I wonder if that was somehow creating a muffled error message. It should have erred as soon as someone tried to make a request with `api=None`. Not sure why it looped forever instead.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4586#issuecomment-434412373:143,message,message,143,https://hail.is,https://github.com/hail-is/hail/pull/4586#issuecomment-434412373,1,['message'],['message']
Integrability,"I also think a fixed result type of RDD will be a problem. My CNV work involves parallelizing file parsing, and this interface wouldn't be compatible with that use case. I think there are a lot of strengths of generating a `(String) => Annotation` parse function, and then letting the client code use that however it likes. Including `sc.textFileLine(...)` in one read function prevents only a little bit of code duplication, and causes major headaches.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-232825352:117,interface,interface,117,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232825352,1,['interface'],['interface']
Integrability,I am convinced this is actually just a bad error message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4668#issuecomment-434267326:49,message,message,49,https://hail.is,https://github.com/hail-is/hail/issues/4668#issuecomment-434267326,1,['message'],['message']
Integrability,"I am in dependency Hell. Spark, in classic Spark fashion, wants a version of Netty from February 2022. The fixed versions of Azure need more recent Netty.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13018#issuecomment-1541021083:8,depend,dependency,8,https://hail.is,https://github.com/hail-is/hail/pull/13018#issuecomment-1541021083,1,['depend'],['dependency']
Integrability,"I apologize for not responding sooner to this. I've been mulling over what to do here as it's been over 4 years since I wrote the first interface. I think your changes are fine, but I need to go through the tests again and figure out what `_mentioned` was originally intended for to make sure this change doesn't break anything subtle. I'm going to have our CI run this SHA so I can see what the failures are.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13192#issuecomment-1603274953:136,interface,interface,136,https://hail.is,https://github.com/hail-is/hail/pull/13192#issuecomment-1603274953,1,['interface'],['interface']
Integrability,"I believe fixes messages need to be in the PR message body to work, so I've added it there.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2858#issuecomment-364973969:16,message,messages,16,https://hail.is,https://github.com/hail-is/hail/pull/2858#issuecomment-364973969,2,['message'],"['message', 'messages']"
Integrability,"I believe the code is behaving as designed. The error message, however, leaves a lot to be desired. A few take-aways:; 1. Hail doesn't support heterogeneous arrays. In situations like these, using an array of tuples has the desired outcome.; 2. The variable `x` in `lambda x:` is already a hail expression and so you don't need to explictly capture it as a `literal`.; a. While support for using hail expressions with `literal` was added in https://github.com/hail-is/hail/pull/4086 (see the issue for motivation), it can only be used when that expression is self-contained (ie it's not dependent on another hail expression, eg referencing an element of a hail array expression or tuple expression etc).; b. Our evaluation strategy is to `eval` the expression, then broadcast the result in a `literal`.; c. `eval` correctly complains that that expression has free variables and so can't be evaluated.; d. This error is ugly and has little to do with what the user wanted to achieve. Off the top of my head, a couple of ways to proceed:; 1. The hardest (but backwards compatible) fix is to somehow provide a good error message that the `x` in `lambda x:` in this particular context is a hail expression containing a reference that you should not use with `literal`.; 2. Remove support for using hail expressions with literal.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624335413:54,message,message,54,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624335413,3,"['depend', 'message']","['dependent', 'message']"
Integrability,I believe we removed PLINK because we didn't want to depend on it. It was historically hard to get a stable URL to a fixed version of the PLINK binary.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9527#issuecomment-701653528:53,depend,depend,53,https://hail.is,https://github.com/hail-is/hail/pull/9527#issuecomment-701653528,1,['depend'],['depend']
Integrability,"I can add it if you'd like, thought we'd prefer to save keystrokes:. ```python; In [3]: class Backend: ; ...: """""" ; ...: Abstract class for backends. ; ...: """""" ; ...: ; ...: @abc.abstractmethod ; ...: def close(self): ; ...: """""" ; ...: Close a Hail Batch backend. ; ...: """""" ; ...: pass ; ...: ; ...: ; ...: class LocalBackend(Backend): . In [4]: n = LocalBackend() . In [5]: n.close() ; ```. This allows us to declare an interface and a void implementation in one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9191#issuecomment-667226454:423,interface,interface,423,https://hail.is,https://github.com/hail-is/hail/pull/9191#issuecomment-667226454,1,['interface'],['interface']
Integrability,"I can't figure out why I'm still getting this warning message:; ```; 2018-04-20 15:31:39 Hail: WARN: modified row key, rescanning to compute ordering...; 2018-04-20 15:31:39 Hail: INFO: Coerced sorted dataset; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3417#issuecomment-383200453:54,message,message,54,https://hail.is,https://github.com/hail-is/hail/pull/3417#issuecomment-383200453,1,['message'],['message']
Integrability,I cannot figure out why the revision isn't getting generated. The build_hail step seems to completely ignore that I've put that dependency into the Makefile.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11645#issuecomment-1075937035:128,depend,dependency,128,https://hail.is,https://github.com/hail-is/hail/pull/11645#issuecomment-1075937035,1,['depend'],['dependency']
Integrability,I cant help you without an error message or description of what didnt work. I recommend waiting for the next release which should come out today.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12844#issuecomment-1501909992:34,message,message,34,https://hail.is,https://github.com/hail-is/hail/issues/12844#issuecomment-1501909992,1,['message'],['message']
Integrability,I changed PCA and toIndexRowMatrix to take a field. Now these all use select_entries so no need to analyze keys or process joins. But I still check that the expression has a matrix table source with good error message using `matrix_table_source`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3262#issuecomment-377599353:210,message,message,210,https://hail.is,https://github.com/hail-is/hail/pull/3262#issuecomment-377599353,1,['message'],['message']
Integrability,"I changed the interface for `Batch.jobs()` to return an iterator of Job objects rather than statuses that were partial. I think this makes more sense to me. To do that, I got rid of parent_ids on Job because they weren't used anywhere and were misleading because we never return the parent_ids when creating Job objects.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7949#issuecomment-578296538:14,interface,interface,14,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-578296538,1,['interface'],['interface']
Integrability,I checked the database and was surprised to see the SKUs weren't necessarily unique to a specific region. But it makes sense when I looked at their API here: https://cloud.google.com/billing/docs/reference/rest/v1/services.skus/list#sku. I think we should put this in and address what happens if they change the SKU of a particular region if that occurs in the future. We'll just get a bunch of error messages with no price updates and it shouldn't impact users. ~~I will also manually check this in Azure.~~ I checked in both GCP and Azure and the updates looked fine with no errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13607#issuecomment-1726229597:401,message,messages,401,https://hail.is,https://github.com/hail-is/hail/pull/13607#issuecomment-1726229597,1,['message'],['messages']
Integrability,I copied `nginx.conf` from a running router and then trimmed out the comments and made our changes (see `access_log` and `log_format`).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8058#issuecomment-583565691:37,rout,router,37,https://hail.is,https://github.com/hail-is/hail/pull/8058#issuecomment-583565691,1,['rout'],['router']
Integrability,"I couldn't get rid of gcloud, sadly. `kubectl` is still to pervasively integrated into what build.yaml does. Although removing it would save us 100s of MB of unnecessary transfer and extraction, using the ci_utils_image for this purpose delays deployment of most services by an unacceptable amount of time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10502#issuecomment-845459536:71,integrat,integrated,71,https://hail.is,https://github.com/hail-is/hail/pull/10502#issuecomment-845459536,1,['integrat'],['integrated']
Integrability,"I created a new multi-branch configuration that should be better for what we are trying to accomplish. This should fix issues 2 and 3. . For the reproducibility of errors, that will probably take both setting the random seed parameter in Hail for all random tests and getting Jenkins to give better error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/335#issuecomment-214377125:305,message,messages,305,https://hail.is,https://github.com/hail-is/hail/issues/335#issuecomment-214377125,1,['message'],['messages']
Integrability,"I decided to try to do this in two passes since making changes to deploy logic always finnicky on its own. I think this does the right thing though. . Does build.yaml support a way to say ""depend on this step x if we are doing x at all""? Redeploying the website will have to happen after the `deploy` step runs in the future and publishes the latest version of the docs to hail-common.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11294#issuecomment-1024606310:189,depend,depend,189,https://hail.is,https://github.com/hail-is/hail/pull/11294#issuecomment-1024606310,1,['depend'],['depend']
Integrability,"I deleted the LinearRegressionFromHcsCommand and associated tests as it'd fallen out of sync with how hcs evolved for T2D, it's independent of the rest of the PR and I can add this functionality back later (at which point I imagine there will be other changes both to hcs and to the stats interfaces more generally).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/517#issuecomment-236385122:289,interface,interfaces,289,https://hail.is,https://github.com/hail-is/hail/pull/517#issuecomment-236385122,1,['interface'],['interfaces']
Integrability,"I did make sure it renders as I intended, and the round trip test means it produces valid type grammar. but I'm hesitant to add a test for exact characters, since if we want to change spacing or something cosmetic then we have to change the test. I feel the same way about assertRaisesRegex checks -- we should be able to make error messages nicer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2998#issuecomment-368885849:333,message,messages,333,https://hail.is,https://github.com/hail-is/hail/pull/2998#issuecomment-368885849,1,['message'],['messages']
Integrability,"I did some more tweaking with the memory and got a new exception. ```; ---------------------------------------------------------------------------; Py4JError Traceback (most recent call last); Input In [3], in <cell line: 2>(); 1 ## Import scores table; ----> 2 score_pd = ht_score.to_pandas(). File <decorator-gen-1091>:2, in to_pandas(self, flatten). File ~/mambaforge/lib/python3.9/site-packages/hail/typecheck/check.py:577, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 574 @decorator; 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_). File ~/mambaforge/lib/python3.9/site-packages/hail/table.py:3340, in Table.to_pandas(self, flatten); 3338 dtypes_struct = table.row.dtype; 3339 collect_dict = {key: hl.agg.collect(value) for key, value in table.row.items()}; -> 3340 column_struct_array = table.aggregate(hl.struct(**collect_dict)); 3341 columns = list(column_struct_array.keys()); 3342 data_dict = {}. File <decorator-gen-1037>:2, in aggregate(self, expr, _localize). File ~/mambaforge/lib/python3.9/site-packages/hail/typecheck/check.py:577, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 574 @decorator; 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_). File ~/mambaforge/lib/python3.9/site-packages/hail/table.py:1231, in Table.aggregate(self, expr, _localize); 1228 agg_ir = ir.TableAggregate(base._tir, expr._ir); 1230 if _localize:; -> 1231 return Env.backend().execute(hl.ir.MakeTuple([agg_ir]))[0]; 1233 return construct_expr(ir.LiftMeOut(agg_ir), expr.dtype). File ~/mambaforge/lib/python3.9/site-packages/hail/backend/py4j_backend.py:99, in Py4JBackend.execute(self, ir, timed); 97 try:; 98 result_tuple = self._jbackend.executeEncode(jir, stream_codec)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12035#issuecomment-1186014691:450,wrap,wrapper,450,https://hail.is,https://github.com/hail-is/hail/issues/12035#issuecomment-1186014691,2,['wrap'],['wrapper']
Integrability,"I did that originally and then switched to the state in the last commit, because of a preference for try/catch blocks to wrap the minimal amount of code for clarity",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4465#issuecomment-425601550:121,wrap,wrap,121,https://hail.is,https://github.com/hail-is/hail/pull/4465#issuecomment-425601550,1,['wrap'],['wrap']
Integrability,I did try it out and the error messages are WAY better!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2916#issuecomment-366339845:31,message,messages,31,https://hail.is,https://github.com/hail-is/hail/pull/2916#issuecomment-366339845,1,['message'],['messages']
Integrability,"I didn't look through everything, but skimmed through quickly. This interface is gonna be way nicer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1811#issuecomment-302128830:68,interface,interface,68,https://hail.is,https://github.com/hail-is/hail/pull/1811#issuecomment-302128830,1,['interface'],['interface']
Integrability,I didn't read your comment carefully enough. Do we want to have all of these dependencies for our project? This is a more general question.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5572#issuecomment-471675774:77,depend,dependencies,77,https://hail.is,https://github.com/hail-is/hail/pull/5572#issuecomment-471675774,1,['depend'],['dependencies']
Integrability,"I discovered this when I tried to run a vcf combiner pipeline. To me, this signals that we need better knowledge of where integration tests live and how to add to them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5033#issuecomment-449468618:122,integrat,integration,122,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449468618,1,['integrat'],['integration']
Integrability,"I do agree that I think all of my proposed interfaces are missing the ability to do things like nest loops, which is part of the reason I'm not sold on any of them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614#issuecomment-558331064:43,interface,interfaces,43,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558331064,1,['interface'],['interfaces']
Integrability,"I do think the solution you've described in the PR message seems better, though, if we can make this work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9304#issuecomment-675767918:51,message,message,51,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-675767918,1,['message'],['message']
Integrability,"I don't disagree. However, we need toString on (scalar) Type because they are used for error messages all over. MatrixTable used to have a bunch of separate types, now it just has a MatrixType. I think some error messages could now use the matrix type. Python also has some matrix type printing logic, these should probably get unified. Once I have printing for the user, it seemed easier to write a (admittedly small) parser than a separate to/from JSON. I admit, apart from user error messages, JSON is natural since that's what we're storing in the metadata file. Do you have a concrete suggestion? I'm not sure quite what to do that's better than this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2825#issuecomment-362082473:93,message,messages,93,https://hail.is,https://github.com/hail-is/hail/pull/2825#issuecomment-362082473,3,['message'],['messages']
Integrability,I don't especially like renaming the Python hail1 api classes KeyTable -> Table and VariantDataset -> MatrixTable. I basically feel like we should leave that interface untouched until we delete it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2588#issuecomment-352157852:158,interface,interface,158,https://hail.is,https://github.com/hail-is/hail/pull/2588#issuecomment-352157852,1,['interface'],['interface']
Integrability,I don't know why you'd ever check the position and not the chromosome. Leave the same interface but have them check the chromosome.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/499#issuecomment-235602871:86,interface,interface,86,https://hail.is,https://github.com/hail-is/hail/pull/499#issuecomment-235602871,1,['interface'],['interface']
Integrability,"I don't love what I've had to do with the deploy config stuff. That's in my opinion the most finicky part of this (has already broken multiple times) and it's mostly our fault, because we overload the `namespace` parameter with both identifying the namespace in Kubernetes and signifying whether the environment is prod or not. All I want really is to change the `domain` to a domain and path prefix, and not have the namespace have such an impact on routing. Like what if `namespace` didn't affect routing, but if the deploy config only gave a domain with no path e.g. `hail.is`, we use subdomains so `batch.hail.is`, but if we provided a domain with a path prefix like `internal.hail.is/dgoldste`, we make the batch root `internal.hail.is/dgoldste/batch`?. Alternative: Actually have and use a `base_path` in the deploy config. This would be used in dev and terra environments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13944#issuecomment-1785575616:451,rout,routing,451,https://hail.is,https://github.com/hail-is/hail/pull/13944#issuecomment-1785575616,2,['rout'],['routing']
Integrability,"I don't see the utility in creating an unnecessary stack trace to see 'method ""variant QC"" requires a split dataset'. I think there is value in having clear, short, stack-trace-free error messages when it's clear what the problem is and what the user needs to do. I think that printing unnecessary stack traces will cause users to view hail even more as a tool in development, and they will be more inclined to ask us about errors rather than try to figure out how whether they made a simple mistake using the interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287149290:188,message,messages,188,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287149290,2,"['interface', 'message']","['interface', 'messages']"
Integrability,"I don't think I actually understand how artifact and snapshot dependencies work in TeamCity. I thought a build by the main build configuration (the regular CI) would trigger a build of the docs build configuration. This was not the case and I'm not sure why. I've set up the docs build to trigger on any change to master. Unfortunately, we have to `compileScala` twice because these are separate builds. I'll add an issue to clean this up and make it more sensible. There's got to be a right way to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/733#issuecomment-244475645:62,depend,dependencies,62,https://hail.is,https://github.com/hail-is/hail/issues/733#issuecomment-244475645,1,['depend'],['dependencies']
Integrability,"I don't think there's any good reason why the other pip packages are in a separate RUN step. In fact, using one pip invocation should ensure we get compatible versions whereas what we have now doesn't ensure that. I have no objections to using the hail/python/pinned-requirements.txt. If we install that and all these extra dependencies in one layer then install the hail wheel after (without -U), I *think* it should see all the dependencies are met and just install the hail package.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12301#issuecomment-1275353805:324,depend,dependencies,324,https://hail.is,https://github.com/hail-is/hail/pull/12301#issuecomment-1275353805,2,['depend'],['dependencies']
Integrability,"I don't understand your first sentence. In the case of a hybrid query and batch script, using Hail Query is totally kosher! What I don't want is dependence on a JVM and Hail Query just to check if a file exists in Google Cloud Storage for a script that is otherwise independent of Hail Query.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-671516865:145,depend,dependence,145,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671516865,1,['depend'],['dependence']
Integrability,"I feel a bit like a cheat here since there's been a fair bit of work since you last reviewed. Most of it was fixes of tiny bugs that the CI revealed. There was [one, kind of notable, change](https://github.com/hail-is/hail/pull/5194/commits/f95e4e0ff1cdd2865cf703aa27f780c7f162316c). I removed Spark from the Dockerfile. It is no longer necessary because the pip install will pull the correct version of Spark. To avoid pulling Spark on each PR build, I cache 2.2.0 (and all our other pip dependencies) in the hail conda env in the docker image. Doing this required that I move our requirements into a requirements file which is parameterized by spark version. A good follow up PR would be to either a) entirely remove dependency on conda or b) generate the conda `environment.yml` from `requirements.txt.in`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333:489,depend,dependencies,489,https://hail.is,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333,2,['depend'],"['dependencies', 'dependency']"
Integrability,"I feel like I probably ought to have more tests, but wasn't sure what else to include. I didn't want to make the `StreamLen` tests depend on the correctness of more complicated IR nodes like `StreamZip`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8783#issuecomment-628105025:131,depend,depend,131,https://hail.is,https://github.com/hail-is/hail/pull/8783#issuecomment-628105025,1,['depend'],['depend']
Integrability,"I figured out the cyclic import. I have the code two ways now. Take your pick:. ```; batch/; gcp/; cloud.py # contains constructor functions for CloudDriver, CloudDisk etc.; utils.py. # has general utils plus the InstanceConfig constructor functions (this is what caused the cyclic import when they were in the same file as the CloudDriver constructor); ```. OR. ```; batch/; cloud/; gcp/; cloud.py. # contains constructor functions for CloudDriver etc.; instance_config.py # has interfaces for constructing InstanceConfig (this can't be in the same file as the CloudDriver constructor function due to cyclic import.; utils.py. # old utils functions; ```. If you're going to carefully go through the code, do you want the other two sets of changes in this PR? Then all of Batch will be completely cloud-agnostic and not just the driver. Up to you how you want me to structure this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-951332977:480,interface,interfaces,480,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-951332977,1,['interface'],['interfaces']
Integrability,"I figured, noticed the double post :). ```; The exception in your above message is coming from the Apply node being inferred as a PVoid by your case _ => PVoid code. Writing the rule for the apply node should fix that.; ```. Right. It's just that I tried to write the rule, and quickly ran across the fact that Seq[IR] would be inferred such that the first IR had a different type from the 2nd or Nth. This is what I had written:. ```scala; case ApplySpecial(name, irs) => {; val it = irs.iterator; val head = it.next(); head.inferSetPType(env). while(it.hasNext) {; val value = it.next(). value.inferSetPType(env); assert(value.pType2 == head.pType2); }. head.pType2; }; ```. With the result in one case that `head.pType2` was bool, `value.pType2` was something else. Without a type union, it wasn't clear to me what to return. One possibility was that I shouldn't handle this node, so I started with that possibility, which I know understand isn't right. The other was that the implementation was wrong, and my first guess there is that one of the IRs dictates the type (say the first), the 2nd is that there is a simple precedence rule, the 3rd is that the type inference procedure has some branching logic over the collection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6594#issuecomment-513007861:72,message,message,72,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-513007861,1,['message'],['message']
Integrability,"I fixed something that was definitely broken, and then added scala dependencies that didn't fix scala being missing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6181#issuecomment-497421947:67,depend,dependencies,67,https://hail.is,https://github.com/hail-is/hail/pull/6181#issuecomment-497421947,1,['depend'],['dependencies']
Integrability,"I fixed the part where the behavior of locus_windows was changed, and now the behavior should be consistent with the previous version. (Some of the error types were changed, but I don't really see that as a breaking interface change).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5690#issuecomment-477306260:216,interface,interface,216,https://hail.is,https://github.com/hail-is/hail/pull/5690#issuecomment-477306260,1,['interface'],['interface']
Integrability,"I get the following version: ; ``; $ java -version; java version ""1.8.0_111""; Java(TM) SE Runtime Environment (build 1.8.0_111-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.111-b14, mixed mode); ``; I'm using virtualenv to run python 2.7 and I think I installed all the dependencies and python libraries that were required. Any further idea on what I can do to get this to work?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319675148:274,depend,dependencies,274,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319675148,1,['depend'],['dependencies']
Integrability,"I guess it depends whether you want up to date or just compatible, the maintainers seem to be of the opinion that you should either always update the lock file immediately or set upper bounds if you're not ok with a certain upgrade, seen [here](https://github.com/jazzband/pip-tools/issues/882). Continuous work, but maybe the right way to go honestly. In that case trivially make a build.yaml step that asserts the lock file is valid and up to date.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11842#issuecomment-1131859471:11,depend,depends,11,https://hail.is,https://github.com/hail-is/hail/pull/11842#issuecomment-1131859471,1,['depend'],['depends']
Integrability,I had problems with calling the Make rules that create batch worker images; but I see that you already remove them and created a script that doesn't depend on config.mk: https://github.com/hail-is/hail/blob/main/batch/gcp-create-worker-image.sh. Thanks for a response! I guess I don't have to worry about config.mk :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11371#issuecomment-1045522380:149,depend,depend,149,https://hail.is,https://github.com/hail-is/hail/pull/11371#issuecomment-1045522380,1,['depend'],['depend']
Integrability,"I have a branch where I've upgraded the dependency to libsimdpp-2.1 and resolved issues around depreciation warnings, this does solve the issue. We would need to discuss if we want to upgrade the dependency, and benchmark against the new version to see if it causes any performance regression. Branch is [here](https://github.com/chrisvittal/hail/tree/libsimdpp-2.1)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955#issuecomment-406297780:40,depend,dependency,40,https://hail.is,https://github.com/hail-is/hail/issues/3955#issuecomment-406297780,2,['depend'],['dependency']
Integrability,"I have a implemented a highly concurrent Python asyncio filesystem that supports GCS and the local file system (and soon S3). It is my intention to ultimately replace the hadoop_* functions with this. The new thing feels pretty fast: copy benchmarks 2-5x faster than gsutil for example, esp. when working with lots of files. Some remarks:; - It is designed to do the minimal number of system calls/API calls per operation so there is serial loops like this anywhere in the code.; - Our short term goal is to use this for the input/output steps in Batch.; - It doesn't support Hadoop (and I'm not super exciting about maintaining that).; - Some things will be much faster because no round trip the JVM. ; - The interface is fully async, so we'll need to build some wrappers if you want a synchronous interface. The async interface will get you concurrency within operations (copy, rmtree), the sync interface only gets you currency within operations.; - The list files operation doesn't support globbing yet.; - There are no docs yet.; - Compared to Hadoop/POSIX, the interface is slightly lower level but it was designed to map well onto the filesystems we want to support. There is no `stat`, for example, but is statfile (which requires the input to be a file) and listfiles (which requires it to be a directory), although we could build that.; - I'd say the code is beta and not quite completely solid but getting close. Here is the AsyncFS interface: https://github.com/hail-is/hail/blob/main/hail/python/hailtop/aiotools/fs.py#L70. Here is an example creating a router filesystem that supports GCS and the local file system: https://github.com/hail-is/hail/blob/main/hail/python/test/hailtop/test_aiogoogle.py#L17. I'd be happy to chat more about what would make this attractive for you guys to switch to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10043#issuecomment-778364838:710,interface,interface,710,https://hail.is,https://github.com/hail-is/hail/pull/10043#issuecomment-778364838,8,"['interface', 'rout', 'wrap']","['interface', 'router', 'wrappers']"
Integrability,"I have forthcoming changes that make the router only accept HTTPS. I've been testing this in my dev namespace. Unfortunately, my batch workers cannot speak to my batch instance anymore. The workers speak to internal-gateway who then tries to proxy to my router over HTTP, but nobody is listening on that port. As long as there is a mix of HTTP-only and HTTPS-only routers, internal-gateway needs a way to know which protocol to use with which router. It's temporary because I intend everyone to speak HTTPS.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8490#issuecomment-610644390:41,rout,router,41,https://hail.is,https://github.com/hail-is/hail/pull/8490#issuecomment-610644390,5,"['protocol', 'rout']","['protocol', 'router', 'routers']"
Integrability,"I have hidden options in `lmmreg` PR: https://github.com/hail-is/hail/pull/1064/files#diff-f6dd198b9e7d7ee72f9ff0042b8de368. As we move to the Python interface, we may need another solution. For now I've written ""(advanced)"" at the end of these option descriptions: https://github.com/hail-is/hail/pull/1064/files#diff-b462644505a1aefc479455f4060b415a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1084#issuecomment-260354398:150,interface,interface,150,https://hail.is,https://github.com/hail-is/hail/pull/1084#issuecomment-260354398,1,['interface'],['interface']
Integrability,"I have no idea what this means. ```; Caused by: is.hail.shadedazure.com.azure.storage.blob.models.BlobStorageException: If you are using a StorageSharedKeyCredential, and the server returned an error message that says 'Signature did not match', you can compare the string to sign with the one generated by the SDK. To log the string to sign, pass in the context key value pair 'Azure-Storage-Log-String-To-Sign': true to the appropriate method call.; If you are using a SAS token, and the server returned an error message that says 'Signature did not match', you can compare the string to sign with the one generated by the SDK. To log the string to sign, pass in the context key value pair 'Azure-Storage-Log-String-To-Sign': true to the appropriate generateSas method call.; Please remember to disable 'Azure-Storage-Log-String-To-Sign' before going to production as this string can potentially contain PII.; Status code 403, (empty body); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906073:200,message,message,200,https://hail.is,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906073,2,['message'],['message']
Integrability,I have one thing left to do: I need to rebuild pr-builder because the apiserver test depends on Flask. I'll do that shortly. Rest of it is ready for review.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5386#issuecomment-466480945:85,depend,depends,85,https://hail.is,https://github.com/hail-is/hail/pull/5386#issuecomment-466480945,1,['depend'],['depends']
Integrability,"I haven't looked at this yet, I have a new use case in seqr for the table code. I need to be able to load data as `RDD[Annotation]` without pulling out a sample/variant key (or pull out a totally custom key). Can I do that easily with the new interface?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-232252890:243,interface,interface,243,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232252890,1,['interface'],['interface']
Integrability,"I haven't tested this error message, as I'm not sure how to replicate the bug scenario, but I think it should work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11321#issuecomment-1031552946:28,message,message,28,https://hail.is,https://github.com/hail-is/hail/pull/11321#issuecomment-1031552946,1,['message'],['message']
Integrability,"I integrated these changes into https://github.com/broadinstitute/hail/pull/652 except I used block instead of block-inline. I also fixed the line height. (Problem was not using display: block, padding was per-line.) With the line-height fixed, font-size: 0.8em feels too small. How do you think it looks now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/644#issuecomment-241179566:2,integrat,integrated,2,https://hail.is,https://github.com/hail-is/hail/pull/644#issuecomment-241179566,1,['integrat'],['integrated']
Integrability,"I intend the Tour of Hail Query to assume no genetics knowledge. Indeed, it probably won't ever mention genetics. I removed the sentence-as-bulleted list. I like them, but I'm not the reader here . I intend for each of these documents to be separate files, and people with git experience can skip the git doc. I agree that we use git in one of the typical ways. However, I've learned that Atom's GitHub support doesn't consider the possibility of PRs from one remote to another. We also have some conventions around git messages and stacked PRs that I think are worth getting in writing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10020#issuecomment-776868568:521,message,messages,521,https://hail.is,https://github.com/hail-is/hail/pull/10020#issuecomment-776868568,1,['message'],['messages']
Integrability,"I know I said I thought this was a reasonable approach a while ago, but Ive been thinking hard about this change since last week, and I think I want us to explore a larger set of designs before committing to this strategy. The approach in this PR doubles down on the functional Code[T] structure, which is something were trying to move away from with CodeBuilder. I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. ```scala; class Emit[C](; val ctx: ExecuteContext,; val cb: EmitClassBuilder[C]) { emitSelf =>. val methods: mutable.Map[(String, Seq[Type], Seq[PType], PType), EmitMethodBuilder[C]] = mutable.Map(). private[ir] def emitVoid(cb: EmitCodeBuilder, ir: IR, mb: EmitMethodBuilder[C], region: StagedRegion, env: E, container: Option[AggContainer], loopEnv: Option[Env[LoopRef]]): Unit = {; cb.startLine(ir.lineNumber); ... implementaiton; cb.endLine(ir.lineNumber); ```. How could we make something like this work? Can we get away without every Code[T] knowing the source line? The JVM represents line numbers as an array of (line start bytecode index, line bytecode length) tuples, and I think it will be possible to produce this more easily. I think part of my concern is that Im not entirely sold by the need to have a whole stack of IR printouts and associated line numbers  right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration  theyve been a source of confusion and frustration in the past, and the intentional paucity of implicits in our current codebase reflects that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9770#issuecomment-742027249:395,interface,interface,395,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742027249,2,"['inject', 'interface']","['injecting', 'interface']"
Integrability,"I like it, in particular I like the use of the adjective ""opposite"". Small proposed tweak to the first sentence, because that sounds similar to ""Both tables must have the same number of keys"", which is included in the description of keys below the portion we're changing. ```; Tables are joined at rows whose key fields have equal values.; The inclusion of a row with no match in the opposite table depends on the; join strategy:. - **inner** -- Only rows with a matching key in the opposite table are included; in the resulting table.; - **left** -- All rows from the left table are included in the resulting table.; If a row in the left table has no match in the right table, then the fields; derived from the right table will be missing.; - **right** -- All rows from the right table are included in the resulting table.; If a row in the right table has no match in the left table, then the fields; derived from the left table will be missing.; - **outer** -- All rows are included in the resulting table. If a row in the right; table has no match in the left table, then the fields derived from the left; table will be missing. If a row in the right table has no match in the left table,; then the fields derived from the left table will be missing.; ```. Below this I had added ""Missing (NA) keys never match"".",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8467#issuecomment-609471408:399,depend,depends,399,https://hail.is,https://github.com/hail-is/hail/pull/8467#issuecomment-609471408,1,['depend'],['depends']
Integrability,"I like this proposal!. On Thu, Jun 29, 2017 at 6:30 PM, Tim Poterba <notifications@github.com>; wrote:. > I think we should do this by default, and remove the ability to annotate; > in this method. The interface is a bit too complicated; >; > ; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/1391#issuecomment-312126181>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/ADVxgbSEIxHUor98MljwApai7csY9q2Zks5sJCWHgaJpZM4MB6sU>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1391#issuecomment-312283421:202,interface,interface,202,https://hail.is,https://github.com/hail-is/hail/issues/1391#issuecomment-312283421,1,['interface'],['interface']
Integrability,"I like this. That + depending on the `release` step ensures that we only submit the benchmarks on the exact sha that we release. When we eventually split these steps out into their own release pipeline, we can just delete the file and use the normal `depends_on: release` behavior to achieve the same result.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14398#issuecomment-2027251314:20,depend,depending,20,https://hail.is,https://github.com/hail-is/hail/pull/14398#issuecomment-2027251314,1,['depend'],['depending']
Integrability,"I loaded gcc 4.9 and java 1.8 and now getting a new error while compiling.This is strange as earlier I dint face any issues.Is there some major changes that happened for code compilation. mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; :compileScala; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type SparkSession in package org.apache.spark.sql,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the pro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:408,depend,dependency,408,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,3,['depend'],"['dependencies', 'dependency']"
Integrability,"I looked at all of these usages. Since the only value types that use the state manager are sets and dicts with locus in the elt/key, it's impossible to construct a failing example for most of these. It's possible for a usage of the RVDContext RVB in `orderedLeftJoinDistinctAndInsert`, but we can replace the implementation of `TableLeftJoinRightDistinct.execute` with a wrapped call to the lowered implementation (not necessary in this PR).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12699#issuecomment-1442231548:371,wrap,wrapped,371,https://hail.is,https://github.com/hail-is/hail/pull/12699#issuecomment-1442231548,1,['wrap'],['wrapped']
Integrability,"I looked over the code and it looks fine, but I'm having trouble understanding the bigger picture of what you're trying to accomplish. I see that you have a new step that creates a test database in the default namespace in the test scope. Then you create the database config secret from this new database. And then deploy_ci depends on it, which makes sense because it needs the secret to be able to create new databases. And this is all only in the test scope. It looks like you cleaned up the build database in the case of dev deploy, which is fine too. > we also create a ""test_instance"" database that will be used as the database instance inside the tests. I don't understand what you wrote here because test_instance database doesn't seem to be used at all. Aren't we still creating the same batch and ci databases? I don't see what the test_instance database is buying you except to be able to make the database config secret that doesn't have the root username and password. I also don't quite understand what's going on in the build_cant_create_database build step. Shouldn't those secrets already exist? Won't this fail?. I'm sorry if I'm missing something obvious.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906:325,depend,depends,325,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906,1,['depend'],['depends']
Integrability,"I mostly rewrote things to fit this interface, but found a pretty significant problem with it -- we lose our informative error messages. Once we've got an `RDD[Annotation]`, we've lost our line context. This new variant expression interface will be different and challenging with our users, and if there's a problem with the expression, I don't want to get a crash from a requirementError from the Variant constructor without any context, or a match error from a `Chr:Pos:Ref:Alt` with too few colon-split fields.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-232824524:36,interface,interface,36,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232824524,3,"['interface', 'message']","['interface', 'messages']"
Integrability,I moved to TBoolean and simplified the error messages. Back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/511#issuecomment-236389770:45,message,messages,45,https://hail.is,https://github.com/hail-is/hail/pull/511#issuecomment-236389770,1,['message'],['messages']
Integrability,"I pushed a few more changes:. I added a nginx server block to handle proxying connections to other namespaces. To map a namespace to the router service IP in gateway, I wrote a new server (router-resolver) that looks up the IP via kubernetes and call it with nginx auth_request. So now you can hit, e.g., blah.cseed.internal.hail.is and it gets forwarded to the cseed:router service.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5873#issuecomment-482439395:137,rout,router,137,https://hail.is,https://github.com/hail-is/hail/pull/5873#issuecomment-482439395,3,['rout'],"['router', 'router-resolver']"
Integrability,"I pushed some additional commits:. - added UnpartitionedRVD, made RVD a cleaner interface; - improved the RVDSpec to/from JSON conversion; - added RVD.write that returns an rvdSpec + stats (partition counts)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2828#issuecomment-362842294:80,interface,interface,80,https://hail.is,https://github.com/hail-is/hail/pull/2828#issuecomment-362842294,1,['interface'],['interface']
Integrability,I put the WIP tag on so it won't merge until you change the commit message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13434#issuecomment-1684203905:67,message,message,67,https://hail.is,https://github.com/hail-is/hail/pull/13434#issuecomment-1684203905,1,['message'],['message']
Integrability,I put the WIP tag on so you can merge this at your leisure in case there's a hidden dependency somewhere.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10983#issuecomment-947197768:84,depend,dependency,84,https://hail.is,https://github.com/hail-is/hail/pull/10983#issuecomment-947197768,1,['depend'],['dependency']
Integrability,"I read what you wrote as you wanting to wrap RunAggScan in ToStream. I don't have time now, but I will re-run the above with toStream around that node. However, the issues that originally precipitated this observation/fix had nothing to do with RunAggScan. They had to do with the fact that we cannot safely wrap ToStream on something that is TContainer, and then cast ToArray on that. Even if we don't find a bug now, this can't be safe, we need instead to handle each specific type of TContainer (ToArray, ToDict, ToSet)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586610981:40,wrap,wrap,40,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586610981,2,['wrap'],['wrap']
Integrability,"I registered the functions inside and outside the actual aggregation to bring the IR size all the way down to about what it was before. It was maybe 140 before I did that, 30 of which belonged to the string concatenation in the error messages :(. . This also hit a bug that I'm fixing in #6740, so it won't be able to go in before that does.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6730#issuecomment-515198365:234,message,messages,234,https://hail.is,https://github.com/hail-is/hail/pull/6730#issuecomment-515198365,1,['message'],['messages']
Integrability,I removed the info message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/414#issuecomment-224777755:19,message,message,19,https://hail.is,https://github.com/hail-is/hail/pull/414#issuecomment-224777755,1,['message'],['message']
Integrability,I removed this dependency instead https://github.com/hail-is/hail/pull/11505,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11483#issuecomment-1059610790:15,depend,dependency,15,https://hail.is,https://github.com/hail-is/hail/pull/11483#issuecomment-1059610790,1,['depend'],['dependency']
Integrability,"I replicated the issue with this:; ```; In [1]: grch37 = hl.get_reference('GRCh37'). In [2]: grch37.add_liftover('src/test/resources/grch37_to_grch38_chr20.over.chain.gz', 'GRCh38'). In [3]: i = hl.parse_locus_interval('1:10000-10000'). In [4]: hl.eval(hl.liftover(i)); ```. The issue is this interval is `Interval(10000, 10000, includesStart=True, includesEnd=False)` which has a length of zero. @patrick-schultz Should this be a valid interval? i.e. start==end and includesStart = True and includesEnd = False. Otherwise, if it is a valid Hail interval, then I'll throw a nicer error message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5174#issuecomment-455713308:586,message,message,586,https://hail.is,https://github.com/hail-is/hail/issues/5174#issuecomment-455713308,1,['message'],['message']
Integrability,I reverted the LSM tree transitive dependency change.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8571#issuecomment-615454573:35,depend,dependency,35,https://hail.is,https://github.com/hail-is/hail/pull/8571#issuecomment-615454573,1,['depend'],['dependency']
Integrability,"I searched [here](https://portal.azure.com#@d6c9f2ea-d3bb-4ca9-8b14-231bac999aa6/blade/Microsoft_OperationsManagementSuite_Workspace/Logs.ReactView/resourceId/%2Fsubscriptions%2F22cd45fe-f996-4c51-af67-ef329d977519%2Fresourcegroups%2Fhaildev%2Fproviders%2Fmicrosoft.operationalinsights%2Fworkspaces%2Fhaildev-logs/source/LogsBlade.AnalyticsShareLinkToQuery/q/H4sIAAAAAAAAA21Sy27CMBC88xVbLiRSqNprUSpVgFpUVCraOzLxJhgcO7LNI2r7710nKRBoDokzmp2dnbVEB6%252FbJb5rPtWZhRiCQnPFciwMpuLwYJ0RKovAQ7ZgCR4RqTOJO5QNEMJXB%252BiRpDjUyjGh0Ez4VFhHqk2PidqhctqUFfUb9is0CG%252BkDdYx4%252BxeuBUdTcJc20gE3X43bJURTaIKTs1G4ePdlXBlGuL4NEFD4eRMqOTc7SgCHwRbooxOxRU4KWpkUFUfSyi0Rg4PDhUHGs%252BUccGMxcXaahUQY%252ByxsM0jbcsyjCv%252BbfPXMn9mC4SC4CLUdha1jKV9GOFKP%252B7fev6h4SFZCJVquElqTQvdj5enbkOVWm%252B2BWyE4rHE1OmtQ3PdH7Q699gUF0avMXF9tmclTHIaKqo%252Fn4zuzBwLbYW%252FAHWaEXyKHGfpUOc5U%252FxCw6DfmU%252Ffc2P%252Fuu%252F8DDpnFzboJaIXQY9jyrbS%252BeN4Pp%252FNeyHpaMPJ%252BLKsmjwj2WQOOXC0yS9DxlTe%252BAIAAA%253D%253D/timespan/2023-04-16T20%3A37%3A28.000Z%2F2023-04-28T20%3A37%3A28.000Z) and couldn't really find anything insightful. It does feel odd that we went that long with so *few* error messages, so maybe some silent error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1561910142:1156,message,messages,1156,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1561910142,1,['message'],['messages']
Integrability,"I see.. if there's a place where it's easy to check for this, I could add an error message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806#issuecomment-301753639:83,message,message,83,https://hail.is,https://github.com/hail-is/hail/issues/1806#issuecomment-301753639,1,['message'],['message']
Integrability,"I should have removed it when I was done. I used it to debug a bunch of stuff. I do not understand why, but, by default, no messages are printed. Setting the level to WARNING at least got my `log.warn` messages to print, of which I temporarily added many during debugging of insert. Anyway, I agree with the latter statement.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7990#issuecomment-579967208:124,message,messages,124,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579967208,2,['message'],['messages']
Integrability,I squashed this in https://github.com/hail-is/hail/pull/14057; more details in the PR message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13998#issuecomment-1834317436:86,message,message,86,https://hail.is,https://github.com/hail-is/hail/issues/13998#issuecomment-1834317436,1,['message'],['message']
Integrability,"I started hail this way:; ```; hailctl config set batch/billing_project hail; hailctl dev config set default_namespace default; HAIL_QUERY_BACKEND=service ipython; ```; ```ipython; In [1]: In [1]: import hail as hl ; ...: ...: ; ...: ...: temp = hl.utils.range_table(100) ; ...: ...: temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True) ; ...: ; Initializing Hail with default parameters...; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.61-15a45cfb9b0f; LOGGING: writing to /Users/dking/projects/hail/hail-20210202-1642-0.2.61-15a45cfb9b0f.log; Traceback (most recent call last):; File ""<ipython-input-1-92be8dd8c99f>"", line 4, in <module>; temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True); File ""</Users/dking/miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-1094>"", line 2, in write; File ""/Users/dking/projects/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/hail/hail/python/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 116, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 395, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backen",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:1069,wrap,wrapper,1069,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601,1,['wrap'],['wrapper']
Integrability,"I suppose we have lost to stand up a working CI (b/c it must talk to batcH) without a working router. Or, really, we've lost the ability to do that without copying the service definition from the router's list. I'm OK with this. I agree `make deploy` not also setting up the service definition is out of sync with our previous behavior. I could engineer some system to have one definition but two uses of the `Service` definition but it just doesn't feel important enough to justify it. We rarely if ever deploy the services by hand without deploying the router first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8504#issuecomment-611711990:94,rout,router,94,https://hail.is,https://github.com/hail-is/hail/pull/8504#issuecomment-611711990,3,['rout'],['router']
Integrability,I switched the docker retry function back to having a wrapper/curried because I got a clash with the kwargs for the name parameter in one function and figured this was safer then trying to rename the parameter to something that will never clash.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8049#issuecomment-592050430:54,wrap,wrapper,54,https://hail.is,https://github.com/hail-is/hail/pull/8049#issuecomment-592050430,1,['wrap'],['wrapper']
Integrability,I tested this by hand by replacing the actual function call with throwing a TimeoutError and making sure both the client and the batch-driver gave the appropriate warning in the log message and didn't proceed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7850#issuecomment-573430599:182,message,message,182,https://hail.is,https://github.com/hail-is/hail/pull/7850#issuecomment-573430599,1,['message'],['message']
Integrability,I think I've fixed the dataproc dependencies: https://github.com/hail-is/hail/pull/8576,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8571#issuecomment-615453991:32,depend,dependencies,32,https://hail.is,https://github.com/hail-is/hail/pull/8571#issuecomment-615453991,1,['depend'],['dependencies']
Integrability,"I think in an ideal world, every project including ours, our dependencies, and our end users, maintain public wide-open requirements files and private fully-pinned lockfiles. Our users know we need X and Y and they pin whatever versions are compatible with the union of our code and theirs. If we don't want our end users to have to do that, we need to compromise by narrowing the window and accepting the faulted but practical middle-ground. @jmarshall, out of curiosity does your group fully pin your dependencies / do you have any thoughts? It would be interesting to hear from the perspectives of end users but also third-party collaborators.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1520506773:61,depend,dependencies,61,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1520506773,2,['depend'],['dependencies']
Integrability,"I think it can still be a little tighter (though it's already a definite improvement over before). How does this sound -- it avoids 'key value' while still being precise:. ```; Tables are joined at rows that have the same value of non-missing key fields.; The inclusion of a row with no matching key in the opposite table depends on the; join strategy:. - **inner** -- Only rows with a matching key in the opposite table are included; in the resulting table.; - **left** -- All rows from the left table are included in the resulting table.; If a row in the left table has no match in the right table, then the fields; derived from the right table will be missing.; - **right** -- All rows from the right table are included in the resulting table.; If a row in the right table has no match in the left table, then the fields; derived from the left table will be missing.; - **outer** -- All rows are included in the resulting table. If a row in the right; table has no match in the left table, then the fields derived from the left; table will be missing. If a row in the right table has no match in the left table,; then the fields derived from the left table will be missing.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8467#issuecomment-609470292:322,depend,depends,322,https://hail.is,https://github.com/hail-is/hail/pull/8467#issuecomment-609470292,1,['depend'],['depends']
Integrability,I think it's transitively checking types in the dependencies of hailtop and we probably import hail in hailtop somewhere even though we really ought not to.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11734#issuecomment-1090454046:48,depend,dependencies,48,https://hail.is,https://github.com/hail-is/hail/pull/11734#issuecomment-1090454046,1,['depend'],['dependencies']
Integrability,I think one answer to this would be to just cut out the Java parts and show just the summary. But I also wrote up some further thoughts for how to improve here: https://dev.hail.is/t/better-python-error-messages-idea/201,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9226#issuecomment-672126952:203,message,messages-idea,203,https://hail.is,https://github.com/hail-is/hail/issues/9226#issuecomment-672126952,1,['message'],['messages-idea']
Integrability,"I think one of the following needs to happen:; 1. we document the pc relate setup sufficiently; 2. we precompute results somewhere that PC-Relate runs and test against that. I feel strongly that any PRs that introduce new testing dependencies must also include the relevant information to install those dependencies, probably in the ""getting started developing"" doc or somewhere like that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3274#issuecomment-377932960:230,depend,dependencies,230,https://hail.is,https://github.com/hail-is/hail/pull/3274#issuecomment-377932960,2,['depend'],['dependencies']
Integrability,"I think the current proposal is to have distributed and ""small"" dimensions, but the distributed dimensions specify their blocking (rather than just being marked ""distributed"" which I think is your proposal). Blocks aren't heterogenous in our current setup (last block can be short) and I think that needs to be clarified in your proposal. Slicing has similar complications. I think the blocking should specify the list of sizes of each block, which makes it closed under slicing. I agree the user interface should do implicit broadcast but in the IR it should be explicit. > If we want to support sparse tensors. I vote we punt on sparse vectors for the time being. (In the sparse case, I think sparsity of the output should be inferred by analysis rather than specified. I think statically knowing the sparsity relationship is going to a rarity.). The key point here is that Patrick's operations are incredibly general so this proposal needs to be paired with an compilation strategy that guarantees his operations are never actually realized directly (e.g. deforestation, fusing operations (e.g. broadcast) into their consumers) while also generating BLAS-level performance. I propose we plan to build against BLAS while we investigate more general codegen strategies. To that end, I've suggested we read up on a few related projects in study group: taco (hat tip Patrick), TensorComprehensions, TVM (tensor virtual machine) and Halide. Might be other relevant ones (TF/XLA?) Few questions to ask:; - Can we steal good ideas for our IR?; - Can we target their IR?; - Can we integrate this with our stack?; - What's the performance like (compile time, runtime)?. We'll need some more prosaic operators:; - constructors: from array with shape, from shape and a function of indices; - get shape.; - index: get an element from a tuple of integers. This is only allowed for local arrays.; - slice: get another ndarray from a sequence of integers, ranges, or array of indices.; - reshape. Maybe local only",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5190#issuecomment-459208120:497,interface,interface,497,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-459208120,1,['interface'],['interface']
Integrability,"I think the iterator interface would be much nicer, but I don't see how it could work with the per-sample aggregations.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1209#issuecomment-268823332:21,interface,interface,21,https://hail.is,https://github.com/hail-is/hail/pull/1209#issuecomment-268823332,1,['interface'],['interface']
Integrability,"I think the only two things I'm stuck on are:; (a) Do we want users to pass a VEPConfig instead of a `config` dictionary (and add documentation)?; (b) What is the best way to expose the VEP command interface so a user can customize it to their setup? I wanted to do something like this, but I don't see how to do this with the bash script being called with an argument `/bin/bash -c ""...."" csq` or `/bin/bash -c ""..."" vep`. ```python3; vep_85_grch37_command = '''; #!/bin/bash. if [ $VEP_CONSEQUENCE -ne 0 ]; then; vcf_or_json=""--vcf""; else; vcf_or_json=""--json""; fi. export VEP_COMMAND=/vep/vep \; ${VEP_INPUT_FILE:+--input_file $VEP_INPUT_FILE} \; --format vcf \; ${vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir=${VEP_DATA_DIR} \; --plugin LoF,human_ancestor_fa:${VEP_DATA_DIR}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:${VEP_DATA_DIR}/loftee_data/phylocsf_gerp.sql,gerp_file:${VEP_DATA_DIR}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT. exec vep.py ""$@""; '''. supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfig(; 'hail-qob-vep-grch37-us-central1',; ['us-central1'],; HAIL_GENETICS_QOB_VEP_GRCH37_IMAGE,; '/vep_data/',; {},; VEPConfig.default_vep_json_typ,; [""/bin/bash"", ""-c"", vep_85_grch37_command, ""vep""],; [""/bin/bash"", ""-c"", vep_85_grch37_command, ""csq_header""],; True,; 'gcp',; ),; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1498124947:198,interface,interface,198,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1498124947,1,['interface'],['interface']
Integrability,I think there's another way in which the union interfaces are bad - they don't reorder top-level struct fields. This isn't especially hard to do and would be a huge QoL improvement. We could also reorder column values automatically.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5406#issuecomment-466484251:47,interface,interfaces,47,https://hail.is,https://github.com/hail-is/hail/issues/5406#issuecomment-466484251,1,['interface'],['interfaces']
Integrability,"I think this code needed to be more complicated than you desired because you're not handling the case where someone can request something like this:. ```python3; j = b.new_job(); j.memory('standard'); j.cpu(1); j.preemptible(False); ```. Your solution would require that any non preemptible job would have to be like this:. ```python3; j = b.new_job(); j.machine_type('n1-standard-1', preemptible=False); ```. I was trying to make things easier for the user, especially once we live in a world where we have amazon, azure, etc. But maybe your interface is fine and we drop the preemptible method and put it on machine_type only. In our design discussions, we had decided that we'd support the former interface and select the correct / cheapest machine type for the user automatically. I think we should have another interface discussion, especially since we might be supporting other clouds, before we finalize this PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10283#issuecomment-841410983:543,interface,interface,543,https://hail.is,https://github.com/hail-is/hail/pull/10283#issuecomment-841410983,3,['interface'],['interface']
Integrability,I think this is a bad error message but should still be an error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4118#issuecomment-411796377:28,message,message,28,https://hail.is,https://github.com/hail-is/hail/pull/4118#issuecomment-411796377,1,['message'],['message']
Integrability,"I think this is closer to the right interface, so I want to think about how to get it in while allowing for future changes that don't break interface. I think `RegressionModel` should be initialized with the data of `y` and `covariates`. It should have a `fit` method that returns a struct with the multivariate regression result, which I can add later without breaking interface. The underlying math is already there (e.g. when computing the null model), just a matter of packaging. It'd be nice for this to work on Tables too. And then `regress_rows`, renamed fit_rows, should only take `x` and other parameters like `test` and `block_size`. This is basically how LMM works, but with the multivariate global math all on the python side.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4535#issuecomment-429487447:36,interface,interface,36,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-429487447,3,['interface'],['interface']
Integrability,I think this is dependent on the authorization PR going in so I can test it's working.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6719#issuecomment-514341053:16,depend,dependent,16,https://hail.is,https://github.com/hail-is/hail/pull/6719#issuecomment-514341053,1,['depend'],['dependent']
Integrability,"I think this is just pip not honoring retries everywhere. The crash appears to occur while downloading a dependency. Sure, I can add a retry to apt as well, though apt seems to be more careful about retrying on its own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9906#issuecomment-767034755:105,depend,dependency,105,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-767034755,1,['depend'],['dependency']
Integrability,"I think this is ready for another look but I put a WIP label on it because I don't want to merge it today while there's still workshop things happening. After it goes in I'll run another scale test. The nginx config is mostly just lifted over from router, with a couple small changes. I wasn't able to just proxy to localhost because notebook couldn't figure out which subdomain the request was going to and everything would 404. Adding a Host header fixed the 404, but messed with the requests to notebook pods so instead I added `workshop.local` and `notebook.local` to `/etc/hosts` on the pod, which I kind of like, which did the trick. I needed two ssl configs, one for nginx and one for aiohttp to use internally, and ended up changing the `ssl-config-notebook` secret to `nginx` and creating a second `ssl-config-notebook-python`, but let me know if this seems off.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10204#issuecomment-806087347:248,rout,router,248,https://hail.is,https://github.com/hail-is/hail/pull/10204#issuecomment-806087347,1,['rout'],['router']
Integrability,"I think this is ready. Adds a createDatabase2 step that adds database migrations. What's a database migration? The idea is that a database starts as an empty database and is built up or modified over time by a series of patches or migrations. The database has a version which is the number of migrations applied (starting from 1). This is stored in the table `{database_name}_migration_version`. Each migration involves running a `.sql` or `.py` script. The logic that applies migrations computes a checksum of these scripts and stores them in the database in table `{database_name}_migrations`. When applying migrations again in the future, these checksums are verified. A create database step now looks like (from the CI tests):. ```; - kind: createDatabase2; name: hello2_database; databaseName: hello2; migrations:; - name: create-tables; script: /io/sql/create-hello2-tables.sql; - name: insert; script: /io/sql/insert.py; inputs:; - from: /repo/ci/test/resources/sql; to: /io/; namespace:; valueFrom: default_ns.name; dependsOn:; - default_ns; - copy_files; ```. migrations is a the list of migrations that need to be applied to get the current version. So the idea is, if you want to change the schema of the database, you just add another migration at the end to make the changes you want. After this goes in, I'll make a separate PR to switch everything to this new createDatabase2 step.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7674#issuecomment-562891346:1024,depend,dependsOn,1024,https://hail.is,https://github.com/hail-is/hail/pull/7674#issuecomment-562891346,1,['depend'],['dependsOn']
Integrability,I think this is subsumed by the generic genotype interface + ongoing unsafe work.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/55#issuecomment-316223325:49,interface,interface,49,https://hail.is,https://github.com/hail-is/hail/issues/55#issuecomment-316223325,1,['interface'],['interface']
Integrability,I think this is the bug that will make me create a second JAR target that doesn't depend on Spark. Infuriating.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13018#issuecomment-1541022803:82,depend,depend,82,https://hail.is,https://github.com/hail-is/hail/pull/13018#issuecomment-1541022803,1,['depend'],['depend']
Integrability,"I think this issue is going to arise often with type imputation. As Tim wrote, you can also use --impute and say str(contig) or give it type string which will overwrite the imputation...but it's going to confuse folks. We could expand on the error message, or maybe we should implicitly convert Int to String for contig in the Variant constructor.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/520#issuecomment-236401286:248,message,message,248,https://hail.is,https://github.com/hail-is/hail/issues/520#issuecomment-236401286,1,['message'],['message']
Integrability,"I think this just isn't something you should do. You should use a newer version of hail than 0.2.60. We admittedly had a slightly rough transition into supporting multiple spark versions. It does seem like that message is a warning not an error, so you may be able to remove ` ""-Xfatal-warnings"", ` option from build.gradle and get things to build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10831#issuecomment-914325862:211,message,message,211,https://hail.is,https://github.com/hail-is/hail/issues/10831#issuecomment-914325862,1,['message'],['message']
Integrability,"I think tying the reset to the iterator is a mistake. First, iterator is the wrong abstraction here. Whole-stage code generation should use the aggregator/array strategy we're using in Emit to generate nothing, conditionals and loops for map, filter and flatMap, respectively. Ideally read ... do stuff ... write will generate an RDD with no per-element iterators at all. I want to make sure this picture is clear. Second, we want to vectorize in the database sense: we want to process multiple rows together in batches. Then overall structure of a stage is a loop over the batches, and and a loop within batches. Thus, the common case should not be we reset after every element, so I think it's the wrong direction to bake it in. The place where we do this should be interface points with the Spark stack which should be looked at with scorn and derision and as the organizing model. Finally, this points to an ongoing difference in our views about the meaning of context. I see context as serving two purposes (neither of which involve reset):. - First, context is a set of resources needed to process a partition that should be released when the partition is complete. For example, I'm working on GenomicsDB which needs to localize a GenomicsDB shard to a local file that needs to be cleaned up when the partition is complete. - Second, it is a way to tell an iterator where to return its value. (This is the ""current"" region business.). I'd be happy to separate these, but I don't see clean way. In no case do I see generic logic to manage the lifetime of regions (e.g. knowing when to call reset) inside the Context.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3365#issuecomment-381180739:768,interface,interface,768,https://hail.is,https://github.com/hail-is/hail/pull/3365#issuecomment-381180739,1,['interface'],['interface']
Integrability,"I think we might need a different name than ""mount"". In Docker, you can only use the same destination mount once. i.e. you can't have this. `--mount file1:/ --mount file2:/`. . @danking Do you have any thoughts on the interface for this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13812#issuecomment-1772965095:218,interface,interface,218,https://hail.is,https://github.com/hail-is/hail/pull/13812#issuecomment-1772965095,1,['interface'],['interface']
Integrability,"I think we need this fix:; ```; commit 4cb998d1c7cbc9954d66c6e39d7fd48b0e936f51 (HEAD -> add-version-endpoint); Author: Daniel King <dking@broadinstitute.org>; Date: Mon Mar 22 17:47:22 2021 -0400. fix. diff --git a/build.yaml b/build.yaml; index 7a100adec8..256ca99c91 100644; --- a/build.yaml; +++ b/build.yaml; @@ -86,7 +86,7 @@ steps:; mkdir repo; cd repo; {{ code.checkout_script }}; - make -C hail python/hail/hail_version python/hail/hail_pip_version; + make -C hail python-version-info; git rev-parse HEAD > git_version; outputs:; - from: /io/repo/auth/sql; diff --git a/ci/test/resources/build.yaml b/ci/test/resources/build.yaml; index 3b1df5214c..b994d2787c 100644; --- a/ci/test/resources/build.yaml; +++ b/ci/test/resources/build.yaml; @@ -27,10 +27,13 @@ steps:; mkdir repo; cd repo; {{ code.checkout_script }}; + make -C hail python-version-info; timeout: 300; outputs:; - from: /io/repo; to: /; + - from: /io/repo/hail/python/hail/hail_version; + to: /hail_version; dependsOn:; - inline_image; - kind: buildImage; @@ -52,6 +55,10 @@ steps:; publishAs: service-base; dependsOn:; - base_image; + - copy_files; + inputs:; + - from: /hail_version; + to: /hail_version; - kind: buildImage; name: hello_image; dockerFile: ci/test/resources/Dockerfile; ```; EDIT: updated with more changes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-804418107:982,depend,dependsOn,982,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-804418107,2,['depend'],['dependsOn']
Integrability,"I think we should do this by default, and remove the ability to annotate in this method. The interface is a bit too complicated",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1391#issuecomment-312126181:93,interface,interface,93,https://hail.is,https://github.com/hail-is/hail/issues/1391#issuecomment-312126181,1,['interface'],['interface']
Integrability,"I think we should find a time to discuss this in person if the following explanation doesn't make sense. . Right now, for small batches, we send one REST request to the server to both create the batch and create the jobs. However, if we want one REST request for an update (ideal for the query service and low latency jobs?), we have to use relative job ids because (1) we don't know the absolute start index of the jobs until we've gotten the start id of the update back from the server and (2) the job dependencies can be a mix of known job ids that have already been previously submitted in a previous creation/update. The negative job IDs are a way to deal with a mix of relative ids within an update and known, submitted job ids. We can simplify things if we require all updates make two requests to the server to (1) get the start id and establish the update and then (2) submit new jobs with all absolute job IDs. I'd have to make sure this will actually simplify things because I also ran into a bifurcation in how the job IDs are handled in `BatchBuilder.create_job()`. We currently populate the spec with a job id before we've made any requests to the server. We need to know how many total jobs there are before we can figure out the job ids because the API for creating a new update requires reserving a block of job IDs which then returns the start id. This complexity is because we allow multiple updates to occur simultaneously to a batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1215919856:504,depend,dependencies,504,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1215919856,1,['depend'],['dependencies']
Integrability,"I think you're slightly misinterpreting this bit:. > Patrick Schultz: I think we have most of the infrastructure needed to have a hail type hold a region. Then a lazily decoding PType can hold a (uniquely owned) region to decode into, invisibly to callers. > Patrick Schultz: In which case I don't think the region argument to loadElement would be needed. > Alex Kotlar:; In which case I don't think the region argument to loadElement would be needed; agreed. > Tim Poterba: ok, I think I'm convinced. In order to do a lazy bgen ptype, we need the *value*, not the type, to hold a region handle. PType doesn't need to be associated with a region -- that's a concept that doesn't really make sense given that PType is really a memory layout spec and codegen interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826#issuecomment-575727189:757,interface,interface,757,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575727189,1,['interface'],['interface']
Integrability,"I thought about the memory thing a bit more. I think we should leave the method, add a warning message that says ""We ignore this method now. It was always confusing. If you need more memory select a concomitant core count.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9598#issuecomment-714488332:95,message,message,95,https://hail.is,https://github.com/hail-is/hail/pull/9598#issuecomment-714488332,1,['message'],['message']
Integrability,"I thought we were going to load the genome reference upon initializing the HailContext. What happens if someone has two VDS's in the same session, but one is GRCh37 and the other is 38? Then we would have to ensure the reference is set to the correct one for every operation on the vds. It wouldn't be too difficult to add a decorator to the Python VDS to set the global reference on each method call. Then for a Join we can check the references are the same explicitly. If we go this route, I think we should not have the HailContext have the reference parameter, and instead have it as an optional parameter on `import_vcf`, `import_plink`, ...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1789#issuecomment-302429644:485,rout,route,485,https://hail.is,https://github.com/hail-is/hail/pull/1789#issuecomment-302429644,1,['rout'],['route']
Integrability,"I tried it in both raw python and pyspark and I got a new error. Seem to be a problem with the profile having too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:734,protocol,protocol,734,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978,2,['protocol'],['protocol']
Integrability,"I tried to use `Try[T]` but it doesn't permit me to specify the subclass of `Throwable`, as such consumers are limited to the interface of `Throwable`. This seems like a better solution. I added a minimal definition of the `Either` right-biased monad, we can grow it if necessary.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1075#issuecomment-259742354:126,interface,interface,126,https://hail.is,https://github.com/hail-is/hail/pull/1075#issuecomment-259742354,1,['interface'],['interface']
Integrability,"I used `RegionValueVariant` to clean up some of the code, and fixed a couple of bugs from #2451 in the process. I also replaced the `aggregatePartitions` method I wrote in e5f87c3 following @danking's comment, which was defined on `OrderedRDD2`, with `aggregateWithContext`, which is defined on a rich wrapper around `RDD`. I put it in the spark package to get access to private methods, making the implementation cleaner. It is now a simple modification of the implementation of `RDD.aggregate`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2423#issuecomment-347624179:302,wrap,wrapper,302,https://hail.is,https://github.com/hail-is/hail/pull/2423#issuecomment-347624179,1,['wrap'],['wrapper']
Integrability,I used the gsutil storage bandwidth tool and confirmed we get 1.2 Gibit / second upload and download speeds from within a 1 core job and 10 Gi storage. Adding more cores didn't change anything. I ran a test job with the copy tool on a 10 Gi random file and matched 1.2 Gibit / second. I'm wondering if the problem is actually workload-dependent and is based on the number of jobs / number of files. The GCS best practices states the initial capacity is 5000 read requests / second per bucket including list operations until the bucket has time to scale up its capacity. https://cloud.google.com/storage/docs/request-rate#best-practices. ```. ==============================================================================; DIAGNOSTIC RESULTS ; ==============================================================================. ------------------------------------------------------------------------------; Latency ; ------------------------------------------------------------------------------; Operation Size Trials Mean (ms) Std Dev (ms) Median (ms) 90th % (ms); ========= ========= ====== ========= ============ =========== ===========; Delete 0 B 5 43.1 6.4 40.9 50.9 ; Delete 1 KiB 5 44.2 12.7 42.5 58.1 ; Delete 100 KiB 5 44.7 10.4 42.8 56.3 ; Delete 1 MiB 5 41.5 3.7 40.2 45.7 ; Download 0 B 5 74.6 7.9 73.2 84.0 ; Download 1 KiB 5 84.3 15.9 80.6 103.4 ; Download 100 KiB 5 81.9 16.0 82.7 99.6 ; Download 1 MiB 5 90.6 6.5 94.5 96.8 ; Metadata 0 B 5 23.6 2.7 23.6 26.3 ; Metadata 1 KiB 5 25.5 2.1 26.9 27.4 ; Metadata 100 KiB 5 26.2 3.6 27.3 29.9 ; Metadata 1 MiB 5 24.0 3.7 23.3 28.4 ; Upload 0 B 5 98.1 16.6 95.5 117.9 ; Upload 1 KiB 5 116.7 21.8 115.5 142.1 ; Upload 100 KiB 5 116.5 17.8 115.1 135.1 ; Upload 1 MiB 5 168.2 18.5 179.6 185.6 . ------------------------------------------------------------------------------; Write Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Write thr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:335,depend,dependent,335,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['depend'],['dependent']
Integrability,"I verified this now works and also verified it fails on current main:; ```; In [2]: from hailtop.hail_logging import *; ...: import logging; ...: configure_logging(); ...: logging.getLogger('foo').info(""hello!""); ...: ; ...: try:; ...: raise ValueError('boom!'); ...: except:; ...: logging.getLogger('foo').exception(""hello!""); {""severity"":""INFO"",""levelname"":""INFO"",""asctime"":""2023-05-10 09:54:36,474"",""filename"":""<ipython-input-2-740eb5422cd6>"",""funcNameAndLine"":""<module>:4"",""message"":""hello!"",""hail_log"":1}; {""severity"":""ERROR"",""levelname"":""ERROR"",""asctime"":""2023-05-10 09:54:36,474"",""filename"":""<ipython-input-2-740eb5422cd6>"",""funcNameAndLine"":""<module>:9"",""message"":""hello!"",""exc_info"":""Traceback (most recent call last):\n File \""<ipython-input-2-740eb5422cd6>\"", line 7, in <module>\n raise ValueError('boom!')\nValueError: boom!"",""hail_log"":1}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13023#issuecomment-1542260535:478,message,message,478,https://hail.is,https://github.com/hail-is/hail/pull/13023#issuecomment-1542260535,2,['message'],['message']
Integrability,I very much like the interface. :+1:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4154#issuecomment-414640137:21,interface,interface,21,https://hail.is,https://github.com/hail-is/hail/pull/4154#issuecomment-414640137,1,['interface'],['interface']
Integrability,"I want cancel_after_n_failures to be on a job group. The things a job group doesn't have which maybe it should is:; - callback; - attributes; - updates. I think updates should be on a batch and not part of a job group. An update can add jobs to multiple job groups. Otherwise, the batches table should only have static fields that apply to the entire batch. I think we can do callbacks and attributes on a job group. I added a PATCH endpoint to be able to update a job group's cancel_after_n_attributes as the hailtop.batch interface was going to automatically generate job groups without any configuration settings. As for the full text search, I think prefix searches are faster with full text search than with a regular index, but I could be wrong. We'd have to benchmark it. > If we made batches simpler, does that ease complexity and decrease code duplication? In particular, what if batches didn't contain jobs at all? Instead, a batch contains exactly one job group. That job group contains zero or more job groups. Job groups manage: resource aggregation, cancellation, etc. I believe my plan is basically already doing this. It might not be clear because I didn't put the migrations in. But basically all of the current batches tables are now indexed by batch_id, job_group_id where the current ""batch"" has job_group_id = 1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12697#issuecomment-1450603163:524,interface,interface,524,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1450603163,1,['interface'],['interface']
Integrability,"I want to respond to some of your objections to unique_ptr. > If you buy into using std::unique_ptr, then everyone who writes or reads the code has to get; their head around the massively confusing and counter-intuitive concept of move semantics (a; form of assignment which modifies the source) and the somewhat bizarre terminology and syntax; used to express that in C++. I agree that move semantics takes getting used to, but I think it is much too integrated into modern C++ to ignore, going far beyond unique_ptr. Writing interfaces that take advantage of move semantics requires understanding rvalue-references in more detail, but for users of those move-enabled interfaces I think the guidelines are easy to teach: a variable will only be modified by moving if it is explicitly tagged with a `std::move`, so all you have to remember is ""after a `std::move(foo)`, the variable `foo` may only be assigned to or deleted."". > And then you get into a whole host of associated design decisions (I'm holding this as a unique_ptr,; but I want to pass it to a function - should I pass it as a raw pointer ? a raw reference ? a reference; to the unique_ptr ?). Keeping in mind the model that letting a function/class `foo` hold a `unique_ptr<Widget>` means explicitly ""`foo` owns this Widget, and is responsible for deleting it or passing ownership somewhere else"", these questions have pretty clear answers. * If a function `bar` takes a `Widget` but isn't concerned with its lifetime management, it should take its argument as a `Widget*` or `Widget&`, with the usual reasoning to choose between them. The caller owns the widget, and the lifetime of `bar` is nested inside that of its caller, so lifetime management isn't an issue.; * If `bar` takes a `Widget` and needs to take ownership, it should take its argument as a `unique_ptr<Widget>`. This serves as documentation that the function is taking over responsibility for deleting the Widget, in a way enforced by the compiler.; * A `unique_ptr<Wid",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3718#issuecomment-396669638:452,integrat,integrated,452,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396669638,3,"['integrat', 'interface']","['integrated', 'interfaces']"
Integrability,"I wanted to chime in on this briefly because I think it a good example use case and its design will influence many future methods, so it is important to get the design right. Thoughts:. - the underscore stuff is a non-starter in my opinion, and too clever by half. A lot of my feedback on your stuff is guided by the general heuristic that you should start by writing down the code you want, and then decide how to implement. You'd never want to write this _ stuff if you didn't have to. - I'm still not quite sure what tablify does (in part because the name is too clever by half and in part because it doesn't appear to always return tables). - But I think the idea of tablify is something we want, which is to convert (possibly indexed expressions) back into relational objects (Table, MatrixTable) because the latter support a wider set of operations and don't have the ""source mismatch problem"". Tim and I discussed this yesterday and we suggest the following interface:. ```; t = build_table(); .set_globals(x = 5, batch = batch); .set_rows(locus = locus, aaf = aaf); .build(); ```. and. ```; mt = build_table_matix(); .set_globals(dataset = dataset); .set_rows(locus = locus, aaf = aaf); .set_entries(GT = GT); .build(); ```. where the input expressions for each part must all come from the same source (or be compatible, e.g., constants) and the resulting (matrix) table inherits the keys from the original table. I think there is an unresolved question about how to handle potential name conflicts (e.g. a column key named locus).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2852#issuecomment-363841964:965,interface,interface,965,https://hail.is,https://github.com/hail-is/hail/pull/2852#issuecomment-363841964,1,['interface'],['interface']
Integrability,"I was having trouble figuring out how to handle the token and the attributes in hailtop.batch_client.aioclient.Batch. When we create an update from a Batch that already existed perhaps in a different process, we don't have the attributes and token. I made a contract where `commit_update` always returns the token and attributes regardless of whether the BatchBuilder already has that infromation. However, we could also get that information available lazily and cache the result. In addition, the `n_jobs` returned to the client are the number of jobs that are committed and not the same as the `n_jobs` in the batches table. Things to do before merging:; 1. Get rid of the batch updates additions to the UI2. ; 2. Double check the GCP LogsExplorer to make sure there are no silent error messages especially with regards to cancellation.; 3. Have @danking look over the SQL stored procedure for `commit_batch_update` to make sure that query is going to perform as good as what is possible given the complexity of the check.; 4. Run a test batch with the old client (I just checked out the current version of main). You need to make sure both create and create-fast are accounted for and succeed. I've been using the following script to make sure we're using the slow path in addition to the fast path with a regular small test job:. ```python3; from hailtop.batch import ServiceBackend, Batch; import secrets. backend = ServiceBackend(billing_project='hail'); b = Batch(backend=backend); # 8 * 256 * 1024 = 2 MiB > 1 MiB max bunch size; for i in range(8):; j1 = b.new_job(); long_str = secrets.token_urlsafe(256 * 1024); j1.command(f'echo ""{long_str}"" > /dev/null'); batch = b.run(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1226043347:258,contract,contract,258,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1226043347,2,"['contract', 'message']","['contract', 'messages']"
Integrability,I was playing whack-a-mole with randomness tests and adding the GCS_REQUESTER_PAYS_BUCKET flag and the flags interface and testing infrastructure was making it impossible for the VEP tests to pass or the other tests for randomness that were failing. I'll revisit this PR again next week to see if your flags PR actually fixed the issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1426177281:109,interface,interface,109,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1426177281,1,['interface'],['interface']
Integrability,I went down that route once before and the main issue is how to trigger the change in batch state to completed. I couldn't figure out how to do that correctly and efficiently.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039624594:17,rout,route,17,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039624594,1,['rout'],['route']
Integrability,"I wonder if, using a consistent template, we could have CI do this, either by looking at the first comment, or by looking at the commit. Look for ""depends on: #hash"" string, much like GitHub does with ""closes #hash"" in commits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7417#issuecomment-548480282:147,depend,depends,147,https://hail.is,https://github.com/hail-is/hail/issues/7417#issuecomment-548480282,1,['depend'],['depends']
Integrability,"I'd probably suggest a better error message if possible, but if you don't want to, go ahead and close",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465#issuecomment-386372161:36,message,message,36,https://hail.is,https://github.com/hail-is/hail/issues/3465#issuecomment-386372161,1,['message'],['message']
Integrability,"I'd propose to do an implicit dependency audit every time you push a new commit. You can still pin versions on published packages, but use unpinned dependencies for CI testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542208997:30,depend,dependency,30,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542208997,2,['depend'],"['dependencies', 'dependency']"
Integrability,I'll look at this once the Lazy Router goes in (#11063),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11072#issuecomment-972974896:32,Rout,Router,32,https://hail.is,https://github.com/hail-is/hail/pull/11072#issuecomment-972974896,1,['Rout'],['Router']
Integrability,I'll pick this one up in medium term since NDArray stuff has involved a lot of fleshing out of numpy integration,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3699#issuecomment-550356741:101,integrat,integration,101,https://hail.is,https://github.com/hail-is/hail/issues/3699#issuecomment-550356741,1,['integrat'],['integration']
Integrability,I'll put point_type back in for the sake of compatibility. This will depend on https://github.com/hail-is/hail/pull/5138 so I'm waiting for that to go in first.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5152#issuecomment-455895729:69,depend,depend,69,https://hail.is,https://github.com/hail-is/hail/pull/5152#issuecomment-455895729,1,['depend'],['depend']
Integrability,I'm a little confused. The snippet in the PR body gives impression that the sphinx injection happens after inserting the headers/footers. Should the raw blocks be the other way around?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10278#issuecomment-813632797:83,inject,injection,83,https://hail.is,https://github.com/hail-is/hail/pull/10278#issuecomment-813632797,1,['inject'],['injection']
Integrability,I'm convinced this is entirely possible in the 0.2 python interface,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/332#issuecomment-422364559:58,interface,interface,58,https://hail.is,https://github.com/hail-is/hail/issues/332#issuecomment-422364559,1,['interface'],['interface']
Integrability,"I'm currently running this branch of CI on a pull request of itself on my own fork of hail, and it nearly passes all tests except for hailtop_batch_* because of requester pays permissions issues and monitoring, because I don't have a service account in my project with all the permissions for broad-ctsa. So unfortunately haven't fully validated that it will _not_ merge a passing PR, but this seemed good enough that we can push it through for azure (since both of these errors are gcp-dependent). If this goes through I can put in a follow-up PR that mirrors the infra resources that CI needs in azure (blob storage, acr permissions, etc.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11053#issuecomment-964437539:487,depend,dependent,487,https://hail.is,https://github.com/hail-is/hail/pull/11053#issuecomment-964437539,1,['depend'],['dependent']
Integrability,"I'm fine removing copy-paste-tokens. They were for a prototype with Terra. We obviously are pursuing a different approach now. Hmm. I suppose old versions of hailctl have no way to know that the fix is to upgrade to a newer version of hailctl? Like, the server can't send a message in the auth failure? We can just ask our local users to upgrade. As long as there's a stable & robust version of query that they can rely on, I think they're happy to upgrade. Which version of hailctl is compatible with new auth?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13531#issuecomment-1767018024:274,message,message,274,https://hail.is,https://github.com/hail-is/hail/issues/13531#issuecomment-1767018024,1,['message'],['message']
Integrability,"I'm going to close this and revisit it after (1) we're handling wrapping for the stream code, and (2) we can generate roughly comparable code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8148#issuecomment-591935928:64,wrap,wrapping,64,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-591935928,1,['wrap'],['wrapping']
Integrability,I'm going to start rejecting PRs with non-descriptive names like this (I would have if I caught it!). It makes changelog generation extremely difficult!. I think it's especially important to have good messages when we're force-merging a change.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6168#issuecomment-495293829:201,message,messages,201,https://hail.is,https://github.com/hail-is/hail/pull/6168#issuecomment-495293829,1,['message'],['messages']
Integrability,I'm happy with this as long as the interface is not public in hailtop.batch. I'll look over it once more carefully to make sure nothing has changed since it was first opened and then take care of the rebase.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11944#issuecomment-1302571268:35,interface,interface,35,https://hail.is,https://github.com/hail-is/hail/pull/11944#issuecomment-1302571268,1,['interface'],['interface']
Integrability,I'm happy with this initial interface: typecheck and typecheck_method. Back to you!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1727#issuecomment-301183677:28,interface,interface,28,https://hail.is,https://github.com/hail-is/hail/pull/1727#issuecomment-301183677,1,['interface'],['interface']
Integrability,I'm just going to close the other PR so that we get the overall in (I think here you've approved both the dependent PR and the stacked content),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8920#issuecomment-640990714:106,depend,dependent,106,https://hail.is,https://github.com/hail-is/hail/pull/8920#issuecomment-640990714,1,['depend'],['dependent']
Integrability,"I'm merging this so Sali can use the new interface and better performance, and I am going to add an additional test in another PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2544#issuecomment-351097552:41,interface,interface,41,https://hail.is,https://github.com/hail-is/hail/pull/2544#issuecomment-351097552,1,['interface'],['interface']
Integrability,I'm not sure of the protocol regarding stacked PRs,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3993#issuecomment-409674187:20,protocol,protocol,20,https://hail.is,https://github.com/hail-is/hail/pull/3993#issuecomment-409674187,1,['protocol'],['protocol']
Integrability,"I'm not sure this is a good idea -- repartition shuffle=False should basically be treated as giving something a maximum number of output partitions, which could be smaller. Still need to figure out where partitions fit into the relational interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3756#issuecomment-422360127:239,interface,interface,239,https://hail.is,https://github.com/hail-is/hail/issues/3756#issuecomment-422360127,1,['interface'],['interface']
Integrability,"I'm not sure what the class loading issues are, there's probably some dependency conflict that's been introduced by pulling in avro. Need to investigate, and possibly relocate avro.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10793#issuecomment-902307018:70,depend,dependency,70,https://hail.is,https://github.com/hail-is/hail/pull/10793#issuecomment-902307018,1,['depend'],['dependency']
Integrability,"I'm not sure, but it's the only difference I could find between session messages that worked on the front end and the messages not showing up from the driver. I want to test this explicitly, but my namespace is currently messed up in Azure with previous database changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11094#issuecomment-984813805:72,message,messages,72,https://hail.is,https://github.com/hail-is/hail/pull/11094#issuecomment-984813805,2,['message'],['messages']
Integrability,"I'm obviously sympathetic to this (I've even argued matrix should not have row/col annotations at all!) But I have a few questions about how this will work:. The main interface problem I see is that there are naturally columnless matrices in our domain (sites files). This PR was motivated by some code that Konrad sent me. Are we going to have two versions of VEP and export_vcf, for example?. Second, it seems we can have either interface purity or performance in the short term. I loosely think the latter is better (I would) so we can productively move people off 0.1 even if that means the interface is less stable (e.g. eventually we'll deprecate/remove the drop functions). I want to say I can be convinced otherwise, but changing a range join to a table shuffle seems like a non-starter.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2649#issuecomment-355362499:167,interface,interface,167,https://hail.is,https://github.com/hail-is/hail/pull/2649#issuecomment-355362499,3,['interface'],['interface']
Integrability,I'm personally in favor of this change adding a switch to the protocol. It avoids running the compiler.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11598#issuecomment-1069292016:62,protocol,protocol,62,https://hail.is,https://github.com/hail-is/hail/pull/11598#issuecomment-1069292016,1,['protocol'],['protocol']
Integrability,I'm pretty sure GitHub will merge this with the commit message (instead of PR title/comments) since it's a single commit. oh well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6272#issuecomment-499524823:55,message,message,55,https://hail.is,https://github.com/hail-is/hail/pull/6272#issuecomment-499524823,1,['message'],['message']
Integrability,"I'm ready to approve, but I'm not sure why the build is failing, and I don't know what the protocol is now for approving PRs failing tests. Seems like it increases the chances of merging a dumb mistake.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4123#issuecomment-412981153:91,protocol,protocol,91,https://hail.is,https://github.com/hail-is/hail/pull/4123#issuecomment-412981153,1,['protocol'],['protocol']
Integrability,I'm reverting this to 1 so Konrad can get some work done this weekend -- will figure out why the client message size is still failing next week.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6444#issuecomment-504634470:104,message,message,104,https://hail.is,https://github.com/hail-is/hail/pull/6444#issuecomment-504634470,1,['message'],['message']
Integrability,"I'm still looking, but I could only find the logs for PR 13509 as PR 13458 is too old. There are no batch worker logs at all for these two instances, but there are a bunch of sys logs. I didn't see an obvious error message, but there's 1000s of sys log messages in there. https://console.cloud.google.com/logs/query;query=resource.type%3D%22gce_instance%22%0Alabels.%22compute.googleapis.com%2Fresource_name%22:%22batch-worker-pr-13509-default-p2aogbaogrsp-highmem-np-zx6w4%22;summaryFields=:false:32:beginning;cursorTimestamp=2023-08-29T20:39:28Z;aroundTime=2023-08-29T20:16:33.950Z;duration=PT24H?project=hail-vdc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13554#issuecomment-1736282516:215,message,message,215,https://hail.is,https://github.com/hail-is/hail/issues/13554#issuecomment-1736282516,2,['message'],"['message', 'messages']"
Integrability,"I'm thinking that I want to have this:. Basic setup for the domain, remote_tmpdir, billing_project, and only select one region; ```; hailctl init; ```. Which will prompt for:; 1. GCP project; 2. Hail domain; 3. region; 4. remote tmpdir location. And set:; - domain; - batch/billing_project; - batch/regions; - batch/remote_tmpdir; - batch/backend; - query/backend. And then print out a message with the default settings with instructions on how to change any of the settings and warnings about the configuration by using `hailctl config set ...`. I'll have the autocomplete PR merged (hopefully) by then so we can make that nice. The trial billing project is automatically used and we don't prompt for that. Then we'll have optional flags for components to configure; ```; hailctl init --container-registry --requester-pays-buckets --extra-query-settings --extra-batch-settings; ```; This would prompt for the same as above as well as advanced regions settings, query settings, and create the container registry. And lastly; ```; hailctl config check; ```; which prints warnings about misspecified regions and tmpdirs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1650549451:386,message,message,386,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1650549451,1,['message'],['message']
Integrability,I'm unassigning myself after our Zulip discussion on the intended job dependency graph in batch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4514#issuecomment-437121438:70,depend,dependency,70,https://hail.is,https://github.com/hail-is/hail/issues/4514#issuecomment-437121438,1,['depend'],['dependency']
Integrability,"I've added a second commit that fixes the remainder of #13191, marking the individual JobResourceFiles within the ResourceGroup as `_mentioned` and hence preventing the undefined resource BatchException previously observed. (Some of the tests in _hail/python/test/hailtop/batch/test_batch.py_ would also need adjusting to account for the re-imagined `_mentioned`.). Having now studied f6fe19c085a9d9ebee23866961cb582a713cc1ad, which introduced `_mentioned` and this error message hint, IMHO this is a reasonable fix. Apart from the code in _backend.py_ to do with `symlink_input_resource_group`, which I haven't looked at, `_mentioned` is maintained solely to decide whether to emit this BatchException hinting to the user that the resource ought to be defined if you're going to use it in `write_output`. In this case, because the filenames are related, `foo.gz.tbi` may well have been created even though only `foo.gz` appears explicitly in the command text, so it may be a false positive (as in #13191's case) to raise the exception. So the conservative thing to do is to suppress the message in these `declare_resource_group` cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13192#issuecomment-1600755633:474,message,message,474,https://hail.is,https://github.com/hail-is/hail/pull/13192#issuecomment-1600755633,2,['message'],['message']
Integrability,I've been letting this cook all day with no unforeseen problems so unless someone redeploys gateway it is the current working version. I've also left the `router-resolver` and `router` changes as two commits in case it might be better to deal with the two services independently.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10207#issuecomment-803130352:155,rout,router-resolver,155,https://hail.is,https://github.com/hail-is/hail/pull/10207#issuecomment-803130352,2,['rout'],"['router', 'router-resolver']"
Integrability,"I've eventually reproduced this. I was a bit thrown that the error message in this issue is different to the one in the op. Here's my work so far:; ```python; # create `variants` heterogeneous array as described in the op; variants = [[""10"", 123, ""G"", ""C""], [""10"", 456, ""T"", ""A""]]. # not sure how the `mt` was created, but not sure it's important for the; # purposes of reproducing the failure; mt = hl.struct(; locus=hl.locus(contig='10', pos=60515, reference_genome='GRCh37'),; alleles=['C', 'T']; ). expr = hl.any(; lambda x:; (mt.locus.contig == hl.literal(x[0])) & \; (mt.locus.position == hl.literal(int(x[1]))) & \; (mt.alleles == hl.literal(x[2:])),; variants; ). hl.eval(expr); ```; This fails in the call to `any` with the following: ; ```; Traceback (most recent call last):; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_typecheck.py"", line 78, in check; return self.coerce(to_expr(x)); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 275, in to_expr; return cast_expr(e, dtype, partial_type); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 281, in cast_expr; dtype = impute_type(e, partial_type); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 129, in impute_type; t = _impute_type(x, partial_type=partial_type); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 179, in _impute_type; ts = {_impute_type(element, partial_type.element_type) for element in x}; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 179, in <setcomp>; ts = {_impute_type(element, partial_type.element_type) for element in x}; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 182, in _impute_type; raise ExpressionException(""Hail does not support heterogeneous arrays: ""; hail.expr.expressions.base_e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:67,message,message,67,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['message'],['message']
Integrability,"I've rebased `jbloom22:lmm_getthisin` onto `jbloom22:py_reg` (which should go in first) and made the parallel modifications (LinearMixedModelCommand is gone, command line relics gone, refactored using RegressionUtils, tests modified to accommodate changing interface, etc). I'll do some command line testing next to be safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1064#issuecomment-272765058:257,interface,interface,257,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-272765058,1,['interface'],['interface']
Integrability,"I've removed the Python `tempfile` approach in favor of adding `new_local_temp_file` to utils and a corresponding function to HailContext in Scala, which currently hardcodes `file:///temp` as the local temp directory. It may be more natural to have a localTmpDir on HailContext like we have tmpDir. ; I see there is a notion of local temp files on TempDir on the Scala side, but it doesn't seem to be used on the Python side. I also don't see if/where we wipe temp files on exit. In any case, I've tested that now it all works nicely on GCP, so ready for feedback/review. I think factoring through `tofile` and `fromfile` is useful for wider interoperability for the same reason that NumPy exposes them, but it's also good if you dont want to actually load the NumPy array into driver memory but just save it to read/copy later, or to load it multiple time without recomputing the BlockMatrix. And I've provided the simpler interface of `to_numpy` and `from_numpy` for the common case. I suspect that (de)serializing over the network and building the local matrix dominates local read/write, so that using a socket isn't going to do much better. I can profile more closely if/when we feel it's high priority to make this faster still.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3114#issuecomment-372165433:642,interoperab,interoperability,642,https://hail.is,https://github.com/hail-is/hail/pull/3114#issuecomment-372165433,2,"['interface', 'interoperab']","['interface', 'interoperability']"
Integrability,"I've told you before dependabot, this doesn't work",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12477#issuecomment-1319090640:21,depend,dependabot,21,https://hail.is,https://github.com/hail-is/hail/pull/12477#issuecomment-1319090640,1,['depend'],['dependabot']
Integrability,I've updated the PR accordingly  feel free to wordsmith the warning message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14700#issuecomment-2384183254:69,message,message,69,https://hail.is,https://github.com/hail-is/hail/pull/14700#issuecomment-2384183254,1,['message'],['message']
Integrability,"I've updated this to allow stream consumers to choose an allocation strategy independently of the producer: the producer is written to allocate temporary regions that it owns, as needed, but the consumer can override that behavior to actually put all temporary allocations in a single region. This is implemented via the two abstract interfaces `StagedRegion` and `StagedOwnedRegion`, which have two implementations, one of which can create temporary regions, and one which cannot. The producer is written using the interface, and the consumer passes in one of the concrete implementations, determining the allocation strategy. Currently, only the non-allocating instance is ever used, which generates the same code we did before, so this should have zero run-time effect, even if we continue to update more stream nodes to use smarter region management. We can figure out separately how to introduce the other strategy (use less memory, but with more allocator overhead) in a controlled way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9106#issuecomment-664591165:334,interface,interfaces,334,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-664591165,2,['interface'],"['interface', 'interfaces']"
Integrability,"If it's an important VCF, it shouldn't be corrupted... My solution to this error message will be to add something like `requirement failed: ref was equal to alt` or something like that",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/361#issuecomment-216535838:81,message,message,81,https://hail.is,https://github.com/hail-is/hail/issues/361#issuecomment-216535838,1,['message'],['message']
Integrability,"If we go through route (2), this project can serve as a prototype C or C++ interface to Hail. This interface could take multiple forms. For example, we could actually re-build our memory representation implementations in C++ and compile SAIGE, at Hail-Query-compile-time (i.e. when we are compiling a *user's* query), to use whatever SType/PType that Hail has decided is the ideal. A simpler approach is to implement one canonical implementation of the Hail types in C++, fork & slightly modify SAIGE to accept these memory representations, compile SAIGE at Java compile time (i.e. in CI or when you run `make` on your laptop) against these mem reps, ship the compiled library with the Hail JAR, and expose it, via JNI, into the Hail Query language. This requires that the Query compiler can call a function which only supports arguments using one particular SType/PType.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13442#issuecomment-1679739816:17,rout,route,17,https://hail.is,https://github.com/hail-is/hail/issues/13442#issuecomment-1679739816,3,"['interface', 'rout']","['interface', 'route']"
Integrability,"If with my changes it is still slower, then let's abandon. My bad for heading us down this route. Seems natural to use binary, but given that pandas probably has native code for parsing TSVs, maybe we're fighting a losing battle.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11368#issuecomment-1057594484:91,rout,route,91,https://hail.is,https://github.com/hail-is/hail/pull/11368#issuecomment-1057594484,1,['rout'],['route']
Integrability,"If you do build a sites-only export routine, now's the time to request it work on Tables in addition to (or instead of, doesn't matter too much) MatrixTables",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4338#issuecomment-421403518:36,rout,routine,36,https://hail.is,https://github.com/hail-is/hail/issues/4338#issuecomment-421403518,1,['rout'],['routine']
Integrability,"In trying to test this (from your branch, ran pip install on /hail/python just in case). ```sh; (hail) alex:~/projects/hail/hail/python:$ hailctl dev deploy -b cseed:batch-web -s deploy_auth,deploy_router,deploy_notebook2; Traceback (most recent call last):; File ""/miniconda3/envs/hail/bin/hailctl"", line 11, in <module>; sys.exit(main()); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/__main__.py"", line 100, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/cli.py"", line 52, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 66, in main; loop.run_until_complete(submit(args)); File ""/miniconda3/envs/hail/lib/python3.6/asyncio/base_events.py"", line 484, in run_until_complete; return future.result(); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 57, in submit; batch_id = await ci_client.dev_deploy_branch(args.branch, steps); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 46, in dev_deploy_branch; self._deploy_config.url('ci', '/api/v1alpha/dev_deploy_branch'), json=data) as resp:; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 1005, in __aenter__; self._resp = await self._coro; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 581, in _request; resp.raise_for_status(); File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; headers=self.headers); aiohttp.client_exceptions.ClientResponseError: 500, message='Internal Server Error'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733:1627,message,message,1627,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733,1,['message'],['message']
Integrability,"Indeed, if i do `hl.literal()`, I get the right error message:; ```; hail.expr.expressions.base_expression.ExpressionException: Hail does not support heterogeneous dicts: found dict with values of types [dtype('str'), dtype('int32')]; ```; But fixing the value types upstream removes the requirement for `hl.literal()` and works",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3886#issuecomment-401931653:54,message,message,54,https://hail.is,https://github.com/hail-is/hail/issues/3886#issuecomment-401931653,1,['message'],['message']
Integrability,"Instead of creating a hail context, I've exposed the top level `init` and `stop` methods in hail2. . Env.hc() will call init if it's not been called yet, and print a message about initializing with default params.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2820#issuecomment-360908387:166,message,message,166,https://hail.is,https://github.com/hail-is/hail/pull/2820#issuecomment-360908387,1,['message'],['message']
Integrability,Is there a new dependency? Just checked out that branch and got missing `catch.hpp`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4816#issuecomment-440640835:15,depend,dependency,15,https://hail.is,https://github.com/hail-is/hail/issues/4816#issuecomment-440640835,1,['depend'],['dependency']
Integrability,Is there any reason not to give `HardCallView` an `Array`-like interface? E.g.; ```scala; def apply(i: Int): Option[Int] = {; setGenotype(i); if (hasGT) Some(getGT) else None; }; ```; That seems like a much more comfortable view interface.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2368#issuecomment-340856253:63,interface,interface,63,https://hail.is,https://github.com/hail-is/hail/pull/2368#issuecomment-340856253,2,['interface'],['interface']
Integrability,"Is there any way the R interface can do things the new way, or do things; need to happen on the Scala side first? Perhaps someone could notify us; when the API has relatively stabilized?. On Thu, Feb 14, 2019, 6:53 AM Tim Poterba <notifications@github.com wrote:. > Yes. We intend to have the Python frontend and scala backend communicating; > only through the methods on the Backend class in backend.py; >; > ; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/5340#issuecomment-463655588>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AAJp7o6ZVZCdyRH4ixYHFdCItQ-wCy9kks5vNXg3gaJpZM4a583l>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5340#issuecomment-463825757:23,interface,interface,23,https://hail.is,https://github.com/hail-is/hail/issues/5340#issuecomment-463825757,1,['interface'],['interface']
Integrability,"It also fails on this simpler example:; ```; In [1]: import hail as hl ; ...: ; ...: temp = hl.utils.range_table(100) ; ...: temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True) ; Initializing Hail with default parameters...; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.61-c548354b6e81; LOGGING: writing to /Users/dking/projects/hail/hail-20210107-1038-0.2.61-c548354b6e81.log; Traceback (most recent call last):; File ""<ipython-input-1-a2e56feaf799>"", line 4, in <module>; temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True); File ""</Users/dking/miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-1092>"", line 2, in write; File ""/Users/dking/projects/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/hail/hail/python/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 114, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 379, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 44, in async_request; raise FatalError(f'Error from server: {result[""value""]}'); FatalError: Error from server: java.util.NoSuchElementExce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:789,wrap,wrapper,789,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693,1,['wrap'],['wrapper']
Integrability,It depends on task completion order.  . And I think you've convinced me.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5015#issuecomment-448851223:3,depend,depends,3,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448851223,1,['depend'],['depends']
Integrability,"It looks like it's called `netcdf` on `brew`. ```; dking@wmb16-359 # brew info netcdf; netcdf: stable 4.6.0 (bottled); Libraries and data formats for array-oriented scientific data; https://www.unidata.ucar.edu/software/netcdf; Not installed; From: https://github.com/Homebrew/homebrew-core/blob/master/Formula/netcdf.rb; ==> Dependencies; Build: cmake ; Required: hdf5 , gcc ; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3273#issuecomment-377930479:326,Depend,Dependencies,326,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377930479,1,['Depend'],['Dependencies']
Integrability,It might be easier to invent a new domain name for VPC-internal clients. `batch.hail.internal`. Add the right DNS routes. Add new root certs to the worker nodes. That ensures our DNS changes don't break anyone else who is trying to talk to `batch.hail.is`. We should maybe talk more about what these DNS changes are breaking so we can plan our network infrastructure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6918#issuecomment-523662443:114,rout,routes,114,https://hail.is,https://github.com/hail-is/hail/pull/6918#issuecomment-523662443,1,['rout'],['routes']
Integrability,"It seems that it does happen after the header is inserted but the footers are inserted after the Sphinx injection for the main block. So, will we need to create a separate CSS file for when the website runs through a second time it is pulling the most up-to-date styling changes?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10278#issuecomment-813640189:104,inject,injection,104,https://hail.is,https://github.com/hail-is/hail/pull/10278#issuecomment-813640189,1,['inject'],['injection']
Integrability,"It seems the test failures are due to:; 1. TemporaryDirectory (and TemporaryFilename); 2. `hailtop.batch.backend.ServiceBackend` absolutely should not use sync `BatchClient`, the async one is right there!; 3. `hailctl batch submit` is broken because of (2); 4. `test_callback` should use async `BatchClient` b/c it is async. TemporaryDirectory & TemporaryFilename use `hailtop.fs`, which is sync. This is nearly the async FS API except:; 1. `isfile` vs `is_file`; 2. `isdir` vs `is_dir`; 3. `stat` returns a `FileListEntry` instead of a `FileStatus`.; 4. `listfiles` vs `ls`. `hailtop.fs.router_fs.RouterFS` is a sync shim between these APIs. So there's basically sync-vs-async and Python-vs-Hail FS APIs. We have:; 1. sync, Python: `hailtop.fs.FS`.; 2. async, Python: does not exist.; 3. async, Hail: `hailtop.aiotools.fs.FS`.; 4. sync, Hail: `hailtop.fs.router_fs.RouterFS`. If we had (2), we could write an async version of TemporaryDirectory and TemporaryFilename and use those in async methods (in particular, in `hail.backend.ServiceBackend`). The high-level need is that we gotta be careful about not interleaving async-sync-async. Your PR reveals that we were inadvertently violating that rule. It seems best to follow the rule and only use `nest_asyncio` when we're in a Jupyter Notebook.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13677#issuecomment-1743171933:598,Rout,RouterFS,598,https://hail.is,https://github.com/hail-is/hail/pull/13677#issuecomment-1743171933,2,['Rout'],['RouterFS']
Integrability,"It sounds like you were using the old command-line interface. Hail has moved to a python API, documented here:; https://hail.is/hail/api.html; https://hail.is/hail/getting_started.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1807#issuecomment-301447221:51,interface,interface,51,https://hail.is,https://github.com/hail-is/hail/issues/1807#issuecomment-301447221,1,['interface'],['interface']
Integrability,"It would lessen the load on aiohttp, to something that may be better suited to handling large messages. Then store the result in gcs, and respond with the address of the object",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5898#issuecomment-484219314:94,message,messages,94,https://hail.is,https://github.com/hail-is/hail/issues/5898#issuecomment-484219314,1,['message'],['messages']
Integrability,"It's a vaguely confusing message, but the part in the brackets indicates that we expected this to fail but instead it passed. The part after the brackets is the default message associated with the `fails_service_backend` annotation; ```; [XPASS(strict)] doesn't yet work on service backend; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11445#issuecomment-1062288146:25,message,message,25,https://hail.is,https://github.com/hail-is/hail/pull/11445#issuecomment-1062288146,2,['message'],['message']
Integrability,It's already a hail dependency.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10366#issuecomment-825911342:20,depend,dependency,20,https://hail.is,https://github.com/hail-is/hail/pull/10366#issuecomment-825911342,1,['depend'],['dependency']
Integrability,"John, I merged in your changes. Let me know if you had something else in mind for an interface. I moved all implementations, besides codeOrdering to PCanonicalNDArray because in my mind it's harder to track implementations across multiple loci, but let me know if you feel PNDArray needs some default methods (for instance, I could imagine numElements to be this way, but decided not to, because its signature really depends on the shape signature, which lives on PCanonicalNDArray)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7734#issuecomment-567051493:85,interface,interface,85,https://hail.is,https://github.com/hail-is/hail/pull/7734#issuecomment-567051493,2,"['depend', 'interface']","['depends', 'interface']"
Integrability,"Julia, sorry I've been letting this sit. I'm totally fine with these changes, but what I want to sort out is the following issue -- . Most of the VDS functions are really built to directly support gnomAD / CCDG activities, and the default function usages should reflect the standard or best practice applications that your team converges on. I think if gnomAD is going to typically use the `use_variant_dataset` mode, we should make that default. Is that the plan? Does it depend on genome/exome data?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11701#issuecomment-1099036554:473,depend,depend,473,https://hail.is,https://github.com/hail-is/hail/pull/11701#issuecomment-1099036554,2,['depend'],['depend']
Integrability,"Just a heads up, the docs aren't rendered. But the interface looks awesome!!!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4004#issuecomment-408972048:51,interface,interface,51,https://hail.is,https://github.com/hail-is/hail/pull/4004#issuecomment-408972048,1,['interface'],['interface']
Integrability,Just fix the error message in AST and I'm happy with it!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/663#issuecomment-242153148:19,message,message,19,https://hail.is,https://github.com/hail-is/hail/pull/663#issuecomment-242153148,1,['message'],['message']
Integrability,"Just saw this. > adds job.entrypoint method. I'm going to argue against this. The goal is not to make Batch a Docker wrapper, in fact, maybe that's an anti-goal. The goal is to build on docker (and other container tools) to create a natural interface for the problems Batch is trying to solve. The expectation is that the commands to be executed will be specified as part of the Job. In particular, I think if you don't specify the command, the command set is empty and no commands get run. It doesn't default to the docker command by default when no command is specified. So I would solve this by just always invoking the image with `--entrypoint <shell>`. > Long term, I think it would be better to use docker engine for LocalBackend, so that container config/create/start between ServiceBackend and LocalBackend could have a common interface. While code sharing often sounds nice, it also has the effect of entangling otherwise separate pieces of code. That means they cannot evolve separately and makes development more difficult. We have lots of plans to evolve the service backend that won't have analogues in the local backend, and I feel this proposal would make those changes harder and make the code more complex to handle two use cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9219#issuecomment-670667174:117,wrap,wrapper,117,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670667174,3,"['interface', 'wrap']","['interface', 'wrapper']"
Integrability,Just saw your final comment. Do you see a problem with shipping our C++ ABI dependencies?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410112524:76,depend,dependencies,76,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410112524,1,['depend'],['dependencies']
Integrability,"Just skimmed the discussion. >> I've been working on an R interface to Hail through the sparklyr package; >; > this also sounds awesome. woah, hell yes. I'll look tomorrow. Our build situation is a bit messed up right now. I'll try to isolate your issue and fix it. Moreover, I should be fixing the build situation for good soon. Can you share a full executor log for an executor that fails? That should have some information about why the spark context got shut down.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-430451868:58,interface,interface,58,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430451868,1,['interface'],['interface']
Integrability,"Just so I understand correctly (and sorry if this is obvious), the current job logs interface is still the same. But if you want a container's logs, then you'll get bytes which the user will have to decode themselves. How does that affect the file download button in the UI and the hailctl batch logs functionality you have? Will you see text or a random byte string?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12666#issuecomment-1426340756:84,interface,interface,84,https://hail.is,https://github.com/hail-is/hail/pull/12666#issuecomment-1426340756,1,['interface'],['interface']
Integrability,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569:323,rout,route-matching,323,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569,1,['rout'],['route-matching']
Integrability,Let me try synchronizing on a lock object instead of this. It's possible that something else (logging?) may be synchronizing on `this`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4068#issuecomment-411201627:11,synchroniz,synchronizing,11,https://hail.is,https://github.com/hail-is/hail/pull/4068#issuecomment-411201627,2,['synchroniz'],['synchronizing']
Integrability,Let's merge this so people don't get horrible error messages for now. I'll make an issue to make typecheck more powerful.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1826#issuecomment-301973652:52,message,messages,52,https://hail.is,https://github.com/hail-is/hail/pull/1826#issuecomment-301973652,1,['message'],['messages']
Integrability,Let's remember to message him when we fix this ticket.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4522#issuecomment-429350451:18,message,message,18,https://hail.is,https://github.com/hail-is/hail/issues/4522#issuecomment-429350451,1,['message'],['message']
Integrability,"Long term, I think it would be better to use docker engine for LocalBackend, so that container config/create/start between ServiceBackend and LocalBackend could have a common interface. If you're interested, after regenie is finished, I can PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9219#issuecomment-670291415:175,interface,interface,175,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670291415,1,['interface'],['interface']
Integrability,"Looking at the auth image, it is down from 2.76GB in main to 674MB. The hail-ubuntu image underneath it has stayed basically the same at about half the new auth image. Nearly all of the 674MB is split evenly between the layer that installs python in hail-ubuntu and the layer that installs the pip dependencies in the auth image. I've not yet inspected the hail-ubuntu layer, but for the pip dependencies the main offenders are:. ```; 77M	/usr/local/lib/python3.7/dist-packages/googleapiclient; 76M	/usr/local/lib/python3.7/dist-packages/botocore; 33M	/usr/local/lib/python3.7/dist-packages/_sass.abi3.so; 29M	/usr/local/lib/python3.7/dist-packages/kubernetes_asyncio; 20M	/usr/local/lib/python3.7/dist-packages/uvloop; 14M	/usr/local/lib/python3.7/dist-packages/pip; 14M	/usr/local/lib/python3.7/dist-packages/cryptography; 8.9M	/usr/local/lib/python3.7/dist-packages/google; 7.9M	/usr/local/lib/python3.7/dist-packages/pygments; 7.0M	/usr/local/lib/python3.7/dist-packages/azure; 5.0M	/usr/local/lib/python3.7/dist-packages/setuptools; 4.2M	/usr/local/lib/python3.7/dist-packages/aiohttp; 2.5M	/usr/local/lib/python3.7/dist-packages/googlecloudprofiler; 2.2M	/usr/local/lib/python3.7/dist-packages/yaml; 2.2M	/usr/local/lib/python3.7/dist-packages/hailtop; 2.0M	/usr/local/lib/python3.7/dist-packages/rich; 1.6M	/usr/local/lib/python3.7/dist-packages/pyasn1_modules; 1.5M	/usr/local/lib/python3.7/dist-packages/boto3; 1.4M	/usr/local/lib/python3.7/dist-packages/pkg_resources; 1.4M	/usr/local/lib/python3.7/dist-packages/oauthlib; 1.1M	/usr/local/lib/python3.7/dist-packages/pycparser; ```. Most of a gigabyte still feels annoyingly bloated but might just have to do for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578#issuecomment-1459376308:298,depend,dependencies,298,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1459376308,2,['depend'],['dependencies']
Integrability,Looks good other than a couple nits on the error message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1005#issuecomment-256483820:49,message,message,49,https://hail.is,https://github.com/hail-is/hail/pull/1005#issuecomment-256483820,1,['message'],['message']
Integrability,Looks good! I'll accept once the dependent PR is accepted,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5458#issuecomment-468026163:33,depend,dependent,33,https://hail.is,https://github.com/hail-is/hail/pull/5458#issuecomment-468026163,1,['depend'],['dependent']
Integrability,Looks like test failure is due to needing to wrap phenotype in array in this line; `top_5_pvals = (vds.linreg('sa.metadata.CaffeineConsumption')`; of; https://github.com/hail-is/hail/blob/master/python/hail/docs/tutorials/expression-language-part-2.ipynb,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2042#issuecomment-318833001:45,wrap,wrap,45,https://hail.is,https://github.com/hail-is/hail/pull/2042#issuecomment-318833001,1,['wrap'],['wrap']
Integrability,"Looks like this now:; ```; In [1]: import hail as hl; ...: mt = hl.utils.range_matrix_table(10, 10); ...: mt = mt.annotate_rows(foo = 'hi', bar = 'bye'); ...: mt2 = mt.annotate_rows(foo = 3, baz = 'hello'); ...: mt.union_cols(mt2, drop_right_row_fields=False); ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); Cell In[1], line 5; 3 mt = mt.annotate_rows(foo = 'hi', bar = 'bye'); 4 mt2 = mt.annotate_rows(foo = 3, baz = 'hello'); ----> 5 mt.union_cols(mt2, drop_right_row_fields=False). File <decorator-gen-1302>:2, in union_cols(self, other, row_join_type, drop_right_row_fields). File ~/projects/hail/hail/python/hail/typecheck/check.py:584, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 581 @decorator; 582 def wrapper(__original_func, *args, **kwargs):; 583 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 584 return __original_func(*args_, **kwargs_). File ~/projects/hail/hail/python/hail/matrixtable.py:3970, in MatrixTable.union_cols(self, other, row_join_type, drop_right_row_fields); 3965 if shared_field_names:; 3966 field_infos = [; 3967 f' {f!r}: left type is {self.row_value.dtype[f]}, right type is {other.row_value.dtype[f]}.'; 3968 for f in shared_field_names; 3969 ]; -> 3970 raise ValueError(; 3971 f'When drop_right_row_fields=True, the matrix tables must '; 3972 f'have distinct row fields. These fields were found to be in both tables:\n' +; 3973 '\n'.join(field_infos) +; 3974 '\nConsider renaming the fields in the right-hand-side matrix table so they are distinct from those fields in the left-hand-side.'; 3975 ); 3977 return MatrixTable(ir.MatrixUnionCols(self._mir, other._mir, row_join_type)). ValueError: When drop_right_row_fields=True, the matrix tables must have distinct row fields. These fields were found to be in both matrix tables:; 'bar': left type is str, right type is str.; 'foo': left type is str, right type is int32.;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13144#issuecomment-1576941505:740,wrap,wrapper,740,https://hail.is,https://github.com/hail-is/hail/pull/13144#issuecomment-1576941505,2,['wrap'],['wrapper']
Integrability,Looks like you need to [update the Google Artifact Registry cleanup policies](https://batch.hail.is/batches/8076011/jobs/210) to account for your new image. Instructions to do so are in the error message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13936#issuecomment-1783512175:196,message,message,196,https://hail.is,https://github.com/hail-is/hail/pull/13936#issuecomment-1783512175,1,['message'],['message']
Integrability,"Looks my comment got lost! Sorry. I said, I'd prefer we didn't copy the HailContext whole hog, but just write a simple wrapper that calls from hail2.HailContext to hail.HailContext, so something like:. ```; class HailContext:; def __init__(args...):; self.hc1 = hail.HailContext(args...). def import_bgen(args...):; return self.hc1.import_bgen(args...).to_hail2(); ```. etc. I don't think you even need docs unless there is something specifically different between the two. That way, we won't need to maintain two versions for things like doc changes and we won't get confused about which one is the ""real"" HailContext. Then, when we're ready to switch over, we can pull the docs across and throw away the stub.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2244#issuecomment-337717963:119,wrap,wrapper,119,https://hail.is,https://github.com/hail-is/hail/pull/2244#issuecomment-337717963,1,['wrap'],['wrapper']
Integrability,"Love it! Though email, not slack integration? ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4509#issuecomment-428052468:33,integrat,integration,33,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-428052468,1,['integrat'],['integration']
Integrability,"Maryam,; I ran this command in Unix:. ```; gunzip -c <file> | cut -f4 | sort | uniq -c; 20709505 A; 20934670 C; 20968049 G; 20693812 T; 25 alt; ```. I think the problem is that the headers from all the files were included in the one file. I'm running another grep now to be sure. I'll fix the error message though!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/317#issuecomment-212477402:299,message,message,299,https://hail.is,https://github.com/hail-is/hail/issues/317#issuecomment-212477402,1,['message'],['message']
Integrability,Maybe ought to wait for #8649 to go in so I can add tests that depend on keying.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8665#issuecomment-621867329:63,depend,depend,63,https://hail.is,https://github.com/hail-is/hail/pull/8665#issuecomment-621867329,1,['depend'],['depend']
Integrability,Maybe we could keep a list here of things that we notice don't have a log message as we notice them?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1616#issuecomment-294024200:74,message,message,74,https://hail.is,https://github.com/hail-is/hail/issues/1616#issuecomment-294024200,1,['message'],['message']
Integrability,"Maybe we should make an issue tag for things to fix before 0.2? We can't fix this without making a breaking interface change, but it's a really easy change that we shouldn't forget to include once we do 0.2. More generally, it seems like it would be good if we knew what we were planning on including before next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1887#issuecomment-305268107:108,interface,interface,108,https://hail.is,https://github.com/hail-is/hail/issues/1887#issuecomment-305268107,1,['interface'],['interface']
Integrability,Maybe we should update the WARNING message to be clear that this is a transient error and we've automatically retried it and there's nothing to be worried about?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649877:35,message,message,35,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649877,1,['message'],['message']
Integrability,Merging in https://github.com/hail-is/hail/pull/14233 causes the failure in `test_union_rows1`. Some strangeness with these new dependencies - running without this commit and everything works fine.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14231#issuecomment-1924903341:128,depend,dependencies,128,https://hail.is,https://github.com/hail-is/hail/pull/14231#issuecomment-1924903341,1,['depend'],['dependencies']
Integrability,"My concern was that not finding a browser when expected would prevent the server from accepting connections, but you may be right. Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 1m default-scheduler Successfully assigned notebook-worker-9szt8 to gke-vdc-non-preemptible-pool-0106a51b-pgxq; Normal SuccessfulMountVolume 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq MountVolume.SetUp succeeded for volume ""default-token-xl2w9""; Normal Pulling 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq pulling image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Pulled 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Successfully pulled image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Created 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Created container; Normal Started 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq ; NAME READY STATUS RESTARTS AGE; notebook-worker-9szt8 0/1 Running 0 49s. Started container; Warning Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:161,Message,Message,161,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218,1,['Message'],['Message']
Integrability,"My main motivation is that I can use up-to-date versions of NumPy, SciPy and Pandas. > I think we're also feeling quite sour on conda at the moment as well. In particular, I had to fix the [environment.yml for LDSC](https://github.com/bulik/ldsc/pull/168) because **recent versions of conda removed scipy==0.18 from their registry**. Wow, that's bad. I did not know conda removes old packages. Are you using some continuous integration?; We are solving these issues by some policy that pull requests have to pass CI building. ; If there is a new version of e.g. Pandas, CI will install it when somebody pushes a commit.; In case the new package version breaks something, CI fails and we get a notification to fix it. This way, we can in general keep up to date with the latest package versions and only rise the minimum version number of dependencies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542194340:424,integrat,integration,424,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542194340,2,"['depend', 'integrat']","['dependencies', 'integration']"
Integrability,"NOTE: This issue is **only** about the error message. We can definitely produce a more insightful error message (perhaps suggesting the use of `.rows()[key_field1, key_field2]`) without also addressing the confusing syntax.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14237#issuecomment-1921963399:45,message,message,45,https://hail.is,https://github.com/hail-is/hail/issues/14237#issuecomment-1921963399,2,['message'],['message']
Integrability,"New config should be correct. I have left the trailing slash on the location path, because without it requests don't appear to make it to the internal router. I think the issue is upstream.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540628849:151,rout,router,151,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540628849,1,['rout'],['router']
Integrability,"New test failure:. ```; + ./gradlew shadowJar archiveZip; Exception in thread ""main"" javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192); ```; Still not sure what to do about transient external dependency failures in the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4536#issuecomment-429360417:281,depend,dependency,281,https://hail.is,https://github.com/hail-is/hail/pull/4536#issuecomment-429360417,1,['depend'],['dependency']
Integrability,"Nice! If you use the phrase ""Fixes #12926"" in your PR message, GitHub will automatically close that ticket when this PR merged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12965#issuecomment-1532156641:54,message,message,54,https://hail.is,https://github.com/hail-is/hail/pull/12965#issuecomment-1532156641,1,['message'],['message']
Integrability,"No, I wanted the equivalent of this:. Inbreeding | `--het` | `vds.annotate_samples_expr(""sa.het = gs.inbreeding(g => va.qc.AF)"")`; Sex Check | `--check-sex` | `vds.impute_sex()....`; etc. I think we're getting close to being able to do this (python interface is stable).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/780#issuecomment-279763636:249,interface,interface,249,https://hail.is,https://github.com/hail-is/hail/issues/780#issuecomment-279763636,1,['interface'],['interface']
Integrability,"No, but its the same string I used to install hail dependencies for Terra's notebooks. I can run a test now",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9502#issuecomment-698497283:51,depend,dependencies,51,https://hail.is,https://github.com/hail-is/hail/pull/9502#issuecomment-698497283,1,['depend'],['dependencies']
Integrability,"No, it's not an HTTP service, it's plain old TCP, so `router` (which is an HTTP router) won't work here. The TCP gateway / router will come next.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9681#issuecomment-722656282:54,rout,router,54,https://hail.is,https://github.com/hail-is/hail/pull/9681#issuecomment-722656282,3,['rout'],['router']
Integrability,"Nobody has ever asked for this. I'm tabling it for now. If it does come up, we should integrate with Hadoop-BAM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/35#issuecomment-208692497:86,integrat,integrate,86,https://hail.is,https://github.com/hail-is/hail/issues/35#issuecomment-208692497,1,['integrat'],['integrate']
Integrability,"Not sure what this error is: . deepest = 'HailException: block matrix must have at least one row'; full = 'is.hail.utils.HailException: block matrix must have at least one row\n\tat is.hail.utils.ErrorHandling$class.fatal(Er...ava:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: HailException: block matrix must have at least one row; E ; E Java stack trace:; E is.hail.utils.HailException: block matrix must have at least one row",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8375#issuecomment-605098820:437,protocol,protocol,437,https://hail.is,https://github.com/hail-is/hail/pull/8375#issuecomment-605098820,1,['protocol'],['protocol']
Integrability,"Not that it helps in the commit history, but at least it's somewhere... this PR fixed that the secrets for batch deployment needed the deploy flag on which secret to use depending on whether we're testing or in production.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6168#issuecomment-495361303:170,depend,depending,170,https://hail.is,https://github.com/hail-is/hail/pull/6168#issuecomment-495361303,1,['depend'],['depending']
Integrability,"Note some highlights from the log:; ```; #12 42.27 ./Bio/tmp/Bio-DB-HTS-2.9 - moving files to ./biodbhts; #12 42.27 - making Bio::DB:HTS; #12 42.40 Checking prerequisites...; #12 42.40 requires:; #12 42.40 ! Bio::Root::Version is not installed; #12 42.40 ; #12 42.40 ERRORS/WARNINGS FOUND IN PREREQUISITES. You may wish to install the versions; #12 42.40 of the modules indicated above before proceeding with this installation; #12 42.40 ; #12 42.40 Run 'Build installdeps' to install missing prerequisites.; ```; ```; #13 138.3 Building and testing Test2-Suite-0.000152 ... ! Installing Test2::V0 failed. See /root/.cpanm/work/1682614674.13506/build.log for details. Retry with --force to force install it.; #13 150.9 FAIL; #13 150.9 --> Working on FFI::CheckLib; #13 150.9 Fetching http://www.cpan.org/authors/id/P/PL/PLICEASE/FFI-CheckLib-0.31.tar.gz ... OK; #13 150.9 Configuring FFI-CheckLib-0.31 ... OK; #13 151.1 ==> Found dependencies: Test2::V0, Test2::Require::EnvVar, Test2::Require::Module; #13 151.1 ! Installing the dependencies failed: Module 'Test2::Require::EnvVar' is not installed, Module 'Test2::V0' is not installed, Module 'Test2::Require::Module' is not installed; #13 151.1 ! Bailing out the installation for FFI-CheckLib-0.31. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12946#issuecomment-1526412230:930,depend,dependencies,930,https://hail.is,https://github.com/hail-is/hail/issues/12946#issuecomment-1526412230,2,['depend'],['dependencies']
Integrability,Note to self to check the logs for error messages before merging!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12199#issuecomment-1254026870:41,message,messages,41,https://hail.is,https://github.com/hail-is/hail/pull/12199#issuecomment-1254026870,1,['message'],['messages']
Integrability,Now `hc.import_vcf('/Users/jbloom/data/bgz_error/sample_plain.vcf.bgz')` on mislabeled plaintext file gives:. ```; FatalError: ZipException: File does not conform to block gzip format. Java stack trace:; java.util.zip.ZipException: File does not conform to block gzip format.; 	at is.hail.io.compress.BGzipInputStream$BGzipHeader.<init>(BGzipInputStream.java:35); ```. This error message is closer to that thrown in the .gz case when reading a plain text file:. ```; FatalError: IOException: not a gzip file. Java stack trace:; java.io.IOException: not a gzip file; 	at org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2129#issuecomment-323611362:380,message,message,380,https://hail.is,https://github.com/hail-is/hail/pull/2129#issuecomment-323611362,1,['message'],['message']
Integrability,"Now depends on https://github.com/hail-is/hail/pull/2723, last pending rv-conversion PR. > +275 2,223. Aw, yiss. @tpoterba Do you want to do the honors on this or should I spin the wheel?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2725#issuecomment-358866976:4,depend,depends,4,https://hail.is,https://github.com/hail-is/hail/pull/2725#issuecomment-358866976,1,['depend'],['depends']
Integrability,"OK, I have a suggestion: the schema for the input to MIS should be node1, node2, [optional v float64], where node1 should be preferred if v >= 0. This is the same as the current interface but we just compute the tiebreaker for each edge, instead of lazily. That gets rid of the expression language. There could be a separate weight-based version that takes edges (node1, node2) and weights (node1, w), where then v = w[node1] - w[node2].",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3425#issuecomment-385024493:178,interface,interface,178,https://hail.is,https://github.com/hail-is/hail/pull/3425#issuecomment-385024493,1,['interface'],['interface']
Integrability,"OK, I think I addressed the comments:; - I'm going to leave the pylint fix for another PR,; - use latest explicitly for remote repositories,; - explicit dependencies on build-stmp instead of find",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5655#issuecomment-475317040:153,depend,dependencies,153,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475317040,1,['depend'],['dependencies']
Integrability,"OK, I will PR improved error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7020#issuecomment-529559470:29,message,messages,29,https://hail.is,https://github.com/hail-is/hail/pull/7020#issuecomment-529559470,1,['message'],['messages']
Integrability,"OK, I won't be able to fix this. @ehigham @patrick-schultz @daniel-goldstein some combo of you three can probably figure it out. The local backend tests that hit requester pays buckets are failing with new Spark. New Spark needs new GCS hadoop connector (see the Dockerfiles). New GCS hadoop connector has [brand new configuration parameters](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/v3.0.0/gcs/INSTALL.md). Somehow I managed to make the normal Spark backend work correctly but the Local backend (which still, afaik, uses Spark & Hadoop for filesystems) is still trying to pick up CI's credentials instead of the test account's credentials. ```; E hail.utils.java.FatalError: GoogleJsonResponseException: 403 Forbidden; E GET https://storage.googleapis.com/storage/v1/b/hail-test-requester-pays-fds32/o/zero-to-nine?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata&userProject=hail-vdc; E {; E ""code"": 403,; E ""errors"": [; E {; E ""domain"": ""global"",; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist)."",; E ""reason"": ""forbidden""; E }; E ],; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist).""; E }; E ; E Java stack trace:; E java.io.IOException: Error accessing gs://hail-test-requester-pays-fds32/zero-to-nine; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1986); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1882); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloud",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236:1048,message,message,1048,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236,1,['message'],['message']
Integrability,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14507#issuecomment-2206596777:204,depend,dependabot,204,https://hail.is,https://github.com/hail-is/hail/pull/14507#issuecomment-2206596777,16,['depend'],['dependabot']
Integrability,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an [`ignore` condition](https://docs.github.com/en/code-security/supply-chain-security/configuration-options-for-dependency-updates#ignore) with the desired `update_types` to your config file. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13576#issuecomment-1709450787:204,depend,dependabot,204,https://hail.is,https://github.com/hail-is/hail/pull/13576#issuecomment-1709450787,1020,['depend'],"['dependabot', 'dependency', 'dependency-updates']"
Integrability,"OK, I've moved it and made the interface as close as I could to the previous `scatter`. One thing is the default value for `n_divisions`. It was 500 before, now I've set it to `None` (i.e. no downsampling). I'm fine either way, but it seems somewhat more intuitive to me for the default to be no downsampling.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5601#issuecomment-473338696:31,interface,interface,31,https://hail.is,https://github.com/hail-is/hail/pull/5601#issuecomment-473338696,1,['interface'],['interface']
Integrability,"OK, a third option:. Gradle has support for something called a Gradle wrapper, a set of distribution scripts that download and run a specific version of Gradle. I just added a Gradle wrapper for 2.14.1 to the master branch. You should now be able to build the local version of Hail with `gradlew installDist` or `./gradlew shadowJar` to build the shadow (fat, uber) jar to run against a Spark cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240309173:70,wrap,wrapper,70,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240309173,2,['wrap'],['wrapper']
Integrability,"OK, great. I'm working on this as we speak, so I'll prototype a draft sync interface and run it by you all.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10043#issuecomment-778403277:75,interface,interface,75,https://hail.is,https://github.com/hail-is/hail/pull/10043#issuecomment-778403277,1,['interface'],['interface']
Integrability,"OK, here's the high level design:. This diff touches a lot of files, but it is conceptually a small; list of connected changes:. 1. TableStageToRVD. This is infrastructure that lets us take a; TableStage and generate a RVD and BroadcastRow (globals). This; is used in...; 2. SparkBackend shuffle lowering. This now uses TableStageToRVD to; create an RVD, uses RVD methods to shuffle, then RVDToTableStage; to produce a TableStage that reads the shuffled RDD.; 3. TableStageDependencies. As I mentioned in our design meeting this; past week, the RDD we generate in the SparkBackend's parallelizeAndComputeWithIndex; needs to depend on the RDDs consumed by RVDToTableStage. In order to; support this, I have added a notion of a `TableStageDependency` to TableStage,; CollectDistributedArray, and BackendUtils.collectDArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9602#issuecomment-709392527:624,depend,depend,624,https://hail.is,https://github.com/hail-is/hail/pull/9602#issuecomment-709392527,1,['depend'],['depend']
Integrability,"OK, here's the most recent failure https://batch.hail.is/batches/8090848/jobs/21993. Don't be duped by my bad log message! There were zero transient errors. I added a log statement that increments the number of errors and prints that message after *every* error, even if it's not transient. . This time it was partition 20053 (we keep moving earlier?). I forgot to catch and rethrow the error with the toString of the input buffer, but I'm not sure there is much to learn from that anyway. FWIW, 20053 was successful in the two previous executions:; 1. https://batch.hail.is/batches/8069235/jobs/21993; 2. https://batch.hail.is/batches/8083195/jobs/21993. Interestingly the peak bytes are not consistent:; ```; 2023-10-24 19:59:47.756 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=58394624, peakBytesReadable=55.69 MiB, chunks requested=5513, cache hits=5501; 2023-10-24 19:59:47.759 : INFO: RegionPool: FREE: 55.7M allocated (7.7M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 9: pool-2-thread-1; ```; ```; 2023-11-08 19:42:40.000 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=61343744, peakBytesReadable=58.50 MiB, chunks requested=5513, cache hits=5501; 2023-11-08 19:42:40.000 : INFO: RegionPool: FREE: 58.5M allocated (10.5M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 10: pool-2-thread-2; ```. Whatever is causing this bug is rare. Approximately once every 31,000 partitions. The CDA IR is the same except for a couple iruid names and the order of the aggregators in the aggregator array is swapped (collect & take vs take & collect). AFAICT, the GCS Java library doesn't do any streaming verification of the hash. We could compute the CRC32c in a streaming manner and fail if/when we get to the end of the object, but this wouldn't work when we read intervals. I'm really mystified.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385:114,message,message,114,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385,2,['message'],['message']
Integrability,"OK, if I add the following additional dependencies in gradle:. ```; 	include(dependency('net.sourceforge.f2j:arpack_combined_all:0.1')); 	include(dependency('com.github.fommil.netlib:native_system-java:1.1')); 	include(dependency('com.github.fommil.netlib:netlib-native_system-linux-x86_64:1.1')); 	include(dependency('com.github.fommil.netlib:netlib-native_ref-linux-x86_64:1.1')); 	include(dependency('com.github.fommil:jniloader:1.1')); ```. it correctly loads on Linux:. > 2018-04-30 00:13:07 JniLoader: INFO: successfully loaded /tmp/jniloader8608320282306924695netlib-native_system-linux-x86_64.so. I'll test the analog on OSX tomorrow. Are you sure we're getting natives on Dataproc now? This definitely worked in the past. I get a 4x speedup (in the 1024 cases):. 214 ms  19.2 ms per loop (mean  std. dev. of 7 runs, 1 loop each). Now we're only 35x slower.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3335#issuecomment-385310865:38,depend,dependencies,38,https://hail.is,https://github.com/hail-is/hail/pull/3335#issuecomment-385310865,6,['depend'],"['dependencies', 'dependency']"
Integrability,"OK, now passes the tests. Not sure if `minRep` should be called in `VariantSubgen` or in `LoadBGenTest` (as I did). All depends on how `VariantSubgen` is used (e.g. if testing minRep)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1656#issuecomment-293058402:120,depend,depends,120,https://hail.is,https://github.com/hail-is/hail/pull/1656#issuecomment-293058402,1,['depend'],['depends']
Integrability,"OK, should be fixed. apiserver depends on hail, runs its own tests now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5624#issuecomment-474138152:31,depend,depends,31,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-474138152,1,['depend'],['depends']
Integrability,"OK, so the big insight is that ""InstanceConfig"" is really just ""ResourcesForAParticularInstance"" (well, and, sometimes, ""ResourcesOfARepresentativeInstance""). I trimmed the InstanceConfig down *significantly* removing the ""vm_config"". Now the InstanceConfig is cheap and easy to create and there's no circularity between vm_config and instance config. I pushed that through everywhere and then abstracted the common create_instance logic for pool and job-private into InstanceCollection. With both of those changes, I was able to modify the ResourceManager's API to expose methods for constructing instance configs. However, the instance config isn't critical to the operation of the ResourceManager. It's just an interface for communicating an instance's resources to the rest of the code base.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956500279:714,interface,interface,714,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956500279,1,['interface'],['interface']
Integrability,"OK, the story is more complicated than I imagined. uniroot was added in post-0.1 devel and made available in the expression language. It hasn't been exposed in the Python interface, but I don't know why. It is straightforward now, but I don't think the IR story has been sorted out yet. I'm going to reopen until it is available in Python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1717#issuecomment-388854826:171,interface,interface,171,https://hail.is,https://github.com/hail-is/hail/issues/1717#issuecomment-388854826,1,['interface'],['interface']
Integrability,"OK, this is passing. Interval is in utils but has a bad import to expr.types that creates a cyclic dependency. I fixed this by allowing a lambda as a typechecker which returns a type or typechecker to defer the import. It looks like this:. ```; @typecheck_method(start=anytype,; end=anytype,; includes_start=bool,; includes_end=bool,; point_type=nullable(lambda: hl.expr.types.hail_type)); def __init__(self, start, end, includes_start=True, includes_end=False, point_type=None):; ...; ```. Tho I think eventually we should move struct and interval out of utils.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8775#issuecomment-627904282:99,depend,dependency,99,https://hail.is,https://github.com/hail-is/hail/pull/8775#issuecomment-627904282,1,['depend'],['dependency']
Integrability,"OK, this is passing. Sorry for another big PR, I think I'm getting close to converging and I'll start spreading things around. Summary of changes:; - break batch into two services: batch2 and batch2-driver; - batch2 handles connections to the client, but has no driver; - driver has the driver and all the control loops; - workers now connect to batch2-driver, not batch2; - batch2 hits batch2-driver after close, cancel and delete to actually handle the jobs; - also removed abort and jsonify from utils and prefer the native aiohttp interfaces. I'll change the batch2 deployment in a later PR to run in triplicate, tolerate preemptibles and have a horizontal autoscaler.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7226#issuecomment-539972072:535,interface,interfaces,535,https://hail.is,https://github.com/hail-is/hail/pull/7226#issuecomment-539972072,1,['interface'],['interfaces']
Integrability,"OK, will change to exec/wait and fix the merge conflict (there was a conflict due to the upgrade to; libsimdpp-2.1, I tried to fix that but may need to do more, or there may be a new issue). My preference is to leave the build-command execution on the C++ side because it's (arguably); easier to read/understand the combined Scala + C++ functionality if the Scala NativeModule is a ; trivial wrapper and all the substance is on one side, in this case in C++. Since that's also acceptable to; you, I'll keep it that way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4211#issuecomment-416093339:392,wrap,wrapper,392,https://hail.is,https://github.com/hail-is/hail/pull/4211#issuecomment-416093339,1,['wrap'],['wrapper']
Integrability,"Of course. Cotton also had a few comments, I will integrate too. On Tuesday, September 20, 2016, Tim Poterba notifications@github.com; wrote:. > Let's get this in. Can you rebase?; > ; > ; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/541#issuecomment-248438901, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgSLomjBSxoQFMUqrmHl1RdXl_mD3ks5qsE7igaJpZM4Jb_3Y; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/541#issuecomment-248439671:50,integrat,integrate,50,https://hail.is,https://github.com/hail-is/hail/pull/541#issuecomment-248439671,1,['integrat'],['integrate']
Integrability,"Oh hmm, just read @danking's last message. Well, up to you all. I don't think it matters as much as long as it's a consistent time, and well-documented.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2243#issuecomment-331485632:34,message,message,34,https://hail.is,https://github.com/hail-is/hail/pull/2243#issuecomment-331485632,1,['message'],['message']
Integrability,"Oh yikes, looks like something in the LLVM dependencies requires/installs python3.8. I wonder whether that is necessary",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12526#issuecomment-1352514979:43,depend,dependencies,43,https://hail.is,https://github.com/hail-is/hail/pull/12526#issuecomment-1352514979,1,['depend'],['dependencies']
Integrability,"Oh, and, yes, nothing broke. Dependencies of failed jobs were cancelled, always run jobs always ran, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10985#issuecomment-954865403:29,Depend,Dependencies,29,https://hail.is,https://github.com/hail-is/hail/pull/10985#issuecomment-954865403,1,['Depend'],['Dependencies']
Integrability,"Oh, shit, I approved. Do we have a working protocol for multi-user reviews? This was clearly not it. I guess the rule should be to dismiss your review if someone else has reviewed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4921#issuecomment-445296624:43,protocol,protocol,43,https://hail.is,https://github.com/hail-is/hail/pull/4921#issuecomment-445296624,1,['protocol'],['protocol']
Integrability,"Oh, woah, that test does look wrong. It's concerning that its suddenly failing. I'm not sure I care too much about tracking down exactly which dependency change caused this. We should fix the test obviously. We should add a test that verifies both `?a` and `?` have the expected data (in particular, that we didn't overwrite one with the other!).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11974#issuecomment-1276854186:143,depend,dependency,143,https://hail.is,https://github.com/hail-is/hail/pull/11974#issuecomment-1276854186,1,['depend'],['dependency']
Integrability,"Ok I just looked at the scala code, and this must have happened around the sex chromosomes when my dataset shifted to haploid (or more specifically, a mix of haploid and diploid calls for male and female). I'll write in the workaround for my own pipeline, but you might want to have a more explicit error message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465#issuecomment-385281263:305,message,message,305,https://hail.is,https://github.com/hail-is/hail/issues/3465#issuecomment-385281263,1,['message'],['message']
Integrability,"Ok, @jigold looks like the only sensible way to do this is to add an. ```; import pytz; ```; ```; now = datetime.datetime.utcnow().replace(tzinfo=pytz.utc); ```; According to a note in the Python 2.7 docs [there are no tzinfo instances in the standard library](https://docs.python.org/2/library/datetime.html#tzinfo-objects). We should add the dependency on pytz to [environment.yml](https://github.com/hail-is/hail/blob/master/python/hail/environment.yml).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2243#issuecomment-331479504:344,depend,dependency,344,https://hail.is,https://github.com/hail-is/hail/pull/2243#issuecomment-331479504,1,['depend'],['dependency']
Integrability,"Ok, I think I sorted out the make->mill dependency propagation. Any real target which invokes mill to build it now depends on a target `FORCE` which is always out-of-date, so mill is always invoked. But mill will not change the modification time if it doesn't need to, so downstream targets aren't forced to be run. For example, we have targets; ```; FORCE:. SHADOW_JAR := out/assembly.dest/out.jar; $(SHADOW_JAR): FORCE; 	$(mill) assembly. PYTHON_JAR := python/hail/backend/hail-all-spark.jar; $(PYTHON_JAR): $(SHADOW_JAR); 	cp -f $(SHADOW_JAR) $@. .PHONY: python-jar; python-jar: $(PYTHON_JAR); ```. If I remove the python jar and invoke make, it runs mill then copies:; ```;  rm python/hail/backend/hail-all-spark.jar.  make python-jar; bash millw assembly; [95/110] compile; [info] compiling 10 Scala sources to /Users/pschulz/hail/mill-worktree/hail/out/compile.dest/classes ...; [info] done compiling; [110/110] assembly; cp -f out/assembly.dest/out.jar python/hail/backend/hail-all-spark.jar; ```. If run again, mill is invoked to check for changes, but as the jar doesn't change it isn't copied again:; ```;  make python-jar; bash millw assembly; [105/110] memory.resources; ```. If I change some scala sources, the jar is updated and copied:; ```;  make python-jar; bash millw assembly; [95/110] compile; [info] compiling 10 Scala sources to /Users/pschulz/hail/mill-worktree/hail/out/compile.dest/classes ...; [info] done compiling; [110/110] assembly; cp -f out/assembly.dest/out.jar python/hail/backend/hail-all-spark.jar; ```. If I change some scala sources in a way that doesn't actually affect the jar, such as modifying comments, mill is smart enough to not change the jar, so it won't be copied again:; ```;  make python-jar; bash millw assembly; [95/110] compile; [info] compiling 1 Scala source to /Users/pschulz/hail/mill-worktree/hail/out/compile.dest/classes ...; [info] done compiling; [105/110] memory.resources; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147#issuecomment-1930325793:40,depend,dependency,40,https://hail.is,https://github.com/hail-is/hail/pull/14147#issuecomment-1930325793,2,['depend'],"['dependency', 'depends']"
Integrability,"Ok, so Cotton's new thing means emitting separate methods by hand is not a thing we do anymore. But there are two factors hurting the benchmark. . One is that the benchmark is hiding the fact that we are spending ~25 seconds serializing and de-serializing JSON for this ndarray. So the real comparison is more like 55 seconds vs 75 seconds, which is a roughly 25% speed improvement. . The other is that `hl.nd.ones` is just an alias for `hl.nd.array(hl.range(shape_product)).map(lambda x: 1).reshape((n_rows, n_cols))`. This is going to create a bunch of row major data, copy it to column major in a pretty cache inefficient way during the reshape, then do the additions. So that's eating some of the time too. We should probably have a way for all the constant methods to not go through regular array. . Anyway, 25% improvement + better interface is a win for now, we can revisit ways to make this faster in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9209#issuecomment-668610293:838,interface,interface,838,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-668610293,1,['interface'],['interface']
Integrability,Ok. I think this code has the correct behavior now. (4) from above should never depend on the deleted event because we don't handle the deletion inside the regular timings now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11429#issuecomment-1054645368:80,depend,depend,80,https://hail.is,https://github.com/hail-is/hail/pull/11429#issuecomment-1054645368,1,['depend'],['depend']
Integrability,"Ok. I thought about this some more. What you've implemented is essentially a ""taint"" in Kubernetes. I ultimately want both taints and something more complicated that will have to be integrated into the scheduler. I think for now your change is self contained and it will be easy to transform later on without too much complexity or breaking changes for users. I think if you want to rename ""label"" to ""taint"" then that might make it clearer what's going on. cc: @daniel-goldstein",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11879#issuecomment-1145343746:182,integrat,integrated,182,https://hail.is,https://github.com/hail-is/hail/pull/11879#issuecomment-1145343746,1,['integrat'],['integrated']
Integrability,"Ok. I was trying to hide the kubernetes error message from the users because I thought it might be confusing. But if you feel that's ok, then I'll make it just report the original error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7484#issuecomment-551253066:46,message,message,46,https://hail.is,https://github.com/hail-is/hail/pull/7484#issuecomment-551253066,1,['message'],['message']
Integrability,"Ok. The limits for the tests will need to be fixed with a REST API for editing a billing limit and a new build step that is `setup_test_batch`. I'll work on that now. But I think this can go in while I'm working on that, but #9355 should depend on the new PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9354#issuecomment-705102869:238,depend,depend,238,https://hail.is,https://github.com/hail-is/hail/pull/9354#issuecomment-705102869,1,['depend'],['depend']
Integrability,"Ok. This exact scenario is what I was worried about when we merge PRs without checking the logs by hand in a full testing scenario. I want a way to check the PR driver, front-end, and worker logs automatically that they don't have ERROR messages. Like test_invariants. For example, I'm still looking at your change for time_since_last_state_change. When I had the code you wanted, there were errors because time_since_last_state_change was None. The current tests would not have caught that. I think we need either a white list of acceptable front-end/driver errors or some kind of threshold for error types. I'll think about it some more once the batch porting is done.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956522959:237,message,messages,237,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956522959,1,['message'],['messages']
Integrability,"Okay, I think all the changes are now directly related to the main changes described in the initial PR message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8917#issuecomment-638942141:103,message,message,103,https://hail.is,https://github.com/hail-is/hail/pull/8917#issuecomment-638942141,1,['message'],['message']
Integrability,Okay. Hopefully this is good. I also pushed an extra branch up with a small debugging class I wrote to decompose a virtual offset in messages. That work is in c323fa335. Let me know if you want to add it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9304#issuecomment-676735526:133,message,messages,133,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-676735526,1,['message'],['messages']
Integrability,"On the issue of how to give people a standardized environment with Hail +; compiler + everything else, one; approach is to build a Bitnami package, which then installs itself into a; directory tree with zero/minimal; dependencies or interactions with anything outside its tree. I've used; that for web services like Jenkins.; It's just one step short of using containers - but since it doesn't require; a containerized OS, I think it works; for laptops etc. I believe the package could have all the stuff we currently manage my; manual install, viz JDK, Spark, Python-3.6,; R, R packages, as well as Hail and a friendly-C++17-capable compiler. All; without perturbing anything else; on the system. See https://bitnami.com. I took a similar approach at PhysicsSpeed, though without using any bitnami; tools because we had less than; zero dollars :-(. I don't know if this adds any value in the containerized/cloud environment,; where custom machine images; are presumably the way to go. But it makes setup easy for standalone use. Regards; Richard. On Thu, Aug 2, 2018 at 10:44 PM Richard Cownie <rcownie@broadinstitute.org>; wrote:. > We have a difference of opinion about the risks involved in using whatever; > compiler happens to show up as $(CXX); > to try to compile arbitrarily large auto-generated C++ files, and maybe; > about what happens when that fails; > and gives an error message about something in the middle of 12000 lines of; > code that bears no obvious relationship; > to what the user is doing. Or when that compiler takes 15 minutes to; > compile it. It's the C++ equivalent of; > the JVM ""no, that's just too much bytecode"". Or worst of all, it compiles; > it but the code gives the wrong answers; > because that particular compiler has a bug, and we never tested the; > combination of our codegen with *that*; > compiler/version.; >; > A couple of years ago I was seeing g++ take 40-60 seconds to compile; > something that clang did in 2 seconds; > (fairly heavily templated cod",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410235287:217,depend,dependencies,217,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410235287,1,['depend'],['dependencies']
Integrability,"On, one thing, I removed delete_batch_database. I don't quite know how to write it with the current setup. I think we should add a new kind of step which is dev-only and only run if you ask for it explicitly (not just as a dependency).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7705#issuecomment-564753932:223,depend,dependency,223,https://hail.is,https://github.com/hail-is/hail/pull/7705#issuecomment-564753932,1,['depend'],['dependency']
Integrability,"Oof, good catch! The thing we're trying to avoid is `e^x` overflowing for large positive `x`. In double precision, the smallest `x` that overflows is 710. So to test that we handle overflow correctly, you can check `sigmoid(710) == 1.0` and `sigmoid(-710) == 0.0` (using approximate equality). Actually, after playing with this, if you just use the simple definition `sigmoid(x) = 1 / (1 + np.exp(-x))`, then `sigmoid(-710)` does overflow, but it returns the right answer since `np.exp(710)` returns `inf`, and `1 / inf == 0.0`. But `math.exp(710)` throws an exception. `hl.exp` seems to have the numpy behavior, so I think the simple version actually works. But we should add the above test. I think wrapping this in an exposed function is a good idea. I agree it should be called `expit`, both for consistency with scipy, and because as you say, `sigmoid` really just means an S shaped function. And if we do expose `expit`, we should probably expose its inverse `logit` too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10606#issuecomment-866034244:701,wrap,wrapping,701,https://hail.is,https://github.com/hail-is/hail/pull/10606#issuecomment-866034244,1,['wrap'],['wrapping']
Integrability,"Oops, sorry, your PR was for the monitoring namespace. I see the problem, CI doesn't have a route for `''`. I'll push a fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536246164:92,rout,route,92,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536246164,1,['rout'],['route']
Integrability,"Options for nested `forAll`:. ``` scala; toProp(for (; j <- forAll(Gen.choose(0, 10000));; k <- forAll(Gen.choose(0, 10000));; ) yield {; val gt = if (j < k) GTPair(j, k) else GTPair(k, j); Genotype.gtPair(Genotype.gtIndex(gt)) == gt; }).check(); ```. ``` scala; forAll(Gen.choose(0, 10000)) { (j: Int) =>; forAll(Gen.choose(0, 10000)) { (k: Int) =>; val gt = if (j < k) GTPair(j, k) else GTPair(k, j); Genotype.gtPair(Genotype.gtIndex(gt)) == gt; }; }.check(); ```. I think I can ditch the `toProp` on the do notation with an implicit conversion. I might be able to support either syntax in a unified way, but I haven't found the time to think about it. There's a little bit of weirdness because you only want `check` to be callable on things that are `Boolean`-valued. The difference between this monad and the `Gen[T]` monad is that this one is a reader monad, collecting a stack of ""read"" variables that can be used by the inner most `forAll` to generate a useful check-failure message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/400#issuecomment-244517801:982,message,message,982,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-244517801,1,['message'],['message']
Integrability,Or have both delete steps depend on both test jobs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13353#issuecomment-1660787777:26,depend,depend,26,https://hail.is,https://github.com/hail-is/hail/pull/13353#issuecomment-1660787777,1,['depend'],['depend']
Integrability,OrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:450); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:486); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:339); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:483); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.backend.spa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:9123,Wrap,WrappedArray,9123,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['Wrap'],['WrappedArray']
Integrability,Our error message on functions that read TSV are much clearer than they used to be. I don't think this needs to be a separate command.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/216#issuecomment-279518768:10,message,message,10,https://hail.is,https://github.com/hail-is/hail/issues/216#issuecomment-279518768,1,['message'],['message']
Integrability,"Our protocol is that the assigned user is the only one who can approve, but that anyone else can block.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4921#issuecomment-445296790:4,protocol,protocol,4,https://hail.is,https://github.com/hail-is/hail/pull/4921#issuecomment-445296790,1,['protocol'],['protocol']
Integrability,"Overall this is good, but I think we should simplify the interface. 1. Require `entry_to_double`. Don't support genotypes or do normalization. 2. Only have the one version that returns the triple. The user can reannotate the original dataset if that's what they want. 3. Write a `VariantDataset.genotype_matrix_pca` in Python that looks at the `.GT` field and does the necessary normalization before calling `pca`. This should be written completely in Python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2454#issuecomment-348531819:57,interface,interface,57,https://hail.is,https://github.com/hail-is/hail/pull/2454#issuecomment-348531819,1,['interface'],['interface']
Integrability,Partially done: https://github.com/cseed/k3/commit/3cfd6b226ec1bd885faa1fae96965435dd78130b. There is still a small number of messages I haven't figured out how to disable.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/41#issuecomment-152272292:126,message,messages,126,https://hail.is,https://github.com/hail-is/hail/issues/41#issuecomment-152272292,1,['message'],['messages']
Integrability,Pass$.transform(LoweringPass.scala:69); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); E 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:454); E 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:490); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:75); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:75); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:63); E 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:342); E 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:487); E 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229:4618,Wrap,WrappedArray,4618,https://hail.is,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229,1,['Wrap'],['WrappedArray']
Integrability,Phew! I finally got to parity. I'm impressed how well the old method wrapping logic worked. Benchmarks with >20% change:. ```; $ hail-bench compare ./0.2.47-3f8cff262dcf-1.json 0.2.47-63dccdda2a44.json ; Failed benchmarks in run 1:; pc_relate_big; large_range_matrix_table_sum; Failed benchmarks in run 2:; pc_relate_big; large_range_matrix_table_sum; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; table_big_aggregate_compile_and_execute 334.8% 13.325 44.608; matrix_table_array_arithmetic 133.9% 10.208 13.665; matrix_table_many_aggs_col_wise 131.4% 34.847 45.781; table_aggregate_counter 126.4% 13.136 16.606; per_row_stats_star_star 124.5% 8.474 10.553; matrix_table_filter_entries_unfilter 123.7% 8.581 10.616; ...; shuffle_key_rows_by_mt 82.8% 35.932 29.742; full_combiner_chr22 23.6% 1644.907 388.414; ----------------------; Geometric mean: 100.0%; Median: 99.2%; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8963#issuecomment-650638060:69,wrap,wrapping,69,https://hail.is,https://github.com/hail-is/hail/pull/8963#issuecomment-650638060,1,['wrap'],['wrapping']
Integrability,"Physical values in the emitter are currently represented by a single scalar JVM value, a Code[T]. The goal of physical values is to make the representation of values more general. In addition, physical values carry a PType, which Code[T] doesn't. The PValue introduced here isn't a ""real"" one, it just wraps the previous Code[T] and carries the PType. I will move towards the more general picture in pieces.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8144#issuecomment-590144918:302,wrap,wraps,302,https://hail.is,https://github.com/hail-is/hail/pull/8144#issuecomment-590144918,1,['wrap'],['wraps']
Integrability,Please keep in mind this PR is part of a much larger refactor of the code and this bad interface will go away.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6238#issuecomment-498324907:87,interface,interface,87,https://hail.is,https://github.com/hail-is/hail/pull/6238#issuecomment-498324907,1,['interface'],['interface']
Integrability,"Proposed interface changes:. class TextTableConfiguration. class TextTableReader. TextTableReader(conf); TextTableReader(delimiter = ""#"", ...). // only read fields ; TextTableReader.read(columnTypes: Map[String, Type], path: String, [select]): (TStruct, RDD[Annotation]). (and JSON). for JSON:. JSONReader.read(t: Type, path: String): RDD[Annotation]. in expr language:. support. Variant(""chr:pos:ref:alt1,...,altN"") . (so Variant(v.toString) == v) and. Variant(chr: String, pos: Int, ref: String, alts: Array[String]). Then we can do:. annotatevariants table -v 'Variant(Chrom, Pos, Ref, Alts.split("",""))'. annotatevariants table -v 'Variant(Variant)'. To get this behavior, you'll have to build the EvalContext from the table type. Add. TStruct.filter(predicate: (Field) => Boolean): (TStruct, Filterer). where. type Filterer = (Annotation) => Annotation. This should make implementating importvariants table simple and elegant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-232740494:9,interface,interface,9,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232740494,1,['interface'],['interface']
Integrability,"Python interface changes:; - filter_variants_all -> drop_variants; - filter_samples_all -> drop_samples; - renamed ""condition"" to ""expr"" in parameter names where appropriate. Removed gradle installDist",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1397#issuecomment-280419318:7,interface,interface,7,https://hail.is,https://github.com/hail-is/hail/pull/1397#issuecomment-280419318,1,['interface'],['interface']
Integrability,"Python versioning is a huge problem. Basically every time we have used unbounded dependency versions, we've gotten burned (some package updates and now Hail is broken for anyone who tries to install it). John could find out that 0.24 is supported, but then we'd have to pin at `<0.25`, so this doesn't solve the problem generally. I think we're also feeling quite sour on conda at the moment as well. In particular, I had to fix the [environment.yml for LDSC](https://github.com/bulik/ldsc/pull/168) because **recent versions of conda removed scipy==0.18 from their registry**.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542183134:81,depend,dependency,81,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542183134,1,['depend'],['dependency']
Integrability,"Re: this interface:; ```scala; def apply(i: Int): Option[Int] = {; setGenotype(i); if (hasGT) Some(getGT) else None; }; ```; It's entirely for performance reasons. We never want to allocate or process `Option`s anywhere, and there's some overhead we can avoid with calling `setGenotype(i)` twice if we use two methods for `hasGtIdx(i: Int): Boolean ` and `getGtIdx(I: Int): Int`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2368#issuecomment-340901365:9,interface,interface,9,https://hail.is,https://github.com/hail-is/hail/pull/2368#issuecomment-340901365,1,['interface'],['interface']
Integrability,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/585#issuecomment-241153168:1314,wrap,wrap,1314,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168,1,['wrap'],['wrap']
Integrability,"Reassigned to Patrick since Arcturus is OOO. The primary concern Arcturus had is that I've called aggregator functions in my Python implementation which need to be consistently agg/scan depending on whether `hardy_weinberg_test` is called as an agg or scan. I'm now convinced we get this for free. The way it works is that a global AggFunc singleton object contains a flag `_as_scan`, which is essentially set in a context manager whenever a function is called as `hl.scan`. This means that if I call `agg.filter` inside `hardy_weinberg_test`, it will look at the AggFunc, which properly has the flag set. It would be incorrect to call `hl.scan.xxx` inside a function in the agg module, but it's perfectly correct to call `hl.agg.xxx`, since the outer context is already managed properly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6816#issuecomment-519535998:186,depend,depending,186,https://hail.is,https://github.com/hail-is/hail/pull/6816#issuecomment-519535998,1,['depend'],['depending']
Integrability,"Rebased, should be ready for a look. I included the compile the decoder once change in this PR. I left in the entry and row field arguments so as not to break the interface, but we can reasonably delete those now that it is playing nicely with the field pruner.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3948#issuecomment-405962693:163,interface,interface,163,https://hail.is,https://github.com/hail-is/hail/pull/3948#issuecomment-405962693,1,['interface'],['interface']
Integrability,"Regarding SSR-only mode. This is the default behavior. SSR is mostly a function of routing. If we allow the client to handle routes, we save the roundtrip in reconciling current app state (current DOM) with the next state (next page's DOM). To ""enable"" this functionality, instead of using `<Link>` use `<a>`. Nextjs has excellent documentation and a responsive maintainer base: https://github.com/zeit/next.js/issues/575",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454796376:83,rout,routing,83,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454796376,2,['rout'],"['routes', 'routing']"
Integrability,"Regarding the `pyspark` issue, it looks like you have to use `--properties-file` and then put that comma-sepearted list as a newline-separated list in a file. That error message is pyspark's rather terrible way of telling you that it doesn't support a `--properties` option. Regarding the old version of VDS, the `master` branch of hail is now an unstable development branch. If you want a consistent user experience with backwards compatible interfaces, please check out and exclusively use the `0.1` branch. Tim discusses the wider change [here](http://discuss.hail.is/t/deployment-changes-branching-off-for-faster-development/261/1). The VDS format will likely change on the scale of days on the `master` branch. Regarding the `hail/scripts` folder, that is a repository of scripts that our build system uses as templates to create a pre-compiled, ready-to-go distribution that only requires a Spark installation. These distributions are available from the Google Storage API at gs://hail-common/distributions. If you're building from source, I recommend following exactly the steps listed [here](https://hail.is/docs/stable/getting_started.html#building-hail-from-source) so as to avoid any future hiccups. NB: the steps for [Running Hail Locally](https://hail.is/docs/stable/getting_started.html#running-hail-locally) are for using the pre-compiled distribution, not for the result of building hail directly from source.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-320242551:170,message,message,170,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-320242551,2,"['interface', 'message']","['interfaces', 'message']"
Integrability,"Remove space in ""Hail Context has already been created"", or make context lowercase to be consistent with other messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1523#issuecomment-285818370:111,message,messages,111,https://hail.is,https://github.com/hail-is/hail/pull/1523#issuecomment-285818370,1,['message'],['messages']
Integrability,"Response:. 1. `simdpp/simd.h`, I changed to a user dependency *and* changed the makefile to grab system dependencies, I agree this seems like a good idea (but I'm not sure, so we'll try it and find out!); 2. `mkdir -p`, I changed them all to `@`, but can `mkdir -p` fail (except for, say, filesystem being full)?; 3. `.DEFAULT_GOAL` good catch, I think we don't want `.DEFAULT` at all, I've removed it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5331#issuecomment-464057989:51,depend,dependency,51,https://hail.is,https://github.com/hail-is/hail/pull/5331#issuecomment-464057989,2,['depend'],"['dependencies', 'dependency']"
Integrability,"Reverted the interface for `select`, `drop`, and `key_by` to requiring a str or list of str rather than varargs. This is because having default values specified by a kwarg was interacting poorly with the varargs. Python3 supports this better. https://stackoverflow.com/questions/13821877/function-call-with-named-unnamed-and-variable-arguments-in-python",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2279#issuecomment-334795157:13,interface,interface,13,https://hail.is,https://github.com/hail-is/hail/pull/2279#issuecomment-334795157,1,['interface'],['interface']
Integrability,"STER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' command is; only run if this option is specified. --help Show this message and exit.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:3899,message,message,3899,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,1,['message'],['message']
Integrability,"See for example https://cloudlogging.app.goo.gl/ziaRD9HKxxca8Nd3A. in which ~15 MJCs have to retry because of `ServerDisconnectedError` or `TimeoutError`. With this PR, I think we would have seen just the three ""two errors observed"" warning messages. Here's a possible extension to this PR that fuses the thinking of both PRs (this one and #12505): use the total delay instead of `errors = 2`. We retry really quickly, so two errors could occur in ~500ms which really isn't enough time for batch driver to fix itself.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12712#issuecomment-1434824106:241,message,messages,241,https://hail.is,https://github.com/hail-is/hail/pull/12712#issuecomment-1434824106,1,['message'],['messages']
Integrability,See: http://dev.hail.is/t/aggregator-interface/120,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4492#issuecomment-428398036:37,interface,interface,37,https://hail.is,https://github.com/hail-is/hail/issues/4492#issuecomment-428398036,1,['interface'],['interface']
Integrability,"Since my interface package isn't ready for release yet, here's a reproducible example using just R and sparklyr, with a Hail jar somewhere. Again, this is happening on the Mac, sparklyr version 0.8.4.9004 (there is probably a newer one on CRAN, I doubt that it matters). ```; data(mtcars); hail_jar <- ""/path/to/your/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sparklyr::invoke(ht, ""count""); ```. Thanks a lot for your continued attention to this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-429475190:9,interface,interface,9,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-429475190,1,['interface'],['interface']
Integrability,Snyk is failing because it suddenly realized that we have dependencies (#security  ). I will fix those issue separately.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1707206872:58,depend,dependencies,58,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1707206872,1,['depend'],['dependencies']
Integrability,"So I just put back the dependencies on `native-lib-prebuilt`. Since that just calls make recursively, it would probably be better to let mill invoke make, but I didn't want to deal with that, and this is a pretty uncommon use case (I think).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147#issuecomment-1930447429:23,depend,dependencies,23,https://hail.is,https://github.com/hail-is/hail/pull/14147#issuecomment-1930447429,1,['depend'],['dependencies']
Integrability,"So I still need to sit down and actually dig through this, but my initial thoughts:. We should have shuffler tests that exercise the shuffler directly (without going through RVD/TableIR). I don't think they need to be totally comprehensive, or be necessarily independent of all hail infrastructure (e.g. encoders/decoders/types/regions, which you're using in the shuffler), but should exercise whatever the basic interface between the shuffle service and the rest of the world is going to be, independent of the specific implementation that shuffles an RVD. To that end, I'd also advocate for defining a backend-agnostic shuffler interface to shuffle TableIRs/RVDs; instead of needing to check the context flag everywhere you might shuffle something, we'd define what an RVD shuffle looks like independent of whether or not we're using the shuffle service or a Spark shuffle, and then the shuffler interface would call the correct implementation depending on what shuffler if was supposed to use. (I'm not absolutely set on the second thing happening immediately, but I do think it's longer-term the right way to think about how the shuffler plugs in, and it feels like it should be a pretty minimal change from what you have right now to get a possibly-imperfect-but-workable interface in.). In short, this feels like two+ separate PRs to me, in that it's composed of pretty distinct, substantial changes that involve a lot of non-trivial choices:; - the actual implementation of the shuffle service, and; - a client for Hail IRs to interact with the shuffle service + wiring things up to actually go through it. and I'm happy to review both pieces of it, but it might be easier + quicker to get the shuffler itself in if you broke that change out separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8205#issuecomment-594214674:413,interface,interface,413,https://hail.is,https://github.com/hail-is/hail/pull/8205#issuecomment-594214674,5,"['depend', 'interface']","['depending', 'interface']"
Integrability,"So I think the root issue here is the unnecessary duplication between `pyRegisterIR` and `pyRegisterIRForServiceBackend`. The only real difference is that one takes and already parsed IR, and the other takes a string and calls the parser. The callers of `pyRegisterIR` in python all call into the parser first, but I don't see any reason it has to make two calls across the python/scala bridge; I think `pyRegisterIR` should just take the IR as a string and call the parser like `pyRegisterIRForServiceBackend` does. With that change, it should be possible to make one a simple wrapper around the other (or maybe even get rid of `pyRegisterIRForServiceBackend` completely). That way the core logic is shared between backends and is getting tested. Let me know if you want help with this, or if you'd like me to make a separate PR for this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14579#issuecomment-2174358685:387,bridg,bridge,387,https://hail.is,https://github.com/hail-is/hail/pull/14579#issuecomment-2174358685,2,"['bridg', 'wrap']","['bridge', 'wrapper']"
Integrability,"So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I don't have a strong feeling if you want to also add a Python-inspired while loop (although I personally would find the similarities misleading given the required differences, I understand others might feel differently). Your while loop should be naturally implementable in terms of mine, so I also suggest we focus on that first. Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Is Patrick's proposal for extra types written up anywhere? I don't like the idea of complicating the type hierarchy for internal bookkeeping like this. So I'm going to remark that in the code generator it is often natural to build data structures to aid the organization of the code generator, and those data structures need not need to be types/IRs. Given that Recur has to be in tail position, and you know exactly when you're existing the loop (branches that don't contain recur nodes). So the compilation looks like:. ```; set initial loop variables; # fall through into loop; Lloop:; ...; # recur; loop variables = new values; goto Lloop; Lan_exit_branch:; result = compile(branch); goto Lafter; Lanother_exit_branch:; result = compile(other_branch); goto Lafter; Lafter:; use result ...; ```. What I would do is ""peel"" off the ifs and lets (anything else?) that can sit in tail position and build a separate data structure for those nodes which I then traverse to emit the above code. Using the stream interface seems wrong to me also. What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. I'll comment more once I've looked over the code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125:45,interface,interface,45,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125,4,"['interface', 'wrap']","['interface', 'interfaces', 'wrapping']"
Integrability,"So the issue is that we used to have `filter` and `explode` inside of aggregations (like `counter` and `collect_as_set`). Now they're placed outside of these operations. There was [a forum post](https://discuss.hail.is/t/breaking-change-redesign-of-aggregator-interface/701) announcing this breaking change. The above to examples should instead be written as:. ```; cut_dict = {'pop': hl.agg.filter(hl.is_defined(mt.meta.pop), hl.agg.counter(mt.meta.pop)),; 'subpop': hl.agg.filter(hl.is_defined(mt.meta.subpop) & hl.is_defined(mt.meta.pop),; hl.agg.collect_as_set(hl.struct(subpop=mt.meta.subpop, pop=mt.meta.pop))); }; ```. The fix for this issue is to change the assertion into an `if` with a `raise` of an error message, probably one that references that discuss post.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4770#issuecomment-452847982:260,interface,interface,260,https://hail.is,https://github.com/hail-is/hail/issues/4770#issuecomment-452847982,2,"['interface', 'message']","['interface', 'message']"
Integrability,"So there's a double regex substitution now in this version. I couldn't figure out how to avoid this without having nice error checking at the exact line there's a problem. For example, `j.command(f'{b}')` right now immediately errors with a nice error message. But if the error checking doesn't come until the massive parallel `_compile` in `Backend.run`, then it will be harder to tell where the error is. I thought about having a `debug_mode` which is on by default that does the double check while the `debug_mode` being off is more efficient.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10716#issuecomment-888459694:252,message,message,252,https://hail.is,https://github.com/hail-is/hail/pull/10716#issuecomment-888459694,1,['message'],['message']
Integrability,"Some initial thoughts:; * I'm not sure we should have implicitly broadcasting operations in the IR. It seems simpler to make broadcast an explicit operation, which we make sure to deforest. In fact, broadcast is a special case of the generic tensor index operation I'll describe below. Implicitly broadcasting operations could be provided in the Python interface, making broadcasts explicit when constructing the IR.; * I'm also not sure how much special treatment we should give to block matrices in the IR. I now like to think of block matrices as just 4-tensors, with matrix operations like matrix multiplication lowering to operations on 4-tensors. If we allow tensors to have some distributed dimensions and some ""small"" dimensions, then at least in the backend we might not need special handling of block structures. It may still be helpful to have a special block matrix/tensor representation at the top level IR, or maybe that should only live in PythonI'm not sure. Here's a proposal for a set of primitive tensor operations. * Outer product: Takes two tensors, T1 and T2, with shapes [n1, ..., ni] and [m1, ..., mj], and entry types t1 and t2, and makes a tensor Out with shape [n1, ..., ni, m1, ..., mj] and entry type (t1, t2). If we want to support sparse tensors, this should take a flag specifying how the sparse structure of the output is determined from those of the inputs. I'll call the possible flags ""and"", ""or"", and ""true"". The ""and"" flag says that Out(n, m) is defined iff T1(n) AND T2(m) are both defined. If we will be multiplying the pairs, or applying any other operation with our default missingness semantics, this is the appropriate setting.; The ""or"" flag says Out(n, m) is defined iff T1(n) OR T2(m) is defined, as is appropriate if we are adding the pairs.; ""true"" just means make Out dense, regardless of the sparsity of the inputs. * Map. I don't think there's much to say here. * Generic index operations (not sure what to call these). I'll first give some example",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772:353,interface,interface,353,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772,1,['interface'],['interface']
Integrability,"Some thoughts:. - This should probably be the default; - Export table is spark dependent. It shouldn't be.; - For the `CONCATENATED` case, we're probably not going to do better than this.; - For the parallel cases, even if we don't handle indexing while the files are being written (which would be the best option), It would be nice to have a `parallelize` kind of functionality that doesn't rely on spark.; - This is less important though since we still use RDD for `export_table` and `export_vcf`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9485#issuecomment-696247576:79,depend,dependent,79,https://hail.is,https://github.com/hail-is/hail/pull/9485#issuecomment-696247576,1,['depend'],['dependent']
Integrability,"Something still isn't right with my configuration. The service and deployment are all up. I can curl to the internal gateway and it shows up in the logs. However, I'm getting 404 with this query: `http://hail.internal/jigold/batch2/healthcheck`. I set up cloud dns to route hail.internal to the internal gateway. I verified no traffic is recorded in the jigold router. I tried adding and removing the `gateway` service account but it made no difference.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6918#issuecomment-524103114:268,rout,route,268,https://hail.is,https://github.com/hail-is/hail/pull/6918#issuecomment-524103114,2,['rout'],"['route', 'router']"
Integrability,Sorry - one more thing I need help with. There's a cyclical import with the RouterFS in `variables.py`. Should I just pylint ignore it?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13224#issuecomment-1677892406:76,Rout,RouterFS,76,https://hail.is,https://github.com/hail-is/hail/pull/13224#issuecomment-1677892406,1,['Rout'],['RouterFS']
Integrability,"Sorry @lgruen these errors were transient issues mostly from our click/dependencies breaking style, but nothing to do with the PR. For CI builds, unfortunately your hail account must be a developer account to see the page. Dan would know whether or not we can do that for your account.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11698#issuecomment-1088742898:71,depend,dependencies,71,https://hail.is,https://github.com/hail-is/hail/pull/11698#issuecomment-1088742898,1,['depend'],['dependencies']
Integrability,"Sorry @tpoterba , there's still more work to do to integrate this throughout Hail without tests failing. That's the next step!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1559#issuecomment-287180359:51,integrat,integrate,51,https://hail.is,https://github.com/hail-is/hail/pull/1559#issuecomment-287180359,1,['integrat'],['integrate']
Integrability,"Sorry I missed your message! The code as written now is plainly wrong: we access a mutable map from two threads without synchronization. We need this change regardless of how it affects error messages. If the tests pass, I'm confident this is fine. Are there components of the system you don't think are well tested by our tests?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13546#issuecomment-1724475471:20,message,message,20,https://hail.is,https://github.com/hail-is/hail/pull/13546#issuecomment-1724475471,3,"['message', 'synchroniz']","['message', 'messages', 'synchronization']"
Integrability,"Sorry for the delay! Every HTTP Status Code has a ""message"" associated with it. That message doesn't vary by request, for 403 its ""Forbidden"". There's a [full list at the MDN](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status). You have to read the body of the request to get rate limit exceeded. I'm sorry, I was playing fast and loose with words, when I said ""message,"" I was referring to the body of the HTTP request, not the `message` field of the aiohttp ClientResponseError. The message, the code and all that come from the HTTP Header, before the body is parsed:; ```; (base) # curl -v https://google.com/123 >/dev/null ; ...; > GET /123 HTTP/1.1; > Host: google.com; > User-Agent: curl/7.69.1; > Accept: */*; > ; ...; < HTTP/1.1 404 Not Found; < Content-Type: text/html; charset=UTF-8; < Referrer-Policy: no-referrer; < Content-Length: 1564; < Date: Thu, 13 May 2021 15:31:43 GMT; < Alt-Svc: h3-29="":443""; ma=2592000,h3-T051="":443""; ma=2592000,h3-Q050="":443""; ma=2592000,h3-Q046="":443""; ma=2592000,h3-Q043="":443""; ma=2592000,quic="":443""; ma=2592000; v=""46,43""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10432#issuecomment-840641717:51,message,message,51,https://hail.is,https://github.com/hail-is/hail/pull/10432#issuecomment-840641717,5,['message'],['message']
Integrability,"Sorry you hit this -- I think I understand what happened. Since we don't include jupyter as a Hail package dependency (it's a large dependency and pulls in a host of transitive dependencies as well), when you ran `jupyter` you picked up a different `jupyter` (probably the conda base environment one, which uses an entirely different Python installation). . I don't want to add jupyter as a dependency, but we can certainly add a note in the tutorials landing page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7136#issuecomment-535590377:107,depend,dependency,107,https://hail.is,https://github.com/hail-is/hail/issues/7136#issuecomment-535590377,4,['depend'],"['dependencies', 'dependency']"
Integrability,"Sorry! I didn't see this because of the review. The root problem is that `raise_for_status` ignores the response body. This is a [known issue in aiohttp](https://github.com/aio-libs/aiohttp/issues/4600). It will be fixed in 4.0.0, but development on that seems slow. There's a variety of solutions to this problem. Every solution avoids aiohttp's raise_for_status and replaces it with something that includes the response body in the error message. A thorough fix to this is to finish the work I started in `httpx.py`. Instead of returning an `aiohttp.ClientSession` we could return a shim class that wraps `aiohttp.ClientSession` and checks the status code itself and raises an error *with the response body*. A smaller fix that only addresses aiogoogle would be to modify `aiogoogle.auth.Session` to:; 1. Not pass `raise_for_status` on to `aiohttp.ClientSession`.; 2. Store raise_for_status as a field on `aiogoogle.auth.Session`.; 2. In `aiogoogle.auth.Session.request`, if `self.raise_for_status` is true and the response status is greater than or equal to 400, retrieve the response body and raise an exception (maybe `HailHTTPError`) that includes the body.; 3. Ensure `is_transient_error` properly handles whatever exception we raise.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10432#issuecomment-852213289:440,message,message,440,https://hail.is,https://github.com/hail-is/hail/pull/10432#issuecomment-852213289,2,"['message', 'wrap']","['message', 'wraps']"
Integrability,"Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`). I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-671357400:402,depend,dependent,402,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671357400,1,['depend'],['dependent']
Integrability,"Sorry, fine meaning just having the submit routes and the test. Not fine with the polling of GitHub with the start time stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9614#issuecomment-714575525:43,rout,routes,43,https://hail.is,https://github.com/hail-is/hail/pull/9614#issuecomment-714575525,1,['rout'],['routes']
Integrability,"Sorry, got wrapped up in other work. I've got things primed to run benchmark comparisons in the morning. Here's some crazy numbers just running one iteration (from this pr):; ```; 2019-10-08 16:46:31,396: INFO: [1/6] Running table_aggregate_linreg20...; 2019-10-08 16:46:44,645: INFO: burn in: 13.24s; 2019-10-08 16:46:55,367: INFO: run 1: 10.72s; 2019-10-08 16:46:55,367: INFO: [2/6] Running table_aggregate_linreg21...; 2019-10-08 16:47:07,836: INFO: burn in: 12.47s; 2019-10-08 16:47:19,376: INFO: run 1: 11.54s; 2019-10-08 16:47:19,376: INFO: [3/6] Running table_aggregate_linreg22...; 2019-10-08 16:47:32,728: INFO: burn in: 13.35s; 2019-10-08 16:47:44,571: INFO: run 1: 11.84s; 2019-10-08 16:47:44,571: INFO: [4/6] Running table_aggregate_linreg23...; 2019-10-08 16:48:37,183: INFO: burn in: 52.61s; 2019-10-08 16:49:28,580: INFO: run 1: 51.40s; 2019-10-08 16:49:28,580: INFO: [5/6] Running table_aggregate_linreg24...; 2019-10-08 16:50:25,410: INFO: burn in: 56.83s; 2019-10-08 16:51:20,184: INFO: run 1: 54.77s; 2019-10-08 16:51:20,184: INFO: [6/6] Running table_aggregate_linreg25...; 2019-10-08 16:52:16,778: INFO: burn in: 56.59s; 2019-10-08 16:53:21,050: INFO: run 1: 64.27s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7134#issuecomment-539702199:11,wrap,wrapped,11,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-539702199,1,['wrap'],['wrapped']
Integrability,"Spark 3.1.1 is out, dataproc image should be updated from the release candidate dependency within a week or so I think. The only remaining issue I think is a weird one, a particular blockmatrix test is failing because json4s can't find a constructor for `BlockMatrixSparsity` objects. Looking into it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10054#issuecomment-790702427:80,depend,dependency,80,https://hail.is,https://github.com/hail-is/hail/pull/10054#issuecomment-790702427,1,['depend'],['dependency']
Integrability,"Still contains debug messages, and needs rebase. All will be fixed after tests pass. Remaining tests not passing are:; <img width=""358"" alt=""Screenshot 2020-02-08 15 17 58"" src=""https://user-images.githubusercontent.com/5543229/74091636-8162d600-4a87-11ea-9750-f2804352d4a3.png"">. Each of these fails with a match error in Emit, either on MakeStream, or StreamRange",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-583773630:21,message,messages,21,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583773630,1,['message'],['messages']
Integrability,"Sure thing. Planning on implementing three top-level convenience methods for converting between relational IRs:; - `t.to_matrix_table` which essentially wraps the python approach you laid out in the creation of this issue; - `bm.to_table` which produces a table where each row corresponds to a row of the original BlockMatrix (will do a write and a read to avoid shuffling, actually have to dig into the RDDs for this one); - `bm.to_matrix_table` which will just compose the previous two methods",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5504#issuecomment-470199897:153,wrap,wraps,153,https://hail.is,https://github.com/hail-is/hail/issues/5504#issuecomment-470199897,1,['wrap'],['wraps']
Integrability,"Sure, the interface is slightly different though to accommodate for some of the new features. It also doesn't support passing python lists instead of hail expr -- could be trivially added if useful though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5601#issuecomment-473303829:10,interface,interface,10,https://hail.is,https://github.com/hail-is/hail/pull/5601#issuecomment-473303829,1,['interface'],['interface']
Integrability,TODO for myself. Any new routes we add need to have a test added to `test_authorized_users_only()`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14016#issuecomment-1834642128:25,rout,routes,25,https://hail.is,https://github.com/hail-is/hail/pull/14016#issuecomment-1834642128,1,['rout'],['routes']
Integrability,"Taking a look at this interface, and I think I prefer it to your other suggestion (lambda va and ga). One question: with `sm = SplitMulti(ds)` - This will now be a class modified in-place with `sm.update_rows()` rather than `sm = sm.update_rows()` (or annotate_rows)?. I ask because it's a bit different from the rest of the `ds` interface and might be a tad bit confusing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2869#issuecomment-364135133:22,interface,interface,22,https://hail.is,https://github.com/hail-is/hail/pull/2869#issuecomment-364135133,2,['interface'],['interface']
Integrability,"Tested this out by running it locally and changing `aiohttp-session` in `docker/requirements.txt` from `2.70` to `2.8.0` and got the following:. ```; + pip-compile --quiet docker/requirements.txt docker/pinned-requirements.txt --output-file=new-pinned.txt; Could not find a version that matches aiohttp-session==2.7.0,==2.8.0 (from -r docker/requirements.txt (line 4)); Tried: 0.0.1, 0.0.1, 0.1.0, 0.1.0, 0.1.1, 0.1.1, 0.1.2, 0.1.2, 0.2.0, 0.2.0, 0.3.0, 0.3.0, 0.4.0, 0.4.0, 0.5.0, 0.5.0, 0.7.0, 0.7.0, 0.7.1, 0.7.1, 0.8.0, 0.8.0, 1.0.0, 1.0.0, 1.0.1, 1.0.1, 1.1.0, 1.1.0, 1.2.0, 1.2.0, 1.2.1, 1.2.1, 2.0.0, 2.0.0, 2.0.1, 2.0.1, 2.1.0, 2.1.0, 2.2.0, 2.2.0, 2.3.0, 2.3.0, 2.4.0, 2.4.0, 2.5.1, 2.5.1, 2.6.0, 2.6.0, 2.7.0, 2.7.0, 2.8.0, 2.8.0, 2.9.0, 2.9.0, 2.10.0, 2.10.0, 2.11.0, 2.11.0; Skipped pre-versions: 2.10.0a0, 2.10.0a0; There are incompatible versions in the resolved dependencies:; aiohttp-session==2.7.0 (from -r docker/pinned-requirements.txt (line 20)); aiohttp-session==2.8.0 (from -r docker/requirements.txt (line 4)); ```. and another example where I added an unrelated pip dependency in the requirements but didn't update the lock file",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11842#issuecomment-1131987651:877,depend,dependencies,877,https://hail.is,https://github.com/hail-is/hail/pull/11842#issuecomment-1131987651,2,['depend'],"['dependencies', 'dependency']"
Integrability,"Tests are passing now. I am going to mark WIP because I want to run benchmarks on Monday before this merges. You can look whenever though. . Some things to look at:. 1. Does `ServiceTaskContext` have a notion of stages or retries, or should those also always be 0? ; 2. Some of the logic with combOps getting region pools is a little different, now depends on where combOp is running . For the most part though, PR is just refactoring to pass `RegionPool` everywhere",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9675#issuecomment-738330547:349,depend,depends,349,https://hail.is,https://github.com/hail-is/hail/pull/9675#issuecomment-738330547,1,['depend'],['depends']
Integrability,"Tests finally all pass. The boundary condition is too fragile (TContainer can be TDict, and we cannot wrap that in ToArray)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586422358:102,wrap,wrap,102,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586422358,1,['wrap'],['wrap']
Integrability,"Tests pushing to the cache is important. For example, if a PR adds a new apt-get dependency, only the first build should have to rebuild the image. Subsequent commits / retries should be fast.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152533590:81,depend,dependency,81,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152533590,1,['depend'],['dependency']
Integrability,Thank @jbloom22 ! Hadn't seen that this had change (was `bool` back in the days). And indeed the error message did not put me on the right track :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4033#issuecomment-408982088:103,message,message,103,https://hail.is,https://github.com/hail-is/hail/issues/4033#issuecomment-408982088,1,['message'],['message']
Integrability,"Thank you all for another round of detailed critique!. OK, I think the only remaining critical fix is to hard-code a mainclass. This is a wee bit complicated because I need to multiplex the ServiceBackendSocketAPI2 and the Worker. I hope to do this tomorrow AM. I'll then dismiss reviews. I also have a list of todos generated by this process which will feedback into some master QoB doc that integrates the two teams necessary todos.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194#issuecomment-1033173934:393,integrat,integrates,393,https://hail.is,https://github.com/hail-is/hail/pull/11194#issuecomment-1033173934,1,['integrat'],['integrates']
Integrability,"Thanks @jmarshall for bringing this to our attention. It looks like while we updated the upper bound here, we did not update our fully-pinned requirements which we use to test in CI, so it did not catch this incompatibility. That being said, I don't think that was necessarily a mistake, because by testing our minimum-compatible-version we make sure not to introduce incompatibilities on that end of the spectrum either.. I think I don't see a good way in which we can confidently support more than one major version of a dependency at a given point in time. Even without the bokeh issue, there could easily be places in our codebase where we use pandas 1.x functionality that has been removed in 2.0. @danking thoughts on moving the pandas pin to >= the 2.x.x version that we test with and <3?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1520406276:523,depend,dependency,523,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1520406276,1,['depend'],['dependency']
Integrability,"Thanks dependabot, but we got this #10120",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10115#issuecomment-786824947:7,depend,dependabot,7,https://hail.is,https://github.com/hail-is/hail/pull/10115#issuecomment-786824947,1,['depend'],['dependabot']
Integrability,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:802,depend,depends,802,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040,1,['depend'],['depends']
Integrability,Thanks for reporting this. I had tried to remove our dependence on `setuptools` but had not done a good enough job clearly.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14630#issuecomment-2243322335:53,depend,dependence,53,https://hail.is,https://github.com/hail-is/hail/issues/14630#issuecomment-2243322335,1,['depend'],['dependence']
Integrability,"Thanks for the explanation! I'm happy to make the change, I was just trying to understand the difference between Host and X-Forwarded-Host a little better before first. So if I understand correctly, for the different headers:; - X-Forwarded-Proto gets passed to the router through the base https server in gateway, which sets X-Forwarded-Proto to `$scheme`, which is always going to be https since that's always going to be the protocol you're using for that server? And so when we use `$updated_scheme` for the blog server in the router's config, it's going to look at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I'm having trouble seeing when `$http_x_forwarded_proto` would ever be absent, although if it is, isn't $scheme always `http` since all traffic from gateway to router is via http? Or am I misunderstanding how this works?; - I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? I don't quite follow your comment about our use of `Host` being wrong, in this case; I *think* I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enough to know what happens if I change it to `proxy_set_header Host $http_host;` instead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548078429:266,rout,router,266,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548078429,4,"['protocol', 'rout']","['protocol', 'router']"
Integrability,"Thanks for the thoughtful review @catoverdrive!. @cseed , your review is requested of the use of TLS. @cseed , I'm also curious of your thoughts about documenting the shuffler interface. I could eventually write a README file in the package that reflects the final outcome of the dev forum posts. We could also leave the dev forum as the only documentation. What do you think?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8361#issuecomment-609933937:176,interface,interface,176,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609933937,1,['interface'],['interface']
Integrability,"Thanks for working on this!. Would it be possible to keep the logout button on every page but add a step where it takes the user to the Auth UI to make it work? Specifically, I'm thinking we could add logic to the `/user` route in `auth/auth/auth.py` such that if we pass in the query parameter `logout`, it calls the same code as the `/logout` endpoint, and then replace the `form` and `button` with something like:. ```html; <a href=""https://auth.hail.is/user?logout"">Log out</a>; ```. The tricky part of that might be getting the CSRF token, but since the `/user` page is only accessible by logged in users (because of the `authenticated_users_only` decorator), I *think* there should always be a CSRF token accessible via `request.cookies[""_csrf""]` (e.g. https://github.com/hail-is/hail/blob/main/web_common/web_common/web_common.py#L93).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14639#issuecomment-2258267657:222,rout,route,222,https://hail.is,https://github.com/hail-is/hail/pull/14639#issuecomment-2258267657,1,['rout'],['route']
Integrability,Thanks! Fix worked (I just rolled a custom jar) so don't worry about integrating immediately on my account.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2456#issuecomment-345872854:69,integrat,integrating,69,https://hail.is,https://github.com/hail-is/hail/pull/2456#issuecomment-345872854,1,['integrat'],['integrating']
Integrability,"Thanks, @tomwhite! This is great. Is there a Hive CLI equivalent of `LIKE PARQUET <file>`? I can't figure out how to get Hive to infer the schema from the Parquet file rather than specifying it explicitly. It would be awesome to be able to query the genotypes, too. It seems like we could write a SerDe (now I'm thinking ImpEx isn't so bad :)) to unpack the genotypes. Does that sound like the right approach?. On a related note, we've played with storing VDS natively as Parquet as (variant, variant annotations, array(genotype)). Even when I ported over some of the GenotypeStream encoding tricks (OD instead of DP, etc.), it was 2-3x larger (using Snappy compression vs. our internal LZ4 compression). That's disappointing, esp. when we have 30+TB datasets on the way. What's worse, simple operations like counting genotypes (`count -g`) are 5-10x in the native representation. Current master:. ```; $ hail importvcf profile225.vcf.bgz write -o profile225.vds read -i profile225.vds count -g; hail: info: timing:; importvcf: 508.829ms; write: 3m6.7s; read: 1.629s; count: 13.934s; $ du -sh profile225.vds; 2.0G profile225.vds; ```. And with the `jg_dataframe1` experimental branch, which uses native Parquet and computes the count using a UDF that computes the sum per array (fastest Parquet-based implementation we've found so far):. ```; $ hail importvcf profile225.vcf.bgz write -o profile225.vds read2 -i profile225.vds count2; hail: info: timing:; importvcf: 492.354ms; write: 5m57.1s; read2: 1.466s; count2: 1m44.1; $ du -sh profile225.vds; 5.4G profile225.vds; ```. That's 2.7x larger and >7x slower. This includes the fact that the Parquet version is only loading the GT field of the genotypes (!). This might be a non-starter for us. We'd love the flexibility and interoperability of standard Parquet. If you have other ideas about how to get Parquet close to what we currently have, I'd love to talk more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/480#issuecomment-234027310:1776,interoperab,interoperability,1776,https://hail.is,https://github.com/hail-is/hail/pull/480#issuecomment-234027310,1,['interoperab'],['interoperability']
Integrability,"Thanks, the lsmtree dependency change unbreaks Google Dataproc compatibility so 'm tagging this high-prio.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8571#issuecomment-615334218:20,depend,dependency,20,https://hail.is,https://github.com/hail-is/hail/pull/8571#issuecomment-615334218,1,['depend'],['dependency']
Integrability,Thanks. Didn't know what the protocol was and didn't want to be pushy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9250#issuecomment-673731776:29,protocol,protocol,29,https://hail.is,https://github.com/hail-is/hail/pull/9250#issuecomment-673731776,1,['protocol'],['protocol']
Integrability,That message is caused by returning an `int` from a Flask request handler. Looks like the endpoint is called `/test`. I don't see that in the CI logs. Can you show me on your laptop later?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5418#issuecomment-466544014:5,message,message,5,https://hail.is,https://github.com/hail-is/hail/pull/5418#issuecomment-466544014,1,['message'],['message']
Integrability,"That will get updated once this PR goes in, while the docs and the client users use (batch not pipeline anymore) will be dependent on upgrading to the next PIP release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8453#issuecomment-612087215:121,depend,dependent,121,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-612087215,1,['depend'],['dependent']
Integrability,That's a super nice interface.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2975#issuecomment-368139786:20,interface,interface,20,https://hail.is,https://github.com/hail-is/hail/pull/2975#issuecomment-368139786,1,['interface'],['interface']
Integrability,"That's all part of the same error. If you resolve the file permissions issue, then the HailContext can be successfully initialized. I'll report this confusing error message to the team.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337918138:165,message,message,165,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337918138,1,['message'],['message']
Integrability,"That's fine for the first two comments, but please address the last comment to have the `delete_batch_azure_instances` depend on the Azure tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13353#issuecomment-1660786852:119,depend,depend,119,https://hail.is,https://github.com/hail-is/hail/pull/13353#issuecomment-1660786852,1,['depend'],['depend']
Integrability,"Thats a great point; ________________________________; From: Patrick Schultz ***@***.***>; Sent: Tuesday, October 4, 2022 7:27 AM; To: hail-is/hail ***@***.***>; Cc: Emma S Kelminson ***@***.***>; Author ***@***.***>; Subject: Re: [hail-is/hail] geom_boxplot (PR #11720). CAUTION: EXTERNAL SENDER. I haven't had a chance to look into this and understand how precomputed and faceting work and interact. If Iris wants to give that some thought, I'd be happy to advise. If there is a real obstruction to doing this in the current design, it would be good for both Iris and I to understand it before starting a redesign. ; Reply to this email directly, view it on GitHub<https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fhail-is%2Fhail%2Fpull%2F11720%23issuecomment-1266817118&data=05%7C01%7Cemma.kelminson001%40umb.edu%7Cf12d7220658149a9bc7a08daa5fb7ba4%7Cb97188711ee94425953c1ace1373eb38%7C0%7C0%7C638004796790915105%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=dry2ZdLcd1beO2WrdhCRT9sV0kjtFm4lKsy2nL0iqnU%3D&reserved=0>, or unsubscribe<https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FARVGM3NWM6AEALMGZIZ2WS3WBQIDVANCNFSM5SGPPZUA&data=05%7C01%7Cemma.kelminson001%40umb.edu%7Cf12d7220658149a9bc7a08daa5fb7ba4%7Cb97188711ee94425953c1ace1373eb38%7C0%7C0%7C638004796791071348%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=iR6L6EoWRp6CivnjxCyKhe%2Bjs%2FYl2p29e%2FVbCxYxgvQ%3D&reserved=0>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11720#issuecomment-1266858564:1669,Message,Message,1669,https://hail.is,https://github.com/hail-is/hail/pull/11720#issuecomment-1266858564,1,['Message'],['Message']
Integrability,"The [TextInputFormat](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html) class clearly comes from hadoop. It's no longer in the location from which we import it. We must get it from some other dependency. OK. So, before my simplification of build.gradle, we used a configuration called `compile` and another one called `testCompile`. [Neither of those exist in modern gradle, apparently](https://docs.gradle.org/current/userguide/java_library_plugin.html#sec:java_library_configurations_graph). I found a side-note about the `compile` configuration [here](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:java_dependency_management_overview) (search for ""compile""):. > **Why no compile configuration?**; > The Java Library Plugin has historically used the compile configuration for dependencies that are required to both compile and run a projects production code. It is now deprecated, and will issue warnings when used, because it doesnt distinguish between dependencies that impact the public API of a Java library project and those that dont. You can learn more about the importance of this distinction in [Building Java libraries](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:building_java_libraries). OK, so, we used to just dump everything into our runtime dependencies. I changed it so that we have three kinds of dependencies:; 1. `shadow`: these are provided by Dataproc/QoB at run-time. They are not in any JAR. They are not on the `testRuntimeClasspath` or `runtimeClasspath`. They are on the `testCompileClasspath` because I [explicitly requested](https://github.com/hail-is/hail/blob/main/hail/build.gradle#L98) that `testCompileOnly` bring in our `shadow` dependencies.; 2. `implementation`: these are included in all class paths and in shadow JARs (but not ""thin"" jars generated by `./gradlew jar`).; 3. `testImplementation`: these are included in test class paths and in shadow JARs. Our t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741:231,depend,dependency,231,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741,2,['depend'],"['dependencies', 'dependency']"
Integrability,"The `*` means that route will be triggered for any request matching the specified URL for any method, be it GET or POST, etc. The reason I needed to make that change is that when envoy makes an authentication request to that endpoint, it uses the HTTP method of the original request. E.g. If I make a POST to https://internal.hail.is/dgoldste/batch/batches/create envoy will authenticate me with a POST request to auth:443/api/v1alpha/verify_dev_credentials. So I can't set that endpoint to be any one method.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12094#issuecomment-1282998946:19,rout,route,19,https://hail.is,https://github.com/hail-is/hail/pull/12094#issuecomment-1282998946,1,['rout'],['route']
Integrability,"The `pre-commit` hook is a little sticky because `pre-commit` installs each tool in its own isolated virtual env, which won't have the dependencies unless we tell `pre-commit` to also install all of our pinned dependencies into the pyright virtualenv. We can configure pyright to use a different virtualenv for all its dependencies, but that would require each developer specifying the name of their virtual environment in `pyproject.toml`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13437#issuecomment-1681248432:135,depend,dependencies,135,https://hail.is,https://github.com/hail-is/hail/pull/13437#issuecomment-1681248432,3,['depend'],['dependencies']
Integrability,"The creeping expansion of the interface is on me as we tried to not break 0.1. I'd appreciate discussing in person what makes the most sense for 0.1. Here are the pieces I think we should separate for devel, though we could consider providing a meta-interface as well that combines some of them for usability. I'm writing (U, S, L) for a local matrix of eigenvectors U, an array of eigenvalues S, and an array of labels L on the rows of U (as with labels for SymmetricMatrix). 1) VDS to a (labeled) symmetric matrix (we have these: GRM, RRM, LD matrix, and Dan is working on a way to read and write them). 2) Symmetric matrix to (U, S, L), which we'll want to write and read. This modularizes the single-core eigen-decomposition bottleneck. 3) Variant-labeled (V, S, L) and VDS with those variants to transport (V, S, L) to sample-labeled (U, S, L). Currently this also requires knowing the number of samples used to make the LDMatrix since that number is used in its normalization. I agree it feels unnatural to need to remember this; to avoid it we'd need an unnormalized version. 4) Sample-labeled (U, S, L) and VDS to global fit of LMM including delta. This is currently a local computation that's been pretty fast in practice but as sample sizes increase we will want to distribute evaluating many values of delta in parallel. Note this step only uses the sample annotations on the VDS, so logically it could also be on KeyTable (which would be the sample KeyTable of the VDS). 5) Sample-labeled (U, S, L), VDS, and delta to per-variant-fit of LMM. This VDS can now contain exactly the variants one wants to fit. (5) should eventually be decomposed as well. The first command should project from Matrix to Matrix (projecting both numeric cells and a list of numeric sample annotations) and some additional small data. Then (6) will do per variant tests starting from after this projection (that is, after what is the BIG computation when you have tens of millions of variants). That way users can",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1984#issuecomment-319971210:30,interface,interface,30,https://hail.is,https://github.com/hail-is/hail/pull/1984#issuecomment-319971210,2,['interface'],['interface']
Integrability,"The current aggregator interface assumes copy is the same as creating a new, uninitialized aggregator of the same class. This is used when copying aggregators to have one per sample when aggregating over rows for example. It is used in `MatrixAggregateRowsByKey`, `MatrixAggregateColsByKey`, and `MatrixMapCols`. I think my `deepCopy` would result in the same functionality because the aggregator being copied is uninitialized. I would be okay with changing `deepCopy` to `copy` and removing the existing `copy` methods or renaming it to something else. @cseed Do you have thoughts on this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3822#issuecomment-400073198:23,interface,interface,23,https://hail.is,https://github.com/hail-is/hail/pull/3822#issuecomment-400073198,1,['interface'],['interface']
Integrability,"The dependencies are used in `project_changed.py`. That's how I broke the CI before -- I changed batch, but the CI tests weren't run because we had no notion of project dependency.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5593#issuecomment-473324852:4,depend,dependencies,4,https://hail.is,https://github.com/hail-is/hail/pull/5593#issuecomment-473324852,2,['depend'],"['dependencies', 'dependency']"
Integrability,"The design of NativeModule's handling of files (which may involve several worker threads; each trying to initialize their own NativeModule referring to the same DLL) assumed that; /bin/mv would do an atomic rename to replace an old DLL with a newer version. But on; MacOS /bin/mv is non-atomic, and leaves a window between deleting the old file and ; replacing it with the new one. I'm working on details of a more robust file-synchronization scheme, once that's ok I'll; backport it here and also address the review comments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-408479218:427,synchroniz,synchronization,427,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-408479218,1,['synchroniz'],['synchronization']
Integrability,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:556,rout,route,556,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358,1,['rout'],['route']
Integrability,"The error message suggests that multiple clients are writing to:; ```; gs://rwalters-hail-tmp/merged_round2_sumstats.fix_lowconf.mt/entries/rows/parts/part-15801-2fde3786-67cb-42ed-8aac-f900cfcc4c00; ```; Which is a Hail Query partition file within a matrix table. This is happening in a flush of GoogleStorageFS after we've filled up the local byte buffer. I don't think this is 0.2.114 because the line numbers don't line up. This appears to be before our recent changes to handle requester pays. A couple thoughts:; 1. We retryTransientErrors on the entire partition. What happens if a partition fails and we automatically retry it? Does the generated code clean up all the output streams when an exception occurs? If not, it's possible that there's still an open connection to GCS when we re-run the partition's generated code gain.; 2. Hail Batch guarantees at least once execution of a job. It's possible that two distinct workers are executing the partition's generated code. Does the Hail Query generated code create a distinct file name for each execution? (I think the answer is yes.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1530268410:10,message,message,10,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1530268410,1,['message'],['message']
Integrability,"The error, some resources are missing. . {""levelname"": ""ERROR"", ""asctime"": ""2019-09-29 03:43:05,260"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory\n response = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/gear/csrf.py\"", line 20, in wrapped\n return await fun(request, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 77, in wrapped\n return await fun(request, userdata, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 417, in post_notebook\n return await _post_notebook('notebook', request, userdata)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:174,message,message,174,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851,1,['message'],['message']
Integrability,"The failure doesn't appear to be related to my changes. Installing the docker requirements, which has `setuptools>=38.6.0`, is trying to upgrade to the latest setuptools (56.0.0). Another dependency might be forcing the upgrade. However, setuptools was installed via apt, not pip, and that is causing this:. ```; Attempting uninstall: setuptools; Found existing installation: setuptools 45.2.0; Not uninstalling setuptools at /usr/lib/python3/dist-packages, outside environment /usr; Can't uninstall 'setuptools'. No files were found to uninstall.; ```. So there's two things I don't understand. I'll keep investigating. I glad I PRed this separately!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10349#issuecomment-824092733:188,depend,dependency,188,https://hail.is,https://github.com/hail-is/hail/pull/10349#issuecomment-824092733,1,['depend'],['dependency']
Integrability,The improved error message isn't working because we catch TypeError instead of ExpressionException,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13280#issuecomment-1646024467:19,message,message,19,https://hail.is,https://github.com/hail-is/hail/issues/13280#issuecomment-1646024467,1,['message'],['message']
Integrability,"The integration test is failing. Otherwise, looks good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1572#issuecomment-287900602:4,integrat,integration,4,https://hail.is,https://github.com/hail-is/hail/pull/1572#issuecomment-287900602,1,['integrat'],['integration']
Integrability,"The interface needs some work, first, but this is probably a ~3 month timeline (the outstanding calls into java are for things like maximal_independent_set, the BlockMatrix linear algebra stuff, and a few utility functions). I'm also happy to take PRs now to change the java.util.HashMaps to java.util.Map",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5340#issuecomment-463827896:4,interface,interface,4,https://hail.is,https://github.com/hail-is/hail/issues/5340#issuecomment-463827896,1,['interface'],['interface']
Integrability,"The issue is `j.wait()` will trigger when the job is complete which is set before the callback occurs. However, even if we change the order `set_state` and `callback` are called (change in interface), there's still the possibility that the callback won't complete before the wait is terminated. Therefore, the correct solution should wait for `d` to be non-empty with a timeout.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5817#issuecomment-482597121:189,interface,interface,189,https://hail.is,https://github.com/hail-is/hail/issues/5817#issuecomment-482597121,1,['interface'],['interface']
Integrability,"The issue seems to be that ci's `/refresh_batch_state` POST route broke. ```; INFO	| 2019-03-02 16:16:17,392 	| ci.py 	| <lambda>:409 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 500 -; ERROR	| 2019-03-02 16:16:17,394 	| ci.py 	| polling_event_loop:400 | Could not poll due to exception: 500 Server Error: INTERNAL SERVER ERROR for url: http://127.0.0.1:5000/refresh_batch_state; ```. edit:. These appear to be the relevant parts of the stack trace:. ```; File ""/hail-ci/ci/ci.py"", line 144, in refresh_batch_state; jobs = batch_client.list_jobs(); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/client.py"", line 202, in list_jobs; jobs = self.api.list_jobs(self.url); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/api.py"", line 41, in list_jobs; response = requests.get(url + '/jobs', timeout=self.timeout). #... requests.exceptions.ReadTimeout: HTTPConnectionPool(host='batch.default', port=80): Read timed out. (read timeout=5); ```. The batch /jobs endpoint appears to be having an issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-468934883:60,rout,route,60,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468934883,1,['rout'],['route']
Integrability,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403:661,wrap,wrap,661,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403,4,['wrap'],"['wrap', 'wrap-in-ToStream', 'wrapping']"
Integrability,"The problem is the exception we actually get tells us no information about the problem other than it is Forbidden. aiohttp.client_exceptions.ClientResponseError: 403, message='Forbidden', url=URL('https://compute.googleapis.com/compute/v1/projects/hail-vdc/zones/us-central1-a/instances/batch-worker-default-highmem-40e6w/detachDisk?deviceName=batch-disk-be5253d5361d45118350&requestId=130727e3-f96c-4f3a-ac86-1df585bc4e1c'",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10432#issuecomment-833643845:167,message,message,167,https://hail.is,https://github.com/hail-is/hail/pull/10432#issuecomment-833643845,1,['message'],['message']
Integrability,"The python ones. I didn't actually read the diff because I was on my phone, just saw ""remove ndarraywrite tesr"" commit message. Thanks",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7002#issuecomment-528642262:119,message,message,119,https://hail.is,https://github.com/hail-is/hail/pull/7002#issuecomment-528642262,1,['message'],['message']
Integrability,"The python/scala interface should be the same, I agree with Jon",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1768#issuecomment-299478117:17,interface,interface,17,https://hail.is,https://github.com/hail-is/hail/pull/1768#issuecomment-299478117,1,['interface'],['interface']
Integrability,The repository in question was created at `2018-10-10T00:32:59Z`. GitHub indicates [no system failures](https://status.github.com/messages) on October the tenth or the ninth.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429026782:130,message,messages,130,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429026782,1,['message'],['messages']
Integrability,The shadow jar file was newer than its dependencies so Make assumed it was good to go. The Make-Way TM would be to make the output of gradle an intermediate step which is then copied to a new file by the jar modification.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9667#issuecomment-722646751:39,depend,dependencies,39,https://hail.is,https://github.com/hail-is/hail/pull/9667#issuecomment-722646751,1,['depend'],['dependencies']
Integrability,"The specialization creates mangled class files, and having just one implementation makes it much easier to reason about the interfaces. I think we can add support for all the numeric types back once we've gotten rid of the old agg path, if anyone cares enough to do that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7529#issuecomment-554389306:124,interface,interfaces,124,https://hail.is,https://github.com/hail-is/hail/pull/7529#issuecomment-554389306,1,['interface'],['interfaces']
Integrability,The suppressed message is:; ```; 	Suppressed: java.lang.Exception: #block terminated with an error; 		at is.hail.shadedazure.reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:99); 		at is.hail.shadedazure.reactor.core.publisher.Mono.block(Mono.java:1742); 		at is.hail.shadedazure.com.azure.storage.common.implementation.StorageImplUtils.blockWithOptionalTimeout(StorageImplUtils.java:133); 		at is.hail.shadedazure.com.azure.storage.blob.specialized.BlobClientBase.getPropertiesWithResponse(BlobClientBase.java:1379); 		at is.hail.shadedazure.com.azure.storage.blob.specialized.BlobClientBase.getProperties(BlobClientBase.java:1348); 		at is.hail.io.fs.AzureStorageFS.$anonfun$openNoCompression$1(AzureStorageFS.scala:223); 		at is.hail.io.fs.AzureStorageFS.$anonfun$handlePublicAccessError$1(AzureStorageFS.scala:175); 		at is.hail.services.package$.retryTransientErrors(package.scala:124); 		at is.hail.io.fs.AzureStorageFS.handlePublicAccessError(AzureStorageFS.scala:174); 		at is.hail.io.fs.AzureStorageFS.openNoCompression(AzureStorageFS.scala:220); 		at is.hail.io.fs.RouterFS.openNoCompression(RouterFS.scala:20); 		at is.hail.io.fs.FS.openNoCompression(FS.scala:322); 		at is.hail.io.fs.FS.openNoCompression$(FS.scala:322); 		at is.hail.io.fs.RouterFS.openNoCompression(RouterFS.scala:3); 		at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); 		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 		at is.hail.services.package$.retryTransientErrors(package.scala:124); 		at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:459); 		at is.hail.backend.service.Main$.main(Main.scala:15); 		at is.hail.backend.service.Main.main(Main.scala); 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodA,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906430:15,message,message,15,https://hail.is,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906430,1,['message'],['message']
Integrability,"The test `testMatrixUnionRowsMemo` is still failing with this change. The error that I am seeing is:; ```; java.util.NoSuchElementException: key not found: RefEquality(MatrixMapRows(MatrixMapRows(MatrixLiteral(...),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8]))))),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8])))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:32); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:47); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:46); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.PruneSuite.checkMemo(PruneSuite.scala:46); 	at is.hail.expr.ir.PruneSuite.testMatrixUnionRowsMemo(PruneSuite.scala:412); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4891#issuecomment-444264952:411,Wrap,WrappedArray,411,https://hail.is,https://github.com/hail-is/hail/pull/4891#issuecomment-444264952,2,['Wrap'],['WrappedArray']
Integrability,"The to-do list is roughly. - [ ] Get genome reference type in Variant, Interval, and Locus constructors in function registry; - [ ] Add default reference to HailContext ; - [ ] Add default reference to a bunch of RDD methods; - [ ] Expose genome reference in Python interface with import methods and as an input argument to TVariant(), etc.; - [ ] Make sure #2226 solves the problem of variant, locus, and interval methods having the correct function generated dependent on the genome reference; - [ ] Double check that if a user adds a new genome reference, it is visible on all workers.; - [ ] Add GenomeReference python class to documentation; - [ ] Convert GenomeReference -> ReferenceGenome (Jon's request); - [ ] Remove methods from Variant that do not take a GenomeReference as a parameter (Right now, everything is still hardcoded as GRCh37). I vaguely remember some debate on whether these functions should be removed from Variant completely and instead only called from GenomeReference.; - [ ] At some point, we may want to change Variant etc. so they store the contigIndex rather than the contig.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2208#issuecomment-332347245:266,interface,interface,266,https://hail.is,https://github.com/hail-is/hail/pull/2208#issuecomment-332347245,2,"['depend', 'interface']","['dependent', 'interface']"
Integrability,"There are a few small cosmetic changes in here that were a result of an updated pylint, but I put those in a separate commit to hopefully make that less confusing. There are a few follow-ups after this that I want to tackle; - simplifying the images for testing query (Dockerfile.hail-build, Dockerfile.hail-base, Dockerfile.hail-run). I think these are the only things that use `base_image` so we might be able to collapse a bunch of these; - updating to python 3.8 to avoid accidentally installing that in some of our images; - trying to produce eStargz images so that buildkit can lazily pull the base image when building new images. I hope that can bring some image build times down even further by not having to localize the installed pip dependencies when making changes to our python code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548:744,depend,dependencies,744,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548,1,['depend'],['dependencies']
Integrability,"There are code paths where ReferenceGenome's `codeSetup` invokes the filesystem. Those must be handled in a static context in order for this to work at all. https://github.com/tpoterba/hail/blob/e5b3fb3d2fba971d0c226094ae0c7ac66f190bbe/hail/src/main/scala/is/hail/variant/ReferenceGenome.scala#L493-L512; ```scala; if (fastaReader != null) {; rg = rg.invoke[String, FS, String, String, Int, Int, ReferenceGenome](; ""addSequenceFromReader"",; localTmpdir,; cb.getFS,; fastaReader.fastaFile,; fastaReader.indexFile,; fastaReader.blockSize,; fastaReader.capacity); }. for ((destRG, lo) <- liftoverMaps) {; rg = rg.invoke[String, FS, String, String, ReferenceGenome](; ""addLiftoverFromFS"",; localTmpdir,; cb.getFS,; lo.chainFile,; destRG); }; rg; ```. The static isn't final or anything, the setter is still currently called from the `FunctionWithFS` interface's `addFS` method, so the `resultWithIndex` closure is still what captures and saves the FS. Like I said, needs some work to make it work always.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9044#issuecomment-652767079:846,interface,interface,846,https://hail.is,https://github.com/hail-is/hail/pull/9044#issuecomment-652767079,1,['interface'],['interface']
Integrability,There are no commits here. Happy to review a pull request with:; 1) some code changes :); 2) a descriptive title and commit message,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6104#issuecomment-491798256:124,message,message,124,https://hail.is,https://github.com/hail-is/hail/pull/6104#issuecomment-491798256,1,['message'],['message']
Integrability,"There are now three check steps:; - check_hail (in the sense of $HAIL_HOME/hail directory), which checks the hail and hailtop packages, using the base image, from the source via `make check-hail`; - check_services, using the services image, except the benchmark service, which installs addition dependencies, from the source via `make -k check-services`, `-k` forces make to check all packages, even if one fails; - check_benchmark_service, which checks benchmark as installed in that image",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9394#issuecomment-685253481:295,depend,dependencies,295,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685253481,1,['depend'],['dependencies']
Integrability,"There is no requirement to support many OSes. We only have to support two platforms: recent Linux and OSX. They have variants but should be mutually compatible. Testing on Dataproc + recent version of Ubuntu or Debian (which we do in the CI) seems fine. OSX has flags for version support, we can just pick a version (10.10, say) and build for that and beyond. As I've said before, we control what we support and there is no need to make this a burden on ourselves. > If you build your own compiler + library, then you risk becoming incompatible with other libraries. Yes, we should ship with all our C++ dependencies. You convinced me of this? Then there is no issue. BLAS is a C library, so no issue there. Right now I think that just means the compiler and the standard library. > Probably not something I could do in the limited time available. Fair. I'm happy with partial progress in in the above direction, but this seems like a step backwards and something we will want to revert soon. I'm not inclined to go in this direction. Not having access to the standard library seems problematic. > Confirmed that this prebuilt libhail.so can run tests with HAIL_ENABLE_CPP_CODEGEN=1 on a dataproc node with the default 1.2 image (debian8 and g++-4.9.2). Did you test submitting jobs to the cluster itself? This can be quite a different environment than the tests. Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we submit cluster_sanity_check.py with and without C++ codegen enabled?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448:604,depend,dependencies,604,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448,1,['depend'],['dependencies']
Integrability,"There's a `--check` flag, I added it to the existing ci `check`. Had to add `black` to the dev dependencies for that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9931#issuecomment-769209896:95,depend,dependencies,95,https://hail.is,https://github.com/hail-is/hail/pull/9931#issuecomment-769209896,1,['depend'],['dependencies']
Integrability,"There's also #11428, which just merged and I forgot to write the changelog message for:. `hailtop.batch.build_python_image` now accepts a `show_docker_output` argument to toggle printing docker's output to the terminal while building container images",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11795#issuecomment-1109887634:75,message,message,75,https://hail.is,https://github.com/hail-is/hail/pull/11795#issuecomment-1109887634,1,['message'],['message']
Integrability,There's an issue here with whether or not data is wrapped in a tuple. You've changed the type on a few Java tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11683#issuecomment-1080639721:50,wrap,wrapped,50,https://hail.is,https://github.com/hail-is/hail/pull/11683#issuecomment-1080639721,1,['wrap'],['wrapped']
Integrability,"There's more work to be done here. This adds a new route to the batch UI for getting a certain job group within a batch. It then, instead of listing all jobs, only lists the jobs that belong directly to the currently viewed job group and also shows the child job groups of the current job group. When picking up this PR I would make sure to go through the Batch development tutorial to make sure you are familiar with dev deploying. Then, read [this](https://github.com/hail-is/hail/blob/main/dev-docs/development-process.md#alternatives-to-dev-deploy) to learn about all the ways you can avoid dev deploying  . If you are only making tweaks in the HTML templates, you don't need to keep deploying for every little change. Instead, run. ```bash; make devserver SERVICE=batch; ```. in your terminal and you'll get a local server that proxies the Batch that your `hail` installation is pointed to. You can then make changes to HTML and refresh your browser to see the results. Note that this is just rendering the HTML locally, and will have any effect on what's deployed, meaning you can't use it for python changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14600#issuecomment-2239854998:51,rout,route,51,https://hail.is,https://github.com/hail-is/hail/pull/14600#issuecomment-2239854998,1,['rout'],['route']
Integrability,"There's not a good reason, this is just how it was originally designed. Whenever a job was cancelled, it would take until the start of the next step for a container's execution to be stopped. I replaced the dependency on this in `Container` with `run_until_done_or_deleted`, but stopped short of deleting the functionality entirely because there were other parts of the worker, specifically `JVMJob` that still relied on it. Hopefully that is no longer the case after the QoB changes, but @danking would know better. We've also both lamented about how it's impossible to use timings currently inside cleanup blocks because it could accidentally re-raise a deleted error. This is a great change, I would just take extra care to test job cancellation to make sure there isn't anywhere that's still relying on this functionality.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11429#issuecomment-1054563382:207,depend,dependency,207,https://hail.is,https://github.com/hail-is/hail/pull/11429#issuecomment-1054563382,1,['depend'],['dependency']
Integrability,"There's some arguments that might let us do that: https://developer.github.com/v3/pulls/#merge-a-pull-request-merge-button I'm not sure exactly what the messages look like. We could at least grab the PR message and supply it as ""additional"" text.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8552#issuecomment-613698193:153,message,messages,153,https://hail.is,https://github.com/hail-is/hail/pull/8552#issuecomment-613698193,2,['message'],"['message', 'messages']"
Integrability,This PR depends on #1466.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1473#issuecomment-284198386:8,depend,depends,8,https://hail.is,https://github.com/hail-is/hail/pull/1473#issuecomment-284198386,1,['depend'],['depends']
Integrability,"This PR is in @catoverdrive path toward backend refactor. I would like to get this in before any further changes occur, 2nd conflict since this was opened & passing all tests. @catoverdrive Can you help me understand why AddHadoopConfiguration went away? Its body appears inlined in GetHadoopConfiguration. As a result the filesystem object (hConf) is no longer passed around, which is moving things in the opposite direction of future PRs related to this one, which will explicitly pass FS objects to all methods that perform file system operations. . edit: The AddHadoopConfiguration default implementation (on the trait didn't go away, but the overriding implementation did, resulting in an apparent noop, and maybe a potential bug, although I don't understand this portion of the codebase as well as I should yet. ```scala; trait FunctionWithHadoopConfiguration {; def addHadoopConfiguration(hConf: SerializableHadoopConfiguration): Unit; }. // No overriding addHadoopConfiguration implementation; def getHadoopConfiguration: Code[SerializableHadoopConfiguration] = {; if (_hconf == null) {; cn.interfaces.asInstanceOf[java.util.List[String]].add(typeInfo[FunctionWithFS].iname); val confField = newField[FS]; val mb = new EmitMethodBuilder(this, ""addHadoopConfiguration"", Array(typeInfo[SerializableHadoopConfiguration]), typeInfo[Unit]); methods.append(mb); mb.emit(confField := mb.getArg[SerializableHadoopConfiguration](1)); _hconf = HailContext.sHadoopConf. def resultWithIndex(print: Option[PrintWriter] = None): Int => F = {; if (localHConf != null); f.asInstanceOf[FunctionWithHadoopConfiguration].addHadoopConfiguration(localHConf); ```. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6263#issuecomment-500879573:1099,interface,interfaces,1099,https://hail.is,https://github.com/hail-is/hail/pull/6263#issuecomment-500879573,1,['interface'],['interfaces']
Integrability,"This also matches the corresponding Numpy error message almost precisely, which is nice. Difference is that ""Index"" is lowercase for Numpy. It's too bad that we can't keep the error message in python land, and the stack trace is utterly useless to a user. I'll make an issue for this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9223#issuecomment-670037279:48,message,message,48,https://hail.is,https://github.com/hail-is/hail/pull/9223#issuecomment-670037279,2,['message'],['message']
Integrability,This and #9304 have a mutual dependency (ugh). Close this in favor of the other.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9309#issuecomment-676736575:29,depend,dependency,29,https://hail.is,https://github.com/hail-is/hail/pull/9309#issuecomment-676736575,1,['depend'],['dependency']
Integrability,This appears to have triggered a new version of some Python dependency which is breaking Sphinx's generation of the docs. I'm going to try updating all our Sphinx versions to the latest version of Sphinx. Hoping for the best.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10282#issuecomment-818271521:60,depend,dependency,60,https://hail.is,https://github.com/hail-is/hail/pull/10282#issuecomment-818271521,1,['depend'],['dependency']
Integrability,"This change will make it so that it no longer goes to the container logs, and instead will be placed in the exception message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8146#issuecomment-591013808:118,message,message,118,https://hail.is,https://github.com/hail-is/hail/issues/8146#issuecomment-591013808,1,['message'],['message']
Integrability,This clashes with the dependency immediately below it. I've added it to my branch of major version bumps that I'll sort out.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546#issuecomment-1062276650:22,depend,dependency,22,https://hail.is,https://github.com/hail-is/hail/pull/11546#issuecomment-1062276650,1,['depend'],['dependency']
Integrability,This depends on #5479 now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5363#issuecomment-479897542:5,depend,depends,5,https://hail.is,https://github.com/hail-is/hail/pull/5363#issuecomment-479897542,1,['depend'],['depends']
Integrability,This doesn't actually affect the hail2 interface,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1610#issuecomment-349721354:39,interface,interface,39,https://hail.is,https://github.com/hail-is/hail/issues/1610#issuecomment-349721354,1,['interface'],['interface']
Integrability,"This is a quadratic task in releases, so it's not really feasible. We just need to be better about writing which interface is used in a post.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1384#issuecomment-349720138:113,interface,interface,113,https://hail.is,https://github.com/hail-is/hail/issues/1384#issuecomment-349720138,1,['interface'],['interface']
Integrability,"This is a useful discussion: https://stackoverflow.com/questions/26619478/are-dependent-pull-requests-in-github-possible. If I create multiple, stacked branches in hail-is/hail and PR them against each other, NOT master (except the first one), then they can be reviewed independently. I might start trying to do this. I still have a bunch more PRs on this RegionValue stuff. :-/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2302#issuecomment-337749177:78,depend,dependent-pull-requests-in-github-possible,78,https://hail.is,https://github.com/hail-is/hail/pull/2302#issuecomment-337749177,1,['depend'],['dependent-pull-requests-in-github-possible']
Integrability,"This is all passing except check_batch is running in an image on python 3.6 and the worker is on python 3.7. The process initializer isn't exposed until 3.7. I think I need to change the base image, but am worried about breaking Hail and other dependencies. Otherwise, I can just pass the key file path and the project and create the credentials and gcs client each time a function is called rather than once per process. This will make it slower for small files.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8368#issuecomment-607588305:244,depend,dependencies,244,https://hail.is,https://github.com/hail-is/hail/pull/8368#issuecomment-607588305,1,['depend'],['dependencies']
Integrability,"This is awesome. Two thoughts:. Objects corresponding to builtin types have py4j wrapper types, e.g.:. ```; >>> samples = vds.query_samples('samples.map(s => s.id).collect()'); >>> samples; [u'C1046::HG02024', ...]; >>> type(samples); <class 'py4j.java_collections.JavaList'>; ```. I think this is in the spirit of python (JavaList is list-like), but code (like ours!) that does things like `isinstance(x, list)` or `isinstance(x, str)` is going to fail. Second, this will work for python collections as described here:. https://www.py4j.org/advanced_topics.html. but won't work for user-defined objects like Variant, etc. that we'll want to introduce. For example, what d you want to return for `vds.query_variants('variants.filter(v => ...).collect()`. Shouldn't you get a python list of python Variant instances? I think to do this we have to do the translation no the python side. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1255#issuecomment-274365803:81,wrap,wrapper,81,https://hail.is,https://github.com/hail-is/hail/pull/1255#issuecomment-274365803,1,['wrap'],['wrapper']
Integrability,This is definitely no longer high-prio -- we'll throw the correct error message now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6223#issuecomment-500771034:72,message,message,72,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-500771034,1,['message'],['message']
Integrability,"This is due to FSs in `hailtop.fs` never getting closed. Unfortunately we exposed functions on a module, not a context manager. Options include; 1. adding a `hailtop.fs.close` method; 2. Using a new `RouterFS` on every `hailtop.fs` method; 3. Have users instantiate a `RouterFS` as a context manager and use that. Among these options I prefer 2 and 3. I think using a standalone function instead of properly allocating a context manager can be a convenience/performance tradeoff. 3 could use a bit of thought though as it adds more user-facing API.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14280#issuecomment-1936521020:200,Rout,RouterFS,200,https://hail.is,https://github.com/hail-is/hail/issues/14280#issuecomment-1936521020,2,['Rout'],['RouterFS']
Integrability,This is failing with this message:. ```; + python3 -m pylint --rcfile pylintrc hailtop; ************* Module hailtop.aiotools.weighted_semaphore; /usr/local/lib/python3.7/dist-packages/hailtop/aiotools/weighted_semaphore.py:2:0: E0401: Unable to import 'sortedcontainers' (import-error); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10752#issuecomment-897018931:26,message,message,26,https://hail.is,https://github.com/hail-is/hail/pull/10752#issuecomment-897018931,1,['message'],['message']
Integrability,"This is great, not having to enumerate the dependencies. Hmm, this is potentially making the build 2x slower. Your branch:. > 17 | build_hail | Complete | Success  (0) | 8 minutes | log. A master deploy a few moments ago:. > 16 | build_hail | Complete | 0 | 4 minutes | log. The cluster might have been under load when your tests run. Can you do a bit of benchmarking to see if this is real?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6248#issuecomment-498529082:43,depend,dependencies,43,https://hail.is,https://github.com/hail-is/hail/pull/6248#issuecomment-498529082,1,['depend'],['dependencies']
Integrability,"This is great, thanks @cseed! I've tried the gradlew option, and it worked well on Debian Jessie with java-8-oracle. `./gradlew installDist` worked, and the majority of the tests passed in `./gradlew check` (4 failed; I can give you the details if this is useful). On Ubuntu 16.04 with java-8-openjdk (the default) I get an error:. ```; :compileJava UP-TO-DATE; :compileScala; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:80: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:98: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:424: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; three errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; ```. Is this a dependency on java-8-oracle do you think?. My immediate problem is solved, as I have hail running now, so this is just out of curiosity really. l think it would be good for new users if you could nail down the dependencies a bit more precisely. For testing and development, it's also really useful to be able to spin up a quick Ubuntu VM, apt-get install a few packages and make a fresh install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240346120:1258,depend,dependency,1258,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240346120,2,['depend'],"['dependencies', 'dependency']"
Integrability,"This is now ready to be reviewed. @danking Could you please help me setup the tests to run on the CI?. @catoverdrive This is an example of the interface and the output generated. There's also a tests file in there. I'm happy to explain the design to you if you'd like. ```python3; from pipeline import Pipeline. p = Pipeline() # initialize a pipeline. # Define mapping for taking a file root to a set of output files; bfile = {'bed': '{root}.bed', 'bim': '{root}.bim', 'fam': '{root}.fam'}. # Import a file as a resource; file = p.read_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.read_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .docker('ubuntu'); .declare_resource_group(tmp1=bfile, ofile=bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:143,interface,interface,143,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282,1,['interface'],['interface']
Integrability,This is obviated by my imminent aggregator registry changes. The L suffix change should be a separate PR. I'll integrate the tests into my branch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1051#issuecomment-262054090:111,integrat,integrate,111,https://hail.is,https://github.com/hail-is/hail/pull/1051#issuecomment-262054090,1,['integrat'],['integrate']
Integrability,This is ready for a second look. I basically tried to do the minimum to change nomenclature for services that aren't auth. I had to put in some backwards compatibility in `batch/job.py` because there could be batches with the old format for userdata in the database with `gsa_key_secret_name` instead of `hail_credentials_secret_name`. I also kept the old field in `hail/python/hailtop/hailctl/auth/user.py`. I'm not sure exactly what depends on that.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10986#issuecomment-947944813:435,depend,depends,435,https://hail.is,https://github.com/hail-is/hail/pull/10986#issuecomment-947944813,1,['depend'],['depends']
Integrability,"This is somewhat intentional, because the only interface to create a Spark dataframe from a Hail table is `Table.to_spark()`, which calls `Table.expand_types()` first to convert Hail-specific data types to their generic forms. See here:; https://github.com/hail-is/hail/blob/b570bb100dca65c9e79f4297514eba1e178cd018/hail/python/hail/table.py#L2596-L2622",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10882#issuecomment-930101834:47,interface,interface,47,https://hail.is,https://github.com/hail-is/hail/issues/10882#issuecomment-930101834,1,['interface'],['interface']
Integrability,"This is super useful, thanks @jigold! A few high level comments:. - I'd love to have this checked in, but I don't think it should be part of the regular tests, esp. when they run against the production database and this is designed to find/stress the limits of the database.; - Also, this seems most useful for benchmarking different database configuration and settings, and we don't want to vary the production database (and in some cases, we can't, like decreasing the database size).; - Therefore, I think we just have a module you can run that takes a database connection settings and n_jobs, batch_size, batch_parallelism and number of replicates, and runs the benchmark, not integrated with the build system. And .sql files to create/clean up tables. When we want to run it, we can just clone the repo and run it directly. Then we can think about wrapping it in a larger test to spinning up database instances with various node and disk sizes and MySQL settings and see how they perform.; - You explore number of jobs and batch size, but I think you should also measure amount of batch insert parallelism. You can use bounded_gather I sent you. Then basically these two tests correspond to parallelism=1 and parallelism=infinity.; - I think you can get rid of the pymysql version. No reason the async version should perform differently, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881:681,integrat,integrated,681,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881,2,"['integrat', 'wrap']","['integrated', 'wrapping']"
Integrability,"This is the current version of gateway in default since yesterday, and since then there haven't been any logs out of router.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10247#issuecomment-810268452:117,rout,router,117,https://hail.is,https://github.com/hail-is/hail/pull/10247#issuecomment-810268452,1,['rout'],['router']
Integrability,"This is what `hailctl` looks like:. ```. Usage: hailctl [OPTIONS] COMMAND [ARGS]... Manage and monitor hail deployments.  Options ;  --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified ;  shell. ;  [default: None] ;  --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified ;  shell, to copy it or customize the ;  installation. ;  [default: None] ;  --help Show this message and exit. ; ;  Commands ;  batch Manage batches running on the batch service managed by the Hail team. ;  config Manage Hail configuration. ;  curl Issue authenticated curl requests to Hail infrastructure. ;  version Print version information and exit. ; ; ```. This is what `hailctl batch submit --help` looks like:. ```. Usage: hailctl batch submit [OPTIONS] SCRIPT [ARGUMENTS]... Submit a batch with a single job that runs SCRIPT with the arguments ARGUMENTS.  Arguments ;  * script PATH Path to the script [default: None] [required] ;  arguments [ARGUMENTS]... [default: None] ; ;  Options ;  --files PATH Files or directories to add to the working directory of the ;  job. ;  [default: None] ;  --name TEXT The name of the batch. ;  --image-name TEXT Name of Docker image for the job ;  [default: (hailgenetics/hail)] ;  --ou",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13109#issuecomment-1561081921:543,message,message,543,https://hail.is,https://github.com/hail-is/hail/pull/13109#issuecomment-1561081921,1,['message'],['message']
Integrability,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:171,rout,router-resolver,171,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251,2,"['mediat', 'rout']","['mediated', 'router-resolver']"
Integrability,"This isn't what I intended with my original design and I'm trying to figure out what you did here. - I intended for entire file, not individual specs, to have a version number. That's why it is in RelationalSpec at the top level.; - FooSpec (e.g. RelationalSpec, RVDSpec, etc.) is supposed to be an abstract class that implements the interface used by the main code for the entity in question. E.g. CodecSpec knows how to build encoders and decoders. It can have many implementations.; - The file version essentially determines which Spec implementations are allowed in the file. Each spec should have a notion of when it was introduced (or deprecated, if we bump the major version) but each implementation should have its own name/version. I was imagining FooRVD2Spec if we needed a new version of the existing RVD specs.; - The type hints in the JSON determine what Spec implementation to use (e.g. UnpartitionedRVDSpec). With this design, you can't change name of an existing spec.; - The Spec is also the intended place for code which matches the interface (which might evolve) with the legacy data in the JSON file (which cannot change).; - The file format is only used to check compatibility. The Spec class hierarchy should drive the decoding of the Specs, not the file format.; - Specs that are not being generated anymore (but still need to be read) should go into a compatibility package. I think following this design will be cleaner. This is exactly the kind of forward-extensibility I designed it to handle cleanly, although I might very well have messed something up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4734#issuecomment-436661120:334,interface,interface,334,https://hail.is,https://github.com/hail-is/hail/pull/4734#issuecomment-436661120,2,['interface'],['interface']
Integrability,This iteration also uses an extremely slow py4j protocol; ```; return [MatrixTable._from_java(jmir) for jmir in jmirs]. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5465#issuecomment-469789909:48,protocol,protocol,48,https://hail.is,https://github.com/hail-is/hail/pull/5465#issuecomment-469789909,1,['protocol'],['protocol']
Integrability,This kinda depends on #2480,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2479#issuecomment-347063979:11,depend,depends,11,https://hail.is,https://github.com/hail-is/hail/pull/2479#issuecomment-347063979,1,['depend'],['depends']
Integrability,"This looks *AWESOME*! Organizationally, I think the tags should be english text for common search terms, and the existing tags you have should go into a `see also` or `dependencies` or something section (For clickable links to the method that this implementation uses)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4089#issuecomment-415077569:168,depend,dependencies,168,https://hail.is,https://github.com/hail-is/hail/pull/4089#issuecomment-415077569,1,['depend'],['dependencies']
Integrability,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731:203,integrat,integrating,203,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731,1,['integrat'],['integrating']
Integrability,"This looks like it would work. Do you think think that after the region value transition, the `Variant`, `AltAllele`, etc., classes will be replaced with their associated views? Or if there were still a need for the Scala objects, maybe they could be wrappers around a small region and a view, so that the view classes become the single location for associated methods.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2434#issuecomment-344609522:251,wrap,wrappers,251,https://hail.is,https://github.com/hail-is/hail/pull/2434#issuecomment-344609522,1,['wrap'],['wrappers']
Integrability,"This means that hail (or something on which hail depends) is trying to call into BLAS. BLAS is a Fortran library and is often shipped with C bindings. The symbol `cblas_dgemv` is a C function. Your machine is likely missing `libcblas`. Can you post the output of these commands:; - `nm -g /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so` (this may say that the file doesn't exist, in which case skip the next command; - `objdump -TC /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so`. Can you also answer these questions:; - What distribution are you using?; - What version of that distribution do you have?; - What package management tool do you use?. If you're on Ubuntu, can you tell me what version of `libatlas-dev` you have installed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-238879581:49,depend,depends,49,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-238879581,1,['depend'],['depends']
Integrability,This needs to be recreated once #2519 (on which this depends) is adapted to handle GenomeReference.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2516#issuecomment-350801948:53,depend,depends,53,https://hail.is,https://github.com/hail-is/hail/pull/2516#issuecomment-350801948,1,['depend'],['depends']
Integrability,"This now gives a nice error message:. ```; >>> hl.uniroot(lambda x: x * x + 1, -1, 1).value; Error summary: HailException: sign of endpoints must have opposite signs, got: f(min) = 2.0, f(max) = 2.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1896#issuecomment-408636691:28,message,message,28,https://hail.is,https://github.com/hail-is/hail/issues/1896#issuecomment-408636691,1,['message'],['message']
Integrability,"This proposal mounts to programming with explicit continuations. It doesn't increase the expressiveness of the loop construct that I can see. Our users are reluctant enough to learn functional programming, I think continuations is one step too far for the user interface. Internally, I don't care as much, although personally I would prefer to code up my solution. @catoverdrive's doing the work, so I'll let them decide. > As a side note, @iitalics stream emitter. Ah, I thought @catoverdrive was referring to IR level streams.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614#issuecomment-559099522:261,interface,interface,261,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559099522,1,['interface'],['interface']
Integrability,This quote from the 2.12.0 collections docs is as depressing and still seemingly misguided:. > (Since version 2.12.0) mutable.Stack is an inelegant and potentially poorly-performing wrapper around List. Use a List assigned to a var instead.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2185#issuecomment-326970456:182,wrap,wrapper,182,https://hail.is,https://github.com/hail-is/hail/pull/2185#issuecomment-326970456,1,['wrap'],['wrapper']
Integrability,"This resolved itself. https://batch.hail.is/batches/8049645/jobs/26. I don't entirely understand the perl build process, but it seems that some dependency somehow broke itself. Matt's comment about updating to a later version stands. We'll track the need for a newer version of VEP at https://github.com/hail-is/hail/issues/13167",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12946#issuecomment-1738209895:144,depend,dependency,144,https://hail.is,https://github.com/hail-is/hail/issues/12946#issuecomment-1738209895,1,['depend'],['dependency']
Integrability,This seems fine to me. I think we should check the worker and driver logs and make sure there's no unexpected messages though before merging.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13876#issuecomment-1772950616:110,message,messages,110,https://hail.is,https://github.com/hail-is/hail/pull/13876#issuecomment-1772950616,1,['message'],['messages']
Integrability,"This seems right to me. In this vein of work, there's one more thing I want: we should build this Dockerfile with the hail wheel and then execute `pylint hail && pylint hailtop`. Pylint will look for uninstalled modules. This will save us from checking in (and eventually deploying) a hail package with bad dependencies. We should probably also run the python tests against this version of hail. This is a tru, local-mode user environment. ```; FROM ubuntu:18.04. ENV LANG C.UTF-8. RUN apt-get update && \; apt-get -y install \; openjdk-8-jdk-headless \; python3 python3-pip && \; rm -rf /var/lib/apt/lists/*. COPY hail.whl pylintrc ./; RUN pip install --no-cache-dir ./hail.whl; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7031#issuecomment-530112882:307,depend,dependencies,307,https://hail.is,https://github.com/hail-is/hail/pull/7031#issuecomment-530112882,1,['depend'],['dependencies']
Integrability,"This should always produce a valid VCF with respect to: https://samtools.github.io/hts-specs/VCFv4.2.pdf. I've changed the behavior of export_vcf so that, like FORMAT field types, unsupported INFO field types must be explicitly converted to String by the user if the user really wants to export them. With good error messages, I think this will cause less confusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2414#issuecomment-343611350:317,message,messages,317,https://hail.is,https://github.com/hail-is/hail/pull/2414#issuecomment-343611350,1,['message'],['messages']
Integrability,This should be fixed once https://github.com/hail-is/hail/pull/5655 goes in and the pr-builder has the standard python dependencies installed for the default python3 image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5624#issuecomment-475123834:119,depend,dependencies,119,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-475123834,1,['depend'],['dependencies']
Integrability,"This should start passing when ""add batch docker dependency .."" https://github.com/hail-is/hail/pull/5655/files merges.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5641#issuecomment-475309489:49,depend,dependency,49,https://hail.is,https://github.com/hail-is/hail/pull/5641#issuecomment-475309489,1,['depend'],['dependency']
Integrability,This should use distinct server blocks rather than routing based on the URL.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-539530177:51,rout,routing,51,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-539530177,1,['rout'],['routing']
Integrability,"This still causes error messages. `hail -l /mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed.Chr${num}.QC.vds.test.log importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr${num}/TopMed_8k.853.vcf.bgz splitmulti annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' count`. `2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@716: Client environment:host.name=nid00014; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@723: Client environment:os.name=Linux; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-431.el6_1.0000.9051-cray_ari_athena_c_cos; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@725: Client environment:os.version=#1 SMP Thu Jan 28 18:37:39 UTC 2016; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@733: Client environment:user.name=schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@741: Client environment:user.home=/home/users/schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@753: Client environment:user.dir=/mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=192.168.0.1:2181,192.168.0.9:2181,192.168.0.17:2181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:24,message,messages,24,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['message'],['messages']
Integrability,This ticket is complete when:. - [x] We merge a PR that modifies the driver memory setting for hailctl dataproc start as described in the previous message. https://github.com/hail-is/hail/pull/14066; - [x] We have received confirmation from the AoU RWB team that the Spark driver heap size is set in a manner that leaves abundant RAM for Python and daemon memory fluctuations.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1836845812:147,message,message,147,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836845812,1,['message'],['message']
Integrability,"This will allow you to go conda-free: https://github.com/hail-is/hail/pull/5655. In particular, I modified batch to be conda-free as part of this PR. I added the common python dependencies (async stuff, including mysql stuff) to the default python3 installation in the image. I think my PR will obviate this one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5616#issuecomment-475252257:176,depend,dependencies,176,https://hail.is,https://github.com/hail-is/hail/pull/5616#issuecomment-475252257,1,['depend'],['dependencies']
Integrability,"This will be fixed in the next release.; If you can't wait until then and you're comfortable patching this yourself, replace the contents of the affected file with the following:; ```yaml; dataproc:; init_notebook.py: gs://hail-common/hailctl/dataproc/0.2.129/init_notebook.py; vep-GRCh37.sh: gs://hail-common/hailctl/dataproc/0.2.129/vep-GRCh37.sh; vep-GRCh38.sh: gs://hail-common/hailctl/dataproc/0.2.129/vep-GRCh38.sh; wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl; pip_dependencies: aiodns==2.0.0|aiohttp==3.9.3|aiosignal==1.3.1|async-timeout==4.0.3|attrs==23.2.0|avro==1.11.3|azure-common==1.1.28|azure-core==1.30.1|azure-identity==1.15.0|azure-mgmt-core==1.4.0|azure-mgmt-storage==20.1.0|azure-storage-blob==12.19.0|bokeh==3.3.4|boto3==1.34.55|botocore==1.34.55|cachetools==5.3.3|certifi==2024.2.2|cffi==1.16.0|charset-normalizer==3.3.2|click==8.1.7|commonmark==0.9.1|contourpy==1.2.0|cryptography==42.0.5|decorator==4.4.2|deprecated==1.2.14|dill==0.3.8|frozenlist==1.4.1|google-auth==2.28.1|google-auth-oauthlib==0.8.0|humanize==1.1.0|idna==3.6|isodate==0.6.1|janus==1.0.0|jinja2==3.1.3|jmespath==1.0.1|jproperties==2.1.1|markupsafe==2.1.5|msal==1.27.0|msal-extensions==1.1.0|msrest==0.7.1|multidict==6.0.5|nest-asyncio==1.6.0|numpy==1.26.4|oauthlib==3.2.2|orjson==3.9.10|packaging==23.2|pandas==2.2.1|parsimonious==0.10.0|pillow==10.2.0|plotly==5.19.0|portalocker==2.8.2|py4j==0.10.9.5|pyasn1==0.5.1|pyasn1-modules==0.3.0|pycares==4.4.0|pycparser==2.21|pygments==2.17.2|pyjwt[crypto]==2.8.0|python-dateutil==2.9.0.post0|python-json-logger==2.0.7|pytz==2024.1|pyyaml==6.0.1|regex==2023.12.25|requests==2.31.0|requests-oauthlib==1.3.1|rich==12.6.0|rsa==4.9|s3transfer==0.10.0|scipy==1.11.4|six==1.16.0|sortedcontainers==2.4.0|tabulate==0.9.0|tenacity==8.2.3|tornado==6.4|typer==0.9.0|typing-extensions==4.10.0|tzdata==2024.1|urllib3==1.26.18|uvloop==0.19.0;sys_platform!=""win32""|wrapt==1.16.0|xyzservices==2023.10.1|yarl==1.9.4|; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14452#issuecomment-2045876651:1918,wrap,wrapt,1918,https://hail.is,https://github.com/hail-is/hail/issues/14452#issuecomment-2045876651,1,['wrap'],['wrapt']
Integrability,"This will enforce even stricter requirements on downstream projects, right? Do we want to do this only for our own builds and use unpinned dependencies in the pip package's setup.py?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11842#issuecomment-1128128100:139,depend,dependencies,139,https://hail.is,https://github.com/hail-is/hail/pull/11842#issuecomment-1128128100,1,['depend'],['dependencies']
Integrability,This will need to be revisited in the context of the DataFrame-style Matrix interface.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/134#issuecomment-226613696:76,interface,interface,76,https://hail.is,https://github.com/hail-is/hail/pull/134#issuecomment-226613696,1,['interface'],['interface']
Integrability,"Tim would you prefer RVD.unify to be separate from RVD.union, such that the caller controls upcast? Interface seems much simpler if RVD.union calls RVD.unify, but may result in unify being called too many times (if the caller unifies rvds, and doesn't realize that they also transitively call RVD.unify because some function the caller directly calls calls RVD.union)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8132#issuecomment-589323266:100,Interface,Interface,100,https://hail.is,https://github.com/hail-is/hail/pull/8132#issuecomment-589323266,1,['Interface'],['Interface']
Integrability,"Tim, I left the integration tests for now. I propose if you want them out, that we leave 2 cases for each of the type-combinations, so that we can inductively prove that our code can infer the correct unified type across nested IR (without the Ref shortcut)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6990#issuecomment-530868509:16,integrat,integration,16,https://hail.is,https://github.com/hail-is/hail/pull/6990#issuecomment-530868509,1,['integrat'],['integration']
Integrability,"Tim, I've added one of the suggested interfaces. To use `protected var _pType2` instead I believe we need to have InferPType extend `IR` inside of IR.scala, e.g `object InferPType extend IR`, by requirement of sealed traits. Anything else you want? Happy to add additional tests, or to move on to the next pType sub-project.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6594#issuecomment-515061792:37,interface,interfaces,37,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515061792,1,['interface'],['interfaces']
Integrability,"Tim, put a WIP on this to block merge. The fix is to add the correct ptype for the recur branch in Emit.emit. However, this has us wrapping PValue(pt, Code._empty), which seems odd (a type and no corresponding value). @catoverdrive could you comment? Should Recur only take type PVoid and infer type PVoid, is the current system correct, or would you prefer an altogether different solution to: https://github.com/hail-is/hail/pull/8250/commits/8df5a452e2e123ffabf42e277164ce3e6c367517",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8250#issuecomment-595827209:131,wrap,wrapping,131,https://hail.is,https://github.com/hail-is/hail/pull/8250#issuecomment-595827209,1,['wrap'],['wrapping']
Integrability,"Tim, would you have any recommendations?. ```sh; Gradle suite > Gradle test > is.hail.expr.ir.IRSuite.regressionTestUnifyBug FAILED; scala.MatchError: +Void (of class is.hail.expr.types.physical.PVoid$); at is.hail.expr.types.physical.PType.setRequired(PType.scala:206); at is.hail.expr.ir.InferPType$.apply(InferPType.scala:209); at is.hail.expr.ir.IR$class.inferSetPType(IR.scala:33); at is.hail.expr.ir.ArrayMap.inferSetPType(IR.scala:232); at is.hail.expr.ir.InferPType$$anonfun$apply$8.apply(InferPType.scala:359); at is.hail.expr.ir.InferPType$$anonfun$apply$8.apply(InferPType.scala:358); ```. This occurs because the IR is the following:. ```sh; ; //a; Literal(array<interval<locus<GRCh37>>>,WrappedArray([20:10277621-20:11898992))); // name; __iruid_11; //body; ApplySpecial(Interval,ArrayBuffer(MakeStruct(ArrayBuffer((locus,ApplySpecial(start,ArrayBuffer(Ref(__iruid_11,interval<locus<GRCh37>>)))))), MakeStruct(ArrayBuffer((locus,ApplySpecial(end,ArrayBuffer(Ref(__iruid_11,interval<locus<GRCh37>>)))))), True(), False())); ```. So I set this IR in the match to return PVoid(). However, in ArrayMap there is a requiredeness setter (which is inspired/copied from InferType's corresponding match:. ```scala; coerce[PStreamable](a.pType2).copyStreamable(body.pType2.setRequired(false)); ```. Which causes a nonsensical operation on PVoid, which is required: true (override final val). As an aside, ApplySpecial seems to allow operations on non-identical types, which would require algebraic types, rather than a singular: any time the types of its`Seq[IR]`'s are different. I have similar issues with other array operations, which want nodes that we currently don't support, including aggregator nodes. Note that I am only calling inferSetPType in Compile (immediately after optimization pass).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6594#issuecomment-512957123:700,Wrap,WrappedArray,700,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-512957123,1,['Wrap'],['WrappedArray']
Integrability,"To summarize from our discussion today:. 1. We want to make the FASTAReaderConfig and Liftover map transient; 2. We'll keep enough information around (like map from name to chain file path, sequence path) such that with a filesystem, the FASTA reader and LiftOvers can be regenerated.; 3. Create a method that synchronizes FASTAReaderConfig and LiftOver state, working name `heal`, that takes a filesystem and sets everything up.; 4. We'll call this method in the resultWithIndex closure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10523#issuecomment-871799587:310,synchroniz,synchronizes,310,https://hail.is,https://github.com/hail-is/hail/pull/10523#issuecomment-871799587,1,['synchroniz'],['synchronizes']
Integrability,"True. I thought at least for the copy-paste tokens that this would be intentional. Looks like you can get access tokens from GCP that last up to 12 hours, but that could be insufficient for large workloads. If we need something arbitrarily long-lived, our current implementation might be our best bet short of some better integration with OIDC.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13934#issuecomment-1785516525:322,integrat,integration,322,https://hail.is,https://github.com/hail-is/hail/pull/13934#issuecomment-1785516525,1,['integrat'],['integration']
Integrability,"Turns out I needed to pull in the transitive breeze dependencies; without these, the Python tests failed while the Scala tests passed. I've re-run the tests on Spark 3 now; let me know if there are any other changes you'd like to see. Unfortunately, there's a small bug I saw on the Spark 3 MLLib side; I'll make sure this gets addressed ASAP: https://issues.apache.org/jira/browse/SPARK-33043",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9524#issuecomment-701717243:52,depend,dependencies,52,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701717243,1,['depend'],['dependencies']
Integrability,"UI won't let me respond about the tests comment. I removed the api1.tests.py file, but left the api1 code because it's still depended on and we should extract that separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2816#issuecomment-360953858:125,depend,depended,125,https://hail.is,https://github.com/hail-is/hail/pull/2816#issuecomment-360953858,1,['depend'],['depended']
Integrability,"Uh-oh, so this built on a previous PR that @tpoterba was reviewing: https://github.com/hail-is/hail/pull/2302. I was afraid this was going to happen at some point. @tpoterba I'd still like your input on the OrderedRDD2 stuff (and any comments you had on the region value stuff) and I will address it with a new round of changes. I'm going to close that pull request but you can still comment there. In the future we should use stacked branches for inter-dependent PRs. See my comment on that PR for a link to a description. I'm going to try to PR the remaining OrderedRDD2 stuff that way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2308#issuecomment-337927797:454,depend,dependent,454,https://hail.is,https://github.com/hail-is/hail/pull/2308#issuecomment-337927797,1,['depend'],['dependent']
Integrability,Unkey the cols seems wrong. It makes the contract of `entries()` much more complicated. I guess a warning or error that makes the user decide between manually unkeying the columns or shuffling would be best? (I guess that means implementing a shuffling option for small data.),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4646#issuecomment-433473971:41,contract,contract,41,https://hail.is,https://github.com/hail-is/hail/issues/4646#issuecomment-433473971,1,['contract'],['contract']
Integrability,"Update the following docs:; annotatevariants_expr.md; HailExpressionLanguage.md; splitmulti.md, these lines:. ```; 108 filtervariants expr -c 'va.info.AC[va.aIndex] < 10' --remove; 118 annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; ```. Update error message in AST. ```; 1905 Hint: For accessing `A'-numbered info fields in split variants, `va.info.field[va.aIndex]' is correct"""""".stripMargin); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/663#issuecomment-242074915:266,message,message,266,https://hail.is,https://github.com/hail-is/hail/pull/663#issuecomment-242074915,1,['message'],['message']
Integrability,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:1223,message,messages,1223,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984,1,['message'],['messages']
Integrability,Very good point. I moved the check to be on the `dependsOn` array of names rather than on the deps array of steps,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8829#issuecomment-631652614:49,depend,dependsOn,49,https://hail.is,https://github.com/hail-is/hail/pull/8829#issuecomment-631652614,1,['depend'],['dependsOn']
Integrability,WTF - I'm getting this error in a PR - https://ci.hail.is/jobs/30344/log. also see this message at the bottom: . 2019-05-30 19:47:48 Hail: WARN: struct{idx: int32} has no field row_idx at <root>.<array>.end for value JInt(10),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6223#issuecomment-497480960:88,message,message,88,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-497480960,1,['message'],['message']
Integrability,"We *should* be handling this error already - there's code that matches this exact message here: https://github.com/hail-is/hail/blob/8362afaefa8829e720f5affc9b482c2568e8299d/hail/src/main/scala/is/hail/services/NettyProxy.scala#L21-L31. Following the logic through, it looks like we retry this kind of exception indefinitely UNLESS the netty `Epoll` is not available...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12980#issuecomment-1545975559:82,message,message,82,https://hail.is,https://github.com/hail-is/hail/issues/12980#issuecomment-1545975559,1,['message'],['message']
Integrability,"We added the list of test dependencies to the ""Running the tests"" subsection of [Getting Started](https://hail.is/docs/devel/index.html#GettingStarted) so I'm going to mark this as closed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-244471032:26,depend,dependencies,26,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-244471032,1,['depend'],['dependencies']
Integrability,"We are installing Hail's dependencies by injecting the requirements.txt dependencies into the deploy.yaml file. We don't have a way to grab the dependencies from the other wheel at the moment. I'd prefer to defer this change to modify -- The PR is big enough as-is, and fixes the modify semantics to be what they are in latest cloudtools. I think we should explore if there's a way to get pip to print the package dependencies from the wheel, and then grep out pyspark and install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6297#issuecomment-500612048:25,depend,dependencies,25,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500612048,5,"['depend', 'inject']","['dependencies', 'injecting']"
Integrability,We can decide the proper policy/procedure to update dependencies later. I think that we should find a way to be made aware of security issues in our dependencies so we can update them should an extraordinary circumstance arise.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4701#issuecomment-435204362:52,depend,dependencies,52,https://hail.is,https://github.com/hail-is/hail/pull/4701#issuecomment-435204362,2,['depend'],['dependencies']
Integrability,"We could use the library, but I thought (1) we were trying to be dependent on external libraries and (2) it's easier for me to see what's being run with the shell commands as it's easier to search for examples.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12542#issuecomment-1371296492:65,depend,dependent,65,https://hail.is,https://github.com/hail-is/hail/pull/12542#issuecomment-1371296492,1,['depend'],['dependent']
Integrability,"We currently depend on bokeh version 1.2, so I'm not surprised that bokeh 2.0 doesn't work with Hail's plotting lib. Going to close this as a non-issue since it's a versioning mismatch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8318#issuecomment-600212727:13,depend,depend,13,https://hail.is,https://github.com/hail-is/hail/issues/8318#issuecomment-600212727,1,['depend'],['depend']
Integrability,"We have a difference of opinion about the risks involved in using whatever; compiler happens to show up as $(CXX); to try to compile arbitrarily large auto-generated C++ files, and maybe; about what happens when that fails; and gives an error message about something in the middle of 12000 lines of; code that bears no obvious relationship; to what the user is doing. Or when that compiler takes 15 minutes to; compile it. It's the C++ equivalent of; the JVM ""no, that's just too much bytecode"". Or worst of all, it compiles; it but the code gives the wrong answers; because that particular compiler has a bug, and we never tested the; combination of our codegen with *that*; compiler/version. A couple of years ago I was seeing g++ take 40-60 seconds to compile; something that clang did in 2 seconds; (fairly heavily templated code generated for an SQL query, so very much in; the same ballpark as parts of Hail),; which contributes to my concern about this, especially on linux where g++; is the default. So in the long run I expect we'll ship a compiler, or specify a compiler.; But that becomes a problem in itself; if we want the shipped compiler to work on a variety of OS'es. When I did; that before it was all Ubuntu-14.04; and Ubuntu-16.04, and it was manageable to build it for two platforms. On Thu, Aug 2, 2018 at 9:59 PM cseed <notifications@github.com> wrote:. > *@cseed* commented on this pull request.; > ------------------------------; >; > In src/main/c/NativeModule.cpp; > <https://github.com/hail-is/hail/pull/3973#discussion_r207422997>:; >; > > +}; > +; > +std::string strip_suffix(const std::string& s, const char* suffix) {; > + size_t len = s.length();; > + size_t n = strlen(suffix);; > + if ((n > len) || (strncmp(&s[len-n], suffix, n) != 0)) return s;; > + return std::string(s, 0, len-n);; > +}; > +; > +std::string get_cxx_name() {; > + char* p = ::getenv(""CXX"");; > + if (p) return std::string(p);; > + // We prefer clang because it has faster compile; > + auto s = run",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410127709:243,message,message,243,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410127709,1,['message'],['message']
Integrability,We have almost everything here. Will make two separate issues for contract types and partitioning,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1158#issuecomment-301549814:66,contract,contract,66,https://hail.is,https://github.com/hail-is/hail/issues/1158#issuecomment-301549814,1,['contract'],['contract']
Integrability,"We may want to use NumPy ndarray as local matrix now, to avoid interface duplication and grab all its functionality, even if there's a performance hit in moving between Python and Java (worst case, we go through disk). I'm going to close this while we strategize, will PR the BlockMatrix updates separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3064#issuecomment-370149825:63,interface,interface,63,https://hail.is,https://github.com/hail-is/hail/pull/3064#issuecomment-370149825,1,['interface'],['interface']
Integrability,"We now have latex integration for formulas, used pervasively in docs along with references to literature. Not sure if this issue calls for more?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/28#issuecomment-279514823:18,integrat,integration,18,https://hail.is,https://github.com/hail-is/hail/issues/28#issuecomment-279514823,1,['integrat'],['integration']
Integrability,"We probably want a function like:. ``` scala; def projectT[T <: AST](maybeT: AST)(implicit m: ASTDefaultExpectedMessage[T]): T =; projectT[T](maybeT, m.message). def projectT[T <: AST](maybeT: AST, message: => String): T =; maybeT match {; case t : T => t; case _ => maybeT.parseError(message); }; ```. that we can use like:. ``` scala; val Lambda(position, parameter, body) = projectT[Lambda](rhs); ```. But we have the additional issue of wanting to assert function arity, so we want some way to say:. ``` scala; val Array(Lambda(position, parameter, body)) = projectArray(projectT[Lambda]); ```. And more generally. ``` scala; val Array(A(...), B(...)) = projectArray(projectT[A], projectT[B]); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/786#issuecomment-247090248:152,message,message,152,https://hail.is,https://github.com/hail-is/hail/issues/786#issuecomment-247090248,3,['message'],['message']
Integrability,"We recently encountered jobs that failed due to syntax errors in the shell script generated by Hail, stemming from code such as. ```python; job.command('touch before'); job.command('\n'.join(f'echo {shlex.quote(msg)}' for msg in messages)); job.command('touch after'); ```. Occasionally `messages` is an empty list, so this evaluates to `job.command('')` and the eventual shell script submitted by Hail contains. ```sh/bin/bash' '-c' '; ; {; touch before; }; {. }; {; touch after; }; ; ```. Shell compound commands like `{  }` must contain at least one command, so this is a syntax error. Empty commands could be rewritten to generate e.g. [`:`](https://pubs.opengroup.org/onlinepubs/9799919799/utilities/V3_chap02.html#tag_19_17) such as. ```sh; ; {; :; }; ; ```. but it seems easier and probably less surprising to just omit them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14700#issuecomment-2372547180:229,message,messages,229,https://hail.is,https://github.com/hail-is/hail/pull/14700#issuecomment-2372547180,2,['message'],['messages']
Integrability,"We should add docs that describe how to do this to:; 1. `hl.default_reference`, obviously; 2. Deprecate the `reference_genome` parameter to `hl.init` and instruct users to use `hl.default_reference`. Inform that this parameter has confusing interactions with ReferenceGenome, so we're removing it.; 3. `hl.ReferenceGenome.__init__` should refer users to that. . I think we should also make a separate PR that improves the `hl.import_vcf` error message. If the backend throws an error like; ```; HailException: Invalid locus '1:249367215' found. Position '249367215' is not within the range [1-249250621] for reference genome 'GRCh37'.; ```; `import_vcf` should catch and wrap with another exception that suggests you use a `reference_genome` parameter or `hl.default_reference`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13856#issuecomment-1771234741:444,message,message,444,https://hail.is,https://github.com/hail-is/hail/issues/13856#issuecomment-1771234741,2,"['message', 'wrap']","['message', 'wrap']"
Integrability,"We'd need a synchronous version of the file system. I thought this was something Cotton wanted to do himself. With regards to the interface changes, I'm not sure if Cotton had something else in mind.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10331#issuecomment-842574828:130,interface,interface,130,https://hail.is,https://github.com/hail-is/hail/pull/10331#issuecomment-842574828,1,['interface'],['interface']
Integrability,We'll pick this up from other dependency changes I'm making,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12928#issuecomment-1520826560:30,depend,dependency,30,https://hail.is,https://github.com/hail-is/hail/pull/12928#issuecomment-1520826560,1,['depend'],['dependency']
Integrability,"We'll use these functions for eigen-decomposition of kernel in linear mixed model, and elsewhere. The Breeze method symEig calls to LAPACK dsyev, which performs poorly relative to dsyevd and dsyevr. See:; http://www.netlib.org/lapack/lawnspdf/lawn183.pdf. http://scicomp.stackexchange.com/questions/11827/flop-counts-for-lapack-symmetric-eigenvalue-routines-dsyev-dsyevd-dsyevx-and-d. I added interfaces to [dsyevd](http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html) and [dsyevr](http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga2ad9f4a91cddbf67fe41b621bd158f5c.html) and used a version of symEigSpeedTest in symEigDSuite to test performance on my mid 2015 MacBook Pro (2.8 GHz Intel Core i7). Here's a plot from dimension 500 to 5500:; [symEig6k.pdf](https://github.com/hail-is/hail/files/512569/symEig6k.pdf). Extrapolating to 10k:; [symEig10k.pdf](https://github.com/hail-is/hail/files/512571/symEig10k.pdf). And to 25k for kicks (though by then we're at 5G of RAM...):; [symEig25k.pdf](https://github.com/hail-is/hail/files/512580/symEig25k.pdf). So at least on a [Wishart matrix](https://en.wikipedia.org/wiki/Wishart_distribution), symEigD (dsyevd) is best, with symEigR (dsyevr) close behind, then svd (dgesdd) about 2.5x worse than symEigD, and then symEig (dsyev) about 10x worse than symEigD. ```; dim svd symEigD symEig symEigR; 500 .092 .051 .234 .038; 500 .088 .050 .217 .046; 500 .093 .047 .229 .041; 1000 .458 .193 1.659 .191; 1000 .430 .184 1.469 .195; 1000 .441 .207 1.464 .183; 1500 1.399 .7245 4.810 .595; 1500 1.407 .5990 4.777 .601; 1500 1.421 .5835 5.236 .627; 2000 3.272 1.479 10.942 1.386; 2000 3.205 1.337 11.006 1.381; 2000 3.473 1.354 10.933 1.366; 2500 6.180 2.519 21.639 2.750; 2500 6.217 2.718 21.772 2.758; 2500 6.580 2.590 21.176 2.661; 3000 10.169 4.117 51.154 4.716; 3000 10.414 4.131 51.602 4.834; 3000 10.709 4.219 46.711 4.794; 3500 15.451 6.549 72.2 7.365; 3500 15.353 7.058 7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/906#issuecomment-251835211:349,rout,routines-dsyev-dsyevd-dsyevx-and-d,349,https://hail.is,https://github.com/hail-is/hail/pull/906#issuecomment-251835211,2,"['interface', 'rout']","['interfaces', 'routines-dsyev-dsyevd-dsyevx-and-d']"
Integrability,"We're moving in the other direction, actually -- we're removing the nice mirrored interfaces in python/scala and replacing them with super fast `sun.misc.unsafe`-based infrastructure and native routines. It's not a good time investment to make a scala interface as nice as the Python interface right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2378#issuecomment-349722748:82,interface,interfaces,82,https://hail.is,https://github.com/hail-is/hail/issues/2378#issuecomment-349722748,4,"['interface', 'rout']","['interface', 'interfaces', 'routines']"
Integrability,"We've recently updated from 0.2.126ish to 0.2.130ish, and encountered some teething issues with the new (to us) metadata server. Jobs using `gsutil` failed as their attempts to get credentials from the server resulted in 404. There seems to have been two problems:. 1. As shown (also via `curl`) in [batch 454410](https://batch.hail.populationgenomics.org.au/batches/454410/jobs/1), our `gsutil` queried for `http://169.254.169.254/computeMetadata/v1/instance/service-accounts` (without a final `/`) which resulted in a 404. I don't know if there's a more elegant way for the server to accept both, rather than just adding a route with and without. 2. With that fixed, [batch 454418](https://batch.hail.populationgenomics.org.au/batches/454418/jobs/1) shows a failure within `GetInstanceScopes()`. This is failing because the metadata server does not implement the `/scopes` endpoint. PR #14019 implemented only so much as is needed for `hail` and `gcloud` to get access tokens for hail GSAs so they can then make API calls to GCS or Hail Batch, but we seem to have needed a bit more. Not sure why you didn't encounter this yourselves: possibly sufficiently different versions of `gsutil` or the cloud SDK, or perhaps you are better at remembering to use `gcloud` rather than `gsutil` than we are!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14566#issuecomment-2132539331:625,rout,route,625,https://hail.is,https://github.com/hail-is/hail/pull/14566#issuecomment-2132539331,1,['rout'],['route']
Integrability,"Welcome @ryerobinson, and thanks for the Pull Request! Could you report exactly the error that you saw when building hail? `sys_platform!='win32'` is necessary to install the dependencies on Windows, so there is probably an alternative solution to the error that works for all platforms.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1230330783:175,depend,dependencies,175,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1230330783,1,['depend'],['dependencies']
Integrability,Welp. OK. Looks like we're stuck on 6.8.21 forever. I have no idea why the CI is NPE'ing. My local system gives all manner of other inexplicable error messages (mostly about class loading). Things are fine when done through `./gradlew test` though. It's just the test jar that seems broken.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8801#issuecomment-629593668:151,message,messages,151,https://hail.is,https://github.com/hail-is/hail/pull/8801#issuecomment-629593668,1,['message'],['messages']
Integrability,"What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace. The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which *should* do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing. The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548102338:681,rout,router,681,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102338,1,['rout'],['router']
Integrability,"What do you mean by validation? . For the 'annotation line' are you suggesting a general error-catching wrapper? I actually really like that, and I'll give it a go. > CNV work; > What I want to do with CNVs is something like ; > ; > ```; > val files: Array[String]; > sc.paralellize(files); > .map { f => readTable(f, config...) }; > .,map (convert to a hail better cnv representation); > ```; > ; > Can't do that if readTable gives you an RDD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233007740:104,wrap,wrapper,104,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233007740,1,['wrap'],['wrapper']
Integrability,What do you think about a solution where we only upload/send the last 50MB of the log file and then worry about a nicer interface later on? This will also patch up a liability we've had where someone could write a huge log file that we'd then have to pay for.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12852#issuecomment-1570828503:120,interface,interface,120,https://hail.is,https://github.com/hail-is/hail/issues/12852#issuecomment-1570828503,1,['interface'],['interface']
Integrability,What's the plan to address the bug you found in maximal independent set's AST route?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3704#issuecomment-395110967:78,rout,route,78,https://hail.is,https://github.com/hail-is/hail/pull/3704#issuecomment-395110967,1,['rout'],['route']
Integrability,Whats the deal with the ordering? I want to eliminate implicit dependencies,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5354#issuecomment-464034964:64,depend,dependencies,64,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464034964,1,['depend'],['dependencies']
Integrability,"When did responding to comments become a ""review""? Super annoying interface change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1778#issuecomment-300001063:66,interface,interface,66,https://hail.is,https://github.com/hail-is/hail/pull/1778#issuecomment-300001063,1,['interface'],['interface']
Integrability,"When python loads a file it runs everything at the top level. That means running statements like `import aiohttp`, `MY_CONSTANT = 5`, and `def foo: `. So importing files can define functions, so they can be used later, but those functions are not run (how could they be?). By putting the imports inside the functions themselves, we defer the import of aiohttp from when the deploy_config is imported to when the functions that depend on `aiohttp` are actually used. If they are never used, `aiohttp` will never be imported! This is why it made `hailctl dev config list` much faster, since it doesn't need to use those functions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11293#issuecomment-1027295448:428,depend,depend,428,https://hail.is,https://github.com/hail-is/hail/pull/11293#issuecomment-1027295448,1,['depend'],['depend']
Integrability,"Which exception was being masked?. We currently use this `deserialize` function to construct reader/writer classes, like MatrixVCFReader. This class does a bunch of work on construction, including throwing user-facing errors. Wrapping these errors in a `MappingException` (which becomes the top-level error, and the one in the summary in Python) is wrong. Obscuring full stack traces is wrong too. The correct thing is to stop doing a bunch of work on class construction, but until we make that change, I think that right now, we should continue peeling off the mapping exception",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6156#issuecomment-494809361:226,Wrap,Wrapping,226,https://hail.is,https://github.com/hail-is/hail/pull/6156#issuecomment-494809361,1,['Wrap'],['Wrapping']
Integrability,"While we're making breaking changes to the `hailctl` interface... there is a beta feature to start/stop Dataproc clusters. Once this is released, it could create some confusion that `hailctl dataproc start` runs `gcloud dataproc clusters create` instead of `gcloud dataproc clusters start`. Likewise for `hailctl dataproc stop` and `gcloud dataproc clusters delete`. Should we rename `hailctl dataproc` start/stop to create/delete?. https://cloud.google.com/dataproc/docs/guides/dataproc-start-stop",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767171070:53,interface,interface,53,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767171070,1,['interface'],['interface']
Integrability,Why are we keeping both if they're almost exactly the same thing? To prep for the python interface?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2225#issuecomment-331205348:89,interface,interface,89,https://hail.is,https://github.com/hail-is/hail/pull/2225#issuecomment-331205348,1,['interface'],['interface']
Integrability,Why is this a breaking change? It's the same interface as before with additional optional parameters.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3489#issuecomment-387263813:45,interface,interface,45,https://hail.is,https://github.com/hail-is/hail/pull/3489#issuecomment-387263813,1,['interface'],['interface']
Integrability,Will experiment with that interface. It's unfortunate that I can't change the order of arguments in the method now without breaking things. The order no longer makes sense as we add a bunch of things at the end.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1984#issuecomment-314618826:26,interface,interface,26,https://hail.is,https://github.com/hail-is/hail/pull/1984#issuecomment-314618826,1,['interface'],['interface']
Integrability,Will integrate with new build system and re-pull.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5906#issuecomment-487407741:5,integrat,integrate,5,https://hail.is,https://github.com/hail-is/hail/pull/5906#issuecomment-487407741,1,['integrat'],['integrate']
Integrability,"Will look at this asap; focused on notebook service, can this wait until that wraps up?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5150#issuecomment-456211174:78,wrap,wraps,78,https://hail.is,https://github.com/hail-is/hail/pull/5150#issuecomment-456211174,1,['wrap'],['wraps']
Integrability,Will split out the disk representation aspect once the other part goes in (one-way dependency),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/406#issuecomment-232000412:83,depend,dependency,83,https://hail.is,https://github.com/hail-is/hail/pull/406#issuecomment-232000412,1,['depend'],['dependency']
Integrability,"Working backwards, we need to not return a 500 on error. We could return a BadRequest error code with the message 'invalid spec' and then handle the MJC database call on the driver. I chose instead to have the worker to post job complete so we get the error message with the stack trace showing up in the UI as having the normal job flow seemed cleaner to me last week then special casing `schedule_job` on the driver. `post job complete` needs a job object to get the status to send back to the driver. However, a `Job` has two concrete implementations and we don't know which the bad job is because we can't get the spec. Furthermore, the `Job` class does a lot of work based on the spec right now. So I thought it was clearer to just create a new class that had the status, but nothing else. After writing this out, it's probably better to have the driver MJC upon error rather than from the worker. The code below would be more complicated. We'd have to get the traceback / error message from the response from the worker. ```python3; try:; await client_session.post(; f'http://{instance.ip_address}:5000/api/v1alpha/batches/jobs/create',; json=body,; timeout=aiohttp.ClientTimeout(total=2),; ); await instance.mark_healthy(); except aiohttp.ClientResponseError as e:; await instance.mark_healthy(); if e.status == 403:; log.info(f'attempt already exists for job {id} on {instance}, aborting'); if e.status == 503:; log.info(f'job {id} cannot be scheduled because {instance} is shutting down, aborting'); raise e; except Exception:; await instance.incr_failed_request_count(); raise; ```. And the error handling would look something like this:. ```python3; try:; body = await job_config(app, record, attempt_id); except Exception:; log.exception('while making job config'); status = {; 'version': STATUS_FORMAT_VERSION,; 'worker': None,; 'batch_id': batch_id,; 'job_id': job_id,; 'attempt_id': attempt_id,; 'user': record['user'],; 'state': 'error',; 'error': traceback.format_exc(),; 'container_s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078:106,message,message,106,https://hail.is,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078,3,['message'],['message']
Integrability,"Wow, talk about a *tour de force* of debugging, well done!!. ---. OK, so this kinda makes sense. We are importing our own copies of the GCS libraries and renaming them all to `is.hail.relocated....`. We do this so that we're not stuck with whatever version Dataproc is including. We pin our dataproc image version to `2.1.2-debian11` (see [here](https://github.com/hail-is/hail/blob/main/hail/python/hailtop/hailctl/dataproc/start.py#L147)) which [was released in January 2023](https://cloud.google.com/dataproc/docs/release-notes#January_23_2023). The latest available version of [Dataproc's Debian images](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1) is 2.1.25-debian11 which depends on GoogleCloudDataproc hadoop connector version [2.2.15](https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/tag/2.2.15) which relies on Google Cloud Storage client library version [2.22.3](https://github.com/GoogleCloudDataproc/hadoop-connectors/commit/8b79f025ef5e8231de827f4c620cd23e230c3489). I have [a PR](https://github.com/hail-is/hail/pull/13732) to upgrade us to 2.27.1 because the library broke retries in versions [2.25.0, 2.27.0). AFAICT, Google's image version page only shows the most recent five. There's no way to go back further in time. Luckily, the way back machine has [a March 2023 capture](https://web.archive.org/web/20230307225815/https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1) which includes our version. 2.1.2-debian11 used Google Cloud Dataproc hadoop connector version [2.2.9](https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/tag/v2.2.9) This version of the hadoop connector was [using some alpha version of a gRPC version of the cloud storage library](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/18f6e9f1c745e1854d76bea9362e2332898d8895/pom.xml#L96C1-L97C1). I'm not sure what's up with that. OK, here's my proposal: let's change that IMAGE_VERSION to the latest one ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13690#issuecomment-1738196645:714,depend,depends,714,https://hail.is,https://github.com/hail-is/hail/issues/13690#issuecomment-1738196645,1,['depend'],['depends']
Integrability,"Yeah, I was having the same trouble. The moral now is the printed representation should roughly have the same information as the interface call that generated it (read_matrix_table, import_vcf, etc.) I made the ""requested type"" optional in the printed representation, and None means use the full type coming from the file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3896#issuecomment-403056631:129,interface,interface,129,https://hail.is,https://github.com/hail-is/hail/pull/3896#issuecomment-403056631,1,['interface'],['interface']
Integrability,"Yeah, adding an `async` copy of each method of `FS` seems right to me. RouterFS is pretty close to having that anyway. `HadoopFS` can just delegate to its sync methods.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13677#issuecomment-1747702915:71,Rout,RouterFS,71,https://hail.is,https://github.com/hail-is/hail/pull/13677#issuecomment-1747702915,1,['Rout'],['RouterFS']
Integrability,"Yeah, agreed. I've never taken inventory of the full test suite. This was the first bug that caused an error message though, previous ones have just been about things being too slow. So we'll need benchmarks to catch that stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7146#issuecomment-536022939:109,message,message,109,https://hail.is,https://github.com/hail-is/hail/pull/7146#issuecomment-536022939,1,['message'],['message']
Integrability,"Yeah, amazing how far the Python interface has come!. Here are the essential changes:; ```; val popOfSample_n = DenseMatrix.zeros[Double](if (mixture) K else 1, N); if (mixture) {; val popDistRV = Dirichlet(popDist_k); (0 until N).foreach(j => popOfSample_n(::, j) := popDistRV.draw()); } else {; popDist_k :/= sum(popDist_k); val popDistRV = Multinomial(popDist_k); (0 until N).foreach(j => popOfSample_n(0, j) = popDistRV.draw()); }; ```; ```; val p =; if (mixture); popOfSample_nBc.value(::, i) dot popAF_k; else; popAF_k(popOfSample_nBc.value(0, i).toInt); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3206#issuecomment-375136911:33,interface,interface,33,https://hail.is,https://github.com/hail-is/hail/pull/3206#issuecomment-375136911,1,['interface'],['interface']
Integrability,"Yeah, it says no such method found. One of many bad expr error messages",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/846#issuecomment-249639159:63,message,messages,63,https://hail.is,https://github.com/hail-is/hail/issues/846#issuecomment-249639159,1,['message'],['messages']
Integrability,"Yeah, no trailing bin. I think we can sanity-check the SPARK_HOME setting in the HailContext constructor to give a more informative error message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319707620:138,message,message,138,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319707620,1,['message'],['message']
Integrability,"Yes totally. You'd just call ParsedLine.getValue. getKey would return an; empty array, since you passed nothing in. On Jul 13, 2016 12:21 AM, ""cseed"" notifications@github.com wrote:. > I haven't looked at this yet, I have a new use case in seqr for the table; > code. I need to be able to load data as RDD[Annotation] without pulling; > out a sample/variant key (or pull out a totally custom key). Can I do that; > easily with the new interface?; > ; > ; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/hail/pull/462#issuecomment-232252890,; > or mute the thread; > https://github.com/notifications/unsubscribe/AKEs6uAzeg--b5QgvgWITjuIpY51afyEks5qVGesgaJpZM4JGtqM; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-232253067:435,interface,interface,435,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232253067,1,['interface'],['interface']
Integrability,Yes! https://github.com/hail-is/hail/pull/4119/files#diff-b173fb9bd584d50afcfa6724954ef3b5R415 (It will be later work to rip out partition keys from the python interface.),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4094#issuecomment-412565015:160,interface,interface,160,https://hail.is,https://github.com/hail-is/hail/pull/4094#issuecomment-412565015,1,['interface'],['interface']
Integrability,"Yes, exactly. We started by building something to compile instances of AsmFunctionN, that's what FunctionBuilder is. MethodBuilder is for adding auxiliary methods to the class being compiled which will be called from the apply method. In the old way, FunctionBuilder was central. Now I'm trying to tease things apart so they are analogous to the core Java constructs: class, method, field. FunctionBuilder will just have a class and a method build for the apply method. Splitting out ClassBuilder will also let us build multiple/auxiliary classes (something done by dependent functions).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8179#issuecomment-592277392:566,depend,dependent,566,https://hail.is,https://github.com/hail-is/hail/pull/8179#issuecomment-592277392,1,['depend'],['dependent']
Integrability,"Yes, it will exit(0), but upon any job failure `state` will be ""failure"", and the STDOUT message will be ""batch {bc_batch.id} complete: failure"" instead of ""Batch completed successfully"" as in the LocalBackend case currently. We rely on exception handling to catch job errors for LocalBackend.run, and the current bug occurs because of an improperly formed ternary expression (we can tell this because `verbose` should not enable `-e`). cc @jigold, @danking, what are your thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9297#issuecomment-675599575:89,message,message,89,https://hail.is,https://github.com/hail-is/hail/pull/9297#issuecomment-675599575,1,['message'],['message']
Integrability,"Yes, thank you - all resolved with a clean build + inclusion of the above dependencies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10747#issuecomment-894259075:74,depend,dependencies,74,https://hail.is,https://github.com/hail-is/hail/issues/10747#issuecomment-894259075,1,['depend'],['dependencies']
Integrability,"You have an error based on where the maven code is being run (not the directory with the pom file).; ```; [[1;31mERROR[m] Failed to execute goal [32morg.apache.maven.plugins:maven-dependency-plugin:2.8:resolve[m [1m(default-cli)[m: [1;31mGoal requires a project to execute but there is no POM in this directory (/). Please verify you invoked Maven from the correct directory.[m -> [1m[Help 1][m; [[1;31mERROR[m] ; [[1;31mERROR[m] To see the full stack trace of the errors, re-run Maven with the [1m-e[m switch.; [[1;31mERROR[m] Re-run Maven using the [1m-X[m switch to enable full debug logging.; [[1;31mERROR[m] ; [[1;31mERROR[m] For more information about the errors and possible solutions, please read the following articles:; [[1;31mERROR[m] [1m[Help 1][m http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException; The command '/bin/sh -c mvn dependency:resolve && rm pom.xml test-jar-with-dependencies.xml' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6050#issuecomment-489712270:183,depend,dependency-plugin,183,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-489712270,3,['depend'],"['dependencies', 'dependency', 'dependency-plugin']"
Integrability,"You should benchmark linear search vs priority queue in the merge itself. You can either implement both and compare on a representative benchmark in context, or code up the merge code itself into a targeted benchmark, something along the lines of:. Generate 10 million (or more, large enough to get a stable measurement) random input integers `Array[A]` in a wrapper class `class A(i: Int)` to simulate `RegionValue`. We're aiming to merge N = ~100 elements, so put 100 into either an array or priority heap, pull out the smallest values, and update them with new input values until the input values and array are both exhausted. Test with N = 10, 100, 1000. If you do the latter, post the benchmark. I think we'll need to re-visit this trade-off again when we switch to C++.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4396#issuecomment-428335101:359,wrap,wrapper,359,https://hail.is,https://github.com/hail-is/hail/pull/4396#issuecomment-428335101,1,['wrap'],['wrapper']
Integrability,"Your application master container failed with exit code 11. There was no human interpretable error message, but googling turned up this:. http://stackoverflow.com/questions/31284799/spark-streaming-job-exited-with-code-11. Hitting ""spark.yarn.max.executor.failures"" would be totally consistent with the observed behavior, although I'm not sure why we didn't get the same error message. http://spark.apache.org/docs/latest/running-on-yarn.html. Again, this is related to the 1.5 bug I mentioned before: executors killed to give resources to other jobs shouldn't be counted as killed. Let's try again with two changes:; 1. I increased max executor failures to 500 in `hail-new-vep`.; 2. Instead of using repartition, use `importvcf -n 1000 /path/to/my.vcf.bgz splitmulti ...`. I think this will fix all the problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/302#issuecomment-211046143:99,message,message,99,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-211046143,2,['message'],['message']
Integrability,"Your previous post includes a warning message that your `SPARK_HOME`, in that shell, is set to:; ```; /opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2; ```; Could you try setting `SPARK_HOME` in the configuration file to `/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2` and then try again to create a `HailContext`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337610577:38,message,message,38,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337610577,1,['message'],['message']
Integrability,"Youre right, it could be useful to have a scala-specific test, but any breaking changes away from old ls behavior will cause the hadoop_ls test to fail (since Python presents our public interface, the more important single test to have). edit: iPhone typos",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7611#issuecomment-558616975:187,interface,interface,187,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-558616975,1,['interface'],['interface']
Integrability,Yup! Mitja and I have been talking and sharing code on this issue. But clearly there is some work to be done for this functionality to be integrated naturally in Hail. And obviously the phasing stuff would be neat :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/104#issuecomment-279858875:138,integrat,integrated,138,https://hail.is,https://github.com/hail-is/hail/issues/104#issuecomment-279858875,1,['integrat'],['integrated']
Integrability,"[1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in parse; ir_map); /miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py:1257: in __call__; answer, self.gateway_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. args = ('xro1961', <py4j.java_gateway.GatewayClient object at 0x7ffa62e9e390>, 'z:is.hail.expr.ir.IRParser', 'parse_value_ir'), kwargs = {}; pyspark = <module 'pyspark' from '/miniconda3/lib/python3.7/site-packages/pyspark/__init__.py'>, s = 'java.lang.RuntimeException: typ: inf",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:1599,wrap,wrapper,1599,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930,1,['wrap'],['wrapper']
Integrability,[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:262) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1404) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1372) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:966) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_392]; 	at sun.net.www.MeteredStream.read(MeteredStream.java:134) ~[?:1.8.0_392]; 	at java.io.FilterInputStream.read(FilterInputStream.java:133) ~[?:1.8.0_392]; 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3460) ~[?:1.8.0_392]; 	at com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:164) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_392]; 	at is.hail.relocated.com.google.cloud.storage.StorageByteChannels$ScatteringByteChannelFacade.read(StorageByteChannels.java:242) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.ApiaryUnbufferedReadableByteChannel.read(ApiaryUnbufferedReadableByteChannel.java:113) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.UnbufferedReadableByteChannelSession$Unbuffered,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:9505,protocol,protocol,9505,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['protocol'],['protocol']
Integrability,[message added](https://github.com/hail-is/hail/issues/400#issuecomment-244517801). I think if the outer most `forAll` was a method on the Suite class we'd be fine. It's return type would be constrained to be the type of the real `forAll` monad. All the inner `forAll`s would be nameless.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/732#issuecomment-244518077:1,message,message,1,https://hail.is,https://github.com/hail-is/hail/pull/732#issuecomment-244518077,1,['message'],['message']
Integrability,"](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html) class clearly comes from hadoop. It's no longer in the location from which we import it. We must get it from some other dependency. OK. So, before my simplification of build.gradle, we used a configuration called `compile` and another one called `testCompile`. [Neither of those exist in modern gradle, apparently](https://docs.gradle.org/current/userguide/java_library_plugin.html#sec:java_library_configurations_graph). I found a side-note about the `compile` configuration [here](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:java_dependency_management_overview) (search for ""compile""):. > **Why no compile configuration?**; > The Java Library Plugin has historically used the compile configuration for dependencies that are required to both compile and run a projects production code. It is now deprecated, and will issue warnings when used, because it doesnt distinguish between dependencies that impact the public API of a Java library project and those that dont. You can learn more about the importance of this distinction in [Building Java libraries](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:building_java_libraries). OK, so, we used to just dump everything into our runtime dependencies. I changed it so that we have three kinds of dependencies:; 1. `shadow`: these are provided by Dataproc/QoB at run-time. They are not in any JAR. They are not on the `testRuntimeClasspath` or `runtimeClasspath`. They are on the `testCompileClasspath` because I [explicitly requested](https://github.com/hail-is/hail/blob/main/hail/build.gradle#L98) that `testCompileOnly` bring in our `shadow` dependencies.; 2. `implementation`: these are included in all class paths and in shadow JARs (but not ""thin"" jars generated by `./gradlew jar`).; 3. `testImplementation`: these are included in test class paths and in shadow JARs. Our test code references",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741:1025,depend,dependencies,1025,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741,1,['depend'],['dependencies']
Integrability,"_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181113-2115-0.2-721af83bc30a.log; Converting vcf /project/ukbiobank/imp/ad.v1/vcf/ukbb.hg38.imputed.chr22.dose.vcf.gz to mt /project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr22.mt; [Stage 1:====> (59 + 24) / 741]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>; 6 mt=""/project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr""+chr+"".mt""; 7 print(""Converting vcf ""+vcf+"" to mt ""+ mt); ----> 8 hl.import_vcf(vcf,force_bgz=True).write(mt). <decorator-gen-891> in write(self, output, overwrite, stage_locally, _codec_spec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:1128,wrap,wrapper,1128,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,3,['wrap'],['wrapper']
Integrability,"_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB); Collecting urllib3==1.26.16; Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB); Collecting uvloop==0.17.0; Using cached uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB); Collecting wrapt==1.15.0; Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB); Collecting xyzservices==2023.7.0; Using cached xyzservices-2023.7.0-py3-none-any.whl (56 kB); Collecting yarl==1.9.2; Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB); Building wheels for collected packages: avro; Building wheel for avro (pyproject.toml): started; Building wheel for avro (pyproject.toml): finished with status 'done'; Created wheel for avro: filename=avro-1.11.2-py2.py3-none-any.whl size=119738 sha256=d7f238f86de270b449b018590930a06270766887328bdb51066eccff2cd696a6; Stored in directory: /home/hadoop/.cache/pip/wheels/e3/a2/1e/5c1be0865f4170a89de34e0a798f32f674a7eaf63a93272c7f; Successfully built avro; Installing collected packages: sortedcontainers, pytz, py4j, commonmark, azure-common, xyzservices, wrapt, uvloop, urllib3, tzdata, typing-extensions, tornado, tenacity, tabulate, six,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:40841,wrap,wrapt,40841,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['wrap'],['wrapt']
Integrability,"_fetchall; > async for row in tx.execute_and_fetchall(sql, args, query_name):\n File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 257, in execute_and_fetchall; > await cursor.execute(sql, args)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 239, in execute; > await self._query(query)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 457, in _query; > await conn.query(q)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 469, in query; > await self._read_query_result(unbuffered=unbuffered)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 683, in _read_query_result; > await result.read()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 1164, in read; > first_packet = await self.connection._read_packet()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 652, in _read_packet; > packet.raise_for_error()\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/protocol.py"", line 219, in raise_for_error; > err.raise_mysql_exception(self._data)\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/err.py"", line 150, in raise_mysql_exception; > raise errorclass(errno, errval); > pymysql.err.OperationalError: (1054, ""Unknown column 'cancelled.id' in 'on clause'""); > ```. This error strikes me as odd because `cancelled.id` has been updated to `cancelled.batch_id` in `delete_prev_cancelled_job_group_cancellable_resources_records`:. [batch/driver/main.py](https://github.com/hail-is/hail/blob/c6e3c660035379e6fe3f96fb4385f8b3c7e8d436/batch/batch/driver/main.py#L1474). Based on the error, it looks like the `main.py` being executed at `/usr/local/lib/python3.9/dist-packages/batch/driver/main.py` is still using the old version of the code, the changes from the PR were not correctly reflected in the environment. Is it possible that we might be missing a `pip install` step to ensure the latest code is deployed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2353226053:1777,protocol,protocol,1777,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2353226053,1,['protocol'],['protocol']
Integrability,"_mode': 420,; 'items': None,; 'optional': None,; 'secret_name': 'default-token-8h99c'},; 'storageos': None,; 'vsphere_volume': None}]},; 'status': {'conditions': [{'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'Initialized'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'Ready'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'ContainersReady'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'PodScheduled'}],; 'container_statuses': [{'container_id': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_id': '',; 'last_state': {'running': None,; 'terminated': None,; 'waiting': None},; 'name': 'main',; 'ready': False,; 'restart_count': 0,; 'state': {'running': None,; 'terminated': {'container_id': None,; 'exit_code': 0,; 'finished_at': None,; 'message': None,; 'reason': None,; 'signal': None,; 'started_at': None},; 'waiting': None}}],; 'host_ip': '10.128.0.8',; 'init_container_statuses': None,; 'message': None,; 'nominated_node_name': None,; 'phase': 'Pending',; 'pod_ip': None,; 'qos_class': 'Burstable',; 'reason': None,; 'start_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal())}}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 53, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurren",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:8574,message,message,8574,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['message'],['message']
Integrability,"_self_kinship). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/methods/relatedness/pc_relate.py in pc_relate(call_expr, min_individual_maf, k, scores_expr, min_kinship, statistics, block_size, ; include_self_kinship); 314 ; 315 if k and scores_expr is None:; --> 316 _, scores, _ = hwe_normalized_pca(call_expr, k, compute_loadings=False); 317 scores_expr = scores[mt.col_key].scores; 318 elif not k and scores_expr is not None:. <decorator-gen-1778> in hwe_normalized_pca(call_expr, k, compute_loadings). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/methods/pca.py in hwe_normalized_pca(call_expr, k, compute_loadings); 99 return _hwe_normalized_blanczos(call_expr, k, compute_loadings); 100 ; --> 101 return pca(hwe_normalize(call_expr),; 102 k,; 103 compute_loadings). <decorator-gen-1780> in pca(entry_expr, k, compute_loadings). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-pac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:1543,wrap,wrapper,1543,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,2,['wrap'],['wrapper']
Integrability,`-_______________________-`. Now depends on #5772,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5710#issuecomment-479908699:33,depend,depends,33,https://hail.is,https://github.com/hail-is/hail/pull/5710#issuecomment-479908699,1,['depend'],['depends']
Integrability,"``>>> import subprocess``; ``>>> old_popen = subprocess.Popen``; ``>>> def wrapped(*args, **kwargs):``; ``... print('args are: ' + str(args))``; ``... print('kwargs are: ' + str(kwargs))``; ``... return old_popen(*args, **kwargs)``; ``... ``; ``>>> subprocess.Popen = wrapped``; ``>>> hc = HailContext()``; ``Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties``; ``Setting default log level to ""WARN"".``; ``To adjust logging level use sc.setLogLevel(newLevel).``; ``Traceback (most recent call last):``; `` File ""<stdin>"", line 1, in <module>``; `` File ""<decorator-gen-422>"", line 2, in __init__``; `` File ""/Users/ih/languages/hail.is/hail/python/hail/typecheck/check.py"", line 226, in _typecheck; return f(*args, **kwargs)``; `` File ""/Users/ih/languages/hail.is/hail/python/hail/context.py"", line 83, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir)``; ``TypeError: 'JavaPackage' object is not callable``. Is this what you were suggesting or am I missing something?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319717790:75,wrap,wrapped,75,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319717790,2,['wrap'],['wrapped']
Integrability,"```; * installing *source* package ncdf4 ...; ** package ncdf4 successfully unpacked and MD5 sums checked; configure.ac: starting; checking for nc-config... no; -----------------------------------------------------------------------------------; Error, nc-config not found or not executable. This is a script that comes with the; netcdf library, version 4.1-beta2 or later, and must be present for configuration; to succeed. If you installed the netcdf library (and nc-config) in a standard location, nc-config; should be found automatically. Otherwise, you can specify the full path and name of; the nc-config script by passing the --with-nc-config=/full/path/nc-config argument; flag to the configure script. For example:. ./configure --with-nc-config=/sw/dist/netcdf4/bin/nc-config. Special note for R users:; -------------------------; To pass the configure flag to R, use something like this:. R CMD INSTALL --configure-args=""--with-nc-config=/home/joe/bin/nc-config"" ncdf4. where you should replace /home/joe/bin etc. with the location where you have; installed the nc-config script that came with the netcdf 4 distribution.; -----------------------------------------------------------------------------------; ERROR: configuration failed for package ncdf4; * removing /usr/local/lib/R/3.3/site-library/ncdf4; ERROR: dependency ncdf4 is not available for package GWASTools; * removing /usr/local/lib/R/3.3/site-library/GWASTools; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3273#issuecomment-377701057:1331,depend,dependency,1331,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377701057,1,['depend'],['dependency']
Integrability,"```; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-11-de5acf23a36f> in <module>(); ----> 1 hl.methods.maximal_independent_set(t.idx, t.foo, tie_breaker = lambda i, j: hl.signum(i - j)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 139 .select()); 140 ; --> 141 edges = t.key_by(None).select('i', 'j'); 142 nodes_in_set = Env.hail().utils.Graph.maximalIndependentSet(edges._jt.collect(), node_t._jtype, joption(tie_breaker_hql)); 143 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 863 row = get_select_exprs('Table.select',; 864 exprs, named_exprs, self._row_indices,; --> 865 protect_keys=True); 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 . ~/projects/hail/python/hail/utils/misc.py in get_select_exprs(caller, exprs, named_exprs, indices, protect_keys); 314 def get_select_exprs(caller, exprs, named_exprs, indices, protect_keys=True):; 315 from hail.expr.expressions import to_expr, ExpressionException, TopLevelReference, Select; --> 316 exprs = [to_expr(e) if not isinstance(e, str) else indices.source[e] for e in exprs]; 317 named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}; 318 assignments = OrderedDict(). ~/projects/hail/python/hail/utils/misc.py in <listcomp>(.0); 314 def get_select_exprs(caller, exprs, named_exprs, indices, protect_k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3706#issuecomment-394395696:1168,wrap,wrapper,1168,https://hail.is,https://github.com/hail-is/hail/issues/3706#issuecomment-394395696,3,['wrap'],['wrapper']
Integrability,```; Building dependency tree...; Reading state information...; [91mE: Unable to locate package python-setuptools; The command '/bin/sh -c apt-get install gcc python-setuptools && pip uninstall crcmod && pip install --no-cache-dir -U crcmod' returned a non-zero code: 100; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7024#issuecomment-529650160:14,depend,dependency,14,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529650160,1,['depend'],['dependency']
Integrability,"```; FatalError Traceback (most recent call last); /tmp/ipykernel_10092/1120523425.py in <cell line: 8>(); 6 # Connection refused: qc-sw-nvpv.c.covid-19-wgs-analysis.internal/10.128.0.182:7337; 7 # https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/pc_related.2E.20how.20slow.3F; ----> 8 relatedness_ht = hl.pc_relate(hq_mt_subset.GT, 0.01, k=10, statistics='kin',; 9 include_self_kinship=include_kinself, block_size=2048); 10 . <decorator-gen-1794> in pc_relate(call_expr, min_individual_maf, k, scores_expr, min_kinship, statistics, block_size, include_self_kinship). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/methods/relatedness/pc_relate.py in pc_relate(call_expr, min_individual_maf, k, scores_expr, min_kinship, statistics, block_size, ; include_self_kinship); 314 ; 315 if k and scores_expr is None:; --> 316 _, scores, _ = hwe_normalized_pca(call_expr, k, compute_loadings=False); 317 scores_expr = scores[mt.col_key].scores; 318 elif not k and scores_expr is not None:. <decorator-gen-1778> in hwe_normalized_pca(call_expr, k, compute_loadings). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/methods/pca.py in hwe_normalized_pca(call_expr, k, compute_loadings); 99 return _hwe_normalized_blanczos(call_exp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:677,wrap,wrapper,677,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,3,['wrap'],['wrapper']
Integrability,"```; echo $SPARK_CLASSPATH; /Users/ih/languages/hail.is/hail/build/libs/hail-all-spark.jar; ```; SPARK_CLASSPATH is set correctly, however, I still get the error message; ```; >>> hc = HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-422>"", line 2, in __init__; File ""/Users/ih/languages/hail.is/hail/python/hail/typecheck/check.py"", line 226, in _typecheck; return f(*args, **kwargs); File ""/Users/ih/languages/hail.is/hail/python/hail/context.py"", line 83, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); TypeError: 'JavaPackage' object is not callable; ```. What do I need to do about the driverClassPath?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319712395:162,message,message,162,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319712395,1,['message'],['message']
Integrability,"```scala; def emit(mbLike: EmitMethodBuilderLike): Code[Unit] =; // wrapped methods can't contain uses of Recur; useValues(mbLike.mb, ir.pType, mbLike.emit.emit(ir, env, EmitRegion.default(mbLike.emit.mb), container, None)); ```. Issue is Emit running before inference",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8012#issuecomment-580367676:68,wrap,wrapped,68,https://hail.is,https://github.com/hail-is/hail/pull/8012#issuecomment-580367676,1,['wrap'],['wrapped']
Integrability,"```scala; private def boundary(node: IR): IR = {; var streamified = streamify(node). if (streamified.typ.isInstanceOf[TStream] && node.typ.isInstanceOf[TArray]) {; streamified = ToArray(streamified); }. if (streamified.typ.isInstanceOf[TArray] && node.typ.isInstanceOf[TStream]); streamified = ToStream(streamified). println(s""\n\nbefore: \n${node} of typ: ${node.typ}\nafter: ${streamified} of typ: ${streamified.typ}\n\n""); assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }; ...; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. 4 failed in IRSuite, including testDictContains, as before:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))). before: ; ToArray(Ref(__iruid_2345,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_2345,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). wrapping `case x: ApplyIR => streamify(x.explicitNode)` doesn't fix. But the greater point, I think, is that if boundary can ever be called on something that was wrapped in ToStream, we cannot just blindly case ToArray on it. Need to cast ToArray, ToDict, ToSet, or do what I did. I prefer the separate ToArray, ToDict, ToSet, over what I have now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586612345:1489,wrap,wrapping,1489,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586612345,2,['wrap'],"['wrapped', 'wrapping']"
Integrability,"`inHemiX` depends on sex. How about `inXPar` and `inXNonPar`, so that Non clearly just refers to Par? This reads as ""in the X pseudo-autosomal region"" or ""in the X non-pseudo-autosomal-region""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/499#issuecomment-235665809:10,depend,depends,10,https://hail.is,https://github.com/hail-is/hail/pull/499#issuecomment-235665809,1,['depend'],['depends']
Integrability,"`pip install -e .`; Defaulting to user installation because normal site-packages is not writeable; Obtaining file:///home/skr/hail2/hail; Installing build dependencies ... done; Checking if build backend supports build_editable ... done; Getting requirements to build editable ... error; error: subprocess-exited-with-error; ;  Getting requirements to build editable did not run successfully.;  exit code: 1; > [14 lines of output]; error: Multiple top-level packages discovered in a flat-layout: ['tls', 'gear', 'hail', 'auth', 'blog', 'infra', 'batch', 'query', 'docker', 'memory', 'devbin', 'gateway', 'website', 'grafana', 'notebook', 'graphics', 'datasets', 'monitoring', 'web_common', 'prometheus', 'letsencrypt'].; ; To avoid accidental inclusion of unwanted files or directories,; setuptools will not proceed with this build.; ; If you are trying to create a single distribution with multiple packages; on purpose, you should not rely on automatic discovery.; Instead, consider the following options:; ; 1. set up custom discovery (`find` directive with `include` or `exclude`); 2. use a `src-layout`; 3. explicitly set `py_modules` or `packages` with a list of names; ; To find more information, look for ""package discovery"" on setuptools docs.; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; error: subprocess-exited-with-error.  Getting requirements to build editable did not run successfully.;  exit code: 1; > See above for output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12844#issuecomment-1502112290:155,depend,dependencies,155,https://hail.is,https://github.com/hail-is/hail/issues/12844#issuecomment-1502112290,1,['depend'],['dependencies']
Integrability,"actually, the non-transitive dependencies is breaking the tests that use the LSM tree. Will sort out those issues.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8571#issuecomment-615348697:29,depend,dependencies,29,https://hail.is,https://github.com/hail-is/hail/pull/8571#issuecomment-615348697,1,['depend'],['dependencies']
Integrability,addresses poor error message in https://github.com/hail-is/hail/issues/4033,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4036#issuecomment-408955855:21,message,message,21,https://hail.is,https://github.com/hail-is/hail/pull/4036#issuecomment-408955855,1,['message'],['message']
Integrability,after discussion with @tpoterba pausing on this until the PContainer interface is cleaned up. Aiming to finish this up next week.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7639#issuecomment-561962333:69,interface,interface,69,https://hail.is,https://github.com/hail-is/hail/pull/7639#issuecomment-561962333,1,['interface'],['interface']
Integrability,"ages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 230, in POST\n body=body))\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 181, in request\n raise ApiException(http_resp=r)\nkubernetes_asyncio.client.rest.ApiException: (422)\nReason: Unprocessable Entity\nHTTP response headers: <CIMultiDictProxy('Audit-Id': '16158e81-8543-457e-8bea-5d5e1a8c39f3', 'Content-Type': 'application/json', 'Date': 'Sun, 29 Sep 2019 03:43:05 GMT', 'Content-Length': '901')>\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""Pod \\\""notebook-worker-9z6tg\\\"" is invalid: [spec.volumes[1].secret.secretName: Required value, spec.volumes[2].secret.secretName: Required value, spec.containers[0].volumeMounts[1].name: Not found: \\\""gsa-key\\\"", spec.containers[0].volumeMounts[2].name: Not found: \\\""user-tokens\\\""]\"",\""reason\"":\""Invalid\"",\""details\"":{\""name\"":\""notebook-worker-9z6tg\"",\""kind\"":\""Pod\"",\""causes\"":[{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[1].secret.secretName\""},{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[2].secret.secretName\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""gsa-key\\\""\"",\""field\"":\""spec.containers[0].volumeMounts[1].name\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""user-tokens\\\""\"",\""field\"":\""spec.containers[0].volumeMounts[2].name\""}]},\""code\"":422}\n\n""}",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:3002,message,message,3002,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851,4,['message'],['message']
Integrability,"ah, you're totally right. Didn't read the interface diff closely enough.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3489#issuecomment-387349360:42,interface,interface,42,https://hail.is,https://github.com/hail-is/hail/pull/3489#issuecomment-387349360,2,['interface'],['interface']
Integrability,"ah---that's just a log message from the readinessProbe sending requests before the server is fully up, I believe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548103041:23,message,message,23,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103041,1,['message'],['message']
Integrability,"ail/hail/python/hail/backend/py4j_backend.py"", line 94, in execute; jir = self._to_java_value_ir(ir); File ""/home/edmund/.local/src/hail/hail/python/hail/backend/spark_backend.py"", line 280, in _to_java_value_ir; return self._to_java_ir(ir, self._parse_value_ir); File ""/home/edmund/.local/src/hail/hail/python/hail/backend/spark_backend.py"", line 276, in _to_java_ir; ir._jir = parse(r(finalize_randomness(ir)), ir_map=r.jirs); File ""/home/edmund/.local/src/hail/hail/python/hail/backend/spark_backend.py"", line 245, in _parse_value_ir; return self._jbackend.parse_value_ir(; File ""/home/edmund/.local/src/hail/.venv/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1304, in __call__; return_value = get_return_value(; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/py4j_backend.py"", line 21, in deco; return f(*args, **kwargs); File ""/home/edmund/.local/src/hail/.venv/lib/python3.8/site-packages/py4j/protocol.py"", line 326, in get_return_value; raise Py4JJavaError(; py4j.protocol.Py4JJavaError: An error occurred while calling o1.parse_value_ir.; : java.util.NoSuchElementException: key not found: __uid_4; at scala.collection.immutable.Map$Map1.apply(Map.scala:114); at is.hail.expr.ir.Env.apply(Env.scala:128); at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:890); at is.hail.expr.ir.IRParser$.$anonfun$ir_value_expr$1(Parser.scala:820); at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); at is.hail.utils.StackSafe$.run(StackSafe.scala:16); at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); at is.hail.expr.ir.IRParser$.$anonfun$parse_value_ir$1(Parser.scala:2072); at is.hail.expr.ir.IRParser$.parse(Parser.scala:2068); at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:2072); at is.hail.backend.spark.SparkBackend.$anonfun$parse_value_ir$2(SparkBackend.scala:710); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); at is.hail.utils.package$.using(package.scala:635); at is.hail.backend.ExecuteContext$.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:9872,protocol,protocol,9872,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['protocol'],['protocol']
Integrability,"al.PContainer. 	at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:631). #### Let's try to return x from streamify catch-all case (instead of if(...) ToStream(x) else x). With:. ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }; ````. Again issue in testArrayAggScan (and 8 others in IRSuite):. Before Lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). After lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(Cal",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:4560,Wrap,WrappedArray,4560,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113,2,['Wrap'],['WrappedArray']
Integrability,ala:50); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); at is.hail.utils.package$.using(package.scala:601); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.SparkBackend.execu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:15777,Wrap,WrappedArray,15777,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Wrap'],['WrappedArray']
Integrability,"also, this error message is WAY better and fully debuggable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1623#issuecomment-290762181:17,message,message,17,https://hail.is,https://github.com/hail-is/hail/issues/1623#issuecomment-290762181,1,['message'],['message']
Integrability,"ame': 'default-token-8h99c',; 'nfs': None,; 'persistent_volume_claim': None,; 'photon_persistent_disk': None,; 'portworx_volume': None,; 'projected': None,; 'quobyte': None,; 'rbd': None,; 'scale_io': None,; 'secret': {'default_mode': 420,; 'items': None,; 'optional': None,; 'secret_name': 'default-token-8h99c'},; 'storageos': None,; 'vsphere_volume': None}]},; 'status': {'conditions': [{'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'Initialized'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'Ready'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'ContainersReady'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'PodScheduled'}],; 'container_statuses': [{'container_id': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_id': '',; 'last_state': {'running': None,; 'terminated': None,; 'waiting': None},; 'name': 'main',; 'ready': False,; 'restart_count': 0,; 'state': {'running': None,; 'terminated': {'container_id': None,; 'exit_code': 0,; 'finished_at': None,; 'message': None,; 'reason': None,; 'signal': None,; 'started_at': None},; 'waiting': None}}],; 'host_ip': '10.128.0.8',; 'init_container_statuses': None,; 'message': None,; 'nominated_node_name': None,; 'phase': 'Pending',; 'pod_ip': None,; 'qos_class': 'Burstable',; 'reason': None,; 'start_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal())}}; File ""/usr/local/lib/python3.6/dist-packages/batc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:8327,message,message,8327,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['message'],['message']
Integrability,"ar/180856479). From Zulip:; > By comparison, on my wired laptop (which should be strictly slower than in GCP), I can download and extract a tar -cvzf archive in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and #7534 is a big quality of life improvement for those of us with large repos running tests on images that are deep on the critical path (the shuffler test is behind 3 images and build hail, which also c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:1212,rout,route,1212,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927,1,['rout'],['route']
Integrability,"ay<struct{key: int32, value: str}>; after: Ref(__iruid_56,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). #### Fix:; ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TIterable]) {; ToStream(node); } else {; node; }; }; }; }; ```. New issues (many more failures in IRSuite, and surely plenty more in other tests):. First of these:; Before Lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))), ArrayFor(ArrayMap(ArrayRange(I32(0),I32(4),I32(1)),__iruid_304,Cast(Ref(__iruid_304,int32),int64)),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),GetTupleElement(Ref(__iruid_302,tuple(int64)),0))))). After lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))), ArrayFor(ToStream(ArrayMap(ToStream(StreamRange(I32(0),I32(4),I32(1))),__iruid_304,Cast(Ref(__iruid_304,int32),int64))),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum() -> Agg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:2313,Wrap,WrappedArray,2313,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113,2,['Wrap'],['WrappedArray']
Integrability,azure.com.azure.storage.common.implementation.StorageImplUtils.blockWithOptionalTimeout(StorageImplUtils.java:133); 		at is.hail.shadedazure.com.azure.storage.blob.specialized.BlobClientBase.getPropertiesWithResponse(BlobClientBase.java:1379); 		at is.hail.shadedazure.com.azure.storage.blob.specialized.BlobClientBase.getProperties(BlobClientBase.java:1348); 		at is.hail.io.fs.AzureStorageFS.$anonfun$openNoCompression$1(AzureStorageFS.scala:223); 		at is.hail.io.fs.AzureStorageFS.$anonfun$handlePublicAccessError$1(AzureStorageFS.scala:175); 		at is.hail.services.package$.retryTransientErrors(package.scala:124); 		at is.hail.io.fs.AzureStorageFS.handlePublicAccessError(AzureStorageFS.scala:174); 		at is.hail.io.fs.AzureStorageFS.openNoCompression(AzureStorageFS.scala:220); 		at is.hail.io.fs.RouterFS.openNoCompression(RouterFS.scala:20); 		at is.hail.io.fs.FS.openNoCompression(FS.scala:322); 		at is.hail.io.fs.FS.openNoCompression$(FS.scala:322); 		at is.hail.io.fs.RouterFS.openNoCompression(RouterFS.scala:3); 		at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); 		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 		at is.hail.services.package$.retryTransientErrors(package.scala:124); 		at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:459); 		at is.hail.backend.service.Main$.main(Main.scala:15); 		at is.hail.backend.service.Main.main(Main.scala); 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 		at java.lang.reflect.Method.invoke(Method.java:498); 		at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at java.util.concurrent.FutureTask.run(FutureTask.java:266); 		at java.util.concurrent.Executors$Runnabl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906430:1320,Rout,RouterFS,1320,https://hail.is,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906430,1,['Rout'],['RouterFS']
Integrability,"b.com/kubernetes/kubernetes/pull/53947. Batch: `kube_event_loop` is always involved. Always:; ```log; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:1170,depend,depending,1170,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389,1,['depend'],['depending']
Integrability,"b/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<decorator-gen-468>"", line 2, in eval_typed; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 199, in eval_typed; return (Env.backend().execute(expression._ir), expression.dtype); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 86, in _to_java_ir; code = r(ir); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 511, in render; return '(ArrayLen {})'.format(r(self.a)); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:2194,wrap,wrapper,2194,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184,1,['wrap'],['wrapper']
Integrability,"b/python3.9/dist-packages/py4j/java_gateway.py:1321: in __call__; return_value = get_return_value(; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . args = ('xro552', <py4j.java_gateway.GatewayClient object at 0x7f01e1182160>, 'o1', 'executeEncode'); kwargs = {}; pyspark = <module 'pyspark' from '/usr/local/lib/python3.9/dist-packages/pyspark/__init__.py'>; s = 'java.lang.RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000'; tpl = JavaObject id=o553; deepest = 'RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000'; full = 'java.lang.RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000\n\tat is.h...ava:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n'; error_id = -1. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); > raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; E hail.utils.java.FatalError: RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000; E ; E Java stack trace:; E java.lang.RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000; E 	at is.hail.annotations.Memory.checkAddress(Memory.java:226); E 	at is.hail.annotations.Memory.loadInt(Memory.java:140); E 	at is.hail.annotations.Region$.loadInt(Region.scala:20); E 	at __C92844etypeDecode.__m92863ord_compareNonnull(Unknown Source); E 	at __C92844etypeDecode.__m92862ord_compareNonnull(Unknown Source); E 	at __C92844etypeDecode.__m92861ord",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:3069,protocol,protocol,3069,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['protocol'],['protocol']
Integrability,"bgen ___________________; [gw0] linux -- Python 3.6.6 /home/hail/.conda/envs/hail/bin/python; 067 ; 068 Import a BGEN file as a matrix table with genotype dosage entry field:; 069 ; 070 >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; 071 ... entry_fields=['dosage'],; 072 ... sample_file=""data/example.8bits.sample""); 073 ; 074 Load a single variant from a BGEN file:; 075 ; 076 >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; UNEXPECTED EXCEPTION: TypeError(""import_bgen: parameter 'variants': expected (None or Sequence[hail.utils.struct.Struct] or hail.expr.expressions.typed_expressions.StructExpression or hail.table.Table), found list: [<StructExpression of type struct{locus: locus<GRCh37>, alleles: array<str>}>]"",); Traceback (most recent call last):. File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 487, in check_all; args_.append(checker.check(arg, name, arg_name)). File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 59, in check; raise TypecheckFailure(). hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/home/hail/.conda/envs/hail/lib/python3.6/doctest.py"", line 1330, in __run; compileflags, 1), test.globs). File ""<doctest hail.methods.impex.import_bgen[2]>"", line 4, in <module>. File ""<decorator-gen-904>"", line 2, in import_bgen. File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 559, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method). File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 513, in check_all; )) from e. TypeError: import_bgen: parameter 'variants': expected (None or Sequence[hail.utils.struct.Struct] or hail.expr.expressions.typed_expressions.StructExpression or hail.table.Table), found list: [<StructExpression of type struct{locus: locus<GRCh37>, alleles: array<str>}>]. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4291#issuecomment-421486590:1745,wrap,wrapper,1745,https://hail.is,https://github.com/hail-is/hail/pull/4291#issuecomment-421486590,1,['wrap'],['wrapper']
Integrability,"bump @patrick-schultz, I've another branch that depends on this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12091#issuecomment-1230454206:48,depend,depends,48,https://hail.is,https://github.com/hail-is/hail/pull/12091#issuecomment-1230454206,1,['depend'],['depends']
Integrability,cala-library-2.12.15.jar:?]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.GoogleStorageFS$$anon$2.close(GoogleStorageFS.scala:308) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159) ~[?:1.8.0_382]; 	at is.hail.utils.package$.using(package.scala:677) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.FS.writePDOS(FS.scala:441) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.FS.writePDOS$(FS.scala:440) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$4(Worker.scala:124) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$4$adapted(Worker.scala:124) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$13(Worker.scala:178) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:14229,Rout,RouterFS,14229,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['Rout'],['RouterFS']
Integrability,"cala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Daniel King: JAR and ZIP are both deleted from google storage. for the record the bad SHA was 5702f5c6b299; ```. A discuss user [is seeing that the JVM crashes when he tries to use hail](http://discuss.hail.is/t/gcp-py4jnetworkerror-answer-from-java-side-is-empty/630):. ```; hl.init(); mt = hl.read_matrix_table('gs://hail-datasets/hail-data/1000_genomes_phase3_autosomes.GRCh37.mt'); mt.describe() ...; mt.rsid.show(5); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:4114,protocol,protocol,4114,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186,2,['protocol'],['protocol']
Integrability,can we make the commit messages more descriptive?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5970#issuecomment-487193594:23,message,messages,23,https://hail.is,https://github.com/hail-is/hail/pull/5970#issuecomment-487193594,1,['message'],['messages']
Integrability,can't do this now as it's technically an interface change./,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7107#issuecomment-533924688:41,interface,interface,41,https://hail.is,https://github.com/hail-is/hail/issues/7107#issuecomment-533924688,1,['interface'],['interface']
Integrability,"cc: @cseed, this is my proposed interface and a couple of examples of aggregators for region values.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2514#issuecomment-348577859:32,interface,interface,32,https://hail.is,https://github.com/hail-is/hail/pull/2514#issuecomment-348577859,1,['interface'],['interface']
Integrability,"ccrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches running on the batch service managed by the Hail team. positional arguments:; {billing,list,get,cancel,delete,log,job,wait}; billing List billing; list List batches; get Get a particular batch's info; cancel Cancel a batch; delete Delete a batch; log Get log for a job; job Get the status and specification for a job; wait Wait for a batch to complete, then print JSON status. optional arguments:; -h, --help show this help message and exit; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:2969,message,message,2969,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006,1,['message'],['message']
Integrability,cher.run(ServerImpl.java:408); 	at java.base/java.lang.Thread.run(Thread.java:834). java.net.SocketTimeoutException: connect timed out; 	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399); 	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242); 	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224); 	at java.base/java.net.Socket.connect(Socket.java:591); 	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177); 	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474); 	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569); 	at java.base/sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:341); 	at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:362); 	at java.base/sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1242); 	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1181); 	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1075); 	at java.base/sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:1009); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:151); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1012); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:196); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.auth.oauth2.Credential.refres,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:5390,protocol,protocol,5390,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,2,['protocol'],['protocol']
Integrability,"ci failure:; ```; hail version: 0.2.109-c163bcb21073; Error summary: AssertionError: assertion failed; self = <test.hail.linalg.test_linalg.Tests testMethod=test_tree_matmul>. @fails_service_backend(); @fails_local_backend(); def test_tree_matmul(self):; nm = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]); m = BlockMatrix.from_numpy(nm, block_size=2); nrow = np.array([[7.0, 8.0, 9.0]]); row = BlockMatrix.from_numpy(nrow, block_size=2); ; > with BatchedAsserts() as b:. test/hail/linalg/test_linalg.py:612: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/hail/linalg/test_linalg.py:96: in __exit__; vals.extend(list(hl.eval(tuple([all_bms[k] for k in bm_keys[batch_start:batch_start + batch_size]])))); <decorator-gen-692>:2: in eval; ???; hail/typecheck/check.py:577: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:191: in eval; return eval_timed(expression)[0]; <decorator-gen-690>:2: in eval_timed; ???; hail/typecheck/check.py:577: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:161: in eval_timed; return Env.backend().execute(MakeTuple([ir]), timed=True)[0]; hail/backend/py4j_backend.py:82: in execute; raise e.maybe_user_error(ir) from None; hail/backend/py4j_backend.py:76: in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); ../../.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321: in __call__; return_value = get_return_value(; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . args = ('xro549', <py4j.clientserver.JavaClient object at 0x7fd0d58f6fb0>, 'o1', 'executeEncode'); kwargs = {}; pyspark = <module 'pyspark' from '/home/edmund/.local/src/hail/.venv/lib/python3.10/site-packages/pyspark/__init__.py'>; s = 'java.lang.AssertionError: assertion failed', tpl = JavaObject id=o550; deepest = 'AssertionError: assertion failed'; full = 'java.lang.AssertionError: asserti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229:814,wrap,wrapper,814,https://hail.is,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229,1,['wrap'],['wrapper']
Integrability,closing until dependency is in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2552#issuecomment-351194636:14,depend,dependency,14,https://hail.is,https://github.com/hail-is/hail/pull/2552#issuecomment-351194636,1,['depend'],['dependency']
Integrability,closing until dependent PRs are merged:; - https://github.com/hail-is/hail/pull/3390; - https://github.com/hail-is/hail/pull/3389,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3392#issuecomment-382172344:14,depend,dependent,14,https://hail.is,https://github.com/hail-is/hail/pull/3392#issuecomment-382172344,1,['depend'],['dependent']
Integrability,"col.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory\n response = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/gear/csrf.py\"", line 20, in wrapped\n return await fun(request, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 77, in wrapped\n return await fun(request, userdata, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 417, in post_notebook\n return await _post_notebook('notebook', request, userdata)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 230, in POST\n body=body))\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:1091,wrap,wrapped,1091,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851,1,['wrap'],['wrapped']
Integrability,"create_vm; await self.compute_client.post(f'/zones/{location}/instances', params=params, json=vm_config); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiocloud/common/base_client.py"", line 30, in post; async with await self._session.post(url, **kwargs) as resp:; File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiocloud/common/session.py"", line 21, in post; return await self.request('POST', url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiocloud/common/session.py"", line 103, in request; return await request_retry_transient_errors(self._http_session, method, url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 770, in request_retry_transient_errors; return await retry_transient_errors(session.request, method, url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 729, in retry_transient_errors; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 147, in request_and_raise_for_status; body=body; hailtop.httpx.ClientResponseError: 400, message='Bad Request', url=URL('https://compute.googleapis.com/compute/v1/projects/hail-vdc/zones/us-central1-a/instances?requestId=e2555a38-1583-47e2-ab15-c3d7ad84e700') body='{\n ""error"": {\n ""code"": 400,\n ""message"": ""Invalid value for field \'resource.scheduling.instanceTerminationAction\': \'DELETE\'. You cannot specify a termination action for a VM instance that has the standard provisioning model (default). To use instance termination action, the VM instance must use the Spot provisioning model."",\n ""errors"": [\n {\n ""message"": ""Invalid value for field \'resource.scheduling.instanceTerminationAction\': \'DELETE\'. You cannot specify a termination action for a VM instance that has the standard provisioning model (default). To use instance termination action, the VM instance must use the Spot provisioning model."",\n ""domain"": ""global"",\n ""reason"": ""invalid""\n }\n ]\n }\n}\n'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11878#issuecomment-1144087728:1267,message,message,1267,https://hail.is,https://github.com/hail-is/hail/pull/11878#issuecomment-1144087728,3,['message'],['message']
Integrability,"crets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.create_job('alpine:3.8', command=['echo', 'right'], parents=[head]); tail = batch.create_job('alpine:3.8', command=['echo', 'tail'], parents=[left, right]); batch.close(); batch.wait(); i = 0; while len(output) != 4:; time.sleep(0.100 * (3/2) ** i); i += 1; if i > 14:; break; > assert len(output) == 4; E assert 5 == 4; E -5; E +4. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:1615,rout,route,1615,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506,1,['rout'],['route']
Integrability,"crossposting from a message I sent to the variants team. ---. #### executive summary. Excess JVM memory use is almost certainly not the issue. I've taken a close look at the import_gvs.py loop and the related Hail Python code. No obvious accumulation of RAM use. AFAICT, the oomkiller keeps killing the pipelines. We need to stop this because the oomkiller (a) acts before the JVM GC can free things and (b) prevents us from getting JVM diagnostics on failure. We control the JVM's max heap with hailctl's --master-memory-fraction (default is 0.8 for 80% of the master machine type's advertised RAM). I suggest we set this down to 0.6 and continue using an n1-highmem-16 driver.; If Hail is (incorrectly) accumulating garbage memory per-group, we'll have a better chance diagnosing that with a running JVM instead of one that's been SIGKILL'ed. To understand what's going on, we gotta see what is using RAM in the n1-highmem-16 case. If I could SSH to the cluster, a simple solution is a screen with top -s 300 -n 100 >memory.log (I'd guess no more than 500KiB per hour of logs) and retrieve that file if the cluster fails. If we could get Google Monitoring set up to retrieve process-level memory statistics from the driver node that should also work. Just to be clear, I don't anticipate any changes to Hail in the next week that would change the memory use of this pipeline. There could be a memory leak, but I have no clews that lead to it. I realize this is an unsatisfying answer. I'm pretty perplexed as to what could be the issue here. #### technical details. We'll call the second to most recent run Run A and the most recent run Run B. Run A (like all runs before it) only manages two sample groups before failing. Run B made it through 50 groups before failing on 51. Why did they fail? The syslog for Run A is clear: the oomkiller killed Run A. We lack syslogs for Run B, so we cannot be certain but the lack of a JVM stack trace suggests to me that (a) the driver failed and (b) the drive",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449:20,message,message,20,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449,1,['message'],['message']
Integrability,"currently, all the c++ code is acquiring things called ""ScalaRegions"" to minimize the amount of changes in this PR. One of the next steps will be to have c++ code rely on the underlying c++ region instead of the ""ScalaRegion"" which is intended to just be a wrapper for the Region object in Scala.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4859#issuecomment-443817760:257,wrap,wrapper,257,https://hail.is,https://github.com/hail-is/hail/pull/4859#issuecomment-443817760,1,['wrap'],['wrapper']
Integrability,"d another one called `testCompile`. [Neither of those exist in modern gradle, apparently](https://docs.gradle.org/current/userguide/java_library_plugin.html#sec:java_library_configurations_graph). I found a side-note about the `compile` configuration [here](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:java_dependency_management_overview) (search for ""compile""):. > **Why no compile configuration?**; > The Java Library Plugin has historically used the compile configuration for dependencies that are required to both compile and run a projects production code. It is now deprecated, and will issue warnings when used, because it doesnt distinguish between dependencies that impact the public API of a Java library project and those that dont. You can learn more about the importance of this distinction in [Building Java libraries](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:building_java_libraries). OK, so, we used to just dump everything into our runtime dependencies. I changed it so that we have three kinds of dependencies:; 1. `shadow`: these are provided by Dataproc/QoB at run-time. They are not in any JAR. They are not on the `testRuntimeClasspath` or `runtimeClasspath`. They are on the `testCompileClasspath` because I [explicitly requested](https://github.com/hail-is/hail/blob/main/hail/build.gradle#L98) that `testCompileOnly` bring in our `shadow` dependencies.; 2. `implementation`: these are included in all class paths and in shadow JARs (but not ""thin"" jars generated by `./gradlew jar`).; 3. `testImplementation`: these are included in test class paths and in shadow JARs. Our test code references these third-party classes:; ```; import breeze.linalg.DenseMatrix; import breeze.linalg._; import breeze.linalg.{*, diag, DenseMatrix => BDM, DenseVector => BDV}; import breeze.linalg.{DenseMatrix => BDM, _}; import breeze.linalg.{DenseMatrix => BDM}; import breeze.linalg.{DenseMatrix, DenseVector, eigSym, svd}; im",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741:1359,depend,dependencies,1359,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741,1,['depend'],['dependencies']
Integrability,"d/.local/src/hail/hail/python/hail/expr/expressions/expression_typecheck.py"", line 80, in check; raise TypecheckFailure from e; hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in <module>; cli.main(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main; run(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file; runpy.run_path(target, run_name=""__main__""); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path; return _run_module_code(code, init_globals, run_name,; File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code; _run_code(code, mod_globals, init_globals,; File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 124, in _run_code; exec(code, run_globals); File ""/home/edmund/.local/src/hail/test.py"", line 10, in <module>; expr = hl.any(lambda x:; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/functions.py"", line 3530, in any; collection = arg_check(ar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:3427,adapter,adapter,3427,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['adapter'],['adapter']
Integrability,debatable. I think it's useful to know this but maybe better conceived as a log message. Let's close and reconsider when its a problem again.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12984#issuecomment-1550324359:80,message,message,80,https://hail.is,https://github.com/hail-is/hail/pull/12984#issuecomment-1550324359,1,['message'],['message']
Integrability,depends on #3377,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3381#issuecomment-381608025:0,depend,depends,0,https://hail.is,https://github.com/hail-is/hail/pull/3381#issuecomment-381608025,1,['depend'],['depends']
Integrability,"depends on your definition of ""in"". 0.0 does compare equal to -0.0 with `==`, but not `java.lang.Double.compare`. Anyhow, this can probably be fixed with a hack.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846#issuecomment-483906188:0,depend,depends,0,https://hail.is,https://github.com/hail-is/hail/issues/5846#issuecomment-483906188,1,['depend'],['depends']
Integrability,"dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * **#14691** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14691?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14690** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14690?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14686** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14686?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14684** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14684?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 1 other dependent PR ([#14747](https://github.com/hail-is/hail/pull/14747) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14747?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14683** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14683?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @ehigham and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14692#issuecomment-2358958754:3080,depend,dependent,3080,https://hail.is,https://github.com/hail-is/hail/pull/14692#issuecomment-2358958754,2,['depend'],"['dependencies', 'dependent']"
Integrability,"dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14691** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14691?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * **#14690** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14690?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14686** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14686?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14684** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14684?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 1 other dependent PR ([#14747](https://github.com/hail-is/hail/pull/14747) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14747?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14683** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14683?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @ehigham and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14691#issuecomment-2357221524:3080,depend,dependent,3080,https://hail.is,https://github.com/hail-is/hail/pull/14691#issuecomment-2357221524,2,['depend'],"['dependencies', 'dependent']"
Integrability,"dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14691** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14691?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14690** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14690?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * **#14686** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14686?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14684** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14684?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 1 other dependent PR ([#14747](https://github.com/hail-is/hail/pull/14747) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14747?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14683** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14683?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @ehigham and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14690#issuecomment-2356990835:3080,depend,dependent,3080,https://hail.is,https://github.com/hail-is/hail/pull/14690#issuecomment-2356990835,2,['depend'],"['dependencies', 'dependent']"
Integrability,"dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14691** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14691?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14690** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14690?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14686** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14686?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * **#14684** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14684?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 1 other dependent PR ([#14747](https://github.com/hail-is/hail/pull/14747) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14747?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14683** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14683?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @ehigham and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14686#issuecomment-2354274670:3080,depend,dependent,3080,https://hail.is,https://github.com/hail-is/hail/pull/14686#issuecomment-2354274670,2,['depend'],"['dependencies', 'dependent']"
Integrability,"does that mean the genome reference is getting parsed once per evaluation? That's a pretty expensive thing to do. I'm OK with going this route for now, but we should make a list of things that we need to fix asap when our infrastructure allows.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3357#issuecomment-380969334:137,rout,route,137,https://hail.is,https://github.com/hail-is/hail/pull/3357#issuecomment-380969334,1,['rout'],['route']
Integrability,"e an image build step feels wrong to me. Each step creates a layer which inflates the image sizes. Hail's images are already too big!. I took your commits and added one of my own that snags the version from the `copy_files` build step. Because I wanted `service_base_image` to depend on `copy_files`, I had to move the whole step after `copy_files`. This is a limitation in build.yaml: a step must appear *after* steps on which it depends. I also had to move `check_services` for the same reason: it depends on `service_base_image`. File dependencies in build.yaml work like this:; 1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This also means all hailtop installs need a hail_version file. I only found two other places that use hailtop. One of them was a completely unused Dockerfile. I deleted that (`Dockerfile.hailtop`). The other was Dockerfile.ci",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401:1292,depend,dependsOn,1292,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401,1,['depend'],['dependsOn']
Integrability,"e(MakeTuple([ir]), timed=timed)[0] for ir in irs]; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/py4j_backend.py"", line 94, in execute; jir = self._to_java_value_ir(ir); File ""/home/edmund/.local/src/hail/hail/python/hail/backend/spark_backend.py"", line 280, in _to_java_value_ir; return self._to_java_ir(ir, self._parse_value_ir); File ""/home/edmund/.local/src/hail/hail/python/hail/backend/spark_backend.py"", line 276, in _to_java_ir; ir._jir = parse(r(finalize_randomness(ir)), ir_map=r.jirs); File ""/home/edmund/.local/src/hail/hail/python/hail/backend/spark_backend.py"", line 245, in _parse_value_ir; return self._jbackend.parse_value_ir(; File ""/home/edmund/.local/src/hail/.venv/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1304, in __call__; return_value = get_return_value(; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/py4j_backend.py"", line 21, in deco; return f(*args, **kwargs); File ""/home/edmund/.local/src/hail/.venv/lib/python3.8/site-packages/py4j/protocol.py"", line 326, in get_return_value; raise Py4JJavaError(; py4j.protocol.Py4JJavaError: An error occurred while calling o1.parse_value_ir.; : java.util.NoSuchElementException: key not found: __uid_4; at scala.collection.immutable.Map$Map1.apply(Map.scala:114); at is.hail.expr.ir.Env.apply(Env.scala:128); at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:890); at is.hail.expr.ir.IRParser$.$anonfun$ir_value_expr$1(Parser.scala:820); at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); at is.hail.utils.StackSafe$.run(StackSafe.scala:16); at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); at is.hail.expr.ir.IRParser$.$anonfun$parse_value_ir$1(Parser.scala:2072); at is.hail.expr.ir.IRParser$.parse(Parser.scala:2068); at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:2072); at is.hail.backend.spark.SparkBackend.$anonfun$parse_value_ir$2(SparkBackend.scala:710); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); at i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:9800,protocol,protocol,9800,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['protocol'],['protocol']
Integrability,"e, cores_mcpu, memory_bytes, storage_bytes, preemptible; + ):; for pool in self.name_pool_config.values():; - if pool.cloud == cloud and pool.worker_type == worker_type and pool.preemptible == preemptible and pool.label == pool_label:; + if (; + pool.cloud == cloud; + and pool.worker_type == worker_type; + and pool.preemptible == preemptible; + and pool.label == pool_label; + ):; result = pool.convert_requests_to_resources(cores_mcpu, memory_bytes, storage_bytes); if result:; actual_cores_mcpu, actual_memory_bytes, acutal_storage_gib = result; return (pool.name, actual_cores_mcpu, actual_memory_bytes, acutal_storage_gib); return None; @@ -322,11 +329,19 @@; if self.jpim_config.cloud != cloud:; return None; return self.jpim_config.convert_requests_to_resources(machine_type, storage_bytes); ; def select_inst_coll(; - self, cloud, machine_type, pool_label, preemptible, worker_type, req_cores_mcpu, req_memory_bytes, req_storage_bytes; + self,; + cloud,; + machine_type,; + pool_label,; + preemptible,; + worker_type,; + req_cores_mcpu,; + req_memory_bytes,; + req_storage_bytes,; ):; if worker_type is not None and machine_type is None:; result = self.select_pool_from_worker_type(; cloud=cloud,; pool_label=pool_label,; would reformat batch/inst_coll_config.py; --- batch/front_end/front_end.py	2022-06-01 03:13:40.462017 +0000; +++ batch/front_end/front_end.py	2022-06-01 03:15:42.270698 +0000; @@ -838,11 +838,18 @@; ); ; inst_coll_configs: InstanceCollectionConfigs = app['inst_coll_configs']; ; result, exc = inst_coll_configs.select_inst_coll(; - cloud, machine_type, pool_label, preemptible, worker_type, req_cores_mcpu, req_memory_bytes, req_storage_bytes; + cloud,; + machine_type,; + pool_label,; + preemptible,; + worker_type,; + req_cores_mcpu,; + req_memory_bytes,; + req_storage_bytes,; ); ; if exc:; raise web.HTTPBadRequest(reason=exc.message); ; would reformat batch/front_end/front_end.py. Oh no!   ; 2 files would be reformatted, 89 files would be left unchanged.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11879#issuecomment-1144135068:2677,message,message,2677,https://hail.is,https://github.com/hail-is/hail/pull/11879#issuecomment-1144135068,1,['message'],['message']
Integrability,"e.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14691** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14691?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14690** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14690?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14686** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14686?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14684** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14684?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 1 other dependent PR ([#14747](https://github.com/hail-is/hail/pull/14747) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14747?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14683** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14683?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @ehigham and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14693#issuecomment-2362149504:3080,depend,dependent,3080,https://hail.is,https://github.com/hail-is/hail/pull/14693#issuecomment-2362149504,12,['depend'],"['dependencies', 'dependent']"
Integrability,e.read(StorageByteChannels.java:242) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.ApiaryUnbufferedReadableByteChannel.read(ApiaryUnbufferedReadableByteChannel.java:113) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.UnbufferedReadableByteChannelSession$UnbufferedReadableByteChannel.read(UnbufferedReadableByteChannelSession.java:31) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.DefaultBufferedReadableByteChannel.read(DefaultBufferedReadableByteChannel.java:81) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.StorageByteChannels$SynchronizedBufferedReadableByteChannel.read(StorageByteChannels.java:84) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.BaseStorageReadChannel.read(BaseStorageReadChannel.java:105) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.GoogleStorageFS$$anon$1.readHandlingRequesterPays(GoogleStorageFS.scala:216) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.GoogleStorageFS$$anon$1.fill(GoogleStorageFS.scala:245) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.FSSeekableInputStream.read(FS.scala:168) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at java.io.DataInputStream.re,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:2708,Synchroniz,SynchronizedBufferedReadableByteChannel,2708,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,2,['Synchroniz'],['SynchronizedBufferedReadableByteChannel']
Integrability,"e/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 1008, in count; return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum(); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 999, in sum; return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 873, in fold; vals = self.mapPartitions(func).collect(); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 776, in collect; port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd()); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/sql/utils.py"", line 63, in deco; return f(*a, **kw); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.; : org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/hail/test/BRCA1.raw_indel.vcf; 	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285); 	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228); 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313); 	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	at org.apache.spark.rdd.RDD$$anonfun$p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-322349367:2533,protocol,protocol,2533,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-322349367,1,['protocol'],['protocol']
Integrability,eChannel.java:65) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.UnbufferedWritableByteChannelSession$UnbufferedWritableByteChannel.writeAndClose(UnbufferedWritableByteChannelSession.java:40) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.DefaultBufferedWritableByteChannel.close(DefaultBufferedWritableByteChannel.java:166) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.StorageByteChannels$SynchronizedBufferedWritableByteChannel.close(StorageByteChannels.java:119) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.StorageException.wrapIOException(StorageException.java:179) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.BaseStorageWriteChannel.close(BaseStorageWriteChannel.java:84) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$close$2(GoogleStorageFS.scala:310) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:280) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$close$1(GoogleStorageFS.scala:310) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at scala.runtime.java8.JFunction0,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:12167,wrap,wrapIOException,12167,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['wrap'],['wrapIOException']
Integrability,"ePath: str, partitionCounts: int64}),filePath))),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/rows,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(idx),JArray(List(JObject(List((start,JObject(List((idx,JInt(0))))), (end,JObject(List((idx,JInt(10))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(10))))), (end,JObject(List((idx,JInt(20))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(20))))), (end,JObject(List((idx,JInt(30))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(30))))), (end,JObject(List((idx,JInt(40))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(40))))), (end,JObject(List((idx,JInt(50))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(50))))), (end,JObject(List((idx,JInt(60))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(60))))), (end,JObject(List((idx,JInt(70))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(70))))), (end,JObject(List((idx,JInt(80))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(80))))), (end,JObject(List((idx,JInt(90))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(90))))), (end,JObject(List((idx,JInt(100))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))))),IndexSpec2(../index,{""name"":""TypedCodecSpec"",""_eType"":""EBaseStruct{first_idx:+EInt64,keys:+EArray[+EBaseSt",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:5736,Wrap,WrappedArray,5736,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693,2,['Wrap'],['WrappedArray']
Integrability,"eadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 03:59:30,730 | server.py | set_state:141 | job 153 changed state: Created -> Complete; INFO | 2018-10-23 03:59:30,730 | server.py | mark_complete:184 | job 153 complete, exit_code 2; INFO | 2018-10-23 03:59:30,737 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,806 | _internal.py | _log:88 | 10.56.142.33 - - [23/Oct/2018 03:59:30] ""GET /jobs HTTP/1.1"" 200 -; INFO | 2018-10-23 03:59:30,815 | _internal.py | _",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:1717,adapter,adapter,1717,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038,1,['adapter'],['adapter']
Integrability,"eam$1(EmitStream.scala:850). ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TContainer]) {; ToStream(node); } else {; node; }; }; }; }; ````. with . ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TContainer]) {; ToStream(node); } else {; node; }; }; }; }. 4 failures in IRSuite, again testArrayAggScan:. Before Lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). After lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(Cal",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:7363,Wrap,WrappedArray,7363,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113,2,['Wrap'],['WrappedArray']
Integrability,"ebook', request, userdata)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 230, in POST\n body=body))\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 181, in request\n raise ApiException(http_resp=r)\nkubernetes_asyncio.client.rest.ApiException: (422)\nReason: Unprocessable Entity\nHTTP response headers: <CIMultiDictProxy('Audit-Id': '16158e81-8543-457e-8bea-5d5e1a8c39f3', 'Content-Type': 'application/json', 'Date': 'Sun, 29 Sep 2019 03:43:05 GMT', 'Content-Length': '901')>\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""Pod \\\""notebook-worker-9z6tg\\\"" is invalid: [spec.volumes[1].secret.secretName: Required value, spec.volumes[2].secret.secretName: Required value, spec.containers[0].volumeMounts[1].name: Not found: \\\""gsa-key\\\"", spec.containers[0].volumeMounts[2].name: Not found: \\\""user-tokens\\\""]\"",\""reason\"":\""Invalid\"",\""details\"":{\""name\"":\""notebook-worker-9z6tg\"",\""kind\"":\""Pod\"",\""causes\"":[{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[1].secret.secretName\""},{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[2].secret.secretName\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""gsa-key\\\""\"",\""field\"":\""spec.containers[0].volumeMounts[1].name\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""user-tokens\\\""\"",\""fiel",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:2560,message,message,2560,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851,1,['message'],['message']
Integrability,"ecur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using the stream interface seems wrong to me also. When I mentioned this, I was thinking we could reuse the region management logic from the stream emitter ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:2259,wrap,wrap,2259,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407,1,['wrap'],['wrap']
Integrability,eduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:944); at is.hail.expr.ir.functions.MatrixWriteBlockMatrix.execute(MatrixWriteBlockMatrix.scala:47); at is.hail.expr.ir.functions.WrappedMatrixToValueFunction.execute(RelationalFunctions.scala:88); at is.hail.expr.ir.Interpret$.run(Interpret.scala:735); at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:14200,Wrap,WrappedMatrixToValueFunction,14200,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Wrap'],['WrappedMatrixToValueFunction']
Integrability,"el; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## 2I installed the atlas-devel , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https://github.com/broadinstitute/hail/files/417544/gradle_check_i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1591,Depend,Dependencies,1591,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,1,['Depend'],['Dependencies']
Integrability,"ent/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleClient` objects around to access old data.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:1725,interface,interfaces---the,1725,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052,1,['interface'],['interfaces---the']
Integrability,"entException: key not found: RefEquality(WriteMetadata(Let(__iruid_369,CollectDistributedArray(StreamZip(ArrayBuffer(MakeStream(ArrayBuffer(Literal(struct{start: int32, end: int32},[0,10]), Literal(struct{start: int32, end: int32},[10,20]), Literal(struct{start: int32, end: int32},[20,30]), Literal(struct{start: int32, end: int32},[30,40]), Literal(struct{start: int32, end: int32},[40,50]), Literal(struct{start: int32, end: int32},[50,60]), Literal(struct{start: int32, end: int32},[60,70]), Literal(struct{start: int32, end: int32},[70,80]), Literal(struct{start: int32, end: int32},[80,90]), Literal(struct{start: int32, end: int32},[90,100])),stream<struct{start: int32, end: int32}>,false), MakeStream(ArrayBuffer(Str(""part-00-""), Str(""part-01-""), Str(""part-02-""), Str(""part-03-""), Str(""part-04-""), Str(""part-05-""), Str(""part-06-""), Str(""part-07-""), Str(""part-08-""), Str(""part-09-"")),stream<str>,false)),ArrayBuffer(__iruid_372, __iruid_373),MakeStruct(ArrayBuffer((oldCtx,Ref(__iruid_372,struct{start: int32, end: int32})), (writeCtx,Ref(__iruid_373,str)))),AssertSameLength),Literal(struct{},[]),__iruid_370,__iruid_371,WritePartition(Let(__iruid_374,GetField(Ref(__iruid_370,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),oldCtx),StreamMap(StreamRange(GetField(Ref(__iruid_374,struct{start: int32, end: int32}),start),GetField(Ref(__iruid_374,struct{start: int32, end: int32}),end),I32(1),false),__iruid_375,MakeStruct(ArrayBuffer((idx,Ref(__iruid_375,int32)))))),Apply(concat,WrappedArray(),ArrayBuffer(GetField(Ref(__iruid_370,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),writeCtx), UUID4(__iruid_290)),str),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:3780,Wrap,WrappedArray,3780,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601,1,['Wrap'],['WrappedArray']
Integrability,"entException: key not found: RefEquality(WriteMetadata(Let(__iruid_465,CollectDistributedArray(StreamZip(ArrayBuffer(MakeStream(ArrayBuffer(Literal(struct{start: int32, end: int32},[0,10]), Literal(struct{start: int32, end: int32},[10,20]), Literal(struct{start: int32, end: int32},[20,30]), Literal(struct{start: int32, end: int32},[30,40]), Literal(struct{start: int32, end: int32},[40,50]), Literal(struct{start: int32, end: int32},[50,60]), Literal(struct{start: int32, end: int32},[60,70]), Literal(struct{start: int32, end: int32},[70,80]), Literal(struct{start: int32, end: int32},[80,90]), Literal(struct{start: int32, end: int32},[90,100])),stream<struct{start: int32, end: int32}>,false), MakeStream(ArrayBuffer(Str(""part-00-""), Str(""part-01-""), Str(""part-02-""), Str(""part-03-""), Str(""part-04-""), Str(""part-05-""), Str(""part-06-""), Str(""part-07-""), Str(""part-08-""), Str(""part-09-"")),stream<str>,false)),ArrayBuffer(__iruid_468, __iruid_469),MakeStruct(ArrayBuffer((oldCtx,Ref(__iruid_468,struct{start: int32, end: int32})), (writeCtx,Ref(__iruid_469,str)))),AssertSameLength),Literal(struct{},[]),__iruid_466,__iruid_467,WritePartition(Let(__iruid_470,GetField(Ref(__iruid_466,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),oldCtx),StreamMap(StreamRange(GetField(Ref(__iruid_470,struct{start: int32, end: int32}),start),GetField(Ref(__iruid_470,struct{start: int32, end: int32}),end),I32(1),false),__iruid_471,MakeStruct(ArrayBuffer((idx,Ref(__iruid_471,int32)))))),Apply(concat,WrappedArray(),ArrayBuffer(GetField(Ref(__iruid_466,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),writeCtx), UUID4(__iruid_386)),str),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:3500,Wrap,WrappedArray,3500,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693,1,['Wrap'],['WrappedArray']
Integrability,"epUntil(Parser.scala:301); E 	at is.hail.expr.ir.IRParser$.ir_value_children(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:1084); E 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:680); E 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1591); E 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:1596); E 	at is.hail.expr.ir.IRParser.parse_value_ir(Parser.scala); E 	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E 	at py4j.Gateway.invoke(Gateway.java:282); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCommand.java:79); E 	at py4j.GatewayConnection.run(GatewayConnection.java:238); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.NoSuchElementException: next on empty iterator; E 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:39); E 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:37); E 	at scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63); E 	at scala.collection.IterableLike$class.head(IterableLike.scala:107); E 	at scala.collection.mutable.WrappedArray.scala$collection$IndexedSeqOptimized$$super$head(WrappedArray.scala:35); E 	at scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:126); E 	at scala.collection.mutable.WrappedArray.head(WrappedArray.scala:35); E 	at is.hail.expr.ir.InferType$.apply(InferType.scala:30). """""". will check tomorrow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:5835,Wrap,WrappedArray,5835,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930,4,['Wrap'],['WrappedArray']
Integrability,"estMethod=test_array_methods>. def test_array_methods(self):; self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5])), False); self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5, 6])), True); ; self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in parse; ir_map); /miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py:1257: in __call__; answer, self.gateway_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. args = ('",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:1329,wrap,wrapper,1329,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930,1,['wrap'],['wrapper']
Integrability,"est_context.py E. ============================================================================================== ERRORS ==============================================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend); hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:99: in __init__; min_block_size, branching_factor, tmp_dir, optimizer_iterations); /miniconda3/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py:1257: in __call__; answer, self.gateway_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . answer = 'xspy4j.Py4JException: Method apply([null, class java.lang.String, class scala.Some, class java.lang.String, class jav...java:79)\\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\tat java.lang.Thread.run(Thread.java:748)\\n'; gateway_client = <py4j.java_gateway.GatewayClient object at 0x11098bc50>, target_id = 'z:is.hail.HailContext', name = 'apply'. def ge",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:1888,wrap,wrapper,1888,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928,1,['wrap'],['wrapper']
Integrability,"estion, note that the Parquet handling has been rewritten since then, see https://issues.apache.org/jira/browse/SPARK-9095):. ```scala; scala> import org.apache.spark.sql._; import org.apache.spark.sql._. scala> import org.apache.spark.sql.types._; import org.apache.spark.sql.types._. scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc); warning: there was one deprecation warning; re-run with -deprecation for details; sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@625f5712. scala> val schema = StructType(Seq(StructField(""foo"", IntegerType, false))); schema: org.apache.spark.sql.types.StructType = StructType(StructField(foo,IntegerType,false)). scala> val df1 = sqlContext.createDataFrame(sc.parallelize(Array(Row(1))), schema); df1: org.apache.spark.sql.DataFrame = [foo: int]. scala> df1.printSchema; root; |-- foo: integer (nullable = false). scala> df1.write.parquet(""temp.df1""); ; scala> val df2 = sqlContext.read.parquet(""temp.df1""); df2: org.apache.spark.sql.DataFrame = [foo: int]. scala> df2.printSchema; root; |-- foo: integer (nullable = true); ```. Then. ```bash; parquet-tools schema part-r-00000-94aa6aa3-4799-4b78-9717-5397c8e983f9.snappy.parquet; message spark_schema {; required int32 foo;; }; ```. Which shows that the field is `required`, not `optional`. Also. ```bash; parquet-tools meta part-r-00000-94aa6aa3-4799-4b78-9717-5397c8e983f9.snappy.parquet; creator: parquet-mr version 1.5.0-cdh5.7.0 (build ${buildNumber}) ; extra: org.apache.spark.sql.parquet.row.metadata = {""type"":""struct"",""fields"":[{""name"":""foo"",""type"":""integer"",""nullable"":false,""metadata"":{}}]} . file schema: spark_schema ; -------------------------------------------------------------------------------------------------------------------------------------------------------------------; foo: REQUIRED INT32 R:0 D:0; ```. Which shows that the Spark SQL schema is stored in the Parquet metadata. So, looks like a bug. Is this causing correctness/perf problems?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1421#issuecomment-281967861:1425,message,message,1425,https://hail.is,https://github.com/hail-is/hail/pull/1421#issuecomment-281967861,1,['message'],['message']
Integrability,"esult_tuple = self._jbackend.executeEncode(jir, stream_codec); ---> 99 (result, timings) = (result_tuple._1(), result_tuple._2()); 100 value = ir.typ._from_encoding(result); 102 return (value, timings) if timed else value. File ~/mambaforge/lib/python3.9/site-packages/py4j/java_gateway.py:1304, in JavaMember.__call__(self, *args); 1298 command = proto.CALL_COMMAND_NAME +\; 1299 self.command_header +\; 1300 args_command +\; 1301 proto.END_COMMAND_PART; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1307 for temp_arg in temp_args:; 1308 temp_arg._detach(). File ~/mambaforge/lib/python3.9/site-packages/hail/backend/py4j_backend.py:21, in handle_java_exception.<locals>.deco(*args, **kwargs); 19 import pyspark; 20 try:; ---> 21 return f(*args, **kwargs); 22 except py4j.protocol.Py4JJavaError as e:; 23 s = e.java_exception.toString(). File ~/mambaforge/lib/python3.9/site-packages/py4j/protocol.py:330, in get_return_value(answer, gateway_client, target_id, name); 326 raise Py4JJavaError(; 327 ""An error occurred while calling {0}{1}{2}.\n"".; 328 format(target_id, ""."", name), value); 329 else:; --> 330 raise Py4JError(; 331 ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; 332 format(target_id, ""."", name, value)); 333 else:; 334 raise Py4JError(; 335 ""An error occurred while calling {0}{1}{2}"".; 336 format(target_id, ""."", name)). Py4JError: An error occurred while calling o83._1. Trace:; java.lang.NegativeArraySizeException: -1966455376; 	at py4j.Base64.encodeToChar(Base64.java:681); 	at py4j.Base64.encodeToString(Base64.java:734); 	at py4j.Protocol.encodeBytes(Protocol.java:154); 	at py4j.ReturnObject.getPrimitiveReturnObject(ReturnObject.java:150); 	at py4j.Gateway.getReturnObject(Gateway.java:188); 	at py4j.Gateway.invoke(Gateway.java:283); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12035#issuecomment-1186014691:2946,protocol,protocol,2946,https://hail.is,https://github.com/hail-is/hail/issues/12035#issuecomment-1186014691,1,['protocol'],['protocol']
Integrability,"et.py in linreg(self, y, covariates, root, min_ac, min_af); 2216 """"""; 2217; -> 2218 jvds = self._jvdf.linreg(y, jarray(env.jvm.java.lang.String, covariates), root, min_ac, min_af); 2219 return VariantDataset(self.hc, jvds); 2220. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 113 if e.args[0].startswith('An error occurred while calling'):; 114 msg = 'An error occurred while calling into JVM, probably due to invalid parameter types'; --> 115 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (msg, e.message, msg)); 116; 117 return deco. FatalError: An error occurred while calling into JVM, probably due to invalid parameter types. Java stack trace:; An error occurred while calling o29.linreg. Trace:; py4j.Py4JException: Method linreg([class java.util.ArrayList, class [Ljava.lang.String;, class java.lang.String, class java.lang.Integer, class java.lang.Double]) does not exist; 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); 	at py4j.Gateway.invoke(Gateway.java:272); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: An error occurred while calling into JVM, probably due to invalid parameter types```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787:1383,message,message,1383,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787,1,['message'],['message']
Integrability,"expr, k, compute_loadings). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/methods/pca.py in hwe_normalized_pca(call_expr, k, compute_loadings); 99 return _hwe_normalized_blanczos(call_expr, k, compute_loadings); 100 ; --> 101 return pca(hwe_normalize(call_expr),; 102 k,; 103 compute_loadings). <decorator-gen-1780> in pca(entry_expr, k, compute_loadings). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/methods/pca.py in pca(entry_expr, k, compute_loadings); 209 'k': k,; 210 'computeLoadings': compute_loadings; --> 211 })).persist()); 212 ; 213 g = t.index_globals(). <decorator-gen-1340> in persist(self, storage_level). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in persist(self, storage_level); 2110 Persisted table.; 2111 """"""; -> 2112 return Env.backend().persist(self); 2113 ; 2114 def unpersist(self) -> 'Table':. /opt/conda/miniconda3/lib/python3.1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:2529,wrap,wrapper,2529,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,1,['wrap'],['wrapper']
Integrability,"fields: user-specified str; 3 fields: user-specified float64; 1 field: imputed int32; (63110, 64048); running omit filter; [Stage 1:> (0 + 1) / 1](63110, 52877); running pc_relate; [Stage 3:==================================================>(12794 + 2) / 12796]2020-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: running PCA using 63110 variants.; [Stage 5:==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:5537,wrap,wrapper,5537,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['wrap'],['wrapper']
Integrability,"fix looks good, want a changelog message tho",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8876#issuecomment-634996787:33,message,message,33,https://hail.is,https://github.com/hail-is/hail/pull/8876#issuecomment-634996787,1,['message'],['message']
Integrability,"forge/lib/python3.9/site-packages/hail/typecheck/check.py:577, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 574 @decorator; 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_). File ~/mambaforge/lib/python3.9/site-packages/hail/table.py:3340, in Table.to_pandas(self, flatten); 3338 dtypes_struct = table.row.dtype; 3339 collect_dict = {key: hl.agg.collect(value) for key, value in table.row.items()}; -> 3340 column_struct_array = table.aggregate(hl.struct(**collect_dict)); 3341 columns = list(column_struct_array.keys()); 3342 data_dict = {}. File <decorator-gen-1037>:2, in aggregate(self, expr, _localize). File ~/mambaforge/lib/python3.9/site-packages/hail/typecheck/check.py:577, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 574 @decorator; 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_). File ~/mambaforge/lib/python3.9/site-packages/hail/table.py:1231, in Table.aggregate(self, expr, _localize); 1228 agg_ir = ir.TableAggregate(base._tir, expr._ir); 1230 if _localize:; -> 1231 return Env.backend().execute(hl.ir.MakeTuple([agg_ir]))[0]; 1233 return construct_expr(ir.LiftMeOut(agg_ir), expr.dtype). File ~/mambaforge/lib/python3.9/site-packages/hail/backend/py4j_backend.py:99, in Py4JBackend.execute(self, ir, timed); 97 try:; 98 result_tuple = self._jbackend.executeEncode(jir, stream_codec); ---> 99 (result, timings) = (result_tuple._1(), result_tuple._2()); 100 value = ir.typ._from_encoding(result); 102 return (value, timings) if timed else value. File ~/mambaforge/lib/python3.9/site-packages/py4j/java_gateway.py:1304, in JavaMember.__call__(self, *args); 1298 command = proto.CALL_COMMAND_NAME +\; 1299 self.command_header +\; 1300 args_command +\;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12035#issuecomment-1186014691:1238,wrap,wrapper,1238,https://hail.is,https://github.com/hail-is/hail/issues/12035#issuecomment-1186014691,2,['wrap'],['wrapper']
Integrability,"form... using builtin-java classes where applicable; 18/01/08 13:51:03 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).; 18/01/08 13:51:03 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 18/01/08 13:51:03 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 13:51:03 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 13:51:03 WARN Utils: Your hostname, <my computer name> resolves to a loopback address: <my local IP>; using <my IP> instead (on interface enp3s0); 18/01/08 13:51:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; ```. And the other initialize hail like this (crashes with the stack trace/error in the issue):; ```; from pyspark import *; from hail import *; conf = SparkConf(); conf.set('spark.sql.files.maxPartitionBytes','60000000000') ; conf.set('spark.sql.files.openCostInBytes','60000000000') ; conf.set('spark.driver.cores','1') #test with 1 core; sc = SparkContext(conf=conf); hc = HailContext(sc); ```. With startup messages looking like this:; ```; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/01/08 15:16:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/01/08 15:16:23 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:6978,interface,interface,6978,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['interface'],['interface']
Integrability,"func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals); 1329 ; 1330 if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; -> 1331 self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); 1332 _assert_type = self._type; 1333 _load_refs = False. <decorator-gen-1332> in write(self, output, overwrite, stage_locally, _codec_spec). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in write(self, output, overwrite, stage_locally, _codec_spec); 1375 hl.current_backend().validate_file_scheme(output); 1376 ; -> 1377 Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); 1378 ; 1379 @typecheck_method(output=str,. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 80 return (value, timings) if timed else value; 81 except FatalError as e:; ---> 82 raise e.maybe_user_error(ir) from None; 83 ; 84 async def _async_execute(self, ir, timed=False):. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/backend/py4j_backend.py in execute(self, ir, timed); 74 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 75 try:; ---> 76 result_tuple = self._jbackend.executeEncode(jir, strea",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:5155,wrap,wrapper,5155,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,1,['wrap'],['wrapper']
Integrability,"g_check; raise TypeError(""{fname}: parameter '{argname}': ""; TypeError: any: parameter 'collection': expected expression of type set<any> or array<any>, found list: [['10', 123, 'G', 'C'], ['10', 456, 'T', 'A']]; ```; So, hail doesn't support heterogeneous arrays. Converting to a homogeneous array:. ```python; variants = [(""10"", 123, [""G"", ""C""]), (""10"", 456, [""T"", ""A""])]. expr = hl.any(; lambda x:; (mt.locus.contig == hl.literal(x[0])) & \; (mt.locus.position == hl.literal(int(x[1]))) & \; (mt.alleles == hl.literal(x[2])),; variants; ). hl.eval(expr). ```; Leads to the following error (which looks like the bug!):; ```; Traceback (most recent call last):; File ""test.py"", line 10, in <module>; expr = hl.any(lambda x:; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/functions.py"", line 3531, in any; return collection.any(f); File ""<decorator-gen-510>"", line 2, in any; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 68, in any; return hl.array(self).fold(lambda accum, elt: accum | f(elt), False); File ""<decorator-gen-518>"", line 2, in fold; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 221, in fold; return collection._to_stream().fold(lambda x, y: f(x, y), zero); File ""<decorator-gen-650>"", line 2, in fold; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 4522, in fold; body = to_expr(f(accum_ref, elt_ref)); File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 364, in f; ret = x(*args)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:5522,wrap,wrapper,5522,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['wrap'],['wrapper']
Integrability,"gies are in order, I totally lead you astray by mentioning the makefile. The Makefile *is* the source of hail version truth, but invoking the makefile inside an image build step feels wrong to me. Each step creates a layer which inflates the image sizes. Hail's images are already too big!. I took your commits and added one of my own that snags the version from the `copy_files` build step. Because I wanted `service_base_image` to depend on `copy_files`, I had to move the whole step after `copy_files`. This is a limitation in build.yaml: a step must appear *after* steps on which it depends. I also had to move `check_services` for the same reason: it depends on `service_base_image`. File dependencies in build.yaml work like this:; 1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This also means all hailtop installs need a hail_version file. I only ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401:1100,depend,dependsOn,1100,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401,1,['depend'],['dependsOn']
Integrability,"gl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: konradk-gsa-key; Optional: false; batch-2554-job-4-8vvgl:; Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-2554-job-4-8vvgl; ReadOnly: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 11m default-scheduler Successfully assigned batch-pods/batch-2554-job-4-main-vsk7h to gke-vdc-preemptible-pool-9c7148b2-4gq2; Warning FailedMount 36s (x5 over 9m46s) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; + kubectl describe pvc -n batch-pods batch-2554-job-4-8vvgl; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Annotations: pv.kubernetes.io/bind-completed: yes; pv.kubernetes.io/bound-by-controller: yes; volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd; Finalizers: [kubernetes.io/pvc-protection]; Capacity: 1Gi; Access Modes: RWO; VolumeMode: Filesystem; Events: <none>; Mounted By: batch-2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:19020,Message,Message,19020,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['Message'],['Message']
Integrability,"google died:; ```; E Log:	{'main': ""ERROR: (gcloud.auth.activate-service-account) There was a problem refreshing your current auth tokens: {u'status': u'UNAVAILABLE', u'message': u'The service is currently unavailable.', u'code': 503}\nPlease run:\n\n $ gcloud auth login\n\nto obtain new credentials, or if you have already logged in with a\ndifferent account:\n\n $ gcloud config set account ACCOUNT\n\nto select an already authenticated account to use.\n""}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5934#issuecomment-486735019:169,message,message,169,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-486735019,1,['message'],['message']
Integrability,"h the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using the stream interface seems wrong to me also. When I mentioned this, I was thinking we could reuse the region management logic from the stream emitter for loops, but I've since changed my mind. I think loops will have hard region management no matter what. > What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. As a side note, @iitalics stream emitter can handle streams of multiple values fine. Effectively, you can make a `Stream[(Code[A], Code[B], Code[C])]`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:2874,interface,interface,2874,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407,2,['interface'],['interface']
Integrability,"hail 0.2.33, python 3.7.3. `hl.init(sc); Traceback (most recent call last):; File ""<console>"", line 1, in <module>; File ""<decorator-gen-1214>"", line 2, in init; File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/context.py"", line 290, in init; _optimizer_iterations,_backend); File ""<decorator-gen-1212>"", line 2, in __init__; File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/context.py"", line 121, in __init__; min_block_size, branching_factor, tmp_dir, optimizer_iterations); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1286, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/transforms/_java_utils.py"", line 237, in wrapper; return func(*args, **kwargs); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/py4j/protocol.py"", line 332, in get_return_value; format(target_id, ""."", name, value)); py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; py4j.Py4JException: Method apply([class org.apache.spark.SparkContext, class java.lang.String, class scala.None$, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8423#issuecomment-607430617:303,wrap,wrapper,303,https://hail.is,https://github.com/hail-is/hail/issues/8423#issuecomment-607430617,5,"['protocol', 'wrap']","['protocol', 'wrapper']"
Integrability,"haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions: Int, shuffle: Boolean)(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:1859,depend,dependencies,1859,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,1,['depend'],['dependencies']
Integrability,"hat seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I manually deploy and now everyone is verifying client certificates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:3392,rout,router-resolver,3392,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,1,['rout'],['router-resolver']
Integrability,"heck_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in persist(self, storage_level); 2110 Persisted table.; 2111 """"""; -> 2112 return Env.backend().persist(self); 2113 ; 2114 def unpersist(self) -> 'Table':. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py in persist(self, dataset); 167 from hail.context import TemporaryFilename; 168 tempfile = TemporaryFilename(prefix=f'persist_{type(dataset).__name__}'); --> 169 persisted = dataset.checkpoint(tempfile.__enter__()); 170 self._persisted_locations[persisted] = (tempfile, dataset); 171 return persisted. <decorator-gen-1330> in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals); 1329 ; 1330 if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; -> 1331 self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); 1332 _assert_type = self._type; 1333 _load_refs = False. <decorator-gen-1332> in write(self, output, overwrite, stage_locally, _codec_spec). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__origina",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:3993,wrap,wrapper,3993,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,2,['wrap'],['wrapper']
Integrability,"heh, wow, the situation wrt kube-dns is pretty bad eh? https://github.com/kubernetes/kubernetes/issues/57659. My understanding is that the other kube-system pods tolerate every taint (by their special kube-system nature), so I think this argument only applies to kube-dns, is that your understanding?. We should stay alert to DNS issues, even with this change, because all our DNS queries will be routed to non-preemptible nodes (see: https://github.com/kubernetes/kubernetes/issues/57659#issuecomment-477009356).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7784#issuecomment-570278981:397,rout,routed,397,https://hail.is,https://github.com/hail-is/hail/pull/7784#issuecomment-570278981,1,['rout'],['routed']
Integrability,high level comment: all the places we call into the JVM need to be wrapped in `_raise_py4j_exception`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1116#issuecomment-262081893:67,wrap,wrapped,67,https://hail.is,https://github.com/hail-is/hail/pull/1116#issuecomment-262081893,1,['wrap'],['wrapped']
Integrability,"hink for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. ah, cool. > Is there a reason you didn't make it a subdomain? I thought we decided we; > preferred that. I thought it would take less time to get a subdirectory working than figure out; how to add a new domain and a cert and deal with DNS. Long term a subdomain makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:2509,rout,routes,2509,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878,1,['rout'],['routes']
Integrability,"hmm. I think this query locks more aggressively, not sure why. I guess the aggregation must be somewhat time consuming. In main, that aggregation happens without locking the billing projects (ergo preventing changes to limits). . I get these spammed in the front end logs:; ```; File ""/usr/local/lib/python3.7/dist-packages/batch/front_end/front_end.py"", line 2237, in post_edit_billing_limits; await _handle_api_error(_edit_billing_limit, db, billing_project, limit); File ""/usr/local/lib/python3.7/dist-packages/batch/front_end/front_end.py"", line 212, in _handle_api_error; await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/front_end/front_end.py"", line 2227, in _edit_billing_limit; await insert() # pylint: disable=no-value-for-parameter; File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 34, in wrapper; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 64, in wrapper; return await fun(tx, *args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/front_end/front_end.py"", line 2212, in insert; (billing_project,),; File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 209, in execute_and_fetchone; await cursor.execute(sql, args); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 469, in query; await self._read_query_result(unbuffered=unbuffered); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 672, in _read_query_result; await result.read(); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 1153, in read; first_packet = await self.connection._read_packet(); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 641, in _read_packet; packe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12652#issuecomment-1416434586:847,wrap,wrapper,847,https://hail.is,https://github.com/hail-is/hail/pull/12652#issuecomment-1416434586,2,['wrap'],['wrapper']
Integrability,"http performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :). P.S.; aiohttp has a place for optimization, we are working on it. There is no single bottleneck, the optimization requires careful improvements in many places, with keeping backward compatibility policy. I don't know the best criteria for choosing. My advice is: look on user API and choose what you like. P.P.S.; `Starlette` has own benefits and compromises as well, but the post is pretty huge already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:2146,rout,router,2146,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040,1,['rout'],['router']
Integrability,"http://blog.wang/wang/blog/, for instance, sends a request to the `blog` service in the `wang` namespace inside our k8s cluster, which is all handled by kubernetes. So the wait basically sends a GET request on the `/wang/blog/` endpoint directly to the `blog` service in that namespace, which never passes through either gateway or router.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548106312:332,rout,router,332,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548106312,1,['rout'],['router']
Integrability,"https://developers.google.com/identity/protocols/oauth2/web-server#exchange-authorization-code. So the google oauth flow example shows this when getting the token:. ```python3; flow.redirect_uri = flask.url_for('oauth2callback', _external=True); ```. Let me see if I can incorporate this behavior instead of what I did.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11147#issuecomment-994998700:39,protocol,protocols,39,https://hail.is,https://github.com/hail-is/hail/pull/11147#issuecomment-994998700,1,['protocol'],['protocols']
Integrability,https://zulipchat.com/api/send-message,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7889#issuecomment-574823386:31,message,message,31,https://hail.is,https://github.com/hail-is/hail/pull/7889#issuecomment-574823386,1,['message'],['message']
Integrability,"ile ""<stdin>"", line 1, in <module>; File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 1008, in count; return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum(); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 999, in sum; return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 873, in fold; vals = self.mapPartitions(func).collect(); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 776, in collect; port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd()); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/sql/utils.py"", line 63, in deco; return f(*a, **kw); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.; : org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/hail/test/BRCA1.raw_indel.vcf; 	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285); 	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228); 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313); 	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-322349367:2483,protocol,protocol,2483,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-322349367,1,['protocol'],['protocol']
Integrability,"iltering in various; ways, then that may not be changed at all by changes to what happen in the real analysis; after the filtering. And this is also influenced by the medium-term goal of having a Hail service; which (amongst other things) can do simple analyses on small data in under ten seconds - in; that realm compilation time could become a critical factor as a serial bottleneck. [The place where persistent cacheing of compiled files helps most of all is in testing,; where you really are running the exact same queries over and over again on the same; small datasets, and in many cases after making small changes which only affect a few; of the queries]. b) We may actually have some version of the locking problem even if we don't try to reuse the files -; since we have several workers on a node, and possibly a master as well, all needing to put code; into a file (or wait for someone else to populate the file) so that they can load it. Depending on ; precisely how Spark manages things (which I wouldn't want to depend on too much anyway). In fact it's essential that all the workers share the same DLL file, because otherwise they'd be; trying to load multiple DLL's defining the same symbols. That aspect of it could be handled by; putting the files into a per-process directory and using in-memory (std::mutex) synchronization.; But y'know, given that we have to write the DLL's out, it just seemed natural to let them persist; (and until debugging it on MacOS, I thought I could manage it with nothing but atomic file-create; and atomic-rename, but that didn't quite pan out). As for writing LLVM IR, it can definitely be done, because that's what Endeca/Oracle did. But there; was such a huge learning curve that only 3 people ever did it successfully (I wasn't one of them),; and debugging seemed very unpleasant and slow. [It was also a masterful achievement in ; job-security-through-obscurity, because no-one in management was going to mess with the; two people who wrote it - u",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385:1453,Depend,Depending,1453,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385,2,"['Depend', 'depend']","['Depending', 'depend']"
Integrability,"imits; await _handle_api_error(_edit_billing_limit, db, billing_project, limit); File ""/usr/local/lib/python3.7/dist-packages/batch/front_end/front_end.py"", line 212, in _handle_api_error; await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/front_end/front_end.py"", line 2227, in _edit_billing_limit; await insert() # pylint: disable=no-value-for-parameter; File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 34, in wrapper; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 64, in wrapper; return await fun(tx, *args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/front_end/front_end.py"", line 2212, in insert; (billing_project,),; File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 209, in execute_and_fetchone; await cursor.execute(sql, args); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 469, in query; await self._read_query_result(unbuffered=unbuffered); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 672, in _read_query_result; await result.read(); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 1153, in read; first_packet = await self.connection._read_packet(); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 641, in _read_packet; packet.raise_for_error(); File ""/usr/local/lib/python3.7/dist-packages/pymysql/protocol.py"", line 221, in raise_for_error; err.raise_mysql_exception(self._data); File ""/usr/local/lib/python3.7/dist-packages/pymysql/err.py"", line 143, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.OperationalError: (1205, 'Lock wait timeout exceeded; try restarting transaction'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12652#issuecomment-1416434586:2075,protocol,protocol,2075,https://hail.is,https://github.com/hail-is/hail/pull/12652#issuecomment-1416434586,1,['protocol'],['protocol']
Integrability,informative commit messages please!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6640#issuecomment-511460325:19,message,messages,19,https://hail.is,https://github.com/hail-is/hail/pull/6640#issuecomment-511460325,1,['message'],['messages']
Integrability,"ing `group_cols_by` for another aggregation method. This is a matrixtable with 2 variants and 245k samples. ```python; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_gtstats_vals = mt.group_cols_by(mt.ancestry).aggregate(gt_stats_ancestry=hl.agg.call_stats(mt.GT, mt.alleles)); mt_gtstats_vals.gt_stats_ancestry.AF.export(af_ancestry_bucket); ```. ```; [Stage 23:> (0 + 1) / 1]; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /tmp/ipykernel_231/1465831350.py in <module>; ----> 1 mt_gtstats_vals.gt_stats_ancestry.AF.export(af_ancestry_bucket). <decorator-gen-634> in export(self, path, delimiter, missing, header). /opt/conda/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. /opt/conda/lib/python3.7/site-packages/hail/expr/expressions/base_expression.py in export(self, path, delimiter, missing, header); 1068 **{output_col_name: hl.delimit(column_names, delimiter)}); 1069 file_contents = header_table.union(file_contents); -> 1070 file_contents.export(path, delimiter=delimiter, header=False); 1071 ; 1072 @typecheck_method(n=int, _localize=bool). <decorator-gen-1190> in export(self, output, types_file, header, parallel, delimiter). /opt/conda/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. /opt/conda/lib/python3.7/site-packages/hail/table.py in export(self, ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:923,wrap,wrapper,923,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,3,['wrap'],['wrapper']
Integrability,"ing's (e.g. debian8), and systems; based on g++-5.x and later with new-ABI std::string by default (e.g. debian9). That; incompatibility is the problem. >Not having access to the standard library seems problematic. You absolutely have access to the full C++11 standard library in dynamically-generated code -; you're compiling with the master node's default compiler, and header files, and using; the default libstdc++.o (libc++.dylib), and it's all fine. And you'll be using whichever flavor; of std::string is the default for that system, which will presumably also be interoperable with any; third-party library packages on that system. The issues we're getting round are:. a) If libhail.so is prebuilt on a new-ABI system *and* uses std::string, then it can't run against; the libstdc++ on an old-ABI system. b) If libhail.so is prebuilt on an old-ABI system, then it can run against either old-ABI or new-ABI; libstdc++, but if it then gets linked against third-party libraries compiled against new-ABI; headers, you'll have two different flavors of std::string floating around in the same program,; which causes confusion at any interfaceswhich pass std::string around. Now if you want to go further in shipping more of the system, then the question of whether to; ship your own libstdc++ (or libc++) is independent of the choice of compiler version. *If* you ; ship your own libstdc++, then you potentially introduce the problems of interoperability with; other libraries on the system). And there's a slight question about whether your libstdc++ will; work against the other systems libc.so, though I think the ABI at that level has been stable enough for long enough that it probably doesn't cause trouble.; ; In short, it's a can of worms. Avoiding std::string in libhail.so keeps the can closed for now.; And I believe dataproc will move to using debian9 images as the default in November, so at; some point the need to support old-ABI systems (debian8) will diminish and possibly go away c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941:1438,interface,interfaceswhich,1438,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941,1,['interface'],['interfaceswhich']
Integrability,"install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); at is.hail.utils.package$.using(package.scala:601); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.S",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:7771,Wrap,WrappedArray,7771,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Wrap'],['WrappedArray']
Integrability,"ipp-3.17.0-py3-none-any.whl (7.4 kB); Installing collected packages: zipp, wheel, tomli, packaging, click, pyproject_hooks, importlib-metadata, build, pip-tools; Successfully installed build-1.0.3 click-8.1.7 importlib-metadata-6.8.0 packaging-23.2 pip-tools-6.13.0 pyproject_hooks-1.0.0 tomli-2.0.1 wheel-0.41.2 zipp-3.17.0. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; + for package in '$@'; + reqs=python/requirements.txt; + pinned=python/pinned-requirements.txt; ++ mktemp; + new_pinned=/tmp/tmp.YoVBQEw8XF; ++ mktemp; + pinned_no_comments=/tmp/tmp.WRSKGgGEB8; ++ mktemp; + new_pinned_no_comments=/tmp/tmp.C8ggaXDHDt; + PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/aws/puppet/bin/:/home/hadoop/.local/bin:/home/hadoop/.local/bin; + pip-compile --quiet python/requirements.txt python/pinned-requirements.txt --output-file=/tmp/tmp.YoVBQEw8XF; WARNING: the legacy dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$' | tr '\n' '\0' | xargs -0 python3 -m pip install -U; Defaulting to user installation because normal site-packages is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-an",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:31386,depend,dependency,31386,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['depend'],['dependency']
Integrability,ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:450); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:486); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:339); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:483); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:9102,Wrap,WrappedArray,9102,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['Wrap'],['WrappedArray']
Integrability,is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerArrayAggsToRunAggsPass$.apply(LoweringPass.scala:89); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:45); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:314); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:308); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:25); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:23); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:12); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:23); 	at is.hail.backend.service.ServiceBackend.userContext(ServiceBackend.scala:122); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11.apply(ServiceBackend.scala:308); 	at is.hail.backend.service.Ser,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:10946,Wrap,WrappedArray,10946,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693,1,['Wrap'],['WrappedArray']
Integrability,is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerArrayAggsToRunAggsPass$.apply(LoweringPass.scala:89); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:45); 	at is.hail.backend.service.ServiceBackend.is$hail$backend$service$ServiceBackend$$execute(ServiceBackend.scala:321); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:337); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:334); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:25); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:23); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:23); 	at is.hail.backend.service.ServiceBackend.userContext(ServiceBackend.scala:132); 	at is.hail.backend.servic,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:11226,Wrap,WrappedArray,11226,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601,1,['Wrap'],['WrappedArray']
Integrability,"istributed"" which I think is your proposal). Blocks aren't heterogenous in our current setup (last block can be short) and I think that needs to be clarified in your proposal. Slicing has similar complications. I think the blocking should specify the list of sizes of each block, which makes it closed under slicing. I agree the user interface should do implicit broadcast but in the IR it should be explicit. > If we want to support sparse tensors. I vote we punt on sparse vectors for the time being. (In the sparse case, I think sparsity of the output should be inferred by analysis rather than specified. I think statically knowing the sparsity relationship is going to a rarity.). The key point here is that Patrick's operations are incredibly general so this proposal needs to be paired with an compilation strategy that guarantees his operations are never actually realized directly (e.g. deforestation, fusing operations (e.g. broadcast) into their consumers) while also generating BLAS-level performance. I propose we plan to build against BLAS while we investigate more general codegen strategies. To that end, I've suggested we read up on a few related projects in study group: taco (hat tip Patrick), TensorComprehensions, TVM (tensor virtual machine) and Halide. Might be other relevant ones (TF/XLA?) Few questions to ask:; - Can we steal good ideas for our IR?; - Can we target their IR?; - Can we integrate this with our stack?; - What's the performance like (compile time, runtime)?. We'll need some more prosaic operators:; - constructors: from array with shape, from shape and a function of indices; - get shape.; - index: get an element from a tuple of integers. This is only allowed for local arrays.; - slice: get another ndarray from a sequence of integers, ranges, or array of indices.; - reshape. Maybe local only? Or clarified in the distributed case. I think we'll need some additional basic operations supported by numpy (e.g. concatenate, tile), but this is a good start.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5190#issuecomment-459208120:1576,integrat,integrate,1576,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-459208120,1,['integrat'],['integrate']
Integrability,"it is the imperative analog of Code[Unit]. In this analogy, a function returning Code[Unit] becomes a function that takes a CodeBuilder, alternatively, a Code[Unit] can become a CodeLabel: the place to jump to run a given computation. In addition to CodeBuilder, I have a imperative implementation of EmitCode that is similar to the proposal in the dev post: IEmitCode. Under the above analog, the proposal in the dev post would become:. ```; trait IEmitCode {; def apply(missing: (CodeBuilder) => Unit, present: (CodeBuilder, PValue) => Unit): Unit; }; ```. However, I took the additional step of ""defunctionalizing"" this picture by using labels instead of functions of code, giving:. ```; case class IEmitCode(Lmissing: CodeLabel, Lpresent: CodeLabel, pc: PCode) {; ...; }; ```. In this model, the emit function will become: `Emit.emit(cb: CodeBuilder, ...): IEmitCode`. The discipline is after calling `emit`, the contents of the code builder, when executed, will jump to one of `Lmissing` or `Lpresent` (labels which are not defined yet) and the consumer can define those labels, and use the expression `pc` in the code after the `Lpresent` label only. Because obviously I haven't converted everything to the imperative style (yet), I wrote routines to convert them back and forth: `EmitCode.fromI { cb => ... }` provides a CodeBuilder and converts a resulting IEmitCode back to an EmitCode, and EmitCode.toI(cb) does the opposite. There is also `(Emit)CodeBuilder.scoped` that will run a code builder function and collect the code as a Code[Unit]. A second idea introduced in this PR is that some PType operations may only be available on a PSettable/PValue, rather than on PCode. The motivation is the canonical array ref implementation, which involves a lot of duplicate code generation. In the current setup, we can have a compound PValue, and this introduces a PCanonicalIndexableValue that has three fields: the base array address, the length and the elements addresss. FYI @patrick-schultz",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413:2032,rout,routines,2032,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413,1,['rout'],['routines']
Integrability,"it's not necessary, since we can just name them separate things. The Python interface is the reason this is breaking.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6071#issuecomment-554499349:76,interface,interface,76,https://hail.is,https://github.com/hail-is/hail/issues/6071#issuecomment-554499349,1,['interface'],['interface']
Integrability,"k (most recent call last):; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 584, in arg_check; return checker.check(arg, function_name, arg_name); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_typecheck.py"", line 80, in check; raise TypecheckFailure from e; hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in <module>; cli.main(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main; run(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file; runpy.run_path(target, run_name=""__main__""); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path; return _run_module_code(code, init_globals, run_name,; File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code; _run_code(code, mod_globals, init_globals,; File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 124, in _run_code; exec(code, run_globals); File ""/home/ed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:3233,adapter,adapter,3233,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['adapter'],['adapter']
Integrability,"k/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API:; >; > https://hail.is/pyhail/getting_started.html; >; > Please give it a spin and let us know if you run into any problems. The; > documentation for the python API is nearly complete, but the Tutorial and; > General Reference section are still being ported to python and will need; > another week or so. Thanks for your patience!; >; > ; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/1218#issuecomment-2723576",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:1488,protocol,protocol,1488,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799,1,['protocol'],['protocol']
Integrability,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:2106,protocol,protocol,2106,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914,1,['protocol'],['protocol']
Integrability,"l last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<decorator-gen-468>"", line 2, in eval_typed; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 199, in eval_typed; return (Env.backend().execute(expression._ir), expression.dtype); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 86, in _to_java_ir; code = r(ir); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 511, in render; return '(ArrayLen {})'.format(r(self.a)); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 2003, in render; return f'(Literal {self._typ._parsable_string()} ' \; File ""/Users/wang/code/hail/hail/python/hail/utils/java.py"", line 159, in escape_str; return Env.jutils().escapePyString(s); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/Users/wang/code/hail/hail/python/hail/utils/java.py"", line 215, in deco; return f(*args, **kwargs); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 327, in get_return_value; py4j.protocol.Py4JError: An error occurred while calling o33.escapePyString; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:3855,protocol,protocol,3855,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184,2,['protocol'],['protocol']
Integrability,"like this (crashes with the stack trace/error in the issue):; ```; from pyspark import *; from hail import *; conf = SparkConf(); conf.set('spark.sql.files.maxPartitionBytes','60000000000') ; conf.set('spark.sql.files.openCostInBytes','60000000000') ; conf.set('spark.driver.cores','1') #test with 1 core; sc = SparkContext(conf=conf); hc = HailContext(sc); ```. With startup messages looking like this:; ```; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/01/08 15:16:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/01/08 15:16:23 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).; 18/01/08 15:16:23 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 18/01/08 15:16:23 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 15:16:23 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 15:16:23 WARN Utils: Your hostname, <my computer name> resolves to a loopback address: <my local IP>; using <my IP> instead (on interface enp3s0); 18/01/08 15:16:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; 18/01/08 15:16:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; 18/01/08 15:16:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:8810,interface,interface,8810,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['interface'],['interface']
Integrability,"llib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 03:59:30,730 | server.py | set_state:141 | job 153 changed state: Created -> Complete; INFO | 2018-10-23 03:59:30,730 | server.py | mark_complete:184 | job 153 complete, exit_code 2; INFO | 2018-10-23 03:59:30,737 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,806 | _internal.py | _log:88 | 10.56.142.33 - - [23/Oct/2018 03:59:30] ""GET /jobs HTTP/1.1"" 200 -; INFO | 2018-10-23 03:59:30,815 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:1798,adapter,adapters,1798,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038,1,['adapter'],['adapters']
Integrability,looks like this is a weird character message maybe? There are some non-ascii chars in there,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5221#issuecomment-459112814:37,message,message,37,https://hail.is,https://github.com/hail-is/hail/issues/5221#issuecomment-459112814,1,['message'],['message']
Integrability,"m(),None),Sum()))), ArrayFor(ArrayMap(ArrayRange(I32(0),I32(4),I32(1)),__iruid_304,Cast(Ref(__iruid_304,int32),int64)),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),GetTupleElement(Ref(__iruid_302,tuple(int64)),0))))). After lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))), ArrayFor(ToStream(ArrayMap(ToStream(StreamRange(I32(0),I32(4),I32(1))),__iruid_304,Cast(Ref(__iruid_304,int32),int64))),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),GetTupleElement(Ref(__iruid_302,tuple(int64)),0))))). java.lang.ClassCastException: is.hail.expr.types.physical.PStream cannot be cast to is.hail.expr.types.physical.PContainer. 	at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:631). #### Let's try to return x from streamify catch-all case (instead of if(...) ToStream(x) else x). With:. ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }; ````. Again issue in testArrayAggScan (and 8 others in IRSuite):. Before Lower: ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:3134,Wrap,WrappedArray,3134,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113,2,['Wrap'],['WrappedArray']
Integrability,m(LoweringPass.scala:50); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); at is.hail.utils.package$.using(package.scala:601); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.S,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:15756,Wrap,WrappedArray,15756,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Wrap'],['WrappedArray']
Integrability,"me/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Hadoop.d -MT build/Hadoop.o -c Hadoop.cpp; c++ -fvisibility=default -dynamiclib -Wl,-undefined,dynamic_lookup -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin build/davies.o build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/NativeCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:2613,Wrap,WrappedArray,2613,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925,1,['Wrap'],['WrappedArray']
Integrability,"mmand.getArguments(AbstractCommand.java:82); 	at py4j.commands.CallCommand.execute(CallCommand.java:77); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<decorator-gen-468>"", line 2, in eval_typed; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 199, in eval_typed; return (Env.backend().execute(expression._ir), expression.dtype); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/wang/code/hail/hail/python/hail/backend/backe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:1904,protocol,protocol,1904,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184,1,['protocol'],['protocol']
Integrability,"mpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). During handling of the above exception, another exception occurred:. Py4JJavaError Traceback (most recent call last); /bmrn/apps/hail/0.2.72-spark-3.1.2/python/hail-0.2.72-py3-none-any.egg/hail/backend/py4j_backend.py in deco(*args, **kwargs); 15 try:; ---> 16 return f(*args, **kwargs); 17 except py4j.protocol.Py4JJavaError as e:. /bmrn/apps/python/miniconda/64/3.7/envs/piranha_0.2.0_20210812/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 327 ""An error occurred while calling {0}{1}{2}.\n"".; --> 328 format(target_id, ""."", name), value); 329 else:. Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.backend.spark.SparkBackend$.apply(SparkBackend.scala:218); 	at is.hail.backend.spark.SparkBackend.apply(SparkBackend.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:4574,protocol,protocol,4574,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,1,['protocol'],['protocol']
Integrability,"mt.alleles[1]) == mt.ss.nt2)),; (-1*mt.ss.ldpred_inf_beta)); .when(((mt.alleles[0] == mt.ss.nt2) &; (mt.alleles[1] == mt.ss.nt1)) |; ((flip_text(mt.alleles[0]) == mt.ss.nt2) &; (flip_text(mt.alleles[1]) == mt.ss.nt1)),; mt.ss.ldpred_inf_beta); .or_missing()). # filter bgen matrixtable down to only SNPs with betas; mt = mt.filter_rows(hl.is_defined(mt.beta)). # filter bgen matrixtable to only include people in scoring sample; mt = mt.filter_cols(hl.is_defined(sampleids[mt.s])). # score sample; mt = mt.annotate_cols(prs=hl.agg.sum(mt.beta * mt.dosage)). # write out table with sample IDs and PRS scores; mt.cols().export('gs://ukbb_prs/prs/UKB_'+pheno+'_PRS_22.txt'). parser = argparse.ArgumentParser(); parser.add_argument(""--phenotype"", help=""name of the sumstat phenotype""); args = parser.parse_args(). try:; start = time.time(); main(args.phenotype); end = time.time(); message = ""Success! Job was completed in %s"" % time.strftime(""%H:%M:%S"", time.gmtime(end - start)); send_message(message); except Exception as e:; send_message(""Fail.""); ```. ""Failure Reason"":; ```; Job aborted due to stage failure: Task 98 in stage 13.0 failed 20 times, most recent failure: Lost task 98.19 in stage 13.0 (TID 22699, ccarey-sw-xt4j.c.ukbb-robinson.internal, executor 68): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:815); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:804); 	at scala.collection.Iterator$$anon$11.next(Iterator",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:2736,message,message,2736,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['message'],['message']
Integrability,"n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory\n response = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/gear/csrf.py\"", line 20, in wrapped\n return await fun(request, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 77, in wrapped\n return await fun(request, userdata, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 417, in post_notebook\n return await _post_notebook('notebook', request, userdata)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 230, in POST\n body=body))\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 181, in request\n raise ApiException(http_resp=r)\nkubernetes_asyncio.client.rest.ApiException: (422)\nReason: Unprocessable Entity\n",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:1220,wrap,wrapped,1220,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851,1,['wrap'],['wrapped']
Integrability,"n, NormalDistribution}; import org.apache.commons.math3.random.JDKRandomGenerator; import org.apache.commons.math3.util.CombinatoricsUtils.factorialLog; import org.apache.hadoop; import org.apache.log4j.{ConsoleAppender, PatternLayout}; import org.apache.spark.SparkContext; import org.apache.spark.SparkException; import org.apache.spark.mllib.linalg.Vectors; import org.apache.spark.mllib.linalg.distributed.{DistributedMatrix, IndexedRow, IndexedRowMatrix}; import org.apache.spark.rdd.RDD; import org.apache.spark.sql.Row; import org.apache.spark.storage.StorageLevel; import org.apache.{hadoop => hd}; import org.json4s.JValue; import org.json4s.JsonAST._; import org.json4s._; import org.json4s.jackson.JsonMethods; import org.json4s.jackson.JsonMethods._; import org.json4s.jackson.Serialization; import org.json4s.jackson.{JsonMethods, Serialization}; import org.json4s.{DefaultFormats, Formats}; import org.sparkproject.guava.util.concurrent.MoreExecutors; ```. We explicitly depend on; - `htsjdk`; - `breeze`; - `json4s`. That leaves:. ```; import org.apache.avro.SchemaBuilder; import org.apache.avro.file.DataFileWriter; import org.apache.avro.generic.{GenericDatumWriter, GenericRecord, GenericRecordBuilder}; import org.apache.commons.io.IOUtils; import org.apache.commons.math3.distribution.ChiSquaredDistribution; import org.apache.commons.math3.distribution.{ChiSquaredDistribution, NormalDistribution}; import org.apache.commons.math3.random.JDKRandomGenerator; import org.apache.commons.math3.util.CombinatoricsUtils.factorialLog; import org.apache.hadoop; import org.apache.log4j.{ConsoleAppender, PatternLayout}; import org.apache.spark.SparkContext; import org.apache.spark.SparkException; import org.apache.spark.mllib.linalg.Vectors; import org.apache.spark.mllib.linalg.distributed.{DistributedMatrix, IndexedRow, IndexedRowMatrix}; import org.apache.spark.rdd.RDD; import org.apache.spark.sql.Row; import org.apache.spark.storage.StorageLevel; import org.apache.{hadoop => hd",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741:3998,depend,depend,3998,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741,1,['depend'],['depend']
Integrability,"n3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); at is.hail.utils.package$.using(package.scala:601); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.SparkBackend.execu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:7792,Wrap,WrappedArray,7792,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Wrap'],['WrappedArray']
Integrability,"n[T] -> (U -> Boolean) -> Gen[Unit]`; - A `Gen[Unit]` is a bit artificial because the test framework halts execution (presumably with an exception) when a counter-example is found. I instead prefer that `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Boolean]`; - Now `Prop.forAll` has the same type as `Gen.flatMap[Boolean]`. It seems the difference between `forAll` and `flatMap` is that `forAll` conceptually preforms a product operation while `flatMap` performs a sampling. However, I think they are, in reality, the same operation: sampling. The implementation for `GenProp3` looks like:. ``` scala; for (i <- 0 until p.count) {; val v1 = g1(p); val v2 = g2(p); val v3 = g3(p); val r = f(v1, v2, v3); if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; }; ```. Which could be re-written as:. ``` scala; for (i <- 0 until p.count) {; (for (v1 <- g1; v2 <- g2; v3 <- g3) {; if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; })(p); }; ```. The primary difference between `flatMap` and `forAll` seems to be in error reporting. We can fix this by noting `Gen[T]` is currently a Reader monad on `Parameters`. If we add a ""forAll stack"" to `Parameters` we could implement `forAll` as:. ``` scala; def forAll[T,U](gt: Gen[T], gu: T -> Gen[U]): Gen[U] =; for (t <- gt; u <- local(pushQuantified(t), gu(t)) yield u. def pushQuantified(x: Any)(Parameters p): Paramters =; new Parameters(p.rng, p.size, p.count, (x :: p.quanitifed)); ```. We complete the Reader monad transformation by adding the `local` operation to `class Gen[T]`. ``` scala; // in class Gen; def local(modify: Parameters -> Parameters, gu: Gen[U]): Gen[U] =; Gen { p => gu(modify(p)) }; ```. Finally, the `check` method can access this stack of quantified variables to provide a useful error message. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/400#issuecomment-238901220:2180,message,message,2180,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238901220,1,['message'],['message']
Integrability,"nes/jdk1.8.0_202.jdk/Contents/Home/include/darwin build/davies.o build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/NativeCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:2875,Wrap,WrappedArray,2875,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925,2,['Wrap'],['WrappedArray']
Integrability,"not moved to table there. Same needs to be done for VDS, this isn't too hard); - Add the non-core methods to `hail.methods` / `hail.genetics.methods`; - some stuff here is much harder than the rest, like `filter_alleles`; - This is mostly just labor, but some require more thought than others, like moving TDT to use hail2 expr; - Support intervals in the `index_*` methods. It's possible now to join by locus, but not using the `annotateLociTable` fast path.; - Move to Python 3 so argument order is preserved; - Test the hail2 api much more rigorously than we do now (at the very least, call each parameter branch for each method!; - Typecheck the expression language. This isn't super trivial, and making a nice system to integrate our `typecheck` module and expressions will require some thoughtful design work.; - Some more organization around the package: monkey patching with `import hail.genetics` is an idea I like, but want to think about the edge cases first. ## Documentation; - Document the `index_*` methods / joins; - Translate the _Hail Overview_ tutorial; - Make new tutorials to replace the 2 expr ones we have; - Fill in docs on api2 methods (they're not all there yet); - Fill in docs on expression language (things like __mul__ on NumericExpression haven't been documented); - Write ""integrative docs"" that provide how-tos for common types of workflows. Show the power of annotate / select / group_by/aggregate, etc. ## Longer term QoL:; - Move over tests to Python as much as possible. I looked at the linear regression suite and it can be moved entirely into Python without many problems.; - Write a type parser in Python. The nested calls into the JVM for Type._from_java make the library feel extremely sluggish on teensy data.; - Integrate RV with C/C++, so we can transmit data much more efficiently between Python and Java.; - Rethink the expr language function registry, because many functions there can be implemented in terms of others in Python.; - add back in de novo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2588#issuecomment-352190554:1571,integrat,integrative,1571,https://hail.is,https://github.com/hail-is/hail/pull/2588#issuecomment-352190554,2,"['Integrat', 'integrat']","['Integrate', 'integrative']"
Integrability,now depends on #3880,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3840#issuecomment-401758997:4,depend,depends,4,https://hail.is,https://github.com/hail-is/hail/pull/3840#issuecomment-401758997,1,['depend'],['depends']
Integrability,"nship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.cr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:6312,wrap,wrapper,6312,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['wrap'],['wrapper']
Integrability,"ob:; ```python; utils.py	retry_long_running:923	in delete_prev_cancelled_job_group_cancellable_resources_records	; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 915, in retry_long_running; return await f(*args, **kwargs)\n File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 959, in loop; await f(*args, **kwargs)\n File ""/usr/local/lib/python3.9/dist-packages/batch/driver/main.py"", line 1485, in delete_prev_cancelled_job_group_cancellable_resources_records; async for target in targets:\n File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 334, in execute_and_fetchall; async for row in tx.execute_and_fetchall(sql, args, query_name):\n File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 257, in execute_and_fetchall; await cursor.execute(sql, args)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 469, in query; await self._read_query_result(unbuffered=unbuffered)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 683, in _read_query_result; await result.read()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 1164, in read; first_packet = await self.connection._read_packet()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 652, in _read_packet; packet.raise_for_error()\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/protocol.py"", line 219, in raise_for_error; err.raise_mysql_exception(self._data)\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/err.py"", line 150, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.OperationalError: (1054, ""Unknown column 'cancelled.id' in 'on clause'""); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2349752340:1741,protocol,protocol,1741,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2349752340,1,['protocol'],['protocol']
Integrability,"oh crap, sorry, didn't see your question. Will definitely try to improve commit message in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9330#issuecomment-680269070:80,message,message,80,https://hail.is,https://github.com/hail-is/hail/pull/9330#issuecomment-680269070,1,['message'],['message']
Integrability,"okay, this passes locally, is there anything I need to do with the build images if I change the dependencies?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10793#issuecomment-902894763:96,depend,dependencies,96,https://hail.is,https://github.com/hail-is/hail/pull/10793#issuecomment-902894763,1,['depend'],['dependencies']
Integrability,"ome/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 364, in f; ret = x(*args); File ""test.py"", line 11, in <lambda>; (mt.locus.contig == hl.literal(x[0])) & \; File ""<decorator-gen-690>"", line 2, in literal; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/functions.py"", line 261, in literal; return literal(hl.eval(to_expr(x, dtype)), dtype); File ""<decorator-gen-668>"", line 2, in eval; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 223, in eval; return eval_timed(expression)[0]; File ""<decorator-gen-666>"", line 2, in eval_timed; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 189, in eval_timed; return _eval_many(expression, timed=True, name='eval_timed')[0]; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 150, in _eval_many; return Env.backend().execute_many(*irs, timed=timed); File ""/home/edmund/.local/src/hail/hail/python/hail/backend/backend.py"", line 38, in execute_many; return [self.execute(MakeTuple([ir]), timed=timed)[0] for ir in irs]; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/backend.py"", line 38, in <listcomp>; return [self.execute(MakeTuple([ir]), timed=timed)[0] for ir in irs]; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/py4j_backend.py"", line 94, in execute; jir = self._to_java_value_ir(ir); File ""/home/edmund/.local/src/hail/hail/python/hail/backend/spark_backend.py"", line 280, in _to_java_value_ir; return self._to_java_ir(ir, self._parse_valu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:8106,wrap,wrapper,8106,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['wrap'],['wrapper']
Integrability,"omment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14691** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14691?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14690** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14690?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14686** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14686?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14684** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14684?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14683** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14683?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @ehigham and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14683#issuecomment-2359375453:2062,depend,dependencies,2062,https://hail.is,https://github.com/hail-is/hail/pull/14683#issuecomment-2359375453,2,['depend'],['dependencies']
Integrability,"on in thread ""Thread-2"" java.lang.OutOfMemoryError: Java heap space; 	at java.util.Arrays.copyOf(Arrays.java:3332); 	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); 	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:649); 	at java.lang.StringBuilder.append(StringBuilder.java:202); 	at py4j.StringUtil.unescape(StringUtil.java:57); 	at py4j.Protocol.getString(Protocol.java:475); 	at py4j.Protocol.getObject(Protocol.java:302); 	at py4j.commands.AbstractCommand.getArguments(AbstractCommand.java:82); 	at py4j.commands.CallCommand.execute(CallCommand.java:77); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<d",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:1386,protocol,protocol,1386,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184,1,['protocol'],['protocol']
Integrability,"on3.9/site-packages/hail/backend/py4j_backend.py:99, in Py4JBackend.execute(self, ir, timed); 97 try:; 98 result_tuple = self._jbackend.executeEncode(jir, stream_codec); ---> 99 (result, timings) = (result_tuple._1(), result_tuple._2()); 100 value = ir.typ._from_encoding(result); 102 return (value, timings) if timed else value. File ~/mambaforge/lib/python3.9/site-packages/py4j/java_gateway.py:1304, in JavaMember.__call__(self, *args); 1298 command = proto.CALL_COMMAND_NAME +\; 1299 self.command_header +\; 1300 args_command +\; 1301 proto.END_COMMAND_PART; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1307 for temp_arg in temp_args:; 1308 temp_arg._detach(). File ~/mambaforge/lib/python3.9/site-packages/hail/backend/py4j_backend.py:21, in handle_java_exception.<locals>.deco(*args, **kwargs); 19 import pyspark; 20 try:; ---> 21 return f(*args, **kwargs); 22 except py4j.protocol.Py4JJavaError as e:; 23 s = e.java_exception.toString(). File ~/mambaforge/lib/python3.9/site-packages/py4j/protocol.py:330, in get_return_value(answer, gateway_client, target_id, name); 326 raise Py4JJavaError(; 327 ""An error occurred while calling {0}{1}{2}.\n"".; 328 format(target_id, ""."", name), value); 329 else:; --> 330 raise Py4JError(; 331 ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; 332 format(target_id, ""."", name, value)); 333 else:; 334 raise Py4JError(; 335 ""An error occurred while calling {0}{1}{2}"".; 336 format(target_id, ""."", name)). Py4JError: An error occurred while calling o83._1. Trace:; java.lang.NegativeArraySizeException: -1966455376; 	at py4j.Base64.encodeToChar(Base64.java:681); 	at py4j.Base64.encodeToString(Base64.java:734); 	at py4j.Protocol.encodeBytes(Protocol.java:154); 	at py4j.ReturnObject.getPrimitiveReturnObject(ReturnObject.java:150); 	at py4j.Gateway.getReturnObject(Gateway.java:188); 	at py4j.Gateway.invoke(Gateway.java:283); 	at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12035#issuecomment-1186014691:2829,protocol,protocol,2829,https://hail.is,https://github.com/hail-is/hail/issues/12035#issuecomment-1186014691,1,['protocol'],['protocol']
Integrability,"one more change required to error message in AST:. ```; s""""""Tried to access index [$i] on array ${ JsonMethods.compact(localT.toJSON(a)) } of length ${ a.length }; | Hint: All arrays in Hail are zero-indexed (`array[0]' is the first element); | Hint: For accessing `A'-numbered info fields in split variants, `va.info.field[va.aIndex]' is correct"""""".stripMargin); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/658#issuecomment-242136648:34,message,message,34,https://hail.is,https://github.com/hail-is/hail/pull/658#issuecomment-242136648,1,['message'],['message']
Integrability,"one,; 'quobyte': None,; 'rbd': None,; 'scale_io': None,; 'secret': None,; 'storageos': None,; 'vsphere_volume': None},; {'aws_elastic_block_store': None,; 'azure_disk': None,; 'azure_file': None,; 'cephfs': None,; 'cinder': None,; 'config_map': None,; 'downward_api': None,; 'empty_dir': None,; 'fc': None,; 'flex_volume': None,; 'flocker': None,; 'gce_persistent_disk': None,; 'git_repo': None,; 'glusterfs': None,; 'host_path': None,; 'iscsi': None,; 'name': 'default-token-8h99c',; 'nfs': None,; 'persistent_volume_claim': None,; 'photon_persistent_disk': None,; 'portworx_volume': None,; 'projected': None,; 'quobyte': None,; 'rbd': None,; 'scale_io': None,; 'secret': {'default_mode': 420,; 'items': None,; 'optional': None,; 'secret_name': 'default-token-8h99c'},; 'storageos': None,; 'vsphere_volume': None}]},; 'status': {'conditions': [{'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'Initialized'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'Ready'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'ContainersReady'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'PodScheduled'}],; 'container_statuses': [{'container_id': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_id': '',; 'last_state': {'running': None,; 'terminated': None,; 'waiting': None},; 'name': 'main',; 'ready': False,; 'restart_count': 0,; 'state': {'running': None,; 'terminated': {'container_id':",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:7900,message,message,7900,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['message'],['message']
Integrability,"oops. It's probably defined below in the same file. It's a decorator that calls `to_expr` on each argument of the function before calling the function. That way, Python values like ints and bools and so on get wrapped without having to do so explicitly inside the function.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2557#issuecomment-351511340:210,wrap,wrapped,210,https://hail.is,https://github.com/hail-is/hail/pull/2557#issuecomment-351511340,1,['wrap'],['wrapped']
Integrability,opy.dcopy(blas.f); 	at org.netlib.arpack.Dsaitr.dsaitr(arpack.f); 	at org.netlib.arpack.Dsaup2.dsaup2(arpack.f); 	at org.netlib.arpack.Dsaupd.dsaupd(arpack.f); 	at dev.ludovic.netlib.arpack.F2jARPACK.dsaupdK(F2jARPACK.java:189); 	at dev.ludovic.netlib.arpack.AbstractARPACK.dsaupd(AbstractARPACK.java:560); 	at dev.ludovic.netlib.arpack.F2jARPACK.dsaupd(F2jARPACK.java:30); 	at dev.ludovic.netlib.arpack.AbstractARPACK.dsaupd(AbstractARPACK.java:536); 	at dev.ludovic.netlib.arpack.F2jARPACK.dsaupd(F2jARPACK.java:30); 	at org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:106); 	at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:385); 	at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:311); 	at org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix.computeSVD(IndexedRowMatrix.scala:231); 	at is.hail.methods.PCA.execute(PCA.scala:41); 	at is.hail.expr.ir.functions.WrappedMatrixToTableFunction.execute(RelationalFunctions.scala:52); 	at is.hail.expr.ir.TableToTableApply.execute(TableIR.scala:3379); 	at is.hail.expr.ir.TableIR.analyzeAndExecute(TableIR.scala:61); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:865); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:59); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:20); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:58); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:63); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:8,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:8074,Wrap,WrappedMatrixToTableFunction,8074,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,1,['Wrap'],['WrappedMatrixToTableFunction']
Integrability,"p connector has [brand new configuration parameters](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/v3.0.0/gcs/INSTALL.md). Somehow I managed to make the normal Spark backend work correctly but the Local backend (which still, afaik, uses Spark & Hadoop for filesystems) is still trying to pick up CI's credentials instead of the test account's credentials. ```; E hail.utils.java.FatalError: GoogleJsonResponseException: 403 Forbidden; E GET https://storage.googleapis.com/storage/v1/b/hail-test-requester-pays-fds32/o/zero-to-nine?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata&userProject=hail-vdc; E {; E ""code"": 403,; E ""errors"": [; E {; E ""domain"": ""global"",; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist)."",; E ""reason"": ""forbidden""; E }; E ],; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist).""; E }; E ; E Java stack trace:; E java.io.IOException: Error accessing gs://hail-test-requester-pays-fds32/zero-to-nine; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1986); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1882); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystemImpl.getFileInfoInternal(GoogleCloudStorageFileSystemImpl.java:861); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystemImpl.getFileInfo(GoogleCloudStorageFileSystemImpl.java:833); E 	at com.google.cloud.hadoop.fs.gcs.Go",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236:1298,message,message,1298,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236,1,['message'],['message']
Integrability,p.java:23) ~[scala-library-2.12.15.jar:?]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.GoogleStorageFS$$anon$2.close(GoogleStorageFS.scala:308) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159) ~[?:1.8.0_382]; 	at is.hail.utils.package$.using(package.scala:677) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.FS.writePDOS(FS.scala:441) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.FS.writePDOS$(FS.scala:440) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$4(Worker.scala:124) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$4$adapted(Worker.scala:124) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$13(Worker.scala:178) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:14210,Rout,RouterFS,14210,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['Rout'],['RouterFS']
Integrability,"park-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<decorator-gen-468>"", line 2, in eval_typed; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 199, in eval_typed; return (Env.backend().execute(expression._ir), expression.dtype); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 86, in _to_java_ir; code = r(ir); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 511, in render; return '(ArrayLen {})'.format(r(self.a)); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 2003, in render; return f'(Literal {self._typ._parsable_string()} ' \; File ""/Users/wang/code/hail/hail/python/hail/utils/java.py"", line 159, in escape_str; return Env.jutils().escapePyString(s); File ""/Users/wang/spark-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:2513,wrap,wrapper,2513,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184,1,['wrap'],['wrapper']
Integrability,"path.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:1703,depend,dependency,1703,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,1,['depend'],['dependency']
Integrability,"pile configuration?**; > The Java Library Plugin has historically used the compile configuration for dependencies that are required to both compile and run a projects production code. It is now deprecated, and will issue warnings when used, because it doesnt distinguish between dependencies that impact the public API of a Java library project and those that dont. You can learn more about the importance of this distinction in [Building Java libraries](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:building_java_libraries). OK, so, we used to just dump everything into our runtime dependencies. I changed it so that we have three kinds of dependencies:; 1. `shadow`: these are provided by Dataproc/QoB at run-time. They are not in any JAR. They are not on the `testRuntimeClasspath` or `runtimeClasspath`. They are on the `testCompileClasspath` because I [explicitly requested](https://github.com/hail-is/hail/blob/main/hail/build.gradle#L98) that `testCompileOnly` bring in our `shadow` dependencies.; 2. `implementation`: these are included in all class paths and in shadow JARs (but not ""thin"" jars generated by `./gradlew jar`).; 3. `testImplementation`: these are included in test class paths and in shadow JARs. Our test code references these third-party classes:; ```; import breeze.linalg.DenseMatrix; import breeze.linalg._; import breeze.linalg.{*, diag, DenseMatrix => BDM, DenseVector => BDV}; import breeze.linalg.{DenseMatrix => BDM, _}; import breeze.linalg.{DenseMatrix => BDM}; import breeze.linalg.{DenseMatrix, DenseVector, eigSym, svd}; import breeze.linalg.{DenseMatrix, DenseVector}; import breeze.linalg.{DenseMatrix, Matrix, Vector}; import breeze.linalg.{Vector => BVector}; import htsjdk.samtools.reference.ReferenceSequenceFileFactory; import htsjdk.samtools.util.BlockCompressedFilePointerUtil; import htsjdk.tribble.readers.{TabixReader => HtsjdkTabixReader}; import org.apache.avro.SchemaBuilder; import org.apache.avro.file.DataFileWri",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741:1766,depend,dependencies,1766,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741,1,['depend'],['dependencies']
Integrability,"port_vcf('src/test/resources/malformed.vcf').count(); hail: info: Coerced sorted dataset; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-4-955ea6a16c80> in <module>(); ----> 1 hc.import_vcf('src/test/resources/malformed.vcf').count(). /Users/tpoterba/hail/python/hail/dataset.py in count(self, genotypes); 1127 """"""; 1128; -> 1129 return dict(self._jvdf.count(genotypes).toJavaMap()); 1130; 1131 def deduplicate(self):. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 109 # deepest = env.jutils.deepestMessage(e.java_exception); 110 # msg = env.jutils.getMinimalMessage(e.java_exception); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$Inte",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:1014,protocol,protocol,1014,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['protocol'],['protocol']
Integrability,"pper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/methods/relatedness/pc_relate.py in pc_relate(call_expr, min_individual_maf, k, scores_expr, min_kinship, statistics, block_size, ; include_self_kinship); 314 ; 315 if k and scores_expr is None:; --> 316 _, scores, _ = hwe_normalized_pca(call_expr, k, compute_loadings=False); 317 scores_expr = scores[mt.col_key].scores; 318 elif not k and scores_expr is not None:. <decorator-gen-1778> in hwe_normalized_pca(call_expr, k, compute_loadings). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/methods/pca.py in hwe_normalized_pca(call_expr, k, compute_loadings); 99 return _hwe_normalized_blanczos(call_expr, k, compute_loadings); 100 ; --> 101 return pca(hwe_normalize(call_expr),; 102 k,; 103 compute_loadings). <decorator-gen-1780> in pca(entry_expr, k, compute_loadings). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/methods/pca.py in pca(entry_expr, k, compute_loadings); 209 'k': k,; 210 'computeLoadings': compute_loadings; --> 211 })).persist()); 212",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:1823,wrap,wrapper,1823,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,1,['wrap'],['wrapper']
Integrability,pr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:36); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.expr.ir.Compile$.$anonfun$apply$4(Compile.scala:45); E 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction(Backend.scala:126); E 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction$(Backend.scala:122); E 	at is.hail.backend.local.LocalBackend.lookupOrCompileCachedFunction(LocalBackend.scala:73); E 	at is.hail.expr.ir.Compile$.apply(Compile.scala:39); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$5(CompileAndEvaluate.scala:66); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:66); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); E 	at is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:8594,Wrap,WrappedArray,8594,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Wrap'],['WrappedArray']
Integrability,presumably related to: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/mysterious.20behavior/near/182390817 (log posted next message down),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7638#issuecomment-561430746:153,message,message,153,https://hail.is,https://github.com/hail-is/hail/issues/7638#issuecomment-561430746,1,['message'],['message']
Integrability,pretNonCompilablePass$.transform(LoweringPass.scala:69); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); E 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:454); E 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:490); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:75); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:75); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:63); E 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:342); E 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:487); E 	at is.hail.utils.ExecutionTimer$.time(Execut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229:4597,Wrap,WrappedArray,4597,https://hail.is,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229,1,['Wrap'],['WrappedArray']
Integrability,pretty sure the solution is to wrap the links in angle brackets,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7147#issuecomment-535985237:31,wrap,wrap,31,https://hail.is,https://github.com/hail-is/hail/issues/7147#issuecomment-535985237,1,['wrap'],['wrap']
Integrability,publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:99); 		at is.hail.shadedazure.reactor.core.publisher.Mono.block(Mono.java:1742); 		at is.hail.shadedazure.com.azure.storage.common.implementation.StorageImplUtils.blockWithOptionalTimeout(StorageImplUtils.java:133); 		at is.hail.shadedazure.com.azure.storage.blob.specialized.BlobClientBase.getPropertiesWithResponse(BlobClientBase.java:1379); 		at is.hail.shadedazure.com.azure.storage.blob.specialized.BlobClientBase.getProperties(BlobClientBase.java:1348); 		at is.hail.io.fs.AzureStorageFS.$anonfun$openNoCompression$1(AzureStorageFS.scala:223); 		at is.hail.io.fs.AzureStorageFS.$anonfun$handlePublicAccessError$1(AzureStorageFS.scala:175); 		at is.hail.services.package$.retryTransientErrors(package.scala:124); 		at is.hail.io.fs.AzureStorageFS.handlePublicAccessError(AzureStorageFS.scala:174); 		at is.hail.io.fs.AzureStorageFS.openNoCompression(AzureStorageFS.scala:220); 		at is.hail.io.fs.RouterFS.openNoCompression(RouterFS.scala:20); 		at is.hail.io.fs.FS.openNoCompression(FS.scala:322); 		at is.hail.io.fs.FS.openNoCompression$(FS.scala:322); 		at is.hail.io.fs.RouterFS.openNoCompression(RouterFS.scala:3); 		at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); 		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 		at is.hail.services.package$.retryTransientErrors(package.scala:124); 		at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:459); 		at is.hail.backend.service.Main$.main(Main.scala:15); 		at is.hail.backend.service.Main.main(Main.scala); 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 		at java.lang.reflect.Method.invoke(Method.java:498); 		at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 		at ja,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906430:1143,Rout,RouterFS,1143,https://hail.is,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906430,1,['Rout'],['RouterFS']
Integrability,"py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.ClassNotFoundException: com.amazonaws.AmazonClientException; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:419); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:352); 	... 27 more. During handling of the above exception, another exception occurred:. Py4JJavaError Traceback (most recent call last); /bmrn/apps/hail/0.2.72-spark-3.1.2/python/hail-0.2.72-py3-none-any.egg/hail/backend/py4j_backend.py in deco(*args, **kwargs); 15 try:; ---> 16 return f(*args, **kwargs); 17 except py4j.protocol.Py4JJavaError as e:. /bmrn/apps/python/miniconda/64/3.7/envs/piranha_0.2.0_20210812/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 327 ""An error occurred while calling {0}{1}{2}.\n"".; --> 328 format(target_id, ""."", name), value); 329 else:. Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.backend.spark.SparkBackend$.apply(SparkBackend.scala:218); 	at is.hail.backend.spark.SparkBackend.apply(SparkBackend.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:2792,protocol,protocol,2792,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,1,['protocol'],['protocol']
Integrability,"r received from the Java gateway into a Python object.; ; For example, string representation of integers are converted to Python; integer, string representation of objects are converted to JavaObject; instances, etc.; ; :param answer: the string returned by the Java gateway; :param gateway_client: the gateway client used to communicate with the Java; Gateway. Only necessary if the answer is a reference (e.g., object,; list, map); :param target_id: the name of the object from which the answer comes from; (e.g., *object1* in `object1.hello()`). Optional.; :param name: the name of the member from which the answer comes from; (e.g., *hello* in `object1.hello()`). Optional.; """"""; if is_error(answer)[0]:; if len(answer) > 1:; type = answer[1]; value = OUTPUT_CONVERTER[type](answer[2:], gateway_client); if answer[1] == REFERENCE_TYPE:; raise Py4JJavaError(; ""An error occurred while calling {0}{1}{2}.\n"".; format(target_id, ""."", name), value); else:; raise Py4JError(; ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; > format(target_id, ""."", name, value)); E py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; E py4j.Py4JException: Method apply([null, class java.lang.String, class scala.Some, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist; E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339); E 	at py4j.Gateway.invoke(Gateway.java:276); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCommand.java:79); E 	at py4j.GatewayConnection.run(GatewayConnection.java:238); E 	at java.lang.Thread.run(Thread.java:748). /miniconda3/envs/hail/lib/python3.6/site-packages/py4j/protocol.py:332: Py4JError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:4086,protocol,protocol,4086,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928,2,['protocol'],['protocol']
Integrability,"r, k, compute_loadings); 99 return _hwe_normalized_blanczos(call_expr, k, compute_loadings); 100 ; --> 101 return pca(hwe_normalize(call_expr),; 102 k,; 103 compute_loadings). <decorator-gen-1780> in pca(entry_expr, k, compute_loadings). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/methods/pca.py in pca(entry_expr, k, compute_loadings); 209 'k': k,; 210 'computeLoadings': compute_loadings; --> 211 })).persist()); 212 ; 213 g = t.index_globals(). <decorator-gen-1340> in persist(self, storage_level). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in persist(self, storage_level); 2110 Persisted table.; 2111 """"""; -> 2112 return Env.backend().persist(self); 2113 ; 2114 def unpersist(self) -> 'Table':. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py in persist(self, dataset); 167 from hail.context import TemporaryFilename; 168 tempfile = TemporaryFilename(prefix=f'persist_{type(dataset).__name__}'); --> 169 persisted = dataset.checkpoint(tempfile.__enter__()); 170 self._persisted_locations[persisted] = (tempfile, dataset); 171 return persisted. <decorator-gen-1330> in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals). /opt/conda/minicon",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:2893,wrap,wrapper,2893,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,2,['wrap'],['wrapper']
Integrability,"rce('vds'), '1kg_chr22_5_samples.vds')); ; vds1 = hl.vds.filter_intervals(vds,; [hl.parse_locus_interval('chr22:start-10754094', reference_genome='GRCh38')],; split_reference_blocks=True); vds2 = hl.vds.filter_intervals(vds,; [hl.parse_locus_interval('chr22:10754094-end', reference_genome='GRCh38')],; split_reference_blocks=True); ; ; vds_union = vds1.union_rows(vds2); > assert hl.vds.to_dense_mt(vds)._same(hl.vds.to_dense_mt(vds_union)). test/hail/vds/test_vds.py:597: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; <decorator-gen-1386>:2: in _same; ???; /usr/local/lib/python3.9/dist-packages/hail/typecheck/check.py:587: in wrapper; return __original_func(*args_, **kwargs_); /usr/local/lib/python3.9/dist-packages/hail/matrixtable.py:3762: in _same; return self._localize_entries(entries_name, cols_name)._same(; <decorator-gen-1276>:2: in _same; ???; /usr/local/lib/python3.9/dist-packages/hail/typecheck/check.py:587: in wrapper; return __original_func(*args_, **kwargs_); /usr/local/lib/python3.9/dist-packages/hail/table.py:3658: in _same; mismatched_globals, mismatched_rows = t.aggregate(hl.tuple((; <decorator-gen-1216>:2: in aggregate; ???; /usr/local/lib/python3.9/dist-packages/hail/typecheck/check.py:587: in wrapper; return __original_func(*args_, **kwargs_); /usr/local/lib/python3.9/dist-packages/hail/table.py:1285: in aggregate; return Env.backend().execute(hl.ir.MakeTuple([agg_ir]))[0]; /usr/local/lib/python3.9/dist-packages/hail/backend/py4j_backend.py:86: in execute; raise e.maybe_user_error(ir) from None; /usr/local/lib/python3.9/dist-packages/hail/backend/py4j_backend.py:76: in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); /usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1321: in __call__; return_value = get_return_value(; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . args = ('xro552', <py4j.java_gateway.GatewayClient object at 0x7f01e1182160>",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:1299,wrap,wrapper,1299,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['wrap'],['wrapper']
Integrability,"ready for a look. Also, the bug in annotatevariants tsv (the length checking on string instead of split) was only in the fatal message, not the actual check.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/324#issuecomment-213010784:127,message,message,127,https://hail.is,https://github.com/hail-is/hail/pull/324#issuecomment-213010784,1,['message'],['message']
Integrability,reassigned since Arcturus has a lot of PRs right now and I had non-randomly assigned this based on the dependent PR's assignment anyway,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7540#issuecomment-555034067:103,depend,dependent,103,https://hail.is,https://github.com/hail-is/hail/pull/7540#issuecomment-555034067,1,['depend'],['dependent']
Integrability,"refactored the four block matrix operators to match for now, I'll later change this interface to be numpy-like and match local matrix",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2995#issuecomment-369619464:84,interface,interface,84,https://hail.is,https://github.com/hail-is/hail/pull/2995#issuecomment-369619464,1,['interface'],['interface']
Integrability,"return CreateDatabaseStep(; + return CreateDatabase2Step(; params,; json['databaseName'],; json['namespace'],; @@ -1111,12 +1113,12 @@ EOF; attributes={'name': self.name},; secrets=[; {; - 'namespace': self.database_server_config_namespace,; + 'namespace': self.namespace,; 'name': 'database-server-config',; 'mount_path': '/sql-config',; }; ],; - service_account={'namespace': DEFAULT_NAMESPACE, 'name': 'ci-agent'},; + service_account={'namespace': self.namespace, 'name': 'admin'},; input_files=input_files,; parents=[self.create_passwords_job] if self.create_passwords_job else self.deps_parents(),; network='private',; @@ -1125,42 +1127,4 @@ EOF; ); ; def cleanup(self, batch, scope, parents):; - if scope in ['deploy', 'dev'] or self.cant_create_database:; - return; -; - cleanup_script = f'''; -set -ex; -; -commands=$(mktemp); -; -cat >$commands <<EOF; -DROP DATABASE IF EXISTS \\`{self._name}\\`;; -DROP USER IF EXISTS '{self.admin_username}';; -DROP USER IF EXISTS '{self.user_username}';; -EOF; -; -until mysql --defaults-extra-file=/sql-config/sql-config.cnf <$commands; -do; - echo 'failed, will sleep 2 and retry'; - sleep 2; -done; -; -'''; -; - self.cleanup_job = batch.create_job(; - CI_UTILS_IMAGE,; - command=['bash', '-c', cleanup_script],; - attributes={'name': f'cleanup_{self.name}'},; - secrets=[; - {; - 'namespace': self.database_server_config_namespace,; - 'name': 'database-server-config',; - 'mount_path': '/sql-config',; - }; - ],; - service_account={'namespace': DEFAULT_NAMESPACE, 'name': 'ci-agent'},; - parents=parents,; - always_run=True,; - network='private',; - regions=[REGION],; - ); + pass; diff --git a/ci/test/resources/build.yaml b/ci/test/resources/build.yaml; index e6f67bb486..662c873590 100644; --- a/ci/test/resources/build.yaml; +++ b/ci/test/resources/build.yaml; @@ -190,7 +190,7 @@ steps:; to: /io/pyproject.toml; dependsOn:; - hello_image; - - kind: createDatabase; + - kind: createDatabase2; name: hello_database; databaseName: hello; image:; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600:3975,depend,dependsOn,3975,https://hail.is,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600,1,['depend'],['dependsOn']
Integrability,"rn gradle, apparently](https://docs.gradle.org/current/userguide/java_library_plugin.html#sec:java_library_configurations_graph). I found a side-note about the `compile` configuration [here](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:java_dependency_management_overview) (search for ""compile""):. > **Why no compile configuration?**; > The Java Library Plugin has historically used the compile configuration for dependencies that are required to both compile and run a projects production code. It is now deprecated, and will issue warnings when used, because it doesnt distinguish between dependencies that impact the public API of a Java library project and those that dont. You can learn more about the importance of this distinction in [Building Java libraries](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:building_java_libraries). OK, so, we used to just dump everything into our runtime dependencies. I changed it so that we have three kinds of dependencies:; 1. `shadow`: these are provided by Dataproc/QoB at run-time. They are not in any JAR. They are not on the `testRuntimeClasspath` or `runtimeClasspath`. They are on the `testCompileClasspath` because I [explicitly requested](https://github.com/hail-is/hail/blob/main/hail/build.gradle#L98) that `testCompileOnly` bring in our `shadow` dependencies.; 2. `implementation`: these are included in all class paths and in shadow JARs (but not ""thin"" jars generated by `./gradlew jar`).; 3. `testImplementation`: these are included in test class paths and in shadow JARs. Our test code references these third-party classes:; ```; import breeze.linalg.DenseMatrix; import breeze.linalg._; import breeze.linalg.{*, diag, DenseMatrix => BDM, DenseVector => BDV}; import breeze.linalg.{DenseMatrix => BDM, _}; import breeze.linalg.{DenseMatrix => BDM}; import breeze.linalg.{DenseMatrix, DenseVector, eigSym, svd}; import breeze.linalg.{DenseMatrix, DenseVector}; import breeze.linal",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741:1417,depend,dependencies,1417,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741,1,['depend'],['dependencies']
Integrability,"runtime in the container status is a little confusing. It's fine for now, but for user interface, runtime should be a top level state, and starting and running should be substates. But again, this is fine for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7354#issuecomment-545104378:87,interface,interface,87,https://hail.is,https://github.com/hail-is/hail/pull/7354#issuecomment-545104378,1,['interface'],['interface']
Integrability,s.scala:95); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerArrayAggsToRunAggsPass$.apply(LoweringPass.scala:89); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:45); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:314); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:308); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:25); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:23); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:12); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:23); 	at is.hail.backend.service.ServiceBackend.userContext(ServiceBackend.scala:122); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11.apply(ServiceBackend.scala:308); 	at is.hail.ba,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:10925,Wrap,WrappedArray,10925,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693,1,['Wrap'],['WrappedArray']
Integrability,s.scala:95); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerArrayAggsToRunAggsPass$.apply(LoweringPass.scala:89); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:45); 	at is.hail.backend.service.ServiceBackend.is$hail$backend$service$ServiceBackend$$execute(ServiceBackend.scala:321); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:337); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:334); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:25); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:23); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:23); 	at is.hail.backend.service.ServiceBackend.userContext(ServiceBackend.scala:132); 	at is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:11205,Wrap,WrappedArray,11205,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601,1,['Wrap'],['WrappedArray']
Integrability,"s=False); 317 scores_expr = scores[mt.col_key].scores; 318 elif not k and scores_expr is not None:. <decorator-gen-1778> in hwe_normalized_pca(call_expr, k, compute_loadings). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/methods/pca.py in hwe_normalized_pca(call_expr, k, compute_loadings); 99 return _hwe_normalized_blanczos(call_expr, k, compute_loadings); 100 ; --> 101 return pca(hwe_normalize(call_expr),; 102 k,; 103 compute_loadings). <decorator-gen-1780> in pca(entry_expr, k, compute_loadings). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/methods/pca.py in pca(entry_expr, k, compute_loadings); 209 'k': k,; 210 'computeLoadings': compute_loadings; --> 211 })).persist()); 212 ; 213 g = t.index_globals(). <decorator-gen-1340> in persist(self, storage_level). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in persist(self, storage_level); 2110 Pers",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:2249,wrap,wrapper,2249,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,2,['wrap'],['wrapper']
Integrability,se/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242); 	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224); 	at java.base/java.net.Socket.connect(Socket.java:591); 	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177); 	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474); 	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569); 	at java.base/sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:341); 	at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:362); 	at java.base/sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1242); 	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1181); 	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1075); 	at java.base/sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:1009); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:151); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1012); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:196); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:470); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:251); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactor,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:5698,protocol,protocol,5698,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,2,['protocol'],['protocol']
Integrability,see the comment - Arcturus approved but was waiting on dependent PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4971#issuecomment-448789826:55,depend,dependent,55,https://hail.is,https://github.com/hail-is/hail/pull/4971#issuecomment-448789826,1,['depend'],['dependent']
Integrability,"self.assertEqual is unittest style. We use pytest now, which rewrites Python assert statements to provide good failure messages. New code should use `assert`, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7611#issuecomment-559186521:119,message,messages,119,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-559186521,1,['message'],['messages']
Integrability,"sertionError: assertion failed; self = <test.hail.linalg.test_linalg.Tests testMethod=test_tree_matmul>. @fails_service_backend(); @fails_local_backend(); def test_tree_matmul(self):; nm = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]); m = BlockMatrix.from_numpy(nm, block_size=2); nrow = np.array([[7.0, 8.0, 9.0]]); row = BlockMatrix.from_numpy(nrow, block_size=2); ; > with BatchedAsserts() as b:. test/hail/linalg/test_linalg.py:612: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/hail/linalg/test_linalg.py:96: in __exit__; vals.extend(list(hl.eval(tuple([all_bms[k] for k in bm_keys[batch_start:batch_start + batch_size]])))); <decorator-gen-692>:2: in eval; ???; hail/typecheck/check.py:577: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:191: in eval; return eval_timed(expression)[0]; <decorator-gen-690>:2: in eval_timed; ???; hail/typecheck/check.py:577: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:161: in eval_timed; return Env.backend().execute(MakeTuple([ir]), timed=True)[0]; hail/backend/py4j_backend.py:82: in execute; raise e.maybe_user_error(ir) from None; hail/backend/py4j_backend.py:76: in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); ../../.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321: in __call__; return_value = get_return_value(; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . args = ('xro549', <py4j.clientserver.JavaClient object at 0x7fd0d58f6fb0>, 'o1', 'executeEncode'); kwargs = {}; pyspark = <module 'pyspark' from '/home/edmund/.local/src/hail/.venv/lib/python3.10/site-packages/pyspark/__init__.py'>; s = 'java.lang.AssertionError: assertion failed', tpl = JavaObject id=o550; deepest = 'AssertionError: assertion failed'; full = 'java.lang.AssertionError: assertion failed\n\tat scala.Predef$.assert(Predef.scala:208)\n\tat is.hail.e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229:1031,wrap,wrapper,1031,https://hail.is,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229,1,['wrap'],['wrapper']
Integrability,"so like to report a similar `orjson.JSONDecodeError: unexpected character: line 1 column 1 (char 0)` bug first reported by https://discuss.hail.is/t/hail-fails-after-installing-it-on-a-single-computer/3653. Hail installed from https://anaconda.org/sfe1ed40/hail; EDIT: the same error occurs after `pip install hail` into a fresh conda env, which produced hail `version 0.2.130-bea04d9c79b5`. Terminal output: ; ```; Python 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail as hl; hl.init(); >>> hl.init(); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.4.1; SparkUI available at http://xxxx:xxxx; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.127-d18228b9bc5b; LOGGING: writing to xxxx.log; >>> hl.utils.range_table(10).collect(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1234>"", line 2, in collect; File ""/xxxx/lib/python3.10/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/xxxx/lib/python3.10/site-packages/hail/table.py"", line 2213, in collect; return Env.backend().execute(e._ir, timed=_timed); File ""/xxxx/lib/python3.10/site-packages/hail/backend/backend.py"", line 188, in execute; result, timings = self._rpc(ActionTag.EXECUTE, payload); File ""/xxxx/lib/python3.10/site-packages/hail/backend/py4j_backend.py"", line 219, in _rpc; error_json = orjson.loads(resp.content); orjson.JSONDecodeError: unexpected character: line 1 column 1 (char 0); ```. Log file:; ```; 2024-04-25 16:07:16.773 Hail: INFO: SparkUI: http://xxxx:xxxx; 2024-04-25 16:07:21.589 Hail: INFO: Running Hail version 0.2.127-d18228b9bc5b; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14049#issuecomment-2077624076:1353,wrap,wrapper,1353,https://hail.is,https://github.com/hail-is/hail/issues/14049#issuecomment-2077624076,1,['wrap'],['wrapper']
Integrability,"sorry my internet was bad and wasn't reloading the bottom of the page for a while. Can respond now:. > As an aside, ApplySpecial seems to allow operations on non-identical types, which would require algebraic types, rather than a singular: any time the types of itsSeq[IR]'s are different. > I have similar issues with other array operations, which want nodes that we currently don't support, including aggregator nodes. Note that I am only calling inferSetPType in Compile (immediately after optimization pass). The ptype inference for apply methods is handled by some stuff I wrote recently. IRFunction now has a `returnPType` method that takes arg ptypes. > PVoid. Void isn't a catch-all type like Nothing in Scala - it's a specific I-don't-return-anything type used by IRs like TableWrite. The exception in your above message is coming from the Apply node being inferred as a `PVoid` by your `case _ => PVoid` code. Writing the rule for the apply node should fix that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6594#issuecomment-513005812:822,message,message,822,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-513005812,1,['message'],['message']
Integrability,"sorry, I didn't quite understand your last comment. Suppose we have a `PFixedLengthArray(length: Int, element: PType, required: Boolean)`. We need an algorithm that will have the following properties:; ```; unify(; PCanonicalArray(PInt32Required, true),; PFixedLengthArray(1, PInt32Required, false)); returns PCanonicalArray(PInt32Required, false); ```. ```; unify(; PFixedLengthArray(1, PInt32Required, false)); PCanonicalArray(PInt32Required, true),; returns PCanonicalArray(PInt32Required, false); ```. ```; unify(; PFixedLengthArray(1, PInt32Required, false),; PFixedLengthArray(1, PInt32Required, true)); returns PFixedLengthArray(1, PInt32Required, false); ```. calling something on the head is wrong, as is matching on the head alone. This is going to be a complicated thing, and I want all the logic (at least for each interface like PArray) in the same place.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7927#issuecomment-576372543:827,interface,interface,827,https://hail.is,https://github.com/hail-is/hail/pull/7927#issuecomment-576372543,1,['interface'],['interface']
Integrability,"src/hail/hail/python/hail/typecheck/check.py"", line 364, in f; ret = x(*args); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 221, in <lambda>; return collection._to_stream().fold(lambda x, y: f(x, y), zero); File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 364, in f; ret = x(*args); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 68, in <lambda>; return hl.array(self).fold(lambda accum, elt: accum | f(elt), False); File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 364, in f; ret = x(*args); File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 364, in f; ret = x(*args); File ""test.py"", line 11, in <lambda>; (mt.locus.contig == hl.literal(x[0])) & \; File ""<decorator-gen-690>"", line 2, in literal; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/functions.py"", line 261, in literal; return literal(hl.eval(to_expr(x, dtype)), dtype); File ""<decorator-gen-668>"", line 2, in eval; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 223, in eval; return eval_timed(expression)[0]; File ""<decorator-gen-666>"", line 2, in eval_timed; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 189, in eval_timed; return _eval_many(expression, timed=True, name='eval_timed')[0]; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 150, in _eval_many; return Env.backen",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:7445,wrap,wrapper,7445,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['wrap'],['wrapper']
Integrability,"sses; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gra",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:3326,Wrap,WrappedArray,3326,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925,1,['Wrap'],['WrappedArray']
Integrability,still a bad error message. almost done with a fix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4078#issuecomment-410711084:18,message,message,18,https://hail.is,https://github.com/hail-is/hail/issues/4078#issuecomment-410711084,1,['message'],['message']
Integrability,t java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399); 	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242); 	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224); 	at java.base/java.net.Socket.connect(Socket.java:591); 	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177); 	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474); 	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569); 	at java.base/sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:341); 	at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:362); 	at java.base/sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1242); 	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1181); 	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1075); 	at java.base/sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:1009); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:151); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1012); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:196); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:470); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:251); 	at com.google.clou,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:5597,protocol,protocol,5597,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,2,['protocol'],['protocol']
Integrability,"t recent call last):; File ""<frozen runpy>"", line 198, in _run_module_as_main; File ""<frozen runpy>"", line 88, in _run_code; File ""/Users/jigold/projects/hail/hail/python/hailtop/aiotools/copy.py"", line 129, in <module>; asyncio.run(main()); File ""/Users/jigold/anaconda3/envs/py311/lib/python3.11/asyncio/runners.py"", line 190, in run; return runner.run(main); ^^^^^^^^^^^^^^^^; File ""/Users/jigold/anaconda3/envs/py311/lib/python3.11/asyncio/runners.py"", line 118, in run; return self._loop.run_until_complete(task); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""uvloop/loop.pyx"", line 1517, in uvloop.loop.Loop.run_until_complete; File ""/Users/jigold/projects/hail/hail/python/hailtop/aiotools/copy.py"", line 119, in main; await copy_from_dict(; File ""/Users/jigold/projects/hail/hail/python/hailtop/aiotools/copy.py"", line 85, in copy_from_dict; await copy(; File ""/Users/jigold/projects/hail/hail/python/hailtop/aiotools/copy.py"", line 49, in copy; async with RouterAsyncFS(local_kwargs=local_kwargs,; File ""/Users/jigold/projects/hail/hail/python/hailtop/aiotools/fs/fs.py"", line 301, in __aexit__; await self.close(); File ""/Users/jigold/projects/hail/hail/python/hailtop/aiotools/router_fs.py"", line 135, in close; await fs.close(); File ""/Users/jigold/projects/hail/hail/python/hailtop/aiocloud/aioazure/fs.py"", line 555, in close; await asyncio.wait([client.close() for client in self._blob_service_clients.values()]); File ""/Users/jigold/anaconda3/envs/py311/lib/python3.11/asyncio/tasks.py"", line 415, in wait; raise TypeError(""Passing coroutines is forbidden, use tasks explicitly.""); TypeError: Passing coroutines is forbidden, use tasks explicitly.; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x103a604d0>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1048aa040>, 1557505.418), (<aiohttp.client_proto.ResponseHandler object at 0x1048aa0b0>, 1557505.649)]']; connector: <aiohttp.connector.TCPConnector object a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13101#issuecomment-1559996129:1050,Rout,RouterAsyncFS,1050,https://hail.is,https://github.com/hail-is/hail/pull/13101#issuecomment-1559996129,1,['Rout'],['RouterAsyncFS']
Integrability,tException: connect timed out; 	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399); 	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242); 	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224); 	at java.base/java.net.Socket.connect(Socket.java:591); 	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177); 	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474); 	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569); 	at java.base/sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:341); 	at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:362); 	at java.base/sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1242); 	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1181); 	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1075); 	at java.base/sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:1009); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:151); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1012); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:196); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:470); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.Cred,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:5495,protocol,protocol,5495,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,2,['protocol'],['protocol']
Integrability,"tHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14747?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14751** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14751?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14747** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14747?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * **#14684** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14684?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 1 other dependent PR ([#14686](https://github.com/hail-is/hail/pull/14686) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14686?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14683** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14683?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @grohli and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14747#issuecomment-2438734907:1094,depend,dependent,1094,https://hail.is,https://github.com/hail-is/hail/pull/14747#issuecomment-2438734907,2,['depend'],"['dependencies', 'dependent']"
Integrability,"tHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14751?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14751** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14751?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * **#14747** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14747?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14684** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14684?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 1 other dependent PR ([#14686](https://github.com/hail-is/hail/pull/14686) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14686?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14683** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14683?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @grohli and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14751#issuecomment-2457987492:1094,depend,dependent,1094,https://hail.is,https://github.com/hail-is/hail/pull/14751#issuecomment-2457987492,2,['depend'],"['dependencies', 'dependent']"
Integrability,tI$25(Emit.scala:816); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at scala.collection.TraversableLike.map(TraversableLike.scala:286); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279); 	at scala.collection.AbstractTraversable.map(Traversable.scala:108); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:815); 	at is.hail.expr.ir.Emit$.$anonfun$apply$4(Emit.scala:99); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:19); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedCode(EmitCodeBuilder.scala:24); 	at is.hail.expr.ir.EmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1044); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1095); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder$(EmitClassBuilder.scala:1095); 	at is.hail.expr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1192); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:97); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:78); 	at is.hail.TestUtils$.eval(TestUtils.scala:256); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$5(TestUtils.scala:366); 	at scala.collection.immutable.Set$Set4.foreach(Set.scala:289); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$4(TestUtils.scala:348); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$4$adapted(TestUtils.scala:339); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:46); 	at is.hail.backend.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10330#issuecomment-827119604:1738,Wrap,WrappedEmitMethodBuilder,1738,https://hail.is,https://github.com/hail-is/hail/pull/10330#issuecomment-827119604,1,['Wrap'],['WrappedEmitMethodBuilder']
Integrability,the error message? or what?. Can you write a docstring for that function?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1854#issuecomment-302792319:10,message,message,10,https://hail.is,https://github.com/hail-is/hail/pull/1854#issuecomment-302792319,1,['message'],['message']
Integrability,"think helped to reduce the number of places that are aware of the notion of a default_region. It's really now just isolated to the `InstanceCollectionManager`, since that's the piece of code that's making the decision ""use the default region when the cluster is small"". I didn't quite like the pattern of retrieving a default from a `LocationMonitor` just to give it right back to the location monitor in the next line. I think this way the `LocationMonitor` API is much simpler, and we can actually remove its `default_location` method entirely as I believe it is no longer used. I can do that in a follow-up PR if you like this approach. One other thing is I wanted to articulate the distinction between the ""region CI needs its jobs in"" and the ""default region that batch will spin up workers in for small clusters"". While they are in practice the same, I found that treating them both as the ""default_region"" tied the logic around jobs for CI closely with internal Batch decisions and made it more confusing for me to reason about. I tried to separate out these two concepts so that in the future when jobs support region-specific scheduling it will be easier to excise the CI-specific code from the Batch scheduler. Another thing that I realized during this process is that Azure has regions and zones just like GCP, though they may differ slightly since we don't need to specify a zone for a VM and such. I am fine with using the term ""location"" to mean ""where we scheduled the VM, either zone or region depending on the cloud provider"", but I would also like to follow up with a sweep that makes this language more precise where possible. For example, the `possible_cloud_locations` function is really just `possible_cloud_regions`, and we could even go so far as mandating a `region` field in the global config instead of having `azure_location` and `gcp_region`, which are synonymous even though named differently. It also leads me to wonder why we only schedule in a single region in Azure?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12078#issuecomment-1207095240:1535,depend,depending,1535,https://hail.is,https://github.com/hail-is/hail/pull/12078#issuecomment-1207095240,1,['depend'],['depending']
Integrability,"this is not a great error message. The problem is you're missing the lz4 dependency, see here:; https://hail.is/docs/0.2/getting_started.html?highlight=lz4#requirements",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7464#issuecomment-550295122:26,message,message,26,https://hail.is,https://github.com/hail-is/hail/issues/7464#issuecomment-550295122,2,"['depend', 'message']","['dependency', 'message']"
Integrability,this is totally possible in the current Python interface.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1073#issuecomment-365624131:47,interface,interface,47,https://hail.is,https://github.com/hail-is/hail/issues/1073#issuecomment-365624131,1,['interface'],['interface']
Integrability,this whole interface is gone,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/49#issuecomment-422364141:11,interface,interface,11,https://hail.is,https://github.com/hail-is/hail/issues/49#issuecomment-422364141,1,['interface'],['interface']
Integrability,"tic/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used in that package. This can be used with Kubernetes. `kubectl create secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=https://hail.auth0.com/api/v2/; ```. Organization of the web app is simple. There is a pa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:2362,interface,interface,2362,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935,1,['interface'],['interface']
Integrability,"tiveMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). During handling of the above exception, another exception occurred:. Py4JJavaError Traceback (most recent call last); /bmrn/apps/hail/0.2.72-spark-3.1.2/python/hail-0.2.72-py3-none-any.egg/hail/backend/py4j_backend.py in deco(*args, **kwargs); 15 try:; ---> 16 return f(*args, **kwargs); 17 except py4j.protocol.Py4JJavaError as e:. /bmrn/apps/python/miniconda/64/3.7/envs/piranha_0.2.0_20210812/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 327 ""An error occurred while calling {0}{1}{2}.\n"".; --> 328 format(target_id, ""."", name), value); 329 else:. Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.backend.spark.SparkBackend$.apply(SparkBackend.scala:218); 	at is.hail.backend.spark.SparkBackend.apply(SparkBackend.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:4448,protocol,protocol,4448,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,1,['protocol'],['protocol']
Integrability,"to 0.1, you mean? That's an interface change :|",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2582#issuecomment-352084718:28,interface,interface,28,https://hail.is,https://github.com/hail-is/hail/pull/2582#issuecomment-352084718,1,['interface'],['interface']
Integrability,"to answer (1), I think the right thing is for us to implement our own hashable immutable data structures (and use frozenset for sets, for instance) for results of Hail computations. I think we have yet to nail down whether this would be a breaking interface change, forcing us to wait until 0.3. To answer (2), you *may* be able to do `hl.stop(); hl.init()` to reset the session, but not sure this will work in every case. The driver should really only die for OOM and faults, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8076#issuecomment-584795090:248,interface,interface,248,https://hail.is,https://github.com/hail-is/hail/issues/8076#issuecomment-584795090,1,['interface'],['interface']
Integrability,"to become C++11-compliant. Systems using the earlier libstdc++; (which also typically have g++-4.8.x/4.9x) have the old-ABI not-fully-compliant std::string; (and std::list, but I've gone 20 years without ever using that). Later versions of libstdc++ have *both* flavors of std::string, but use namespaces to allow them; to coexist (but not to be interchangeable, so interfaces between old-ABI and new-ABI are; problematic). https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html. ""In the GCC 5.1 release libstdc++ introduced a new library ABI that includes new implementations of std::string and std::list. These changes were necessary to conform to the 2011 C++ standard which forbids Copy-On-Write strings and requires lists to keep track of their size. In order to maintain backwards compatibility for existing code linked to libstdc++ the library's soname has not changed and the old implementations are still supported in parallel with the new ones. This is achieved by defining the new implementations in an inline namespace so they have different names for linkage purposes, e.g. the new version of std::list<int> is actually defined as std::__cxx11::list<int>. Because the symbols for the new implementations have different names the definitions for both versions can be present in the same library."". OSX doesn't have this ABI-compatibility issue because for several years it has been using; libc++ as the default library, and libc++ is a post-C++11 rewrite-from-scratch implementation; of the required standard-library functionality. My understanding is that it is perfectly feasible to mix and match different g++/clang++compiler; versions and different libstdc++ versions. It just doesn't happen very much because both the; g++ version and the libstdc++ are chosen at the same time, early in building a Linux distribution,; and then are frozen throughout the release's lifetime to ensure interoperability of binaries. So debian8's version of g++ and libstdc++ doesn't change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424797612:2382,interoperab,interoperability,2382,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424797612,1,['interoperab'],['interoperability']
Integrability,"ugh. This must depend on another PR I have open, to remove that assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5740#issuecomment-478736550:15,depend,depend,15,https://hail.is,https://github.com/hail-is/hail/pull/5740#issuecomment-478736550,1,['depend'],['depend']
Integrability,ugh. the printed form of UnsafeRow is a public API? surely no one actually depends on this,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8191#issuecomment-592571143:75,depend,depends,75,https://hail.is,https://github.com/hail-is/hail/pull/8191#issuecomment-592571143,1,['depend'],['depends']
Integrability,unassigning while I figure out dependency issues.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12433#issuecomment-1307291111:31,depend,dependency,31,https://hail.is,https://github.com/hail-is/hail/pull/12433#issuecomment-1307291111,1,['depend'],['dependency']
Integrability,"unless -- the dependent PR was fully rebased, and so the SHA of the merged master was identical! That's actually possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5787#issuecomment-480269446:14,depend,dependent,14,https://hail.is,https://github.com/hail-is/hail/pull/5787#issuecomment-480269446,1,['depend'],['dependent']
Integrability,"update: `calculateKeyRanges` doesn't work correctly when there's only one element (which we never hit, since `coerce` considers those to be sorted), which was causing the shuffle to drop the only moved element in some of my tests (which is why the tests were running so much faster than the non-shuffling version). . I did fix something in the ReorderedPartitions dependencies that was causing an extra partition to be included in the dependencies during the `union_rows`, and now this branch (sans explode_rows changes) is running at about the same speed as SplitMulti was previously, although there's still generally noticeable bump for the first iteration. so @tpoterba I think this is probably set for review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4076#issuecomment-410349941:364,depend,dependencies,364,https://hail.is,https://github.com/hail-is/hail/pull/4076#issuecomment-410349941,2,['depend'],['dependencies']
Integrability,"urden(key_name='gene',; [13:46:55]	[:makeHailDocs] variant_keys='va.genes',; [13:46:55]	[:makeHailDocs] single_key='false',; [13:46:55]	[:makeHailDocs] agg_expr='gs.map(g => g.gt).max()',; [13:46:55]	[:makeHailDocs] y='sa.burden.pheno',; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] Exception raised:; [13:46:55]	[:makeHailDocs] Traceback (most recent call last):; [13:46:55]	[:makeHailDocs] File ""/usr/lib64/python2.7/doctest.py"", line 1315, in __run; [13:46:55]	[:makeHailDocs] compileflags, 1) in test.globs; [13:46:55]	[:makeHailDocs] File ""<doctest default[0]>"", line 7, in <module>; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] File ""<decorator-gen-233>"", line 2, in linreg_burden; [13:46:55]	[:makeHailDocs] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 119, in handle_py4j; [13:46:55]	[:makeHailDocs] 'Error summary: %s' % (msg, e.message, Env.hc().version, msg)); [13:46:55]	[:makeHailDocs] FatalError: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Java stack trace:; [13:46:55]	[:makeHailDocs] An error occurred while calling o3918.linregBurden. Trace:; [13:46:55]	[:makeHailDocs] py4j.Py4JException: Method linregBurden([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class [Ljava.lang.String;]) does not exist; [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); [13:46:55]	[:makeHailDocs] 	at py4j.Gateway.invoke(Gateway.java:272); [13:46:55]	[:makeHailDocs] 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); [13:46:55]	[:makeHailDocs] 	at py4j.commands.CallCommand.execute(CallCommand.java",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203:1430,message,message,1430,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203,1,['message'],['message']
Integrability,"use a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14533?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14533** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14533?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> ; * **#14509** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14509?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14514** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14514?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 1 other dependent PR ([#14554](https://github.com/hail-is/hail/pull/14554) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14554?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14517** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14517?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14533#issuecomment-2098889324:1094,depend,dependent,1094,https://hail.is,https://github.com/hail-is/hail/pull/14533#issuecomment-2098889324,2,['depend'],"['dependencies', 'dependent']"
Integrability,"way_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. args = ('xro1961', <py4j.java_gateway.GatewayClient object at 0x7ffa62e9e390>, 'z:is.hail.expr.ir.IRParser', 'parse_value_ir'), kwargs = {}; pyspark = <module 'pyspark' from '/miniconda3/lib/python3.7/site-packages/pyspark/__init__.py'>, s = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])'; tpl = JavaObject id=o1962, deepest = 'NoSuchElementException: next on empty iterator'; full = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])\n\tat is.hail.expr.ir.IR$class.typ(IR....a:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: NoSuchElementException: next on empty iterator; E ; E Java stack trace:; E java.lang.RuntimeException: typ: inference failure: ; E (MakeArray Array[Int32]); E 	at is.hail.expr.ir.IR$class.typ(IR.scala:34); E 	at is.hail.expr.ir.MakeArray.typ(IR.scala:135); E 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:889); E 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:680); E 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Pa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:3109,protocol,protocol,3109,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930,1,['protocol'],['protocol']
Integrability,we can add a check in CI to look at commit messages and reject if the only difference between the branch and main is a single commit that's fewer than X chars or somethign,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9594#issuecomment-725023994:43,message,messages,43,https://hail.is,https://github.com/hail-is/hail/pull/9594#issuecomment-725023994,1,['message'],['messages']
Integrability,we have better error messages now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202#issuecomment-317747531:21,message,messages,21,https://hail.is,https://github.com/hail-is/hail/issues/1202#issuecomment-317747531,1,['message'],['messages']
Integrability,we've got another approach to wrapping now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7448#issuecomment-602325937:30,wrap,wrapping,30,https://hail.is,https://github.com/hail-is/hail/pull/7448#issuecomment-602325937,1,['wrap'],['wrapping']
Integrability,what do you think about keeping these interfaces private while we digest the change and how we want things to look in Python? That way we don't commit to anything right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4995#issuecomment-448331406:38,interface,interfaces,38,https://hail.is,https://github.com/hail-is/hail/pull/4995#issuecomment-448331406,1,['interface'],['interfaces']
Integrability,"with the new mypy update, mypy complains if we don't use the type stubs for our dependencies. This is fixed in main (we add the type stubs) but not in the previously released pip hail, because well it's already released. One option is we decide we don't like this requirement and disable that for mypy (though I do enjoy having the type hints). The problem remains that we lint the released version with the `setup.cfg` on main, so this will fail if we ever tighten our linting. It's not clear to me why we want to lint already-released hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11502#issuecomment-1061947409:80,depend,dependencies,80,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1061947409,1,['depend'],['dependencies']
Integrability,"xample error msgs:; ```; In [2]: vds.linreg([]); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-2-b8bbc41a5ebd> in <module>(); ----> 1 vds.linreg([]). /Users/tpoterba/hail/python/hail/dataset.py in linreg(self, y, covariates, root, min_ac, min_af); 2216 """"""; 2217; -> 2218 jvds = self._jvdf.linreg(y, jarray(env.jvm.java.lang.String, covariates), root, min_ac, min_af); 2219 return VariantDataset(self.hc, jvds); 2220. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 113 if e.args[0].startswith('An error occurred while calling'):; 114 msg = 'An error occurred while calling into JVM, probably due to invalid parameter types'; --> 115 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (msg, e.message, msg)); 116; 117 return deco. FatalError: An error occurred while calling into JVM, probably due to invalid parameter types. Java stack trace:; An error occurred while calling o29.linreg. Trace:; py4j.Py4JException: Method linreg([class java.util.ArrayList, class [Ljava.lang.String;, class java.lang.String, class java.lang.Integer, class java.lang.Double]) does not exist; 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); 	at py4j.Gateway.invoke(Gateway.java:272); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787:1011,protocol,protocol,1011,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787,1,['protocol'],['protocol']
Integrability,"y current rough list of things to be done before hail2 is as usable as hail1. It's still pretty long!. ## Necessary code work:; - Add the rest of the core methods from VDS/KT to api2 (#2591 does most for KT, order_by is the only outstanding KT method that's not moved to table there. Same needs to be done for VDS, this isn't too hard); - Add the non-core methods to `hail.methods` / `hail.genetics.methods`; - some stuff here is much harder than the rest, like `filter_alleles`; - This is mostly just labor, but some require more thought than others, like moving TDT to use hail2 expr; - Support intervals in the `index_*` methods. It's possible now to join by locus, but not using the `annotateLociTable` fast path.; - Move to Python 3 so argument order is preserved; - Test the hail2 api much more rigorously than we do now (at the very least, call each parameter branch for each method!; - Typecheck the expression language. This isn't super trivial, and making a nice system to integrate our `typecheck` module and expressions will require some thoughtful design work.; - Some more organization around the package: monkey patching with `import hail.genetics` is an idea I like, but want to think about the edge cases first. ## Documentation; - Document the `index_*` methods / joins; - Translate the _Hail Overview_ tutorial; - Make new tutorials to replace the 2 expr ones we have; - Fill in docs on api2 methods (they're not all there yet); - Fill in docs on expression language (things like __mul__ on NumericExpression haven't been documented); - Write ""integrative docs"" that provide how-tos for common types of workflows. Show the power of annotate / select / group_by/aggregate, etc. ## Longer term QoL:; - Move over tests to Python as much as possible. I looked at the linear regression suite and it can be moved entirely into Python without many problems.; - Write a type parser in Python. The nested calls into the JVM for Type._from_java make the library feel extremely sluggish on tee",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2588#issuecomment-352190554:991,integrat,integrate,991,https://hail.is,https://github.com/hail-is/hail/pull/2588#issuecomment-352190554,1,['integrat'],['integrate']
Integrability,"y in count(self, genotypes); 1127 """"""; 1128; -> 1129 return dict(self._jvdf.count(genotypes).toJavaMap()); 1130; 1131 def deduplicate(self):. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 109 # deepest = env.jutils.deepestMessage(e.java_exception); 110 # msg = env.jutils.getMinimalMessage(e.java_exception); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:1382,protocol,protocol,1382,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['protocol'],['protocol']
Integrability,"y"", line 364, in f; ret = x(*args); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 68, in <lambda>; return hl.array(self).fold(lambda accum, elt: accum | f(elt), False); File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 364, in f; ret = x(*args); File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 364, in f; ret = x(*args); File ""test.py"", line 11, in <lambda>; (mt.locus.contig == hl.literal(x[0])) & \; File ""<decorator-gen-690>"", line 2, in literal; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/functions.py"", line 261, in literal; return literal(hl.eval(to_expr(x, dtype)), dtype); File ""<decorator-gen-668>"", line 2, in eval; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 223, in eval; return eval_timed(expression)[0]; File ""<decorator-gen-666>"", line 2, in eval_timed; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 189, in eval_timed; return _eval_many(expression, timed=True, name='eval_timed')[0]; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 150, in _eval_many; return Env.backend().execute_many(*irs, timed=timed); File ""/home/edmund/.local/src/hail/hail/python/hail/backend/backend.py"", line 38, in execute_many; return [self.execute(MakeTuple([ir]), timed=timed)[0] for ir in irs]; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/backend.py"", line 38, in <listcomp>; return [self.execute(Make",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:7773,wrap,wrapper,7773,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['wrap'],['wrapper']
Integrability,"y4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.ClassNotFoundException: com.amazonaws.AmazonClientException; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:419); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:352); 	... 27 more. During handling of the above exception, another exception occurred:. Py4JJavaError Traceback (most recent call last); /bmrn/apps/hail/0.2.72-spark-3.1.2/python/hail-0.2.72-py3-none-any.egg/hail/backend/py4j_backend.py in deco(*args, **kwargs); 15 try:; ---> 16 return f(*args, **kwargs); 17 except py4j.protocol.Py4JJavaError as e:. /bmrn/apps/python/miniconda/64/3.7/envs/piranha_0.2.0_20210812/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 327 ""An error occurred while calling {0}{1}{2}.\n"".; --> 328 format(target_id, ""."", name), value); 329 else:. Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.backend.spark.SparkBackend$.apply(SparkBackend.scala:218); 	at is.hail.backend.spark.SparkBackend.apply(SparkBackend.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:2918,protocol,protocol,2918,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,1,['protocol'],['protocol']
Integrability,"yeah, encoders/decoders don't wrap within structs. That's not hard to fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7396#issuecomment-547147774:30,wrap,wrap,30,https://hail.is,https://github.com/hail-is/hail/issues/7396#issuecomment-547147774,1,['wrap'],['wrap']
Integrability,"yeah, if/else should be wrapped in parens. Will submit a patch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2653#issuecomment-355436505:24,wrap,wrapped,24,https://hail.is,https://github.com/hail-is/hail/issues/2653#issuecomment-355436505,1,['wrap'],['wrapped']
Integrability,"yeah, master succeeds. . I'm worried this is related to the synchronization in a sneaky way. We haven't built anything to support a multi-user JVM, so there could be things that go wrong.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4068#issuecomment-411199876:60,synchroniz,synchronization,60,https://hail.is,https://github.com/hail-is/hail/pull/4068#issuecomment-411199876,1,['synchroniz'],['synchronization']
Integrability,"yes, broke up first line because it was too long. I think this method should just be removed, really. We have num_samples, count_variants, and query_genotypes that make it easy to do all of this stuff. The interface is also weird",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1503#issuecomment-284853192:206,interface,interface,206,https://hail.is,https://github.com/hail-is/hail/pull/1503#issuecomment-284853192,1,['interface'],['interface']
Integrability,"yes, but it does use modified functionality (namely info_to_keep is now potentially different). (which means transitive dependencies would no longer work in the same way; and VarDP=hl.cond(row.info.VarDP > vardp_outlier is totally different since that argument is removed)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7247#issuecomment-540742636:120,depend,dependencies,120,https://hail.is,https://github.com/hail-is/hail/pull/7247#issuecomment-540742636,1,['depend'],['dependencies']
Integrability,"yes, looks like this depends on ForwardLets.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5740#issuecomment-478750679:21,depend,depends,21,https://hail.is,https://github.com/hail-is/hail/pull/5740#issuecomment-478750679,1,['depend'],['depends']
Integrability,"ylinux2014_x86_64.whl (269 kB); Building wheels for collected packages: avro; Building wheel for avro (pyproject.toml): started; Building wheel for avro (pyproject.toml): finished with status 'done'; Created wheel for avro: filename=avro-1.11.2-py2.py3-none-any.whl size=119738 sha256=d7f238f86de270b449b018590930a06270766887328bdb51066eccff2cd696a6; Stored in directory: /home/hadoop/.cache/pip/wheels/e3/a2/1e/5c1be0865f4170a89de34e0a798f32f674a7eaf63a93272c7f; Successfully built avro; Installing collected packages: sortedcontainers, pytz, py4j, commonmark, azure-common, xyzservices, wrapt, uvloop, urllib3, tzdata, typing-extensions, tornado, tenacity, tabulate, six, regex, pyyaml, python-json-logger, pyjwt, pygments, pycparser, pyasn1, protobuf, portalocker, pillow, packaging, orjs; on, oauthlib, numpy, nest-asyncio, multidict, markupsafe, jmespath, idna, humanize, google-crc32c, frozenlist, dill, decorator, charset-normalizer, certifi, cachetools, avro, attrs, asyncinit, async-timeout, yarl, typer, scipy, rsa, rich, requests, python-dateutil, pyasn1-modules, plotly, parsimonious; , jproperties, jinja2, janus, isodate, googleapis-common-protos, google-resumable-media, deprecated, contourpy, cffi, aiosignal, requests-oauthlib, pycares, pandas, google-auth, cryptography, botocore, azure-core, aiohttp, s3transfer, msrest, google-auth-oauthlib, google-api-core, bokeh, azure-storage; -blob, azure-mgmt-core, aiodns, msal, google-cloud-core, boto3, azure-mgmt-storage, msal-extensions, google-cloud-storage, azure-identity; Attempting uninstall: packaging; Found existing installation: packaging 23.2; Uninstalling packaging-23.2:; Successfully uninstalled packaging-23.2; Successfully installed aiodns-2.0.0 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 asyncinit-0.2.4 attrs-23.1.0 avro-1.11.2 azure-common-1.1.28 azure-core-1.29.3 azure-identity-1.14.0 azure-mgmt-core-1.4.0 azure-mgmt-storage-20.1.0 azure-storage-blob-12.17.0 bokeh-3.2.2 boto3-1.28.41 botocore-1.31.; 41 cache",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:41753,wrap,wrapt,41753,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['wrap'],['wrapt']
Integrability,"you were the reviewer for the dependent PR, so you did fully approve the change. It went in as one commit instead of two, which saves the CI some cycles and slightly obfuscates the git history ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5787#issuecomment-480268873:30,depend,dependent,30,https://hail.is,https://github.com/hail-is/hail/pull/5787#issuecomment-480268873,1,['depend'],['dependent']
Integrability,zure.reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:99); 		at is.hail.shadedazure.reactor.core.publisher.Mono.block(Mono.java:1742); 		at is.hail.shadedazure.com.azure.storage.common.implementation.StorageImplUtils.blockWithOptionalTimeout(StorageImplUtils.java:133); 		at is.hail.shadedazure.com.azure.storage.blob.specialized.BlobClientBase.getPropertiesWithResponse(BlobClientBase.java:1379); 		at is.hail.shadedazure.com.azure.storage.blob.specialized.BlobClientBase.getProperties(BlobClientBase.java:1348); 		at is.hail.io.fs.AzureStorageFS.$anonfun$openNoCompression$1(AzureStorageFS.scala:223); 		at is.hail.io.fs.AzureStorageFS.$anonfun$handlePublicAccessError$1(AzureStorageFS.scala:175); 		at is.hail.services.package$.retryTransientErrors(package.scala:124); 		at is.hail.io.fs.AzureStorageFS.handlePublicAccessError(AzureStorageFS.scala:174); 		at is.hail.io.fs.AzureStorageFS.openNoCompression(AzureStorageFS.scala:220); 		at is.hail.io.fs.RouterFS.openNoCompression(RouterFS.scala:20); 		at is.hail.io.fs.FS.openNoCompression(FS.scala:322); 		at is.hail.io.fs.FS.openNoCompression$(FS.scala:322); 		at is.hail.io.fs.RouterFS.openNoCompression(RouterFS.scala:3); 		at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); 		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 		at is.hail.services.package$.retryTransientErrors(package.scala:124); 		at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:459); 		at is.hail.backend.service.Main$.main(Main.scala:15); 		at is.hail.backend.service.Main.main(Main.scala); 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 		at java.lang.reflect.Method.invoke(Method.java:498); 		at is.hail.JVMEntryway$1.run(JVMEntryway.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906430:1116,Rout,RouterFS,1116,https://hail.is,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906430,1,['Rout'],['RouterFS']
Integrability,"}),start),GetField(Ref(__iruid_374,struct{start: int32, end: int32}),end),I32(1),false),__iruid_375,MakeStruct(ArrayBuffer((idx,Ref(__iruid_375,int32)))))),Apply(concat,WrappedArray(),ArrayBuffer(GetField(Ref(__iruid_370,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),writeCtx), UUID4(__iruid_290)),str),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/rows/parts/,Some((gs://danking/workshop-test/1kg.mt/index/,+PCStruct{idx:+PInt32})),None)),Some(TableStageDependency(WrappedArray()))),Begin(ArrayBuffer(WriteMetadata(MakeArray(ArrayBuffer(GetField(WritePartition(MakeStream(ArrayBuffer(Literal(struct{},[])),stream<struct{}>,false),Str(""part-0""),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/globals/parts/,None,None)),filePath)),array<str>),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/globals,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(),JArray(List(JObject(List((start,JObject(List())), (end,JObject(List())), (includeStart,JBool(true)), (includeEnd,JBool(true)))))),null,Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_369,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_376,GetField(Ref(__iruid_376,struct{fil",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:4395,Wrap,WrappedArray,4395,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601,1,['Wrap'],['WrappedArray']
Integrability,"}),start),GetField(Ref(__iruid_470,struct{start: int32, end: int32}),end),I32(1),false),__iruid_471,MakeStruct(ArrayBuffer((idx,Ref(__iruid_471,int32)))))),Apply(concat,WrappedArray(),ArrayBuffer(GetField(Ref(__iruid_466,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),writeCtx), UUID4(__iruid_386)),str),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/rows/parts/,Some((gs://danking/workshop-test/1kg.mt/index/,+PCStruct{idx:+PInt32})),None)),Some(TableStageDependency(WrappedArray()))),Begin(ArrayBuffer(WriteMetadata(MakeArray(ArrayBuffer(GetField(WritePartition(MakeStream(ArrayBuffer(Literal(struct{},[])),stream<struct{}>,false),Str(""part-0""),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/globals/parts/,None,None)),filePath)),array<str>),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/globals,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(),JArray(List(JObject(List((start,JObject(List())), (end,JObject(List())), (includeStart,JBool(true)), (includeEnd,JBool(true)))))),null,Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_465,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_472,GetField(Ref(__iruid_472,struct{fil",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:4115,Wrap,WrappedArray,4115,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693,1,['Wrap'],['WrappedArray']
Integrability,";  * name TEXT [default: None] [required] ;  * storage_account TEXT Storage account in which the cluster's container exists. [default: None] [required] ;  * http_password TEXT Web password for the cluster [default: None] [required] ;  * script TEXT Path to script. [default: None] [required] ;  arguments [ARGUMENTS]... You should use -- if you want to pass option-like arguments through. [default: None] ; ;  Options ;  --help Show this message and exit. ; . (base) dking@wm28c-761 hail % hailctl dataproc submit --help ; ; Usage: hailctl dataproc submit [OPTIONS] NAME SCRIPT [ARGUMENTS]... ; ; Submit the Python script at path SCRIPT to a running Dataproc cluster with name NAME. ; You may pass arguments to the script being submitted by listing them after the script; however, if you ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13447#issuecomment-1681403012:4072,message,message,4072,https://hail.is,https://github.com/hail-is/hail/pull/13447#issuecomment-1681403012,1,['message'],['message']
Integrability,";  * name TEXT [default: None] [required] ;  * script TEXT [default: None] [required] ;  arguments [ARGUMENTS]... You should use -- if you want to pass option-like arguments through. [default: None] ; ;  Options ;  --files TEXT Comma-separated list of files to add to the working directory of the Hail application. ;  --pyfiles TEXT Comma-separated list of files (or directories with python files) to add to the PYTHONPATH. ;  --properties -p TEXT Extra Spark properties to set. [default: None] ;  --gcloud_configuration TEXT Google Cloud configuration to submit job (defaults to currently set configuration). [default: None] ;  --dry-run --no-dry-run Print gcloud dataproc command, but don't run it. [default: no-dry-run] ;  --region TEXT Compute region for the cluster. [default: None] ;  --help Show this message and exit. ; ; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13447#issuecomment-1681403012:6693,message,message,6693,https://hail.is,https://github.com/hail-is/hail/pull/13447#issuecomment-1681403012,1,['message'],['message']
Integrability,";  --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified ;  shell. ;  [default: None] ;  --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified ;  shell, to copy it or customize the ;  installation. ;  [default: None] ;  --help Show this message and exit. ; ;  Commands ;  batch Manage batches running on the batch service managed by the Hail team. ;  config Manage Hail configuration. ;  curl Issue authenticated curl requests to Hail infrastructure. ;  version Print version information and exit. ; ; ```. This is what `hailctl batch submit --help` looks like:. ```. Usage: hailctl batch submit [OPTIONS] SCRIPT [ARGUMENTS]... Submit a batch with a single job that runs SCRIPT with the arguments ARGUMENTS.  Arguments ;  * script PATH Path to the script [default: None] [required] ;  arguments [ARGUMENTS]... [default: None] ; ;  Options ;  --files PATH Files or directories to add to the working directory of the ;  job. ;  [default: None] ;  --name TEXT The name of the batch. ;  --image-name TEXT Name of Docker image for the job ;  [default: (hailgenetics/hail)] ;  --output -o [text|yaml|json] [default: text] ;  --help Show this message and exit. ; ; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13109#issuecomment-1561081921:2064,message,message,2064,https://hail.is,https://github.com/hail-is/hail/pull/13109#issuecomment-1561081921,1,['message'],['message']
Modifiability, 	at is.hail.io.AbstractTypedCodecSpec.decodeArrays$(CodecSpec.scala:54); E 	at is.hail.io.TypedCodecSpec.decodeArrays(TypedCodecSpec.scala:19); E 	at is.hail.expr.ir.Interpret$.$anonfun$run$1(Interpret.scala:80); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:79); E 	at is.hail.expr.ir.Interpret$.interpret$1(Interpret.scala:67); E 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:110); E 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:58); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$foldConstants$1(FoldConstants.scala:47); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.FoldConstants$.foldConstants(FoldConstants.scala:13); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.runOpt$1(Optimize.scala:15); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(Execu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:6496,Rewrite,RewriteBottomUp,6496,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Rewrite'],['RewriteBottomUp']
Modifiability," 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2274,config,config,2274,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability," RV. 4. The hash function on (options, source) is now beefed up to cope with having only a; few distinct values of options; and is modified with the output of ""$(CXX) --version"",; so that when you upgrade your compiler, you won't get hits on modules compiled with the old; compiler. 5. build.gradle has a new target ""nativeLibPrebuilt"", for updating the prebuilt/lib/linux-x86-64; or prebuilt/lib/darwin. 6. The committed prebuilt libraries are built thus:. darwin - On my MacOS laptop, with the default (clang-based) compiler, -march=sandybridge; From my reading, I believe this should be compatible withall MacBook Pro's; released since 2011, and all versions of MacOS since 10.9 (the first to use; libc++ rather than libstdc++ as the default C++ library) - we're now at 10.13,; with 10.14 arriving some time in the fall. linux-x86-64 - Built on my home desktop running Ubuntu-16.04 LTS, and g++-5.0.4, with; -fabi-version=9. In theory this should work with all systems based on g++5.x and; later. I made some effort to move std::string out of the interfaces between prebuilt; and dynamic code, which gives it some chance of working on systems based on; g++-4.x, but haven't tested that. I'm planning to fire up VM's either in cloud or under VirtualBox, to test this against Ubuntu-14.04,; Ubuntu-18.04, and the latest stable RHEL, which should cover most of the bases. In the interest of getting this committed, I have not made changes relating to logging and; error messages. The DLL's are still in the jar, and I think it has to stay that way because; all nodes need to see libhail.so. The header files are also in the jar, and have to be; unpacked in a convoluted way, and that could probably be simplified if/when we change; the approach to packaging. Once this goes in, I can follow it with a PR which adds the NativePackDecoder in RowStore.scala,; controlled by whether environment variable ""HAIL_ENABLE_CPP_CODEGEN"" is defined; (so defaulting to using the JVM bytecode CompiledPackDecoder).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863:2496,variab,variable,2496,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863,1,['variab'],['variable']
Modifiability, SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename scripts/release.sh; ++ basename scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'REMOTE is unset or empt,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:15138,variab,variables,15138,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['variab'],['variables']
Modifiability," The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough  in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:1607,config,config,1607,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability," add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using the stream interface seems wrong to me also. When I mentioned this, I was thinking we could reuse the region management logic from the stream emitter for loops, but I've since changed my mind. I think loops will have hard region management no matter what. > What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. As a side note, @iitalics stream emitter can handle streams of multiple values",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:2605,variab,variable,2605,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407,1,['variab'],['variable']
Modifiability, com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1012); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:196); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:470); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:251); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); 	at is.hail.io.fs.HadoopFSURL.<init>(HadoopFS.scala:76); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:88); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:85); 	at is.hail.io.fs.FS.exists(FS.scala:618); 	at is.hail.io.fs.FS.exists$(FS.scala:618); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:85); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:7006,config,configure,7006,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,2,['config'],['configure']
Modifiability," for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-yrxul/batch/tmp; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. Use an existing bucket and give permissions:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init ; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp/foo; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. User does not give permissions to existing remote tmpdir:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: n ; WARNING: Please verify service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com has the role ""roles/storage.objectAdmin"" or both ""roles/storage.objectViewer"" and ""roles/storage.objectCreator"" roles for bucket hail-batch-jigold-oxmmp.; Which ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:2431,CONFIG,CONFIGURATION,2431,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['CONFIG'],['CONFIGURATION']
Modifiability," log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 17/10/19 08:45:43 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.2.0.cloudera1; /_/. Using Python version 2.7.5 (default, Aug 4 2017 00:39:18); SparkSession available as 'spark'.; >>> import hail; >>> hc = hail.HailContext(); log4j:ERROR setFile(null,false) call failed.; java.io.FileNotFoundException: hail.log (Permission denied); 	at java.io.FileOutputStream.open0(Native Method); 	at java.io.FileOutputStream.open(FileOutputStream.java:270); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:213); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:133); 	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294); 	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165); 	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104); 	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842); 	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768); 	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648); 	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514); 	at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:440); 	at is.hail.HailContext$.configureLogging(HailContext.scala:132); 	at is.hail.HailContext$.apply(HailContext.scala:159); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198:1679,config,config,1679,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198,1,['config'],['config']
Modifiability," much as ~12 GB/s of aggregate bandwidth. We seem to have room for improvement, but this seems good enough for now. # On AWS, GCS -> S3. | Files | Bytes | Time | Rate |; | ----- | ----- | ---- | ---- |; | 1 | 5.4 GB | 34 seconds | 154.5 MB/s |; | 1 | 42.9 GB | 4 minutes | 161.6 MB/s |; | 200 | 5.4 GB | 35 seconds | 151.1 MB/s |; | 40000 | 5.4 GB | 4 minutes | 22.0 MB/s |. # On GCP, S3 -> GCS. | Files | Bytes | Time | Rate |; | ----- | ----- | ---- | ---- |; | 1 | 5.4 GB | 17 seconds | 304.2 MB/s |; | 1 | 42.9 GB | 3 minutes | 235.5 MB/s |; | 200 | 5.4 GB | 20 seconds | 267.8 MB/s |; | 40000 | 5.4 GB | 6 minutes | 13.3 MB/s |. # machine parsable form; ```; [{'config': 'one',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/one',; 'times': [34.76],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/one'},; {'config': 'some',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/some',; 'times': [35.527],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/some'},; {'config': 'many',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/many',; 'times': [244.154],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/many'},; {'config': 'huge',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/huge',; 'times': [265.719],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/huge'},; {'config': 'one',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/one',; 'times': [17.65],; 'to': 'gs://1-day/tmp/test-copy/target/dking-benchmark/one'},; {'config': 'some',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/some',; 'times': [20.048],; 'to': 'gs://1-day/tmp/test-copy/target/dking-benchmark/some'},; {'config': 'many',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/many',; 'times': [402.267],; 'to': 'gs://1-day/tmp/test-copy/target/dking-benchmark/many'},; {'config': 'huge',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/huge',; 'times': [182.355],; 'to': 'gs://1-day/tmp/test-copy/target/dking-benchmark/huge'}]. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10752#issuecomment-897651697:1410,config,config,1410,https://hail.is,https://github.com/hail-is/hail/pull/10752#issuecomment-897651697,5,['config'],['config']
Modifiability," recipe continues but it is also passed literally to the shell*. The docs page you linked directly addresses our use case and suggests we put the command inside of a make variable (thus triggering normal backslash-newline rules rather than the special recipe line rules). > Sometimes you want to split a long line inside of single quotes, but you dont want the backslash/newline to appear in the quoted content. This is often the case when passing scripts to languages such as Perl, where extraneous backslashes inside the script can change its meaning or even be a syntax error. One simple way of handling this is to place the quoted string, or even the entire command, into a make variable then use the variable in the recipe. In this situation the newline quoting rules for makefiles will be used, and the backslash/newline will be removed. If we rewrite our example above using this method:; > ; > ```; > HELLO = 'hello \; > world'; > ; > all : ; @echo $(HELLO); > ```; > ; > we will get output like this:; > ; > ```; > hello world; > ```; >; > If you like, you can also use target-specific variables (see [Target-specific Variable Values](https://www.gnu.org/software/make/manual/html_node/Target_002dspecific.html)) to obtain a tighter correspondence between the variable and the recipe that uses it. It seems to me like there are not any great choices. Putting the JSON into a Make variable seems too magical and likely to confuse a newbie editing this file. Using escaped double quotes is less legible than literal JSON. Putting the whole JSON array on one line is quite long. I guess we can go with double quotes for now. I tested on Make 3.81 and Make 4.4.1. The first EDIT and the original comment follow for context. ---. EDIT: Nope, I still appear to be wrong. Hold on. ---. I have bash 3.2.57; ```; (base) dking@wm28c-761 /tmp % make print-shell; /bin/sh; (base) dking@wm28c-761 /tmp % /bin/sh --version; GNU bash, version 3.2.57(1)-release (arm64-apple-darwin22); Copyright (C) 2007 Fr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324:1346,rewrite,rewrite,1346,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324,3,"['Variab', 'rewrite', 'variab']","['Variable', 'rewrite', 'variables']"
Modifiability," spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough  in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/pyt",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:1654,config,config,1654,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability," speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## 2I installed the atlas-devel , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1384,plugin,plugins,1384,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,1,['plugin'],['plugins']
Modifiability," use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); /hail/test/BRCA1.raw_indel.vcf MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2; >>> ; ```. ----------------; When I executed the command in local mode , there seems to hava some result:; ```; [root@tele-1 ~]# python; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; >>> hc = HailContext();; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/opt/NfsDir/UserDir/wanghn/BRCA1.raw_indel.vcf').write('/opt/NfsDir/UserDir/wanghn/BRCA1.raw_indel_1.vds'); hail: info: No multiallelics detected.; hail: info: Coerced unsorted dataset; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; ```; ------------------------; How can I check if my spark configuration meet the requirement of the hail?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506:2769,config,configuration,2769,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506,1,['config'],['configuration']
Modifiability,"&& binds tighter than ||, I don't see how the parentheses make any difference? (Also, we should use curlies, which are for precedence as opposed to parens which trigger sub-shells to start and can be confusing wrt environment variables)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9571#issuecomment-705055793:226,variab,variables,226,https://hail.is,https://github.com/hail-is/hail/pull/9571#issuecomment-705055793,1,['variab'],['variables']
Modifiability,"(@johnc1231 I snuck the LowerBlockMatrixIR change in here that I was using for illustration purposes, since it's just a refactor to make use of some legibility improvements, but I'm happy to back that out into a separate PR.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8658#issuecomment-621293435:120,refactor,refactor,120,https://hail.is,https://github.com/hail-is/hail/pull/8658#issuecomment-621293435,1,['refactor'],['refactor']
Modifiability,"(and to be clear, I'm onboard with the idea of a single command which gets you from zero to simple batch pipelines; jury is out on the Artifact Registry. I think making sure that's configured correctly might be better upstreamed to Sam. Normal users might not have permission to enable/disable things like that.).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1662843157:181,config,configured,181,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1662843157,1,['config'],['configured']
Modifiability,"(i) -> O(i, i)` makes a square matrix whose diagonal is T. * `T(i) -> O(i, j)` and `T(i) -> O(j, i)` broadcast T over a matrix in the two possible directions. Now let T be a 2-tensor. * `T(i, j) -> O(i)` is the vector of row-sums of T. * `T(i, i) -> O(i)` is the diagonal of T. * `T(i, i) -> O()` is the trace of T. * `T(i, j) -> O(j, i)` is transposition. How do we represent matrix multiplication? Let T1 and T2 be 2-tensors. Then letting `T = Out(T1, T2, ""and"").map((x, y) => x * y)`, the matrix product is given by. * `T(i, j, j, k) -> O(i, k)`. In general, an index operation on T requires specifying an output tensor O (including its shape, though you can deduce that in non-broadcasting cases), a set of index variables (eg. ""i, j, k""), and an assignment of a variable to each dimension of T and O. . More abstractly, let DT and DO be the sets of dimensions of T and O. An index operation consists of a set I and two functions DT -> I <- DO, a ""cospan"". These operations compose by cospan composition, which involves a pushout (a disjoint union and a quotient). Let i: DT -> I and o: DO -> I be the two maps assigning index variables. It helps to consider some special cases (compare to the examples above):. * If i is surjective, and o is identity, this is extracting a diagonal from T. * If i is injective, and o is identity, this is a broadcast. * If i is identity, and o is surjective, this embeds T as a diagonal of a higher-dimensional output tensor. * If i is identity, and o is injective, this is a pure aggregation, summing out some dimensions of T. To make composition easy to compute, we could represent an index operation using a union-find structure for I. In other words, an index operation consists of a union-find structure I, and two arrays of points of I, encoding the two functions above. Then the composition of (T, I1, M) and (M, I2, O) is (T, I', O), where I' is computed by taking the union I1+I2, then for each dimension of M, unioning the assigned points of I1 and I2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772:3409,variab,variables,3409,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772,1,['variab'],['variables']
Modifiability,"(nb: if `hailctl dev config show` lists `default: carolin`, then the URL on your laptop will be http://localhost:5000/carolin/website/docs/...)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10238#issuecomment-809610358:21,config,config,21,https://hail.is,https://github.com/hail-is/hail/pull/10238#issuecomment-809610358,1,['config'],['config']
Modifiability,"**c++ -v**; Using built-in specs.; COLLECT_GCC=c++; COLLECT_LTO_WRAPPER=/opt/local/libexec/gcc/x86_64-apple-darwin15/4.9.3/lto-wrapper; Target: x86_64-apple-darwin15; Configured with: /opt/local/var/macports/build/_opt_mports_dports_lang_gcc49/gcc49/work/gcc-4.9.3/configure --prefix=/opt/local --build=x86_64-apple-darwin15 --enable-languages=c,c++,objc,obj-c++,lto,fortran,java --libdir=/opt/local/lib/gcc49 --includedir=/opt/local/include/gcc49 --infodir=/opt/local/share/info --mandir=/opt/local/share/man --datarootdir=/opt/local/share/gcc-4.9 --with-local-prefix=/opt/local --with-system-zlib --disable-nls --program-suffix=-mp-4.9 --with-gxx-include-dir=/opt/local/include/gcc49/c++/ --with-gmp=/opt/local --with-mpfr=/opt/local --with-mpc=/opt/local --with-isl=/opt/local --disable-isl-version-check --with-cloog=/opt/local --disable-cloog-version-check --enable-stage1-checking --disable-multilib --enable-lto --enable-libstdcxx-time --with-as=/opt/local/bin/as --with-ld=/opt/local/bin/ld --with-ar=/opt/local/bin/ar --with-bugurl=https://trac.macports.org/newticket --with-pkgversion='MacPorts gcc49 4.9.3_0' --with-build-config=bootstrap-debug; Thread model: posix; gcc version 4.9.3 (MacPorts gcc49 4.9.3_0) . **sysctl -a | grep machdep.cpu**; machdep.cpu.tsc_ccc.denominator: 0; machdep.cpu.tsc_ccc.numerator: 0; machdep.cpu.thread_count: 8; machdep.cpu.core_count: 4; machdep.cpu.address_bits.virtual: 48; machdep.cpu.address_bits.physical: 36; machdep.cpu.tlb.shared: 512; machdep.cpu.tlb.data.large: 32; machdep.cpu.tlb.data.small: 64; machdep.cpu.tlb.inst.large: 8; machdep.cpu.tlb.inst.small: 64; machdep.cpu.cache.size: 256; machdep.cpu.cache.L2_associativity: 8; machdep.cpu.cache.linesize: 64; machdep.cpu.arch_perf.fixed_width: 48; machdep.cpu.arch_perf.fixed_number: 3; machdep.cpu.arch_perf.events: 0; machdep.cpu.arch_perf.events_number: 7; machdep.cpu.arch_perf.width: 48; machdep.cpu.arch_perf.number: 4; machdep.cpu.arch_perf.version: 3; machdep.cpu.xsave.extended_state1:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543:167,Config,Configured,167,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543,2,"['Config', 'config']","['Configured', 'configure']"
Modifiability,"*Update*. If it helps, our configuration includes three VMs. This includes a master and two workers with autoscaling enabled. We have tried using n1-himem-8 and n1-himem-64 machines. Both configurations failed with similar errors. The one above is form the n1-himem-64 configuration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12083#issuecomment-1213381420:27,config,configuration,27,https://hail.is,https://github.com/hail-is/hail/issues/12083#issuecomment-1213381420,3,['config'],"['configuration', 'configurations']"
Modifiability,*sigh*. ```; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); ```. Azure's `StorageOutputStream.close` method is not idempotent in the version that we use. It has been made idempotent in `12.18.0`. I would be surprised if spark let us upgrade to a version that recent,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12968#issuecomment-1532328901:811,adapt,adapted,811,https://hail.is,https://github.com/hail-is/hail/pull/12968#issuecomment-1532328901,1,['adapt'],['adapted']
Modifiability,- [ ] strip `extends Serializable` from KeyTable,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1386#issuecomment-279695804:13,extend,extends,13,https://hail.is,https://github.com/hail-is/hail/issues/1386#issuecomment-279695804,1,['extend'],['extends']
Modifiability,.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:53); E 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); E 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); E 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:454); E 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:490); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:75); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:75); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229:4365,adapt,adapted,4365,https://hail.is,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229,1,['adapt'],['adapted']
Modifiability,.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); 	at is.hail.io.fs.HadoopFSURL.<init>(HadoopFS.scala:76); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:88); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:85); 	at is.hail.io.fs.FS.exists(FS.scala:618); 	at is.hail.io.fs.FS.exists$(FS.scala:618); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:85); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3$adapted(LocalBackend.scala:223); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:144); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:144); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$2(LocalBackend.scala:223); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:249); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$2(LocalBackend.scala:314); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(LocalBackend.scala:309); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1$adapted(LocalBackend.scala:308); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); 	at is.hail.utils.package,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:1907,adapt,adapted,1907,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,4,['adapt'],['adapted']
Modifiability,.hail.expr.ir.Emit.emitI(Emit.scala:815); 	at is.hail.expr.ir.Emit$.$anonfun$apply$4(Emit.scala:99); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:19); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedCode(EmitCodeBuilder.scala:24); 	at is.hail.expr.ir.EmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1044); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1095); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder$(EmitClassBuilder.scala:1095); 	at is.hail.expr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1192); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:97); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:78); 	at is.hail.TestUtils$.eval(TestUtils.scala:256); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$5(TestUtils.scala:366); 	at scala.collection.immutable.Set$Set4.foreach(Set.scala:289); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$4(TestUtils.scala:348); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$4$adapted(TestUtils.scala:339); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:46); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:276); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$1(ExecuteContext.scala:40); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:39); 	at is.hail.TestUtils$.assertEvalsTo(TestUtils.scala:339); 	at is.hail.TestUtils$.assertEvalsTo(TestUtils.scala:314); 	at is.hail.expr.ir.IRSuite.testStreamLenUnconsumedInnerStream(IRSuite.scala:1800); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10330#issuecomment-827119604:2300,adapt,adapted,2300,https://hail.is,https://github.com/hail-is/hail/pull/10330#issuecomment-827119604,1,['adapt'],['adapted']
Modifiability,".toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(de",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:1622,parameteriz,parameterization,1622,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803,1,['parameteriz'],['parameterization']
Modifiability,"/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3149,config,config,3149,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability,"/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; inde",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2516,config,config,2516,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability,"0 azure-mgmt-storage-20.1.0 azure-storage-blob-12.17.0 bokeh-3.2.2 boto3-1.28.41 botocore-1.31.; 41 cachetools-5.3.1 certifi-2023.7.22 cffi-1.15.1 charset-normalizer-3.2.0 commonmark-0.9.1 contourpy-1.1.0 cryptography-41.0.3 decorator-4.4.2 deprecated-1.2.14 dill-0.3.7 frozenlist-1.4.0 google-api-core-2.11.1 google-auth-2.22.0 google-auth-oauthlib-0.8.0 google-cloud-core-2.3.3 google-cloud-storag; e-2.10.0 google-crc32c-1.5.0 google-resumable-media-2.5.0 googleapis-common-protos-1.60.0 humanize-1.1.0 idna-3.4 isodate-0.6.1 janus-1.0.0 jinja2-3.1.2 jmespath-1.0.1 jproperties-2.1.1 markupsafe-2.1.3 msal-1.23.0 msal-extensions-1.0.0 msrest-0.7.1 multidict-6.0.4 nest-asyncio-1.5.7 numpy-1.25.2 oaut; hlib-3.2.2 orjson-3.9.5 packaging-23.1 pandas-2.1.0 parsimonious-0.10.0 pillow-10.0.0 plotly-5.16.1 portalocker-2.7.0 protobuf-3.20.2 py4j-0.10.9.5 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycares-4.3.0 pycparser-2.21 pygments-2.16.1 pyjwt-2.8.0 python-dateutil-2.8.2 python-json-logger-2.0.7 pytz-2023.3.post; 1 pyyaml-6.0.1 regex-2023.8.8 requests-2.31.0 requests-oauthlib-1.3.1 rich-12.6.0 rsa-4.9 s3transfer-0.6.2 scipy-1.11.2 six-1.16.0 sortedcontainers-2.4.0 tabulate-0.9.0 tenacity-8.2.3 tornado-6.3.3 typer-0.9.0 typing-extensions-4.7.1 tzdata-2023.3 urllib3-1.26.16 uvloop-0.17.0 wrapt-1.15.0 xyzservices; -2023.7.0 yarl-1.9.2. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; python3 -m pip uninstall -y hail; WARNING: Skipping hail as it is not installed.; python3 -m pip install build/deploy/dist/hail-0.2.124-py3-none-any.whl --no-deps; Defaulting to user installation because normal site-packages is not writeable; Processing ./build/deploy/dist/hail-0.2.124-py3-none-any.whl; Installing collected packages: hail; Successfully installed hail-0.2.124. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; hailctl config set query/backend spark; </p>; </details>",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:45012,config,config,45012,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['config'],['config']
Modifiability,"00 on error. We could return a BadRequest error code with the message 'invalid spec' and then handle the MJC database call on the driver. I chose instead to have the worker to post job complete so we get the error message with the stack trace showing up in the UI as having the normal job flow seemed cleaner to me last week then special casing `schedule_job` on the driver. `post job complete` needs a job object to get the status to send back to the driver. However, a `Job` has two concrete implementations and we don't know which the bad job is because we can't get the spec. Furthermore, the `Job` class does a lot of work based on the spec right now. So I thought it was clearer to just create a new class that had the status, but nothing else. After writing this out, it's probably better to have the driver MJC upon error rather than from the worker. The code below would be more complicated. We'd have to get the traceback / error message from the response from the worker. ```python3; try:; await client_session.post(; f'http://{instance.ip_address}:5000/api/v1alpha/batches/jobs/create',; json=body,; timeout=aiohttp.ClientTimeout(total=2),; ); await instance.mark_healthy(); except aiohttp.ClientResponseError as e:; await instance.mark_healthy(); if e.status == 403:; log.info(f'attempt already exists for job {id} on {instance}, aborting'); if e.status == 503:; log.info(f'job {id} cannot be scheduled because {instance} is shutting down, aborting'); raise e; except Exception:; await instance.incr_failed_request_count(); raise; ```. And the error handling would look something like this:. ```python3; try:; body = await job_config(app, record, attempt_id); except Exception:; log.exception('while making job config'); status = {; 'version': STATUS_FORMAT_VERSION,; 'worker': None,; 'batch_id': batch_id,; 'job_id': job_id,; 'attempt_id': attempt_id,; 'user': record['user'],; 'state': 'error',; 'error': traceback.format_exc(),; 'container_statuses': {k: None for k in tasks},; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078:1768,config,config,1768,https://hail.is,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078,1,['config'],['config']
Modifiability,"1. Move the classes into is.hail.rvd. They aren't used anywhere else, are they?. 2. @maccum is working over LD prune, the GeneralRDD stuff shouldn't be necessary anymore. I think she's just working on optimization, so I feel like her current code is an improvement and should go in before the performance improvements are ready. 3. I agree, but, yeah, SKAT can't go until we get expr ndarray. 4. MatrixTable.same just uses zip, you can rewrite it in terms of zip based on my above comments. You might also need a version of zip that returns a RDD[T]. Compare RVD.map and RVD.mapPartitions. 5. I think the right thing here is an OrderedRVD.orderedIntervalJoin. Compare @patrick-schultz recent OrderedRVD.orderedJoin (pending): https://github.com/hail-is/hail/pull/3159/files#diff-b173fb9bd584d50afcfa6724954ef3b5R127",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3186#issuecomment-374449723:436,rewrite,rewrite,436,https://hail.is,https://github.com/hail-is/hail/pull/3186#issuecomment-374449723,1,['rewrite'],['rewrite']
Modifiability,"2/commits/7afc4a5b599a233a4e4b40bb9c7a260b062dd925; - This also includes all CORS bits. With the caveat that this is my first attempt at Kubernetes events, I think this moves things in the right direction. We now have an authenticated, push-notification system for an arbitrary number of notebooks. There are a few issues with it currently, mostly in handling closed web socket connections in gevent, which I will move away from in the iteration after Wednesday, but I handle dead socket errors and they don't *seem* to accumulate over time. I also need to implement a reconnection system on the client. The neat thing about this synchronizes sessions between refresh. So if you have N collaborators all on the same window (or more likely, you have 2 windows open), they will all get consistent state as quickly as Kubernetes knows it. This may not seem useful atm, but it allows us to get really fine-grained view into svc/pod uptime. This also should be much faster, provided we don't overburden the server with watchers (can be solved using server implementation as well), say by using an interval of a second, because we query kubernetes directly, rather than hitting the liveness endpoint by traveling over public internet and then being proxied at the boundary by nginx. We know within ms of the true state. Remaining q is whether this completely replicates the liveness endpoint. I also tried to make the serializing the kubernetes object the domain of the caller; I like this because the called can stop thinking about whether something implements __getitem__, and can specify pretty arbitrary transformations on that data (see lines 172-210, 331, 360, and all other calls to marshall_json), and allows us to use one function to serialize most (any?) kubernetes resource. Some small perf hit of course for an added function call, and tail recursion. Formatting around comments sucks; if you have a standard python formatting config, please share; using vs code default for autopep8. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459839071:2219,config,config,2219,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459839071,1,['config'],['config']
Modifiability,2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 0; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:317); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:299); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:317); 		at is.hail.io.fs.FSPositionedOutputStream.write(FS.scala:227); 		at java.io.DataOutputStream.write(DataOutputStream.java:107); 		at is.hail.fs.FSSuite.$anonfun$testSeekMoreThanMaxInt$1(FSSuite.scala:347); 		at is.hail.fs.FSSuite.$anonfun$testSeekMoreThanMaxInt$1$adapted(FSSuite.scala:341); 		at is.hail.utils.package$.using(package.scala:635); 		... 26 more. test is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt FAILURE; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:9208,adapt,adapted,9208,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['adapt'],['adapted']
Modifiability,"3.2.57; ```; (base) dking@wm28c-761 /tmp % make print-shell; /bin/sh; (base) dking@wm28c-761 /tmp % /bin/sh --version; GNU bash, version 3.2.57(1)-release (arm64-apple-darwin22); Copyright (C) 2007 Free Software Foundation, Inc.; ```. Looks like this was an intentionally backwards incompatible change [in Make 4.0](https://git.savannah.gnu.org/cgit/make.git/tree/NEWS?h=4.0&id=52191d9d613819a77a321ad6c3ab16e1bc73c381#n18) which removed the POSIX-compatible behavior on which our Makefile relies:; ```; * WARNING: Backward-incompatibility!; If .POSIX is specified, then make adheres to the POSIX backslash/newline; handling requirements, which introduces the following changes to the; standard backslash/newline handling in non-recipe lines:; * Any trailing space before the backslash is preserved; * Each backslash/newline (plus subsequent whitespace) is converted to a; single space; ```. They seem to have [broken the behavior in order to fix something else](https://git.savannah.gnu.org/cgit/make.git/commit/?id=391456a) and then added a [`.POSIX`](https://git.savannah.gnu.org/cgit/make.git/commit/?id=88f1bc8) escape hatch for Makefiles that want POSIX compatibility. For the Makefile I shared above, I get equivalent behavior with and without a `.POSIX:` fake target. I suspect Make 4.x treats them differently. In particular, the problem is not the shell, it's our Makes behaving differently. Make 3 replaces `\\\n` with ` ` before calling the shell whereas Make 4 appears to preserve the `\\\n` to the shell (regardless of the presence of quotes in either case). In particular, Make 4 appears to treat recipe lines differently from how it treats all other lines in Makefiles. I don't like the noise of the backslash-quotes. The end of the doc page you linked suggests using a Make variable when you intentionally want normal Makefile backslash-newline interpretation. That feels a bit too clever. Let's try just demanding POSIX-compatibility. I've pushed that change. Can you give it a try?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324:4087,variab,variable,4087,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324,1,['variab'],['variable']
Modifiability,"4.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 241. Traceback (most recent call last):; File ""/hail-vep/vep.py"", line 218, in <module>; main(action, consequence, tolerate_parse_error, block_size, input_file, output_file, part_id, vep_cmd); File ""/hail-vep/vep.py"", line 199, in main; results = run_vep(vep_cmd, input_file, block_size, consequence, tolerate_parse_error, part_id, os.environ); File ""/hail-vep/vep.py"", line 127, in run_vep; raise ValueError(f'VEP command {vep_cmd} failed with non-zero exit status {proc.returncode}\n'; ValueError: VEP command ['/vep/vep', '--input_file', '/io/input', '--format', 'vcf', '--json', '--everything', '--allele_number', '--no_stats', '--cache', '--offline', '--minimal', '--assembly', 'GRCh38', '--fasta', '/vep_data//homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz', '--plugin', 'LoF,loftee_path:/vep/ensembl-vep/Plugins/,gerp_bigwig:/vep_data//gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/vep_data//human_ancestor.fa.gz,conservation_file:/vep_data//loftee.sql', '--dir_plugins', '/vep/ensembl-vep/Plugins/', '--dir_cache', '/vep_data/', '-o', 'STDOUT'] failed with non-zero exit status -9; VEP error output:; Smartmatch is experimental at /vep/ensembl-vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 241.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1802287224:1879,plugin,plugin,1879,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1802287224,9,"['Plugin', 'plugin']","['Plugins', 'plugin']"
Modifiability,"507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1014,config,configuration,1014,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914,1,['config'],['configuration']
Modifiability,"609, 'cost': 1.1510333711392028e-06, 'msec_mcpu': 0, 'status': {'version': 5, 'worker': 'batch-worker-pr-12955-default-rrlcxki12v8r-standard-0e2wl', 'batch_id': 74, 'job_id': 1, 'attempt_id': 'gAaTm8', 'user': 'test', 'state': 'succeeded', 'format_version': 7, 'resources': [{'name': 'az/vm/Standard_D8ds_v4/spot/eastus/1682899200000', 'quantity': 32}, {'name': 'az/disk/E4_LRS/eastus/1546300800000', 'quantity': 1024}, {'name': 'az/ip-fee/1024/2021-12-01', 'quantity': 32}, {'name': 'az/service-fee/2021-12-01', 'quantity': 250}], 'region': 'eastus', 'start_time': 1682966178997, 'end_time': 1682966179606, 'container_statuses': {'main': {'name': 'batch-74-job-1-main', 'state': 'succeeded', 'timing': {'pulling': {'start_time': 1682966179035, 'finish_time': 1682966179224, 'duration': 189}, 'setting up overlay': {'start_time': 1682966179224, 'finish_time': 1682966179253, 'duration': 29}, 'setting up network': {'start_time': 1682966179253, 'finish_time': 1682966179253, 'duration': 0}, 'running': {'start_time': 1682966179253, 'finish_time': 1682966179340, 'duration': 87}, 'uploading_log': {'start_time': 1682966179340, 'finish_time': 1682966179361, 'duration': 21}, 'uploading_resource_usage': {'start_time': 1682966179361, 'finish_time': 1682966179407, 'duration': 46}}, 'container_status': {'started_at': 1682966179253, 'finished_at': 1682966179340, 'state': 'finished', 'exit_code': 0, 'out_of_memory': False}}}, 'timing': {'setup_io': {'start_time': 1682966178998, 'finish_time': 1682966178999, 'duration': 1}, 'configuring xfsquota': {'start_time': 1682966178999, 'finish_time': 1682966179035, 'duration': 36}, 'populating secrets': {'start_time': 1682966179035, 'finish_time': 1682966179035, 'duration': 0}, 'adding cloudfuse support': {'start_time': 1682966179035, 'finish_time': 1682966179035, 'duration': 0}, 'post-job finally block': {'start_time': 1682966179578}}}, 'spec': {'always_copy_output': False, 'job_id': 1, 'process': {'command': ['true'], 'image': 'haildev.azurecr.io/ubun",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12955#issuecomment-1530133228:2298,config,configuring,2298,https://hail.is,https://github.com/hail-is/hail/pull/12955#issuecomment-1530133228,2,['config'],['configuring']
Modifiability,6_290(Unknown Source); 	at __C372collect_distributed_array_matrix_native_writer.apply_region4_318(Unknown Source); 	at __C372collect_distributed_array_matrix_native_writer.apply_region2_501(Unknown Source); 	at __C372collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at __C372collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$16(BackendUtils.scala:91); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90); 	at is.hail.backend.service.Worker$.$anonfun$main$12(Worker.scala:167); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$11(Worker.scala:166); 	at is.hail.backend.service.Worker$.$anonfun$main$11$adapted(Worker.scala:164); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.backend.service.Worker$.main(Worker.scala:164); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	... 11 more. Logs; Main; Log ; 2023-09-24 17:23:30.055 JVMEntryway: ERROR: Exception encountered in QoB cancel thread.; org.newsclub.net.unix.SocketClosedException: Not open; 	at org.newsclub.net.unix.AFCore.validFdOrException(AFCore.java:90) ~[jvm-entryway.jar:?]; 	at org.newsclub.net.unix.AFSocketImpl$AFInputStreamImpl.read(AFSocketImpl.java:510) ~[jvm-entryway.jar:?]; 	at java.io.DataInputStream.readInt(DataInputStream.java:388) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$2.run(JVMEntryway.java:136) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888:4138,adapt,adapted,4138,https://hail.is,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888,1,['adapt'],['adapted']
Modifiability,"8 is reconciled, to protect against k8 operational errors. Obviously that means our record of state is a point of failure, but db failure modes and uptime solutions are well defined across cloud/db provider vendors (we can minimize lock-in as well). The idea seemed to be that we don't particularly care how k8 works, or how much state it persists; the contract is with our users, and we should satisfy . * K8 does not store all state indefinitely. It's more like a rotating log: https://stackoverflow.com/questions/40636021/how-to-list-kubernetes-recently-deleted-pods . For users, and for investors, we want to have a permanent record of all user interactions. * Some kinds of data may be awkward to store and query within pod labels. For instance, how much user state do we want to store in labels? How do we store operation graphs / history?; ; * aggregation operations across users or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there either is, or should be a way of querying one selector across all resources). . * Performance, even for very basic queries. An initial assumption, may prove to be incorrect, but I think we will find a fast db to be much faster than K8, even if we could directly query etcd. This is based on my experience with Amazon data stores, and general impression/experience with google cs products.; * Open issue on this: even directly accessed, etcd is slow, doesn't index selectors https://github.com/kubernetes/kubernetes/issues/4817. * We may want to",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:1713,config,configurable,1713,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290,1,['config'],['configurable']
Modifiability,931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159) ~[?:1.8.0_382]; 	at is.hail.utils.package$.using(package.scala:677) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.FS.writePDOS(FS.scala:441) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.FS.writePDOS$(FS.scala:440) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$4(Worker.scala:124) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$4$adapted(Worker.scala:124) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$13(Worker.scala:178) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.main(Worker.scala:177) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Main$.main(Main.scala:14) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Main.main(Main.scala,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:14595,adapt,adapted,14595,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['adapt'],['adapted']
Modifiability,":270); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:213); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:133); 	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294); 	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165); 	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104); 	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842); 	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768); 	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648); 	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514); 	at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:440); 	at is.hail.HailContext$.configureLogging(HailContext.scala:132); 	at is.hail.HailContext$.apply(HailContext.scala:159); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-289>"", line 2, in __init__; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198:2369,config,configureLogging,2369,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198,1,['config'],['configureLogging']
Modifiability,:66); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:66); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); E 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:158); E 	at is.hail.backend.local.LocalBackend.lowerDistributedSort(LocalBackend.scala:331); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:67); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:14); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:167); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:161); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.mutable.ResizableArray.foreac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:10184,Rewrite,RewriteBottomUp,10184,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Rewrite'],['RewriteBottomUp']
Modifiability,; adding 'hailtop/fs/__init__.py'; adding 'hailtop/fs/fs.py'; adding 'hailtop/fs/fs_utils.py'; adding 'hailtop/fs/router_fs.py'; adding 'hailtop/fs/stat_result.py'; adding 'hailtop/hailctl/__init__.py'; adding 'hailtop/hailctl/__main__.py'; adding 'hailtop/hailctl/deploy.yaml'; adding 'hailtop/hailctl/describe.py'; adding 'hailtop/hailctl/auth/__init__.py'; adding 'hailtop/hailctl/auth/cli.py'; adding 'hailtop/hailctl/auth/create_user.py'; adding 'hailtop/hailctl/auth/delete_user.py'; adding 'hailtop/hailctl/auth/login.py'; adding 'hailtop/hailctl/batch/__init__.py'; adding 'hailtop/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/initialize.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/utils.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; adding 'hailtop/hailctl/batch/billing/cli.py'; adding 'hailtop/hailctl/config/__init__.py'; adding 'hailtop/hailctl/config/cli.py'; adding 'hailtop/hailctl/config/config_variables.py'; adding 'hailtop/hailctl/dataproc/__init__.py'; adding 'hailtop/hailctl/dataproc/cli.py'; adding 'hailtop/hailctl/dataproc/cluster_config.py'; adding 'hailtop/hailctl/dataproc/connect.py'; adding 'hailtop/hailctl/dataproc/deploy_metadata.py'; adding 'hailtop/hailctl/dataproc/diagnose.py'; adding 'hailtop/hailctl/dataproc/gcloud.py'; adding 'hailtop/hailctl/dataproc/modify.py'; adding 'hailtop/hailctl/dataproc/start.py'; adding 'hailtop/hailctl/dataproc/submit.py'; adding 'hailtop/hailctl/dataproc/utils.py'; adding 'hailtop/hailctl/dev/__init__.py'; adding 'hailtop/hailctl/dev/ci_client.py'; adding 'hailtop/hailctl/dev/cli.py'; adding 'hailtop/hailctl/dev/config.py'; adding 'hailtop/hailctl/hdinsight/__init__.py'; adding 'hailtop/hailctl/hdinsight/cli.py'; adding 'hailtop/hailctl/hdinsight/start.py'; adding 'hailtop/hailctl/hdinsight/submit.py'; adding 'hailtop/utils/__init__.py'; adding 'hailtop/utils/filesize,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:27501,config,config,27501,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['config'],['config']
Modifiability,"; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; ; def init_parser(parser):; @@ -35,6 +35,6 @@ def main(args):; }; ; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); with open(config_file, 'w') as f:; f.write(json.dumps(config)); -- ; 2.23.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:4230,config,config,4230,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,9,['config'],['config']
Modifiability,"; total 32; drwxr-xr-x 8 teamcity www-data 4096 Sep 7 18:55 ./; drwxr-xr-x 4 root root 4096 Sep 7 18:16 ../; drwxrwxr-x 6 teamcity teamcity 4096 Sep 7 18:33 .BuildServer/; drwxr-xr-x 5 teamcity www-data 4096 Sep 7 18:55 .gradle/; drwxr-xr-x 13 teamcity teamcity 4096 Aug 22 19:22 TeamCity/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent1/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent2/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent3/; ```. ### Update the `init.d` scripts. #### `/etc/init.d/teamcity`. ```; #!/bin/sh; ### BEGIN INIT INFO; # Provides: teamcity ; # Required-Start: $local_fs $network; # Required-Stop: $local_fs; # Default-Start: 2 3 4 5; # Default-Stop: 0 1 6; # Short-Description: teamcity ; # Description: teamcity build server; ### END INIT INFO; # /etc/init.d/teamcity - startup script for teamcity; export TEAMCITY_DATA_PATH=""/home/teamcity/.BuildServer""; export TEAMCITY_SERVER_OPTS=-Djava.awt.headless=true # Configure TeamCity for use on a headless OS. case $1 in; start); start-stop-daemon --start -c teamcity --exec /home/teamcity/TeamCity/bin/teamcity-server.sh start; ;;. stop); start-stop-daemon --start -c teamcity --exec /home/teamcity/TeamCity/bin/teamcity-server.sh stop; ;;. esac. exit 0; ```. #### `/etc/init.d/teamcityAgents`. ```; #!/bin/bash; ### BEGIN INIT INFO; # Provides: teamcityAgents ; # Required-Start: $local_fs $network; # Required-Stop: $local_fs; # Default-Start: 2 3 4 5; # Default-Stop: 0 1 6; # Short-Description: teamcityAgents ; # Description: TeamCity build agents ; ### END INIT INFO. USER=""teamcity""; AGENTS=(TeamCityAgent1 TeamCityAgent2 TeamCityAgent3). case ""$1"" in; start); for agent in ${AGENTS[@]}; do; start-stop-daemon --start -c teamcity --exec /home/teamcity/$agent/bin/agent.sh start; done; ;;; stop); for agent in ${AGENTS[@]}; do; start-stop-daemon --start -c teamcity --exec /home/teamcity/$agent/bin/agent.sh stop; done; ;;; *); echo ""usage start/stop""; exit 1; ;;. esac. e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/675#issuecomment-245383790:1337,Config,Configure,1337,https://hail.is,https://github.com/hail-is/hail/issues/675#issuecomment-245383790,1,['Config'],['Configure']
Modifiability,"> > I'm not sure this change is thorough enough. Is there a way for a bucket to get partially mounted but have config['mounted'] still be False?; > ; > I suppose this is exactly what happened when the user was able to create a duplicate mount. The unmount succeeded, but it did not rid the worker of the fuse process. Perhaps the more appropriate thing to do is instead of relying on the outcome of the unmount operation, we should check for the existence of the FUSE process. Is it worth making a separate PR that explicitly manages the gcsfuse processes PIDs? If yes, can you do that?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12975#issuecomment-1533440682:111,config,config,111,https://hail.is,https://github.com/hail-is/hail/pull/12975#issuecomment-1533440682,1,['config'],['config']
Modifiability,"> > New config should be correct.; > > As an example of this slash issue, the following config (deployed right now) doesn't work.; > ; > I'm confused, is it correct, or does it not work? If there's an issue upstream, it needs to get fixed, too. The new config addresses the requested changes on the config. When this pr was created there was a claim that the redirection to service.internal was not found in gateway. That appears to be not the case (or at least the internal router doesn't seem to be receiving those requests) and still needs to be investigated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-541102715:8,config,config,8,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541102715,4,['config'],['config']
Modifiability,"> @cseed, do you recall why you chose to delete and then use apply in ci/create_database.py?. I think no reason. I wasn't aware of the race condition and didn't have an alternative for applying changes at hand. This looks good to me. I ran a few examples but I couldn't actually tell how `--save-config` changes the output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10217#issuecomment-808525352:296,config,config,296,https://hail.is,https://github.com/hail-is/hail/pull/10217#issuecomment-808525352,1,['config'],['config']
Modifiability,"> A user reported this error concurrent.futures._base.TimeoutError with no stack trace while copying files in a batch job. . This. No stack trace. If you look at this output, the previous stack trace is part of the WARNING message. ```; INFO:deploy_config:deploy config file not found: None; INFO:hailtop.aiocloud.aiogoogle.credentials:using credentials file /gsa-key/key.json: GoogleServiceAccountCredentials for XXXXX@PROJECT.iam.gserviceaccount.com; WARNING:hailtop.utils:Encountered 2 errors (current delay: 0.2). My stack trace is File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/copy.py"", line 128, in <module>; asyncio.run(main()); File ""/usr/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 735, in retry_transient_errors; st = ''.join(traceback.format_stack()); . Most recent error was; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 729, in retry_transient_errors; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 134, in request_and_raise_for_status; resp = await self.client_session._request(method, url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 634, in _request; break; File ""/usr/local/lib/python3.7/dist-packages/aiohttp/helpers.py"", line 721, in __exit__; raise asyncio.TimeoutError from None; concurrent.futures._base.TimeoutError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817#issuecomment-1117642330:263,config,config,263,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117642330,1,['config'],['config']
Modifiability,"> As an aside, we should definitely have a `HAIL_BATCH_BACKEND` and associated config variables. There is no end to my annoyance that `hb.Batch()` gives me a local backend batch.; > ; > It seems to me that, given the precedent of `hailctl dataproc submit`, `hailctl batch submit` conveys the intent to use QoB or Batch-in-Batch, not ""local mode Batch"" or ""local mode Hail"". It seems very reasonable to have a `--local-mode-query` override (I think we should ignore local mode Batch as much as possible since container-in-container is fraught).; > ; > We need a better name for local mode Spark or Query. I'm slowly realizing that lots of people don't realize you can use Hail on a laptop. Are there other tools that have already settled on terminology here?. @danking Wait so shall I set `HAIL_QUERY_BACKEND=batch`? I can follow up with a PR that adds a similar configuration variable for batch so we don't default to the local backend, but it would be nice to keep this behavior consistent from the start",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12471#issuecomment-1324259408:79,config,config,79,https://hail.is,https://github.com/hail-is/hail/pull/12471#issuecomment-1324259408,4,"['config', 'variab']","['config', 'configuration', 'variable', 'variables']"
Modifiability,"> CVE-2019-11245 is a vulnerability in k8s that causes some (all?); > containers without a runAsUser configuration to run; > as user id 0, i.e. root. Jupyter refuses to start as root.; > This change enables Jupyter to start successfully. Yeah, causes all containers to run as root upon restart of the container.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6678#issuecomment-512828240:101,config,configuration,101,https://hail.is,https://github.com/hail-is/hail/pull/6678#issuecomment-512828240,1,['config'],['configuration']
Modifiability,"> Even tho ArrayBuilder is invariant, the specialized versions extend ArrayBuilder[Object] and implement all the generic, unspecialized methods. That worries me, but I don't know why it would ever get called. Maybe for backward compatibility to code compiled without specialization?. I noticed this as well and figured it was for compatibility with Java code that doesn't have the source-level knowledge of `@specialized`. AFAICT, it shouldn't matter unless we're writing Java code or want super tiny byte code sizes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1549#issuecomment-287083666:63,extend,extend,63,https://hail.is,https://github.com/hail-is/hail/pull/1549#issuecomment-287083666,1,['extend'],['extend']
Modifiability,"> Found it. [#12221 (comment)](https://github.com/hail-is/hail/pull/12221#discussion_r988432928). Hm, this feels like a different change to me. This change doesn't allow the use of a `HAIL_BATCH_BUCKET` env variable. It looks to me like the argument was against being able to specify deprecated arguments, not against env variables.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12799#issuecomment-1476923117:207,variab,variable,207,https://hail.is,https://github.com/hail-is/hail/pull/12799#issuecomment-1476923117,2,['variab'],"['variable', 'variables']"
Modifiability,"> Hmm. Maybe the env-setup script should also set the default region/zone to us-central1-a?. Yep, that would remove the need for --zone or --region. We should probably document that part of the configuration, since its likely what prevented the get-credentials command from working for me as provided.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5022#issuecomment-449278737:194,config,configuration,194,https://hail.is,https://github.com/hail-is/hail/pull/5022#issuecomment-449278737,1,['config'],['configuration']
Modifiability,"> I don't see what the test_instance database is buying you. CI needs the ability to create databases. In production, it gets this power from the default/database-server-config that has the database root credentials that was set up by hand with the cluster and the database. Now, that means for CI to run in the tests, the default namespace needs a database-server-config. We used to copy the production one, giving the tests root in the database. Not good. Instead what I do is create a new database, call test_instance, and use the test instance credentials to create database-server-config. Now, you can't create databases within this database, but you can create tables and do other happy database stuff. So test_instance is the database that the test CI is going to give out when it needs to create databases. That means CI roughly needs to know if it can create databases (it is root) or it has a database but can't create ones (e.g. test CI), in which the create database step needs to do slightly different things. Does that help?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7683#issuecomment-562888944:170,config,config,170,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562888944,3,['config'],['config']
Modifiability,"> I haven't checked if new bokeh supports old pandas. Nor do I know if we have old pandas usage lurking in the codebase. Can we make our pinned-requirements.txt use pandas 2.0, fix whatever issues arise, but leave requirements.txt flexible for folks?. I think there are compromises either way, but I would be surprised if this just worked. It seems very easy to accidentally adopt new functionality so at that point why even have a lower-bound? I think that, while it's very hard to make sure that all our dependencies are compatible at all possible version combinations, and these things will happen, it just feels like an easily-avoidable lie to say we support 1.x and 2.x but then use functionality exclusive to 2.x. So I'm ok keeping the bounds more relaxed if we can guarantee that *our* usage of pandas is compatible with both. FWIW, I think that our primary dependencies release major versions infrequently enough that it is reasonable to only support the most recent major version, in much the same way that we don't support python versions indefinitely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1520500573:231,flexible,flexible,231,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1520500573,1,['flexible'],['flexible']
Modifiability,"> I kinda prefer a fresh file with freshly written tests? I feel like it's a bit hard to get a total view of what is and is not tests when we're using annotations. I agree a single file feels nice, but I'm a little hesitant to copy paste tests. Unless you think I should move where these tests are? That also feels weird. Does the following pytest invocation make you feel better about markers?. ```; (hail) dgoldste@wmce3-cb7 hail % pytest --collect-only -m qobtest hail/python/test; ============================================================================== test session starts ===============================================================================; platform darwin -- Python 3.9.17, pytest-7.4.0, pluggy-1.3.0; rootdir: /Users/dgoldste/hail/hail/python/test; configfile: pytest.ini; plugins: anyio-4.0.0, xdist-2.5.0, instafail-0.5.0, timeout-2.1.0, devtools-0.12.2, asyncio-0.21.1, timestamper-0.0.9, metadata-3.0.0, html-1.22.1, forked-1.6.0; asyncio: mode=auto; collected 8218 items / 8133 deselected / 85 selected. <Package hail>; <Module test_context.py>; <Function test_init_hail_context_twice>; <Function test_top_level_functions_are_do_not_error>; <Function test_tmpdir_runs>; <Module test_randomness.py>; <Function test_table_explode>; <Package backend>; <Module test_service_backend.py>; <Function test_tiny_driver_has_tiny_memory>; <Function test_big_driver_has_big_memory>; <Function test_tiny_worker_has_tiny_memory>; <Function test_big_worker_has_big_memory>; <Function test_regions>; <Package expr>; <Module test_expr.py>; <UnitTestCase Tests>; <TestCaseFunction test_aggregators>; <TestCaseFunction test_densify_table>; <TestCaseFunction test_scan>; <Package genetics>; <Module test_reference_genome.py>; <Function test_reference_genome>; <Function test_reference_genome_sequence>; <Function test_reference_genome_liftover>; <Function test_read_custom_reference_genome>; <Package matrixtable>; <Module test_grouped_matrix_table.py>; <UnitTestCase Tests>; <TestCaseFunct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13620#issuecomment-1720268851:775,config,configfile,775,https://hail.is,https://github.com/hail-is/hail/pull/13620#issuecomment-1720268851,2,"['config', 'plugin']","['configfile', 'plugins']"
Modifiability,"> I realize this is a flip-flop and probably a bit frustrating, but I think leaving the ComplexPTypes on the abstract PLocus, PCall, PInterval seems preferable to needing to define a `representation` on those classes. We can move the ComplexPType inheritance to the concrete implementations of those interfaces when we get rid of the usages of `representation` outside the ptypes package.; > ; > This is a small refactor, right?. The issue is that without declaring the representation type on those classes we will need more casts (for instance line 17 of BinarySearch now needs one), and this is inherently the same thing as assuming PCanonical* rather than the abstract class (so we should not cast, either use the abstract class and define representation's type, or match on the specific implementation).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7712#issuecomment-566681403:247,inherit,inheritance,247,https://hail.is,https://github.com/hail-is/hail/pull/7712#issuecomment-566681403,2,"['inherit', 'refactor']","['inheritance', 'refactor']"
Modifiability,"> I think I agree with you if it is your intention for this javascript to only be used on pages served at hail.is?. So right now it's only used on pages hosted at hail.is/ (everything in the www folder). Longer term we should unify these efforts, so that www (hail.is), docs, workshop, notebook, etc have a unified appearance. Blog is different, since we're using a CMS (content management) solution, although I could be put under a common umbrella as well. One option is to use NextJS, which is my preferred React solution: https://ghost.org/docs/api/v3/nextjs/, although I'm not sure that is a solution that would get buy-in. I could make a solution in python as well and extend our current system as far as it will go.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8476#issuecomment-610034709:674,extend,extend,674,https://hail.is,https://github.com/hail-is/hail/pull/8476#issuecomment-610034709,1,['extend'],['extend']
Modifiability,"> I think I can address collectively by removing all hailctl options that pass through to gcloud. This removes the question of providing them twice, makes all the commands consistent. Do you mean remove all options that are simple pass throughs (such as `--num-worker-local-ssds`) or all options that are also gcloud options (such as `--project`)? The latter could be difficult, since there are some gcloud options that hailctl also needs to read, like `hailctl dataproc start` using `--project` to set requester pays configuration, extending `--initialization-actions` with notebook/VEP init scripts, setting a higher default disk size when `--vep` is specified, etc. or `hailctl dataproc submit` automatically zipping `--py-files`. > I think this also addresses the issue hailctl dataproc submit not supporting --, because you can specify it twice: once to break out of hailctl options, and once to break out of gcloud options to specify options the script being submitted: hailctl dataproc submit --halictl-option -- --gcloud-options -- --script-options and-parameters. Nice. That would solve the problem. I would guess submitting script arguments is more common than using gcloud options here, so it would be nice for the gcloud arguments group to be optional, so that `hailctl dataproc submit cluster -- --script-options` would work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-758100479:518,config,configuration,518,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758100479,2,"['config', 'extend']","['configuration', 'extending']"
Modifiability,"> I think I'm on board with the RVDSpec stuff you've proposed. I still want to rewrite the names of the MatrixTableSpec and TableSpec, since these should have been abstract from the beginning. You can't rename concrete Specs. RelationalSpec was the abstract one. You can debate about what should be in RelationalSpec vs. the concrete members (e.g. maybe partitionCounts shouldn't be in there.). RelationalSpecs are very abstract: they are just a collection of ""components"", whatever those are, with an assumption there is one component called the ""global"" component.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4734#issuecomment-436733090:79,rewrite,rewrite,79,https://hail.is,https://github.com/hail-is/hail/pull/4734#issuecomment-436733090,1,['rewrite'],['rewrite']
Modifiability,"> I think if it's not too hard of a change, we should add the files with encoded secrets to something like `infra/gcp/data/...`. This makes it clear that these files have a different purpose and gives some indication that they're specific to your repo. If you want to also add prefixing the file name with the repo, then that would make it even clearer. But if it's too much work, don't bother. Maybe something like `infra/gcp/data/hail-is/` etc. I've put all deployment-specific configs in an `$ORGANIZATION_DOMAIN` subfolder now, which hopefully should avoid any collisions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11724#issuecomment-1173211731:480,config,configs,480,https://hail.is,https://github.com/hail-is/hail/pull/11724#issuecomment-1173211731,1,['config'],['configs']
Modifiability,"> I think we can do that with a rewrite rule that rewrites TableCount(tir) if tir.partitionCounts.isDefined. Ah, nice. Now that we can embed relational IRs inside value IRs inside relational IRs, this will be great.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5015#issuecomment-448844086:32,rewrite,rewrite,32,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448844086,2,['rewrite'],"['rewrite', 'rewrites']"
Modifiability,"> I think we should just always mount the deploy config. That might trigger some latent bugs where we don't do quite the right thing, but we can fix those. @danking Just realized you already took care of that in https://github.com/hail-is/hail/pull/9848/files#diff-21972f0c9ab321a85ffb4de98353c86fdea4992a5eb0d48c3489b977513b1da0! Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-773006387:49,config,config,49,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-773006387,1,['config'],['config']
Modifiability,"> I want to also have a discussion about how a slice is actually different from a ""canonical"" ndarray, especially after #10619.; > ; > I think of `SNDArrayPointer` as a pointer to data, and variables for shape/strides. Isn't that exactly what a slice is, too? Where are the important differences? I'd think copy semantics are one, but are there others?. I think the main difference is that `SNDArrayPointer` points to the location in memory that stores shape/strides as well as the data, while `SNDArraySlice` points directly to the first element, with shape/strides living only on the stack. #10619 doesn't look like it changes that, though perhaps after that change there's no need for `SNDArrayPointer` any more, and we can only use `SNDArraySlice`. IIRC, MLIR uses basically that design.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10614#issuecomment-871402875:190,variab,variables,190,https://hail.is,https://github.com/hail-is/hail/pull/10614#issuecomment-871402875,1,['variab'],['variables']
Modifiability,"> I'm curious if there's a way to rewrite this to be more clear about the state machine?. Yes, this is quite confusing as written, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6581#issuecomment-509610153:34,rewrite,rewrite,34,https://hail.is,https://github.com/hail-is/hail/pull/6581#issuecomment-509610153,1,['rewrite'],['rewrite']
Modifiability,"> I'm happy to switch to debian:9.6 (that's the same as debian:stretch). But as far as I can tell, it has 1.10.3. Why not use the official docker image nginx:1.15.8? It does everything our custom Dockerfile does, pins the version, and removes 50% of the lines in our custom config. The only thing that I see it not doing that we may want is the jwt auth request module, but that isn't needed currently. https://github.com/nginxinc/docker-nginx/blob/baa050df601b5e798431a9db458e16f53b1031f6/mainline/stretch/Dockerfile",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5244#issuecomment-460363335:274,config,config,274,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460363335,1,['config'],['config']
Modifiability,"> I'm inclined to set `HAIL_QUERY_BACKEND=batch` by default, though I can see how this would also be useful to run a local-mode Spark-Hail. I'm happy to be overruled here but I like the ""just copy their config"" approach for configurations like these where both the local and batch backend could make sense, so the behavior is as consistent as is reasonable across environments. It will Just Work in the way you want if the user has the backend set to batch in their config ;)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12471#issuecomment-1323968044:203,config,config,203,https://hail.is,https://github.com/hail-is/hail/pull/12471#issuecomment-1323968044,3,['config'],"['config', 'configurations']"
Modifiability,"> I'm not sure this change is thorough enough. Is there a way for a bucket to get partially mounted but have config['mounted'] still be False?. I suppose this is exactly what happened when the user was able to create a duplicate mount. The unmount succeeded, but it did not rid the worker of the fuse process. Perhaps the more appropriate thing to do is instead of relying on the outcome of the unmount operation, we should check for the existence of the FUSE process",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12975#issuecomment-1533438375:109,config,config,109,https://hail.is,https://github.com/hail-is/hail/pull/12975#issuecomment-1533438375,1,['config'],['config']
Modifiability,"> If we ever ban old versions of Hail from the cluster, then we can also eliminate the log4j2 reconfiguration. New versions of Hail work fine without any runtime log configuration (thanks to QoBAppender). We might want to do this if we get rid of GSA keys. We can't have any more jars that presume the existence of some key file. It would also be a good time to fully delete the `memory` service, even though old jars should be able to tolerate that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941#issuecomment-1527963692:166,config,configuration,166,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1527963692,1,['config'],['configuration']
Modifiability,"> In the course of this work I also fixed a problem with the staged code generated by the copyFromType methods -- the addresses to copy from were never bound to variables, so in nested types, we ended up duplicating a lot of code (an array of Tuple10s of Tuple10s of Tuple10s would duplicate the top `loadElement` 1000x!). Could you show me where I forgot to bind address returns? This was an oversight; I understand the cost of not binding address-generating code (Need to call Code.store and load that, instead of re-running the generating function)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8099#issuecomment-586410150:161,variab,variables,161,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586410150,1,['variab'],['variables']
Modifiability,"> In trying to test this (from your branch, ran pip install on /hail/python just in case). You're running this locally, or with `hailctl dev deploy`? I assume the latter because the former is essentially impossible. ~/.hail/token is no longer used and you can delete it. You'll need a valid tokens.json to run the dev deploy. Once the dev deploy runs, your local configuration is irrelevant. It sounds like your dev deploy was successful. You're getting failures in your deployed services. You need to look into your namespace to debug them. In particular, for auth to run, you're going to need a copy of auth-oauth2-client-secret from the production namespace. To log in inside the dev namespace, you'll have to add the callback to the list of registered callbacks at Google. You can copy mine as an example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285:363,config,configuration,363,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285,1,['config'],['configuration']
Modifiability,> Is there an easy way to specify at the end once migrations have completed successfully to restart the deployments you cancelled once the migrations have succeeded?. This is an interesting idea. I think it would effectively require merging the database and the deploy steps. We can't re-deploy the existing config because that's the wrong (old) one.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7855#issuecomment-573739953:308,config,config,308,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573739953,1,['config'],['config']
Modifiability,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256:65,config,config,65,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256,1,['config'],['config']
Modifiability,"> Just want to verify, first cut for EType will be like the existing Type (with missingness), right? Then, Type, EType and PType will be free to evolve separately: we can remove missingness for Type, add non-encoded alternate representations to PTypes, and add alternate encodings (e.g. struct of arrays as arrays of structs) to ETypes. Yes, exactly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4734#issuecomment-437416979:145,evolve,evolve,145,https://hail.is,https://github.com/hail-is/hail/pull/4734#issuecomment-437416979,1,['evolve'],['evolve']
Modifiability,"> Oh, I misunderstood, I thought you were suggesting changing our FROM to stretch/9.6.; > ; > I think that should be fine, but can we do it as a separate PR since it seems orthogonal to this change which we're trying to get in for the demo tomorrow? (And in general orthogonal changes should be separate PRs so discussion on one part doesn't hold up the other parts.). Yes, although the gzip settings issued in this pr will be different between the two version. 1.10.3 doesn't have gzip on by default. I understand the value of conservative updates before public demonstrations, so will do what you ask. Btw, the full config if relying on nginx:10.15.8 goes from:. ```; FROM debian:9.5. RUN apt-get update -y && \; apt-get install -y nginx && \; rm -rf /var/lib/apt/lists/*. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf. RUN ln -sf /dev/stdout /var/log/nginx/access.log; RUN ln -sf /dev/stderr /var/log/nginx/error.log. CMD [""nginx"", ""-g"", ""daemon off;""]; ```. to . ```; FROM nginx:1.15.8. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf; ```. kind of neat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467:618,config,config,618,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467,1,['config'],['config']
Modifiability,"> One option is to extend NormalizeNames to take a prefix. Yeah, or use generate uids instead of counting up from 0. I think ForwardLets should take a flag for this (or be parameterized with a name generator).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5710#issuecomment-483514236:19,extend,extend,19,https://hail.is,https://github.com/hail-is/hail/pull/5710#issuecomment-483514236,2,"['extend', 'parameteriz']","['extend', 'parameterized']"
Modifiability,"> Sorry - one more thing I need help with. There's a cyclical import with the RouterFS in `variables.py`. Should I just pylint ignore it?. So it looks like that circular import is because of `config_variables`, but `config_variables` is only ever used in `hailctl` not elsewhere in `hailtop`:. ```; hailctl/config/cli.py; 10:from hailtop.config.variables import ConfigVariable, config_variables; 46: for var, var_info in config_variables().items():; 56: if parameter not in config_variables():; 62: config_variable_info = config_variables()[parameter]; 86: from hailtop.config import config_variables, get_user_config # pylint: disable=import-outside-toplevel; 99: config_items = {var.name: var_info.help_msg for var, var_info in config_variables().items()}. config/__init__.py; 4:from .variables import ConfigVariable, config_variables; 14: 'config_variables',. config/variables.py; 6:_config_variables = None; 30:def config_variables():; 34: global _config_variables; 36: if _config_variables is None:; 37: _config_variables = {; 112: return _config_variables; ```. Can you move `config_variables` into a file in `hailtop/hailctl/config` and keep the `ConfigVariable` enum in `hailtop/config/variables.py`? Then you can't have a circular reference because `hailtop` can't depend on `hailctl`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13224#issuecomment-1677917371:91,variab,variables,91,https://hail.is,https://github.com/hail-is/hail/pull/13224#issuecomment-1677917371,15,"['Config', 'config', 'variab']","['ConfigVariable', 'config', 'variables']"
Modifiability,"> Thanks for the explanation! I'm happy to make the change, I was just trying to understand the difference between Host and X-Forwarded-Host a little better before first.; > ; > So if I understand correctly, for the different headers:; > ; > * X-Forwarded-Proto gets passed to the router through the base https server in gateway, which sets X-Forwarded-Proto to `$scheme`, which is always going to be https since that's always going to be the protocol you're using for that server? And so when we use `$updated_scheme` for the blog server in the router's config, it's going to look at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I. Yep. In fact everything request to a Hail service (besides a lets-encrypt path) gets redirected to https. > Or am I misunderstanding how this works?. Nope, you have it correct. $http_x_forwarded_proto should never be absent, and would be fine to use instead of $updated_scheme (but I'd prefer one of those two, rather than https, because otherwise we're not relying on our upstream infrastructure). > * I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? . Yeah that's right. Host refers to the current server (or in the proxied case, what gateway set Host to). X-Forwarded-Host is set by gateway to be the $http_host at the time it proxies the request to router, which is going to be blog.hail.is. > I don't quite follow your comment about our use of `Host` being wrong, in this case; I _think_ I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enoug",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548082569:555,config,config,555,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548082569,1,['config'],['config']
Modifiability,"> The approach in this PR doubles down on the functional Code[T] structure. I don't see anything in the design that prevents us from moving away from `Code[T]`, but it does have to support it for now. > I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. This is also the interface I would like to see for CodeBuilder. And you're right, making that change would allow methods taking a `CodeBuilder` to not need a line number argument. I agree that's better. I may have gotten a bit of tunnel vision in the middle of the giant mechanical refactoring :) I will make this change. > I think part of my concern is that Im not entirely sold by the need to have a whole stack of IR printouts and associated line numbers  right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. I think most of this PR is necessary for debug information with the fully lowered IR line numbers. It doesn't do anything to propagate line numbers through IR lowerings, which is what would be needed to support line numbers at earlier compiler stages. > Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration. My main reason for that approach was to manage the number of changes required in this PR. We could follow up on this with making line number arguments explicit in manageable chunks. But personally this seems like the ideal use case for implicit arguments. And as `Code` goes away, the number of places with implicit line number arguments should go down significantly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9770#issuecomment-742042975:598,refactor,refactoring,598,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742042975,1,['refactor'],['refactoring']
Modifiability,"> The changes around `toJSON` seem to be unrelated to the introduction of `VType`. Can you explain the motivation there?. This was added as a step towards a greater refactoring effort where I applied a number of changes to try and make various backend implementations look the same.; `Type`, `TableType` and `MatrixType` had a `toJSON` method, with the exception of `BlockMatrixType`. Since these are all virtual types, it seemed like a simple change to unify these methods.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14678#issuecomment-2341475173:165,refactor,refactoring,165,https://hail.is,https://github.com/hail-is/hail/pull/14678#issuecomment-2341475173,1,['refactor'],['refactoring']
Modifiability,"> The hailctl dataproc subcommand now has --beta, --configuration=, --dry-run, --project= and --zone=. These apply to all commands. There is a GcloudRunner object that takes these options, is set to the click context user obj field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with click, the subcommand options must go on the subcommand, so hailctl dataproc stop --dry-run is an error. Nice. It's great that these are handled at the `hailctl dataproc` level instead of having to remember to account for them in every `hailctl dataproc` subcommand. That's going to resolve a lot of inconsistencies (like #9587). A nitpick though... is there a better name for the click context attribute than ""obj""?. > hailctl no longer takes --region (for gcloud dataproc commands). I compute region in GcloudRunner by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this). If consistency with `gcloud dataproc` is desired, I think the opposite (determining zone from cluster region) would be preferable. `gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:52,config,configuration,52,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393,1,['config'],['configuration']
Modifiability,> The new config addresses the requested changes on the config. I don't know what this means. You didn't answer the question.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-541104185:10,config,config,10,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541104185,2,['config'],['config']
Modifiability,"> This has significantly improved the simplicity of the parser, so much so that much of the logic therein could be simplified further, though I think that's beyond the scope of this change. Agreed. But as my follow up will be a complete rewrite of the parser, I definitely don't want to do more to simplify the current one. > I like the separation of type-checking and parsing, however I'd prefer in your implementations of `typecheck` that you assert one thing at a time. That way when things fail, it'll be clear which assertion was fired (ie if `(a && b && c)` fails, you don't know if it's `a` or `b` or `c`, whereas; > ; > ```scala; > assert(a); > assert(b); > assert(c); > ```; > ; > would give you that information. Good suggestion. I only moved assertions here, didn't add any new ones, but I don't mind splitting up some that I moved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13990#issuecomment-1809203400:237,rewrite,rewrite,237,https://hail.is,https://github.com/hail-is/hail/pull/13990#issuecomment-1809203400,1,['rewrite'],['rewrite']
Modifiability,"> This is a small refactor, right?. Yes, no problem",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7712#issuecomment-566671854:18,refactor,refactor,18,https://hail.is,https://github.com/hail-is/hail/pull/7712#issuecomment-566671854,1,['refactor'],['refactor']
Modifiability,"> This node can be used to group together multiple nodes with lowering implementations; > let us generate a TableValue that can go into the tail of relational functions that will take longer to lower. Sorry, I didn't sleep well last night and I must be slow today. I don't think I understand either of these. Can you give me examples?. In thinking about how this is intended to be used, I'm actually starting to formulate a different picture: what I think we want is. ```; case class DistributedArray(; contexts: IR, globals: IR, cname: String, gname: String, body: IR); extends TableIR; ```. where DistributedArray has the same signature as CollectDistributedArray, but is a TableIR instead of a (value)IR and should be able to be rendered as an RVD. In particular, this is something we can construct from a TableStage during the lowering process when we hit something that can't be lowered. In LowreTableIR we'd have:. ```; case TableToTableApply(child, f) =>; TableToTableApply(lower(child).toDistributedArray, f); ```. where lower(child) returns a TableStage.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8028#issuecomment-589006380:571,extend,extends,571,https://hail.is,https://github.com/hail-is/hail/pull/8028#issuecomment-589006380,1,['extend'],['extends']
Modifiability,"> To use protected var _pType2 instead I believe we need to have InferPType extend IR inside of IR.scala, e.g object InferPType extend IR, by requirement of sealed traits. You can use `protected[ir] var _pType2: PType`, which would be fine and let you keep the current structure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6594#issuecomment-515062679:76,extend,extend,76,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515062679,2,['extend'],['extend']
Modifiability,"> Two comments, and a meta-comment:; > ; > * I had looked over async http libraries and had preferred aiohttp over sanic because (1) ""aiohttp"" is a blessed aio library, (2) performance seemed comparable, (4) aiohttp seemed like a simpler solution which was attractive because the microservices are looking more and more like services and less like web servers (even more so moving all the rendering to the front end with the web app, the legacy version of scorecard using jinja is not the representative case). Did you look at aiohttp?; > * From the code:; > > Global variables that are modified ...; > ; > ; > I don't want to have to think about shared state and locking. I want a shared-nothing architecture in the microservices where the only globals are true constants and threads communication by sending immutable data through queues.; > * Finally, a meta-comment. I started reviewing this when it was just ujson, I did a bit of research about json packages to understand your choices and when I came back, the PR had expanded with all the async stuff. I would have approved the ujson stuff. The async stuff could have been a separate PR. Nobody wants to review a moving target, so the scope of a change should be roughly frozen when you assign a PR and additional changes should be minimized and restricted to that scope. You're welcome to have an open PR with no reviewer if you're still fleshing out the scope, of course. Thanks!. In response:. 1) aiohttp is an option, but appears to be generally considered slow on a per-response basis (published benchmarks, haven't had a chance to try it), even potentially slower than flask. It seems wrong to choose something slower if there are are reasonable alternatives.; 2) The globals were a feature of the initial implementation (the GitHub cache). It felt outside of the scope of my PR to change that to some queue solution. Meta comment. Ok. I didn't think it had been looked at, and expanded what it did pretty quickly, as I realized that ujso",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051:568,variab,variables,568,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051,1,['variab'],['variables']
Modifiability,> What was the issue with the third variable?. Needed the full path: gcr.io/$(PROJECT)/$(1):$$(shell ...),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5829#issuecomment-481372325:36,variab,variable,36,https://hail.is,https://github.com/hail-is/hail/pull/5829#issuecomment-481372325,1,['variab'],['variable']
Modifiability,"> Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. We take special care to ensure our system is as efficient as possible when reading or writing to this native format. So, it's partly a sociological thing. On the practical end of things, Hail's native formats (for Tables and Matrix Tables) are a partitioned binary format. The partitioned part means Hail can use many cores in parallel to process and write the dataset. The binary part means that Hail need not use unnecessarily large (in terms of bytes) representations of values. These three things together make writing the native formats use less time, use less memory, and be more reliable. ---. > One thing I noticed is the mt_hwe_vals variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. Yes, after you aggregate you get back an MT with a different column key. ---. The `entries` method converts your matrix table from a compact and efficient matrix into a ""long"" and inefficient table. I generally recommend avoiding it if you can. However, if you only have a handful of ancestries, I wouldn't expect this to be *that* bad. You can just write the MT itself:. ```python3; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)); mt_hwe_vals = mt_hwe_vals.select_rows().select_cols() # drop irrelevant row and column fields; mt_hwe_vals.write(bucket + '/hwe.ht'); ```. ---. > I tried modifying the code to what is shown below but I'm still having the same issue. Just to be clear it's the exact same error ""Container exited with a non-zero exit code 137. ""? This makes me think we have an issue with `entries`, because, even though it's not great, it shouldn't be blowing RAM here. Can you share the log file from your previous or next attempt?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636:755,variab,variable,755,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636,1,['variab'],['variable']
Modifiability,"> Yeah, I imagine we do. `InferPType` isn't used right now, correct? I'll add the new case, just checking if there's a test that would have caught this. Yeah, I edited my comment. Totally not used right now, which is why you didn't notice. I think making an issue is also fine, like you said it would be an enhancement, not something necessary for what you wrote to work. Looks really cool btw.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7682#issuecomment-562812319:307,enhance,enhancement,307,https://hail.is,https://github.com/hail-is/hail/pull/7682#issuecomment-562812319,1,['enhance'],['enhancement']
Modifiability,"> Your tool should also examine the first word of the MAKEFLAGS variable and look for the character n. If this character is present then make was invoked with the -n option and your tool should stop without performing any operations. Added. > Your tool should be sure to write back the tokens it read, even under error conditions. This includes not only errors in your tool but also outside influences such as interrupts (SIGINT), etc. You may want to install signal handlers to manage this write-back. I mean, I doubt anyone is sending signals other than SIGKILL to our build system, but I added some signal handlers that just `sys.exit(0)` which triggers the finally (I checked). > We also get a lot of warning: jobserver unavailable: using -j1. Add +' to parent make rule.warnings when runningmake jvm-test`. This is because our C++ backend uses make to drive compilation (wtf). I strip MAKEFLAGS before calling gradle now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6923#issuecomment-524446104:64,variab,variable,64,https://hail.is,https://github.com/hail-is/hail/pull/6923#issuecomment-524446104,1,['variab'],['variable']
Modifiability,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170:268,config,config,268,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170,1,['config'],['config']
Modifiability,"> ah yes - the identity used to create the SAS token needs to have a control plane role on the Storage Account - Owner, Contributor, or (most specific) Storage Account Key Operator Service Role... Is that a manageable role to configure for testing or should I try to explore alternatives in the generation?. Thanks! This is totally fine, I'll just configure the SP that we use for the inter-cloud tests with the key operator role and re-run the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13140#issuecomment-1579202097:226,config,configure,226,https://hail.is,https://github.com/hail-is/hail/pull/13140#issuecomment-1579202097,2,['config'],['configure']
Modifiability,> elimination of default_namespace. This is also unfortunately tricky :/// as clients require `default_namespace` exist in container's deploy config,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14056#issuecomment-1879295919:142,config,config,142,https://hail.is,https://github.com/hail-is/hail/pull/14056#issuecomment-1879295919,1,['config'],['config']
Modifiability,"> it seems to provide visibility into what happened during the last run of lets encrypt?. Yes. As far as I know, certbot needs the previous config to do a renew (which I'm not doing yet). > I think the ""sidecar"" approach is simpler than this one (no extra nginx instance, no secrets, no service, no k8s secret creation privileges). We beef up the nginx pod to have a second container sharing a letsencrypt volume (which we've already defined in this PR). You can't mount volumes to multiple pods. You can't even mount volumes to the SAME pod if you want to do rolling updates (because the new instance can't launch because the old one is mounting the volume). I think this means volumes for certs and web root are out. volumes only work for replicated StatefulSets where you can take down one instance at a time for updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868:140,config,config,140,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868,1,['config'],['config']
Modifiability,"> nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):; > ; > > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode.; > ; > Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission.; > ; > See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957).; > ; > [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now. Thanks, yeah, I shared this with Cotton yesterday. We need to be careful seems to be the conclusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479640942:338,config,config,338,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640942,1,['config'],['config']
Modifiability,"> nginx should need the services (i.e. domain names) to exist, not the deployments. Ah, that makes sense. I suppose that's why the service definition of `router` used to exist along with gateway's k8s config.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10736#issuecomment-891312778:201,config,config,201,https://hail.is,https://github.com/hail-is/hail/pull/10736#issuecomment-891312778,1,['config'],['config']
Modifiability,"> otherwise we need to wait until EmitStream1 is ripped out. That will be very soon. > I will propose some code changes to deal with this reuse issue. In general, I want the picture that Code[_] cannot be placed in multiple locations. Absolutely. I have some WIP from before I shifted focus to streams that makes a `Code[_]` throw an exception at the site where it is used a second time. But it's tangled up with some other changes I was experimenting with, mostly to enable using variables which could be either locals or fields, with the decision being made after the complete `Code[_]` is assembled. I'd be happy to revisit that when lowering is unblocked, or let you do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8207#issuecomment-593417586:481,variab,variables,481,https://hail.is,https://github.com/hail-is/hail/pull/8207#issuecomment-593417586,1,['variab'],['variables']
Modifiability,"> should be explicitly representable in the IR. How would that work with MakeArray?. > MakeStream is most naturally a push stream. I'm not sure I see that. If you don't duplicate the consumer, the push code is the same. The question is, do you use a variable + switch to track where you are in the MakeStream, or do you use the program counter via labels, where you get the latter from the former by code duplication.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8148#issuecomment-591932796:250,variab,variable,250,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-591932796,1,['variab'],['variable']
Modifiability,"> submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; > if any request fails, raise an exception, which is caught by outer submit, which retries a configurable number of times, logging a configurable number of errors. I haven't dug into the PR yet, but will just remark I'm going to argue pretty strenuously to maintain our current model here: infinitely retry transient errors with exponential backoff and no retry of non-transient errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875#issuecomment-574377227:188,config,configurable,188,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574377227,2,['config'],['configurable']
Modifiability,"> the TableIR doesn't define partitionCounts. statically known partition counts are used to optimize `.count()` when we know the partition sizes. Here that doesn't apply, so I don't think you need to define that method (it inherits `def partitionCounts = None`). > perhaps ""LiftLiterals"" was changed to ""LiftNonCompilable"". Yes, it was. No need to write a rule for this. Separately, I think we should delete the checklist. It'll never be correct, since it's not checked against the codebase. To add an IR node, one needs to understand the compiler, and we can't adequately document that in a bullet list right now (over time, things should get simpler). . If a node is missing from somewhere it needs to appear, then tests should catch that case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6689#issuecomment-513935766:223,inherit,inherits,223,https://hail.is,https://github.com/hail-is/hail/pull/6689#issuecomment-513935766,1,['inherit'],['inherits']
Modifiability,">>> hc = hail.HailContext(); log4j:ERROR setFile(null,false) call failed.; java.io.FileNotFoundException: hail.log (Permission denied); 	at java.io.FileOutputStream.open0(Native Method); 	at java.io.FileOutputStream.open(FileOutputStream.java:270); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:213); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:133); 	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294); 	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165); 	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104); 	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842); 	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768); 	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648); 	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514); 	at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:440); 	at is.hail.HailContext$.configureLogging(HailContext.scala:132); 	at is.hail.HailContext$.apply(HailContext.scala:159); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198:2120,config,configureRootCategory,2120,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198,1,['config'],['configureRootCategory']
Modifiability,"@@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; in",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3316,config,config,3316,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability,"@Sun-shan According to the error message you posted, Spark itself cannot find `/hail/test/BRCA1.raw_indel.vcf`:; ```; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.; : org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/hail/test/BRCA1.raw_indel.vcf; ```. Looking at that error message, it looks like Spark is interpreting your path as a local file system path, _not_ a hadoop path. Moreover, earlier in your posted output this line:; ```; 17/08/15 08:58:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; ```; suggests that you're not actually connecting to a Spark cluster with a properly configured Hadoop installation. ---. Your Spark cluster appears improperly configured. I'm not sure if `pyspark` is even connecting to your cluster. You might try looking at [this StackOverflow post](https://stackoverflow.com/questions/34642292/cant-connect-pyspark-to-master) about connecting `pyspark` to a Spark cluster. I strongly recommend running `pyspark` again and executing:; ```; spark.sparkContext.master; ```; This should print the URL of your Spark master node. If this prints a String starting with `local`, then you're definitely not connecting to a Spark cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-322539635:769,config,configured,769,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-322539635,2,['config'],['configured']
Modifiability,"@bw2 that package name is a lie, sadly. The [maven repository page](https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-20_2.11/5.5.1) for `org.elasticsearch:elasticsearch-spark-20_2.11:5.5.1` lists `org.apache.spark:spark-core_2.11:2.1.0` as a dependency, which is decidedly not 2.0. We'll have to use elasticsearch-spark 5.1.2. It's a bit annoying. You'll have to extend the [spark version-specific logic](https://github.com/hail-is/hail/pull/2049/files#diff-c197962302397baf3a4cc36463dce5eaR44) in `build.gradle`. You'll want to bind a new name, something like `elasticsearchSparkVersion`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2049#issuecomment-320335957:387,extend,extend,387,https://hail.is,https://github.com/hail-is/hail/pull/2049#issuecomment-320335957,1,['extend'],['extend']
Modifiability,"@bw2, it looks like you're picking up the router_fs, which should only be used when you have the QoB backend enabled. What settings do you have for `hailctl config get query/backend` and `HAIL_QUERY_BACKEND`? You can prefer the Spark backend with: `hailctl config set query/backend spark` and `HAIL_QUERY_BACKEND=spark`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12540#issuecomment-1363220199:157,config,config,157,https://hail.is,https://github.com/hail-is/hail/issues/12540#issuecomment-1363220199,2,['config'],['config']
Modifiability,"@catoverdrive yes, but that would be normal behavior. We register a series of compression codecs when creating the spark configuration/hadoop configuration/HailContext that hadoop uses to dispatch reading of the file to the appropriate input stream class, it does this based on a method in the codec classes like so in `BGZipCodec.java`; ```java; @Override; public String getDefaultExtension() {; return "".bgz"";; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5513#issuecomment-469540554:121,config,configuration,121,https://hail.is,https://github.com/hail-is/hail/pull/5513#issuecomment-469540554,2,['config'],['configuration']
Modifiability,"@cseed ; I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. I've updated the PR description with this text so it ends up in the squashed commit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061:48,config,config-hail-root,48,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061,1,['config'],['config-hail-root']
Modifiability,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-252825829:58,config,configured,58,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829,1,['config'],['configured']
Modifiability,"@cseed All set! Still not sure why 0.0.0.0 was needed in this case, but not Dan's config; first assumption is that JupyterLab sets this as default, and not sure. why listening on localhost was insufficient (first guess is that the docker image didn't specify EXPOSE 8888?). Still need to provide finer-grained status updates, based on more than status.phase (inspect container during the MODIFIED watch event). Also. need to re-implement auth_request to deal with (ignore) the ~30 requests subsequent to the redirect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460097832:82,config,config,82,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097832,1,['config'],['config']
Modifiability,@cseed Back to you. Can you please verify the `codec` variable is being used correctly?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1458#issuecomment-283766450:54,variab,variable,54,https://hail.is,https://github.com/hail-is/hail/pull/1458#issuecomment-283766450,1,['variab'],['variable']
Modifiability,"@cseed I don't really like how this looks with lists as the inputs to the commands. To me, it's much harder to read and modify. I like the commands looking as much like writing a shell script as possible. Maybe others feel differently though... Also, you can't do something like this ` ' '.join([task.ofile for task in shapeit_tasks])` because you'll lose information about the dependencies before `command` sees the original resource inputs. What I really want is a version of f-string interpolation where I parse and detect the variables (known and unknown), handle them properly by either creating new resources or adding dependencies to the Task, and then execute the Python formatting code inside the curly braces. I'm not sure if it is possible to do this. If it is, it's probably complicated and we'll have to use the Python `ast` and `parser` modules and call `eval` ourselves. ; ```python; from pyapi import Pipeline; p = Pipeline(). bfile_root = 'gs://jigold/input'; bed = bfile_root + '.bed'; bim = bfile_root + '.bim'; fam = bfile_root + '.fam'. p.write_input(bed=bed, bim=bim, fam=fam). subset = p.new_task(); subset = (subset; .label('subset'); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--make-bed', '--out', subset.tmp1]); .command(['awk', ""'{ print $1, $2}'"", subset.tmp1 + '.fam', ""| sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }'"",; '>', subset.tmp2]); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--remove', subset.tmp2,; '--make-bed', '--out', subset.tmp2])). shapeit_tasks = []; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .command(['shapeit', '--bed-file', subset.ofile, '--chr ', contig, '--out', shapeit.ofile])); shapeit_tasks.append(shapeit). merger = p.new_task(); merger = (merger; .label('merge'); .command(['cat', ' '.join([task.ofile for task in shapeit_tasks]), '>>', merger.ofile)). p.write_output(merger.ofile + "".haps"", ""gs://jigold/final_output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-447469039:530,variab,variables,530,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-447469039,1,['variab'],['variables']
Modifiability,"@cseed I know you didn't approve the final design document. In summary,. 1. We print a warning to the user if they specify the memory in the batch user library that we're ignoring this parameter. Memory is deduced from CPU and the worker configuration in the front end ignoring the resource request.; 2. There's a concept of reserved vs. unreserved space. Each job gets 5 Gi per core requested in the reserved space. We try and reserve unreserved space if needed for storage needs that are bigger than the reserved space with a semaphore. If we can't get enough unreserved space, then we still give the user the reserved space at 5Gi per core for their container on the worker data disk in addition to the extra disk at /io that is the full storage request. Example:. Storage request is 375Gi and 1 CPU.; User gets 5 Gi for their container.; We spin up a 375Gi disk. For a local ssd with 16 cores, there's 16*5 Gi or 80 Gi in the reserved space. The reserved space for us is 20 Gi (I can set this back to 25 Gi, but I thought 100 Gi being the minimum disk size for persistent SSD data disks was a nice number). This means the unreserved space that is first come first serve is 275 Gi. If the data disk is a 100Gi persistent SSD, then the unreserved space is 0 Gi and any job that requests more storage than 5 Gi per core will have to spin up a new disk.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9598#issuecomment-714610813:238,config,configuration,238,https://hail.is,https://github.com/hail-is/hail/pull/9598#issuecomment-714610813,1,['config'],['configuration']
Modifiability,@cseed I refactored the resource stream into utils. Back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1780#issuecomment-302461856:9,refactor,refactored,9,https://hail.is,https://github.com/hail-is/hail/pull/1780#issuecomment-302461856,1,['refactor'],['refactored']
Modifiability,"@cseed I've split the packages up so they can be imported from within the appropriate init method. Edit: re-running ./gradlew shadowJar fixes this. Tests are not passing, but seemingly not because of any of these changes. If I checkout last commit in master, they also fail. Furthermore, I can restore all changes from last FS PR manually, and no benefit. ```; HEAD is now at 117c365c3 [ci] also handle batch Ready state (#5909); (hail) alex:~/projects/hail/hail/python:$ pytest test/ -x; ======================================================================================= test session starts ========================================================================================; platform darwin -- Python 3.6.8, pytest-3.8.0, py-1.7.0, pluggy-0.8.1; rootdir: /Users/alex/projects/hail/hail/python, inifile:; plugins: xdist-1.22.2, metadata-1.8.0, html-1.19.0, forked-1.0.2; collected 591 items . test/hail/test_context.py E. ============================================================================================== ERRORS ==============================================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:816,plugin,plugins,816,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928,1,['plugin'],['plugins']
Modifiability,"@cseed This PR can be merged. I ran the comparison on the cloud between current master and this branch with UKBB Wave 1 Chr21 (20GB) with the exact same cluster configuration (Liam's default settings). Ran this command:. ```; %%timeit -n 1. hc.import_bgen(bgen_file, sample_file = sample_file).count(); ```. Got 3min21sec for master and 3min24sec for my branch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2239#issuecomment-337996806:161,config,configuration,161,https://hail.is,https://github.com/hail-is/hail/pull/2239#issuecomment-337996806,1,['config'],['configuration']
Modifiability,"@cseed added these print statements. I've never used the output in debugging in production except to quickly realize a job is one of a given user's when I was looking at worker performance. Early on, I might have checked it to make sure the Docker config resource options and volume mounts were correct. But I can add those back when debugging if needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9184#issuecomment-667107337:248,config,config,248,https://hail.is,https://github.com/hail-is/hail/pull/9184#issuecomment-667107337,1,['config'],['config']
Modifiability,@cseed am I right that the thinking on this has evolved with move to Python?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/614#issuecomment-279583150:48,evolve,evolved,48,https://hail.is,https://github.com/hail-is/hail/issues/614#issuecomment-279583150,1,['evolve'],['evolved']
Modifiability,@cseed unrelated but I think you've configured your local git email and name to be hail-ci-leader,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5814#issuecomment-481693314:36,config,configured,36,https://hail.is,https://github.com/hail-is/hail/pull/5814#issuecomment-481693314,1,['config'],['configured']
Modifiability,"@cseed, digesting from this and our in-person conversation:. I added a steps about job dependencies and batch closure. Regarding ""a way to refer to individual inputs/outputs"", in the original post, I gave the example of:. ```; - type: exec; name: build-jar; image: hail-pr-builder; namespace: ns; command: [""./gradlew"", ""test"", ""shadowJar""]; outputs:; - ""build/libs/hail-all-spark.jar""; - type: exec; name: pytests; image: hail-pr-builder; dependsOn:; - build-jar; command: [""./run-python-tests-using-input-jar.sh""]; ```. The outputs of a job are stated explicitly by the definition. For the `pytests` job in the example above, the input from `build-jar` is placed at `/inputs/build-jar`. Regarding ""specify a series of stacked containers to execute"", I don't see a straightforward way to implement this. It's tricky enough to have a ""anti-init""/""finalizer"" container. Inter-job I/O will be handled by batch. The user controls the image and the command and the environment variables of the build step, so they can arrange for permission to copy results to a bucket they own. Are we worried about the setting of user's wanting to run untrustworthy software? They already run arbitrary software on cloud instances that have plenty of latent credentials. I think we can at least punt on this until other functionality is in. Local disk sounds like a nice thing to add eventually. Agreed, that sounds like a nice model. I'll consider it as I envision a persistent batch system.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-459537502:973,variab,variables,973,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-459537502,1,['variab'],['variables']
Modifiability,"@cseed: I feel somewhat guilty about duplicating a large amount of `AnnotateVariantsTable` code here, almost all of it. However, it isn't totally easy to refactor. I'm also happy to rename ""GenomicIndex"" to something better.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/438#issuecomment-227626931:154,refactor,refactor,154,https://hail.is,https://github.com/hail-is/hail/pull/438#issuecomment-227626931,1,['refactor'],['refactor']
Modifiability,"@danking . This should be ready to look at. Stacks on #5452. Once that PR is merged, changes are:. 1) notebook.html: organize into form (notebook-form.html) and notebook state (notebook-state.html) components.; 2) add modified versions of notebook-api. Namely I don't use the marshaling procedure you didn't like, and refactor as much of the JS stuff as I can into synchronous http requests.; * modifies /notebook routes , adds `marshall_notebook`, `get_live_user_notebooks`, `wait_websocket`, and replace any calls to `session['pod_name']` and `session['svc_name`] with equivalent versions based on `session['notebook']`, which contains the notebook object of the existing session. Changes mainly contained within commit: https://github.com/hail-is/hail/pull/5476/commits/2f180ed0bfb3b0dfb7224df1ef6afba0e1a9cbfc (the following pr only renames notebook-obj.html to notebook-state.html). Basically feels like a synchronous / refresh-based version of what we had on app.hail.is, with less state insight (uses only the websocket-based reachability check). Upcoming PR will restore fine-grained state updates via JS/websocket. cc @cseed. Images:; <img width=""1302"" alt=""screen shot 2019-02-28 at 3 06 46 pm"" src=""https://user-images.githubusercontent.com/5543229/53595163-88d8ea00-3b6a-11e9-841b-7dbf6981c990.png"">. <img width=""1301"" alt=""screen shot 2019-02-28 at 3 06 51 pm"" src=""https://user-images.githubusercontent.com/5543229/53595148-7ced2800-3b6a-11e9-9428-5290b5ee1dc7.png"">. <img width=""1301"" alt=""screen shot 2019-02-28 at 3 09 17 pm"" src=""https://user-images.githubusercontent.com/5543229/53595276-d35a6680-3b6a-11e9-930e-5ef0757e181e.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5476#issuecomment-468419208:318,refactor,refactor,318,https://hail.is,https://github.com/hail-is/hail/pull/5476#issuecomment-468419208,1,['refactor'],['refactor']
Modifiability,"@danking @catoverdrive OK, I think it's ready for another review. I made the suggested changes, as well as the changes we talked about on Tuesday during check-in. There is a ton of indirection now, which might be confusing but is good for future expansion. The matrix and table metadata are now polymorphic `RelationalSpec`s and have a `name` field so new relational types can be added. Here is `sample.hmt/metadata.json.gz`:. ```; {; ""components"": {; ""cols"": {; ""rel_path"": ""cols\/rows"",; ""name"": ""RVDComponentSpec""; },; ""rows"": ...,; ""partition_counts"": {; ""counts"": [; 346; ],; ""name"": ""PartitionCountsComponentSpec""; }; },; ""matrix_type"": ...,; ""references_rel_path"": ""references"",; ""hail_version"": ""devel-e6ef439"",; ""file_version"": 65536,; ""name"": ""MatrixTableSpec""; }; ```. A MT has currently has five components: globals, cols, rows, entries and partition_counts. The first four are `RVDComponents`, the counts its own thing. I wanted to make the references a component, but they need to be loaded before the types are parse, so `RelationalSpec`s have a path to the references. Components are future expandable. The MT directory structure has metadata and four directories for each main component which is a Table directory. Those directory names are stored in the metadata, so they can be changed or even point elsewhere. A Table directory has two directories: globals and rows, which are RVDs. Again, the directories are stored in the metadata and I use that here: the globals RVD for rows and cols tables are the rows RVD of the globals table of the MT (if you can understand this sentence you've got a handle on this PR.). RVDs now store their own metadata, the RVDSpec. A sample rows RVD metadata for the rows table of an MT, `sample.hmt/rows/rows/metadata.json.gz` looks like:. ```; {; ""jRangeBounds"": [],; ""partFiles"": [; ""part-0""; ],; ""codecSpec"": ...,; ""orvdType"": ...,; ""name"": ""OrderedRVDSpec""; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2858#issuecomment-364814856:295,polymorphi,polymorphic,295,https://hail.is,https://github.com/hail-is/hail/pull/2858#issuecomment-364814856,1,['polymorphi'],['polymorphic']
Modifiability,"@danking Curious for your thoughts on this refactor. I was getting pretty turned around myself with the various credential classes and I think this is closer to what we want in a keyless world. Ideally the batch worker should just be able to request credentials (in the form of an access token) for a given identity with just the identity's ID. LMK if you're in favor of this or not, or if you would like to see it folded into the metadata server PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14125#issuecomment-1879054756:43,refactor,refactor,43,https://hail.is,https://github.com/hail-is/hail/pull/14125#issuecomment-1879054756,1,['refactor'],['refactor']
Modifiability,"@danking Looks like I'm still failing to configure a couple of settings related to references on the `ServiceBackend` but you can feel free to start looking. You'll notice that I made quite a substantial refactor in `ServiceBackend.scala` in an attempt to harmonize the scala backends a bit more. The rationale behind the refactor is I was having a hard time working with the various thunks passed around there. I saw them as a bit of poor-man's-object way to capture some state from the input file while keeping the `ServiceBackend` stateless. IMO there's no harm in keeping the `ServiceBackend` just as stateful as the other backends since it is single use. So I lifted a lot of that state into backend-creation time and created a harder delineation between which part of the input is for configuring the backend and which part is for the action being performed. This made it easier to reuse a couple of methods like `tableType` and such. I'm happy to take suggestions on ways to trim down this PR, but I thought you'd want to take a look at the whole thing given the time-sensitivity.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13797#issuecomment-1766521995:41,config,configure,41,https://hail.is,https://github.com/hail-is/hail/pull/13797#issuecomment-1766521995,4,"['config', 'refactor']","['configure', 'configuring', 'refactor']"
Modifiability,"@danking Thanks for taking this over! I commented out the mark_unscheduled if the sidecar fails for debugging. The sidecar is running, but I'm getting an error because it's trying to run the top level code in batch.py. Either sidecar.py needs to be separate or we need to reconfigure batch.py so it doesn't run that code. ```; Traceback (most recent call last):; File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.6/dist-packages/batch/sidecar.py"", line 14, in <module>; from .batch import REFRESH_INTERVAL_IN_SECONDS, HAIL_POD_NAMESPACE, KUBERNETES_TIMEOUT_IN_SECONDS; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 83, in <module>; db = BatchDatabase.create_synchronous('/batch-user-secret/sql-config.json'); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 23, in create_synchronous; run_synchronous(cls.__init__(db, config_file)); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 15, in run_synchronous; return loop.run_until_complete(coro); File ""uvloop/loop.pyx"", line 1451, in uvloop.loop.Loop.run_until_complete; File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 210, in __init__; await super().__init__(config_file); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 27, in __init__; with open(config_file, 'r') as f:; FileNotFoundError: [Errno 2] No such file or directory: '/batch-user-secret/sql-config.json'; ```. To see the logs `kubectl -n namespace logs pod_name cleanup`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6600#issuecomment-510272888:871,config,config,871,https://hail.is,https://github.com/hail-is/hail/pull/6600#issuecomment-510272888,2,['config'],['config']
Modifiability,@danking This should be really close to having the tests passing. The only other thing left to do once it passes in GCP is to copy the data over to Azure and make a default Azure configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1339804624:179,config,configuration,179,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1339804624,1,['config'],['configuration']
Modifiability,@danking What do you think about having a version ID inside the JAR file (MANIFEST???). We already download the JAR file on the worker. Not sure how much extra time it would be to look for the version inside the JAR (maybe cache this?) and then pass the right argument configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12222#issuecomment-1258646909:269,config,configuration,269,https://hail.is,https://github.com/hail-is/hail/pull/12222#issuecomment-1258646909,1,['config'],['configuration']
Modifiability,"@danking should internal_base_url in wait-for.py be https? Do we have anything that should route through http?. As an alternative, I think it would be reasonable to always rewrite port: 443 to https:// in that function. I can PR if you're ok with that. edit: The comment states that the protocol should match hailtop.config. Not sure why that is, besides sharing gateway probably? In any case, this function doesn't match that. We're always in the `self._location == 'external'` space I think, which means https according to hailtop.config",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548098055:172,rewrite,rewrite,172,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548098055,3,"['config', 'rewrite']","['config', 'rewrite']"
Modifiability,"@danking, @cseed An attempt to use --notebook-dir failed (Didn't understand the path). Will make another attempt to set this as a config, but if not, I think we should defer folder creation as an improvement to jgscm, I'll open an issue. Have forked jgscm, and have identified what appears a likely path to the fix (they don't specify the full blob path, gs://bucket/blob). As an aside, jgscm is effectively unmaintained. 2 of the problems I've encountered have issues dating to May & August (last accepted PR was April 2018). After we patch in the fixes needed (dependencies, folder creation), I think we should consider publishing a separate package from our fork (say jgscm2), unless we want to maintain jgscm in our repo, which may be less desirable from a licensing perspective based on our earlier convos (jgscm is MIT, but I believe you still may prefer to not mix codebases?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480335780:130,config,config,130,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480335780,1,['config'],['config']
Modifiability,"@danking. Here what I have done in my environment ( AWS EMR ); * Create EMR without installing hail; * Update PATH ( this is needed or I get an error with `hailctl not found` at the installation step); ```sh; export PATH=$PATH:/home/hadoop/.local/bin; ```; * Clone latest commit of Hail; ```sh; cd /tmp; git clone --depth 1 https://github.com/broadinstitute/hail.git; cd hail/hail/; ```; * Edit `build.gradle` and add `exclude group: 'org.scala-lang', module: 'scala-reflect'`; * Build Hail; ```sh; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.2; ```; * Symlink hail-all-spark.jar into /opt ( At the EMR creation step (before hail installation) I edit the `spark-defaults` properties in order to link `hail-all-spark.jar`... This config was needed & works successfuly for an old version of Hail (0.2.60)... can be revisit if not appropriate for recent version; ```sh; sudo mkdir /opt/hail/; sudo ln -sf /home/hadoop/.local/lib/python3.9/site-packages/hail/backend /opt/hail/backend; ```; * start pyspark; ```sh; $ pyspark; Python 3.9.18 (main, Oct 25 2023, 05:26:35) ; [GCC 7.3.1 20180712 (Red Hat 7.3.1-17)] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/. Using Python version 3.9.18",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949:772,config,config,772,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949,1,['config'],['config']
Modifiability,"@danking; ```; [root@mg hail]# echo $HAIL_HOME; /opt/Software/hail; [root@mg hail]# echo $PYTHONPATH; :/opt/Software/hail/python:/opt/cloudera/parcels/SPARK2/lib/spark2/python:/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip; [root@mg hail]# cd /opt/Software/hail/python; [root@mg python]# ls; hail; [root@mg python]# cd /opt/cloudera/parcels/SPARK2/lib/spark2/python; [root@mg python]# ls; docs lib MANIFEST.in pylintrc pyspark README.md run-tests run-tests.py setup.cfg setup.py test_support; [root@mg python]# cd /opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/; [root@mg lib]# ls; py4j-0.10.4-src.zip PY4J_LICENSE.txt pyspark.zip; [root@mg lib]# echo $SPARK_CLASSPATH; /opt/Software/hail/build/libs/hail-all-spark.jar; [root@mg lib]# cd /opt/Software/hail/build/libs/; [root@mg libs]# ls; hail-all-spark.jar; ```; the configuration file:; ```; export SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2; export HAIL_HOME=/opt/Software/hail; export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337442177:847,config,configuration,847,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337442177,1,['config'],['configuration']
Modifiability,"@iris-garden I think you make great points! And I agree, across many PRs we probably do want to be analyzing the security impacts at every stage, not just as a one-off ""when we're done it will be X"" analysis in the ticket... So I guess in my mind the _only_ real reason for using the issue-level review would be for tracking the impact of non-code changes (like configuration updates to production). I will try to make the templates reflect that distinction",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14666#issuecomment-2329290981:362,config,configuration,362,https://hail.is,https://github.com/hail-is/hail/pull/14666#issuecomment-2329290981,1,['config'],['configuration']
Modifiability,@iris-garden Would you be interested in collaborating on this set of changes? I know you were starting to think about environment variables and configuration as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13224#issuecomment-1663830418:130,variab,variables,130,https://hail.is,https://github.com/hail-is/hail/pull/13224#issuecomment-1663830418,2,"['config', 'variab']","['configuration', 'variables']"
Modifiability,@jbloom22 I realized that updating the length of the array after creating it doesn't work because of the variable size of the missingness bits that get allocated.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3151#issuecomment-373180986:105,variab,variable,105,https://hail.is,https://github.com/hail-is/hail/pull/3151#issuecomment-373180986,1,['variab'],['variable']
Modifiability,"@jigold I assigned to you since this essentially re-opens #3715 with your comments addressed. After experimenting, I felt that it didn't make sense at this point to design flexible ndarray checking at the level of typecheckers. The requirements vary quite a bit between these two functions. If you feel strongly, let's discuss. I'll leave Tim's review un-dismissed until we've agreed where to put these functions, but no need to wait on it w.r.t reviewing (once you're back).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3873#issuecomment-401470955:172,flexible,flexible,172,https://hail.is,https://github.com/hail-is/hail/pull/3873#issuecomment-401470955,1,['flexible'],['flexible']
Modifiability,"@jigold I stood up batch/ci in my own project from `hail-is/hail:main` and then deployed this branch, taking notes of any changes I needed to make and all seemed to work out OK. I think that's about as much as I can properly test this without trying things out in haildev/hail-vdc. The steps were as follows:. 1. Generate the configmaps used by gateway/internal-gateway. These will have the routing configuration for production services (I've edited the bootstrap instructions to match); `make -C gateway envoy-xds-config && make -C internal-gateway envoy-xds-config`; 2.  wait a few seconds for CI to quietly update these configmaps with information about testing namespaces  (can manually verify changes with `download-configmap gateway-xds-config`); 3. Deploy the new versions of gateway/internal-gateway; `make -C gateway deploy NAMESPACE=default && make -C internal-gateway NAMESPACE=default`. This worked for me in my project with no downtime, but either way I would probably do the same thing as with the previous PR where I test it in azure before making changes to hail-vdc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095#issuecomment-1293703346:326,config,configmaps,326,https://hail.is,https://github.com/hail-is/hail/pull/12095#issuecomment-1293703346,7,['config'],"['config', 'configmap', 'configmaps', 'configuration']"
Modifiability,@jigold Lint failed because the `is_developer` variable is no longer used,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12972#issuecomment-1533377221:47,variab,variable,47,https://hail.is,https://github.com/hail-is/hail/pull/12972#issuecomment-1533377221,1,['variab'],['variable']
Modifiability,"@jigold Notebook has no tests, so it doesn't have an environment.yml that needs to be updated in the hail-ci-build-image. This configuration is only for the run-time service.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5451#issuecomment-467637462:127,config,configuration,127,https://hail.is,https://github.com/hail-is/hail/pull/5451#issuecomment-467637462,1,['config'],['configuration']
Modifiability,@jigold this is a minimal adaptation of #3466 which avoids exposing RowMatrix by putting an export command on BlockMatrix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3500#issuecomment-386445838:26,adapt,adaptation,26,https://hail.is,https://github.com/hail-is/hail/pull/3500#issuecomment-386445838,1,['adapt'],['adaptation']
Modifiability,"@jigold this needs gcloud installed and configured to work in cluster, specifically 'GOOGLE_APPLICATION_CREDENTIALS'. Will work on that after some higher priority items are in, I believe it is sufficient to have this working on our local machines for now (manual user creation). Will unassign for now, and re-assign when gcloud is configured. Any suggestions on how to get that configured on the cluster would be much appreciated too :). cc @cseed, @danking",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5618#issuecomment-479316954:40,config,configured,40,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-479316954,3,['config'],['configured']
Modifiability,"@jigold. > I ran a test job with the copy tool on a 10 Gi random file and matched 1.2 Gibit / second. Does this mean something like:; ```; j = b.new_job(); j.image('hailgenetics/hail:0.2.118'); j.command('python3 -m hailtop.copy ...'); ```; Or did you use a `read_input`? I'm curious if something about how we configure the input container could affect this. I doubt it, but wanted to confirm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1599420578:310,config,configure,310,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1599420578,1,['config'],['configure']
Modifiability,"@lfrancioli look at the built docs (under TeamCity, artifacts, index):; https://ci.hail.is/repository/download/HailSourceCode_HailMainline_BuildDocs/9716:id/www/hail/types.html#set-t. There is an issue of variable naming: your a is implicit (not named), and your b is our a. So for example:; ```; add(a: T): Set[T]  Returns the result of adding the element b to Set a.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1491#issuecomment-284773672:205,variab,variable,205,https://hail.is,https://github.com/hail-is/hail/pull/1491#issuecomment-284773672,1,['variab'],['variable']
Modifiability,"@mhebrard I notice you're using `sudo make`, I suspect this means that Hail's code is running under a modified `PATH` that lacks `pip-compile`. We'll fix our install-on-cluster target to have a ""make the artifact"" and an ""install"" step that are separate (so you can install as root but build as a normal user). In the mean time, apply this patch:. ```; diff --git a/hail/Makefile b/hail/Makefile; index dabe146d3a..e12ac791c4 100644; --- a/hail/Makefile; +++ b/hail/Makefile; @@ -349,7 +349,7 @@ install: $(WHEEL); hailctl config set query/backend spark; ; .PHONY: install-on-cluster; -install-on-cluster: $(WHEEL) check-pip-lockfile; +install-on-cluster: $(WHEEL); sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$$' | tr '\n' '\0' | xargs -0 $(PIP) install -U; -$(PIP) uninstall -y hail; $(PIP) install $(WHEEL) --no-deps. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13445#issuecomment-1690045571:523,config,config,523,https://hail.is,https://github.com/hail-is/hail/issues/13445#issuecomment-1690045571,1,['config'],['config']
Modifiability,"@mhebrard I would expect that `MutableSettings` to come from your Spark installation. I think there are two problems:; 1. For some reason, our gradle configuration is including `MutableSettings`. We should get rid of that, but I haven't yet figured out how to do that.; 2. Normally (1) isn't a problem because your Spark installation appears on the class path before Hail does. It seems to me that this isn't true in your case. This is probably caused by linking the Hail JAR into `/opt`. Is `/opt/hail/backend` on your class path? Why do you link the backend directory into `/opt`? The Hail JAR should be distributed automatically by Spark. You shouldn't need to do anything special after you `pip install` / `make install-on-cluster`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1777706782:150,config,configuration,150,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1777706782,1,['config'],['configuration']
Modifiability,"@mhebrard The only way I can imagine that we would mutate your environment is if we are accidentally installing `pyspark`. `install-on-cluster` takes pains to avoid that:; ```; install-on-cluster: $(WHEEL) check-pip-lockfile; 	sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$$' | tr '\n' '\0' | xargs -0 $(PIP) install -U; 	-$(PIP) uninstall -y hail; 	$(PIP) install $(WHEEL) --no-deps; 	hailctl config set query/backend spark; ```. But that is broken somehow? When you ran `install-on-cluster` did you see a `pip` output indicating that pyspark got installed?. Can you check if pyspark is installed via pip now? `pip show pyspark` (should say its not installed). If it is installed, try uninstalling it `pip uninstall pyspark`. You might also try removing the first line of `install-on-cluster` entirely. That will leave you without Hail's dependencies installed, but if `pyspark-shell` is still the right version of Scala, then I suspect the issue is that line.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1769053906:433,config,config,433,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1769053906,1,['config'],['config']
Modifiability,"@mhebrard can you try applying this diff and then building?. ```diff; diff --git a/hail/build.gradle b/hail/build.gradle; index d4bdd879f0..1b65904484 100644; --- a/hail/build.gradle; +++ b/hail/build.gradle; @@ -100,6 +100,7 @@ configurations {; hailJar.extendsFrom implementation; hailJar {; exclude group: 'org.scala-lang', module: 'scala-library'; + exclude group: 'org.scala-lang', module: 'scala-reflect'; exclude group: 'org.apache.spark'; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1777721812:229,config,configurations,229,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1777721812,2,"['config', 'extend']","['configurations', 'extendsFrom']"
Modifiability,@patrick-schultz I addressed your comments in the second commit and added sum/count aggregators. I also refactored the tests to test them; they're otherwise still the same.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6501#issuecomment-506923400:104,refactor,refactored,104,https://hail.is,https://github.com/hail-is/hail/pull/6501#issuecomment-506923400,1,['refactor'],['refactored']
Modifiability,"@patrick-schultz I'm sending this back to you because I made some pretty drastic changes trying to fix some errors. The biggest non-refactoring change that the original this introduces is that we can't parse IR for a persisted block matrix reader if the persisted block matrix doesn't exist. (This makes some amount of sense if you consider that we also can't parse the IR for a native block matrix reader if the file doesn't exist.). This led me down a rabbit hole of test failures since we're parsing IR/types a fair number of times, through the execution and after we get the result. After fiddling with it for a little bit, I removed UnpersistBlockMatrix. I'm not sure what I was thinking when I added it. I re-added an ""unpersist"" function to the backend to handle unpersisting BlockMatrices. It differs from the current Table/MatrixTable unpersist functions in that we only pass the id of the thing we want to unpersist, not the entire IR, since that's all we need.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9421#issuecomment-691300692:132,refactor,refactoring,132,https://hail.is,https://github.com/hail-is/hail/pull/9421#issuecomment-691300692,1,['refactor'],['refactoring']
Modifiability,"@tpoterba . This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the construct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:126,extend,extends,126,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437,1,['extend'],['extends']
Modifiability,"@tpoterba @cseed. I ran into some issues building images that I addressed as a part of this PR. The main issue was that we needed the `beta` `gcloud` commands, so I added a line to the Dockerfile to load those. I also made a couple changes to the Docker build commands to ensure we at least reuse all images that are available in our GCR. I added `hail-pr-builder` as a cache source in an attempt to take advantage of successful local builds. This, unfortunately, does not allow us to reuse successful layers from a failing build. I noticed that if I build and fail once using `--cache-from`, then I cannot re-use the succeeding layers of the failed build, regardless of whether I supplied `--cache-from`. I also added `:latest` to the `docker images` command because I have a couple tagged images from earlier iterations of this script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4362#issuecomment-424066734:502,layers,layers,502,https://hail.is,https://github.com/hail-is/hail/pull/4362#issuecomment-424066734,2,['layers'],['layers']
Modifiability,"@tpoterba @konradjk A workaround for this issue, should you encounter it again, is to disable the conscrypt library with this dataproc config:. `dataproc:dataproc.conscrypt.provider.enable: 'false'`. Capturing a core file provided a little more detail, but google support cannot explain why it happens. Cheers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053#issuecomment-444135177:135,config,config,135,https://hail.is,https://github.com/hail-is/hail/issues/3053#issuecomment-444135177,1,['config'],['config']
Modifiability,"@tpoterba @patrick-schultz Ok, I switched to elif style and made fixes so that the surrounding code also followed the elif style. I'd like to merge this and push off any further discussions to another PR. IMO, the no-else-return style is almost always nicer when I want to:; - bind a variable half way through an if chain, or; - have a complex condition (see the one with a while loop around 3400) that demands a nested if (I have to duplicate the common fall-through case)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5913#issuecomment-486809766:284,variab,variable,284,https://hail.is,https://github.com/hail-is/hail/pull/5913#issuecomment-486809766,1,['variab'],['variable']
Modifiability,"@tpoterba I added [another PR](https://github.com/hail-is/hail/pull/1613) which adds a `./configure` script which walks the user through setting a spark version (in the future we can add other parameters too). Perhaps that's the best way to manage this going forward?. If the gradle.properties file doesn't exist, our gradle script errors and asks the user to run `./configure`. The `./configure` script queries the user for sparkVersion and generates a valid `gradle.properties` file. Afterwards, the user can execute gradle normally without any `-D` parameters. Users may still override the `sparkVersion` variable on the command line by specifying `-PsparkVersion=2.1.1`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1608#issuecomment-290198582:90,config,configure,90,https://hail.is,https://github.com/hail-is/hail/pull/1608#issuecomment-290198582,4,"['config', 'variab']","['configure', 'variable']"
Modifiability,@tpoterba I just realized I forgot to propagate the FUSE config through to worker jobs. Should I be and I got lucky that the singular test is just doing everything driver-side? Or is there a test we can write to ensure that worker jobs access the FASTA data?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736#issuecomment-1478404495:57,config,config,57,https://hail.is,https://github.com/hail-is/hail/pull/12736#issuecomment-1478404495,1,['config'],['config']
Modifiability,@tpoterba I thought the conversions would lift `Int`s to `Double`s before unifying the type variables. What are struct attributes? I am a unsure that our conversions would work for struct field types.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1630#issuecomment-291173170:92,variab,variables,92,https://hail.is,https://github.com/hail-is/hail/pull/1630#issuecomment-291173170,1,['variab'],['variables']
Modifiability,"@tpoterba I was expecting we'd change the function registry to require the same type variable for left and right, i.e.:. ```scala; register(""=="", (a: Any, b: Any) => a == b, null)(TTHr, TTHr, boolHr); register(""!="", (a: Any, b: Any) => a != b, null)(TTHr, TTHr, boolHr); ```. Did that not work correctly?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1630#issuecomment-291159078:85,variab,variable,85,https://hail.is,https://github.com/hail-is/hail/pull/1630#issuecomment-291159078,1,['variab'],['variable']
Modifiability,"@tpoterba Is this any progress on this issue? We have a bunch of VCFs generated by SV programs ( Delly, GenomeStrip, Manta and Lumpy) that have imprecise SV variants that use these type of field formats. . VCF 4.2 spec; ```; 1.2.5 Alternative allele field format; Symbolic alternate alleles for imprecise structural variants:; ##ALT=<ID=type,Description=description>; The ID field indicates the type of structural variant, and can be a colon-separated list of types and subtypes. ID; values are case sensitive strings and may not contain whitespace or angle brackets. The first level type must be one; of the following:;  DEL Deletion relative to the reference;  INS Insertion of novel sequence relative to the reference;  DUP Region of elevated copy number relative to the reference; 2;  INV Inversion of reference sequence;  CNV Copy number variable region (may be both deletion and duplication); The CNV category should not be used when a more specific category can be applied. Reserved subtypes include:;  DUP:TANDEM Tandem duplication;  DEL:ME Deletion of mobile element relative to the reference;  INS:ME Insertion of a mobile element relative to the reference ; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413#issuecomment-386162935:848,variab,variable,848,https://hail.is,https://github.com/hail-is/hail/issues/3413#issuecomment-386162935,1,['variab'],['variable']
Modifiability,"@tpoterba OK, this is ready for final review. Flags are now duplicated in Python so that service backend can perform all actions without starting a JVM. I have a test that verifies the flag sets, their envvars, and the default values, are all the same. I preserved the randomness behavior. We can address that in a separate PR. The flags now use the Hail `configuration_of` machinery which checks, in order:; - an explicit value (not relevant to flags); - a deprecated environment variable (these are the current flag envvars); - an environment variable with a mechanically derived name (e.g. `HAIL_QUERY_NO_WHOLE_STAGE_CODEGEN`); - the hail configuration file (usually: ""~/.config/hail/config.ini"") under the section ""query"". FWIW, hail configuration files look like this:. ```; (base) dking@wm28c-761 hail % cat ~/.config/hail/config.ini ; [query]; backend = spark; jar_url = gs://hail-query/jars/dking/0wfcw2e6sma9/f4fb19e3d387d6efc6cf0f19b95bec59c95b793a.jar. [batch]; remote_tmpdir = gs://1-day/dktmp/; billing_project = hail. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12423#issuecomment-1411200434:481,variab,variable,481,https://hail.is,https://github.com/hail-is/hail/pull/12423#issuecomment-1411200434,8,"['config', 'variab']","['config', 'configuration', 'variable']"
Modifiability,"@tpoterba fixed the config issue and changed n_partitions to ensure workers are scheduled for the FASTA reading. I tested this on a single batch worker so the jobs overlapped and flexed the shared mount code, but we don't really have a guarantee in our test setup because batch has no way to force collocation of jobs (and even so we can't exactly force that the runtimes will overlap). I suppose if there's an issue here it will bubble up as a nondeterministic failure. Not great but perhaps good enough for now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736#issuecomment-1499151688:20,config,config,20,https://hail.is,https://github.com/hail-is/hail/pull/12736#issuecomment-1499151688,1,['config'],['config']
Modifiability,"A test failed because `hailctl config unset` now returns an error if the config variable does not exist. Let me know if you think we should maintain the current behavior -- otherwise, I slightly modified the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13224#issuecomment-1677602623:31,config,config,31,https://hail.is,https://github.com/hail-is/hail/pull/13224#issuecomment-1677602623,3,"['config', 'variab']","['config', 'variable']"
Modifiability,"According to the makefile documentation https://www.gnu.org/software/make/manual/html_node/Splitting-Recipe-Lines.html:; ```; Notice how the backslash/newline pair was removed inside the string quoted with double quotes (""""), ; but not from the string quoted with single quotes (''). This is the way the default shell (/bin/sh) ; handles backslash/newline pairs. If you specify a different shell in your makefiles it may treat them differently.; ```. Seems you (or `brew`) may have configured `make` to use something other than `/bin/sh`.; Quick way to verify:. ```Makefile; .PHONY: print-shell; print-shell:; 	@echo $(SHELL); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1890075303:484,config,configured,484,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1890075303,1,['config'],['configured']
Modifiability,"Actually I just went ahead and looked at this PR to make sure highcpus were indeed highcpus. They are!; <img width=""1476"" alt=""Screen Shot 2021-12-09 at 11 56 56 AM"" src=""https://user-images.githubusercontent.com/106194/145441305-fec38573-9c66-4a95-9fb7-0e6dc3a7c2e9.png"">. I also grabbed all the VM details and all the things that should be different (vm name, Nic name, etc.) are different. The userData is myseriously null, but its null for every VM in Azure currently (other PRs, namespaces, and default). ```; {; ""additionalCapabilities"": null,; ""applicationProfile"": null,; ""availabilitySet"": null,; ""billingProfile"": {; ""maxPrice"": -1.0; },; ""capacityReservation"": null,; ""diagnosticsProfile"": null,; ""evictionPolicy"": ""Delete"",; ""extendedLocation"": null,; ""extensionsTimeBudget"": null,; ""hardwareProfile"": {; ""vmSize"": ""Standard_F16s_v2"",; ""vmSizeProperties"": null; },; ""host"": null,; ""hostGroup"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/virtualMachines/batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5"",; ""identity"": {; ""principalId"": null,; ""tenantId"": null,; ""type"": ""UserAssigned"",; ""userAssignedIdentities"": {; ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.ManagedIdentity/userAssignedIdentities/batch-worker"": {; ""clientId"": ""890af904-42f1-4136-810a-c52f4e132c6b"",; ""principalId"": ""b952a3bb-1091-4f11-803b-9d5199219a27""; }; }; },; ""instanceView"": null,; ""licenseType"": null,; ""location"": ""eastus"",; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5"",; ""networkProfile"": {; ""networkApiVersion"": null,; ""networkInterfaceConfigurations"": null,; ""networkInterfaces"": [; {; ""deleteOption"": ""Delete"",; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Network/networkInterfaces/batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5-nic"",; ""primary"": null,; ""resourceGroup"": ""dgoldste""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:738,extend,extendedLocation,738,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,1,['extend'],['extendedLocation']
Modifiability,"Actually, v95 might be configured differently, so we do need to do something else...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1832748615:23,config,configured,23,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1832748615,1,['config'],['configured']
Modifiability,"Added a region argument, and mandated that users have either configured a region or are using `--region`. I believe older versions of gcloud assume the region is the default region for your project if you don't have anything specified, which causes users to have random errors when they update gcloud that they ask about on Zulip. This way, everyone gets a good error message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8281#issuecomment-597307671:61,config,configured,61,https://hail.is,https://github.com/hail-is/hail/pull/8281#issuecomment-597307671,1,['config'],['configured']
Modifiability,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:759,rewrite,rewrite,759,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516,4,['rewrite'],['rewrite']
Modifiability,Addressed comments. ; - Refactored to a separate module and added module-level tests. ; - Cleaned up TypeChecker interface to call recursively down,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1727#issuecomment-299293790:24,Refactor,Refactored,24,https://hail.is,https://github.com/hail-is/hail/pull/1727#issuecomment-299293790,1,['Refactor'],['Refactored']
Modifiability,"After some reading, I am still not sure what exactly the difference is between dummy coding and one-hot encoding. Suppose there is a categorical variable with $n$ categories. The [referenced Stack Exchange question](https://stats.stackexchange.com/questions/224051/one-hot-vs-dummy-encoding-in-scikit-learn) suggests that a one-hot encoding converts the categorical variable to $n$ indicator variables (one for each category) and that a dummy coding converts the categorical variable to $n-1$ indicator variables. With these definitions, the dummy coding is the one-hot encoding without one of the indicator variables. However, from the prototype implementation in this issue, the [scikit-learn one-hot encoder documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), and the [dummy variable Wikipedia article](https://en.m.wikipedia.org/wiki/Dummy_variable_(statistics)), I get the impression that dummy coding and one-hot encoding are synonyms and that there is no real distinction. Anyway, I would like to work on this issue. I will base my implementation on the prototype, and perhaps we can add a parameter to drop one of the indicator variables similar to what the [scikit-learn one-hot encoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) has.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13601#issuecomment-1932779413:145,variab,variable,145,https://hail.is,https://github.com/hail-is/hail/issues/13601#issuecomment-1932779413,8,['variab'],"['variable', 'variables']"
Modifiability,"After talking to Cotton, I'm going to refactor the client code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6245#issuecomment-498401222:38,refactor,refactor,38,https://hail.is,https://github.com/hail-is/hail/pull/6245#issuecomment-498401222,1,['refactor'],['refactor']
Modifiability,"After the first commit, lots of unrelated type errors popped up from mypy. I think this is because I have the new mypy installed and it's actually catching more errors that were there all along. It might help to see the errors that it gave me (also visible in old CI builds of the PR:. ```; ci/github.py:508: error: Incompatible types in assignment (expression has type ""bool"", variable has type ""Optional[str]""); ci/github.py:551: error: Incompatible types in assignment (expression has type ""bool"", variable has type ""Optional[str]""); ci/github.py:554: error: Incompatible types in assignment (expression has type ""bool"", variable has type ""Optional[str]""); ci/github.py:574: error: Unsupported operand types for > (""int"" and ""None""); ci/github.py:574: note: Left operand is of type ""Optional[int]""; ci/github.py:575: error: Unsupported operand types for + (""None"" and ""int""); ci/github.py:575: note: Left operand is of type ""Optional[int]""; ci/github.py:817: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; ci/github.py:828: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; ci/github.py:840: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; ci/github.py:842: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; ci/github.py:849: error: Item ""MergeFailureBatch"" of ""Union[Batch, Any, MergeFailureBatch]"" has no attribute ""id""; ci/github.py:849: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; Found 11 errors in 1 file (checked 19 source files); ```. It might be helpful to look at the first commit and last commit in isolation. Or if you'd like I can make a separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11530#issuecomment-1062214775:378,variab,variable,378,https://hail.is,https://github.com/hail-is/hail/pull/11530#issuecomment-1062214775,3,['variab'],['variable']
Modifiability,"Agreed on plan with @astheeggeggs to use this branch for his immediate needs, while pulling in pieces of this code in new PR once 0.1 and refactoring of RegressionUtils is done.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1709#issuecomment-298405321:138,refactor,refactoring,138,https://hail.is,https://github.com/hail-is/hail/pull/1709#issuecomment-298405321,1,['refactor'],['refactoring']
Modifiability,"Agreed that `hailctl init` is maybe the wrong name b/c `hailctl` also controls Dataproc. `hailctl batch init` seems right to me. I agree that `hailctl config init` seems wrong if we're creating buckets. On the issue of login, I think we can appease both points of view. If you're not logged in, have `hailctl batch init` do this:; ```; You are not currently logged in to Hail. Redirecting you to a login screen. ... user does login flow ... In the future, you can use hailctl auth login to login to Hail. ... continue with hailctl batch init ...; ```. I think we should punt on addressing domain. Broad users don't need to interact with it at all. What are the other concerns about how this is developing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1662837421:151,config,config,151,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1662837421,1,['config'],['config']
Modifiability,"Ah there is some upcast that happens before we get to case class TableUnion:. ```scala; case class TableUnion(children: IndexedSeq[TableIR]) extends TableIR {; assert(children.forall(c => c.typ.rowType == children.head.typ.rowType)); println(""ALL SAME""); ```. If this is added, ""ALL SAME"" gets printed 16 times after ""RESULT"". Not sure why that isn't 8 (maybe due to a lowering pass causing a copy?) or 24, but that's less important. edit: Nope, these tables must be interpreted as being of both non-missing types in the ""maybeNull"" column by the import function, or I'm specifying the type wrong:. ```python; def union(...):; left_key = self.key.dtype; for i, ht, in enumerate(tables):; if left_key != ht.key.dtype:; raise ValueError(...); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8008#issuecomment-580777705:141,extend,extends,141,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-580777705,1,['extend'],['extends']
Modifiability,"Ah yeah good point I forgot about that. You have to construct a string to avoid truncation a la:; ```; (base) dking@wm28c-761 /tmp % cat foo.py; def test():; assert False, 'b' * 1000; =========================================== test session starts ============================================; (base) dking@wm28c-761 /tmp % pytest foo.py; platform darwin -- Python 3.10.9, pytest-7.4.3, pluggy-1.3.0; rootdir: /private/tmp; configfile: pytest.ini; plugins: xdist-2.5.0, timeout-2.2.0, instafail-0.5.0, devtools-0.12.2, asyncio-0.21.1, timestamper-0.0.9, metadata-3.0.0, html-1.22.1, anyio-4.2.0, forked-1.6.0, accept-0.1.9, image-diff-0.0.11; asyncio: mode=strict; collected 1 item . foo.py F [100%]. ================================================= FAILURES =================================================; ___________________________________________________ test ___________________________________________________. def test():; > assert False, 'b' * 1000; E AssertionError: bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb; E assert False. foo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14151#issuecomment-1889800019:424,config,configfile,424,https://hail.is,https://github.com/hail-is/hail/pull/14151#issuecomment-1889800019,2,"['config', 'plugin']","['configfile', 'plugins']"
Modifiability,"Ah, I deleted the global-config as a part of switching to gcp-broad. Regardless, we'll need to address that as terraform is iteratively improved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12964#issuecomment-1531832089:25,config,config,25,https://hail.is,https://github.com/hail-is/hail/pull/12964#issuecomment-1531832089,1,['config'],['config']
Modifiability,"Ah, I see. If those paths point to the same location then it shouldn't make any difference. This error almost certainly means that `pyspark` cannot find your hail jar. I suspect that Spark 2.2.x has dropped support for the `SPARK_CLASSPATH` environment variable. Can you try starting `pyspark` with these options:; ```; pyspark \; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --conf=spark.driver.extraClassPath=$HAIL_HOME/build/libs/hail-all-spark.jar \; --conf=spark.executor.extraClassPath=./hail-all-spark.jar. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337639898:253,variab,variable,253,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337639898,1,['variab'],['variable']
Modifiability,"Ah, I'm actually wrong, it was defined in the Makefile itself originally. We do have a `TOKEN` variable defined in `config.mk` but `config.mk` is only used by services Makefiles. I restored the definition of TOKEN into `upload_qob_jar.sh`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12825#issuecomment-1489163821:95,variab,variable,95,https://hail.is,https://github.com/hail-is/hail/pull/12825#issuecomment-1489163821,3,"['config', 'variab']","['config', 'variable']"
Modifiability,"Ah, OK, this is actually quite sensible. I have to tell gradle that this is a test jar, I do that by saying I want my class path to look like the test class path at runtime. A wrinkle is that I have to explicitly request our own code too (that's the first diff line). I'm actually quite pleased that our grade file has become a bit more standard and less custom.; ```diff; task shadowTestJar(type: ShadowJar) {; archiveClassifier = 'spark-test'; + from sourceSets.test.output; + configurations = [project.configurations.testRuntimeClasspath]; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1710347695:479,config,configurations,479,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1710347695,2,['config'],['configurations']
Modifiability,"Ah, thanks! This is just the gear => config rename now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9396#issuecomment-685904525:37,config,config,37,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685904525,1,['config'],['config']
Modifiability,"Ah, yeah, I can't use it as a feature yet: the running CI isn't creating global-config. Fixing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9777#issuecomment-738459457:80,config,config,80,https://hail.is,https://github.com/hail-is/hail/pull/9777#issuecomment-738459457,1,['config'],['config']
Modifiability,"Ah, yes that's right, I'll add that to the config.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10235#issuecomment-812185045:43,config,config,43,https://hail.is,https://github.com/hail-is/hail/pull/10235#issuecomment-812185045,1,['config'],['config']
Modifiability,"Ah, yes, ex nihilio, should've taken a latin class. I added these to uncurated:; - [security] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the tru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:333,rewrite,rewrite,333,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,1,['rewrite'],['rewrite']
Modifiability,"Ah, you must have a file at `~/.hail/.zuliprc`. This should really live inside hail-is, right? I've made it a SOPS encoded file, placed it in hail-is, and imported the secret. This is what it was before:; ```; resource ""kubernetes_secret"" ""zulip_config"" {; count = fileexists(""~/.hail/.zuliprc"") ? 1 : 0; metadata {; name = ""zulip-config""; }. data = {; "".zuliprc"" = fileexists(""~/.hail/.zuliprc"") ? file(""~/.hail/.zuliprc"") : """"; }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12991#issuecomment-1564546844:331,config,config,331,https://hail.is,https://github.com/hail-is/hail/pull/12991#issuecomment-1564546844,1,['config'],['config']
Modifiability,"All of these things I completely agree with!. 1. I was lazy and used the Jinja template engine to parse and find the variable declarations. I need to write a custom parser, but wanted to figure out exactly what we're going to support. Which makes me worried that I don't want to implement an expr language or it should be minimal. 2. Tim suggested something similar: `%%IN bfile%%` and `%%OUT ofile%%. Requires the custom parser. See comment 1 above. 3. I was also concerned about the formatting of arrays. I tried using lambdas for comment 4 and it got complicated. I like your proposal but want to think about it more. 4. PLINK, etc. output lots of files and you specify the file root and then it outputs a bunch of files with different extensions. We must be able to support this and make it easy for users. I agree with your suggestion. I'll try that in the example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-446742931:117,variab,variable,117,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-446742931,1,['variab'],['variable']
Modifiability,"All that messy state twiddling is because Scala's `Iterator` is the wrong model for most things we use it for, which is why I made `FlipbookIterator`. Using that, what you have would become; ```scala; private class BgenRecordStateMachine(; ctx: RVDContext,; p: BgenPartition,; settings: BgenSettings; ) extends StateMachine[RegionValue] {; private[this] val bfis = p.makeInputStream; private[this] val rv = RegionValue(ctx.region); private[this] val rvb = ctx.rvb; ; def isValid: Boolean = p.isValid; def value: RegionValue = rv; def advance() { p.advance(); findNextVariant() }; private def findNextVariant() {; // same as existing advance(), but without advancing p; }. findNextVariant() // make sure iterator is initialized in first valid state; }; ```; giving `BgenPartition` a `FlipbookIterator` style interface, with `isValid`, `value`, and `advance()` instead of `hasNext()` and `next()`. Then to create a new iterator `FlipbookIterator(new BgenRecordStateMachine(...))`. But honestly, what you had was clear enough, so if you benchmarked and the allocation isn't an issue, you should do whatever you find most readable. I've been conditioned to avoid `Option` in low-level code, but I don't have a good intuition for when it is or isn't actually a problem.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3893#issuecomment-404156507:303,extend,extends,303,https://hail.is,https://github.com/hail-is/hail/pull/3893#issuecomment-404156507,1,['extend'],['extends']
Modifiability,"All the args for each genotype and struct are converted to strings with repr and stored. For certain things like genotype there's no reason to have it extend history_mixin, since we can produce a sensible `repr` without recording the args.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2209#issuecomment-328143714:151,extend,extend,151,https://hail.is,https://github.com/hail-is/hail/pull/2209#issuecomment-328143714,1,['extend'],['extend']
Modifiability,Also @cseed @jigold your thoughts on this small refactor appreciated if you have time.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3783#issuecomment-398382857:48,refactor,refactor,48,https://hail.is,https://github.com/hail-is/hail/pull/3783#issuecomment-398382857,1,['refactor'],['refactor']
Modifiability,Also `WatchedBranch.config`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6142#issuecomment-494491481:20,config,config,20,https://hail.is,https://github.com/hail-is/hail/issues/6142#issuecomment-494491481,1,['config'],['config']
Modifiability,Also could someone please make sure this works on macOS and there aren't any dumb linux/glibc portability issues. I tried to keep it pretty POSIX-y.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6927#issuecomment-524446990:94,portab,portability,94,https://hail.is,https://github.com/hail-is/hail/pull/6927#issuecomment-524446990,1,['portab'],['portability']
Modifiability,"Also, I created `gs://hail-common/vep/vep/GRCh37`, `gs://hail-common/vep/vep/GRCh38`; directories with VEP configs and loftee data files, so you can now run ; ```; gcloud dataproc clusters create $CLUSTER; ...; --initialization-actions gs://hail-common/hail-init.sh,gs://hail-common/vep/vep/GRCh37/vep85-GRCh37-init.sh. or . --initialization-actions gs://hail-common/hail-init.sh,gs://hail-common/vep/vep/GRCh38/vep85-GRCh38-init.sh; ```; along with ; ```; gs://hail-common/vep/vep/GRCh37/vep85-GRCh37-gcloud.properties. or . gs://hail-common/vep/vep/GRCh38/vep85-GRCh38-gcloud.properties; ```. though the init.sh script ties the cluster to a particular genome build. . Also, it would be nice if hail could throw an error if trying to annotate a GRCh37 callset with GRCh38 VEP, etc. Would it make sense to put this check in the VEP command?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1779#issuecomment-299721946:107,config,configs,107,https://hail.is,https://github.com/hail-is/hail/pull/1779#issuecomment-299721946,1,['config'],['configs']
Modifiability,"Also, wrt the `hail` alias, that only sets the environment variable for that single execution of `python`. You will need to run:; ```bash; export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python; ```; before running `./gradlew test`, otherwise it's very likely that you will see a variety of errors related to Spark. I am surprised that you saw an error about Breeze natives. An inappropriate `$PYTHON_PATH` should trigger a failure much earlier than the section of code that uses of Breeze natives.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-281862423:59,variab,variable,59,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281862423,1,['variab'],['variable']
Modifiability,"Also, yes, the `registerNumeric` would require some sort of polymorphic implementation, which this is not.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1196#issuecomment-267986559:60,polymorphi,polymorphic,60,https://hail.is,https://github.com/hail-is/hail/pull/1196#issuecomment-267986559,1,['polymorphi'],['polymorphic']
Modifiability,And a typical interaction for a current 2.0.2 user:. ```bash; dking@wmb16-359 # gradle compileScala . FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.413 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); ; using default version: 2.0.2; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total time: 4.418 secs,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613#issuecomment-290201637:353,config,configure,353,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201637,3,"['Config', 'config']","['Configuring', 'configure']"
Modifiability,"And this is on a laptop with an SSD, right? it'll be even worse on the cloud, I think. Should we parameterize this behavior?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6291#issuecomment-500455081:97,parameteriz,parameterize,97,https://hail.is,https://github.com/hail-is/hail/pull/6291#issuecomment-500455081,1,['parameteriz'],['parameterize']
Modifiability,Another thing to explore is how much the block matrix write is spending in compression. That might not be helping out overall. It might be worth modifying BlockMatrix to make the compression optional (and add some features from Matrix/Table will make the file format a bit more flexible.),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3335#issuecomment-385312301:278,flexible,flexible,278,https://hail.is,https://github.com/hail-is/hail/pull/3335#issuecomment-385312301,1,['flexible'],['flexible']
Modifiability,"Arcturus -- I'm assigning this to you, but please don't take off the WIP tag as merging this will cause Batch to shutdown (database migration). If we're not prepared for it, then it could cause an extended outage.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9441#issuecomment-691229759:197,extend,extended,197,https://hail.is,https://github.com/hail-is/hail/pull/9441#issuecomment-691229759,1,['extend'],['extended']
Modifiability,Are you running this locally?; ```; (cd hail/python && python3 -m mypy --config-file ../../setup.cfg hailtop); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11734#issuecomment-1090451328:73,config,config-file,73,https://hail.is,https://github.com/hail-is/hail/pull/11734#issuecomment-1090451328,1,['config'],['config-file']
Modifiability,"As an aside, we should definitely have a `HAIL_BATCH_BACKEND` and associated config variables. There is no end to my annoyance that `hb.Batch()` gives me a local backend batch. It seems to me that, given the precedent of `hailctl dataproc submit`, `hailctl batch submit` conveys the intent to use QoB or Batch-in-Batch, not ""local mode Batch"" or ""local mode Hail"". It seems very reasonable to have a `--local-mode-query` override (I think we should ignore local mode Batch as much as possible since container-in-container is fraught). We need a better name for local mode Spark or Query. I'm slowly realizing that lots of people don't realize you can use Hail on a laptop. Are there other tools that have already settled on terminology here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12471#issuecomment-1324143394:77,config,config,77,https://hail.is,https://github.com/hail-is/hail/pull/12471#issuecomment-1324143394,2,"['config', 'variab']","['config', 'variables']"
Modifiability,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:49,config,config,49,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336,1,['config'],['config']
Modifiability,"Assigned John, I think @cseed is busy. John, this PR has 2 features:; 1) Remove catch-all server block, for service-specific blocks, in better keeping with router.nginx.conf. 2) Allow prefix matches only on the exact, slash-less url. Meaning /prometheusss$haxor doesn't work, but /prometheus does. This is most easily accomplished with an exact match location block, because by Nginx semantics, regex-containing locations cannot be elided with a root proxy_pass (one with a trailing /), because nginx wants a static prefix to remove, and regex prevents that. So to accomplish this with regex would require more LOC, namely a URL rewrite rule inside the location block.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-554498117:629,rewrite,rewrite,629,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-554498117,1,['rewrite'],['rewrite']
Modifiability,"B .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used in that package. This can be used with Kubernetes. `kubectl create secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:1987,variab,variables,1987,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935,1,['variab'],['variables']
Modifiability,"BLAS is actually installed in /usr/lib64/atlas. Spark was not finding the lib for some reason. ; ; The solution was to add the following config to the spark-submit command line. --conf spark.executor.extraClassPath=""/usr/lib64/atlas"" . It would be useful to add this to the documentation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7008#issuecomment-529740667:137,config,config,137,https://hail.is,https://github.com/hail-is/hail/issues/7008#issuecomment-529740667,1,['config'],['config']
Modifiability,"Basically done, I just have to go through and rewrite the LAPACK calls to use the new interface (so far I've only done so with NDArrayInverse). If you have any thoughts on the actual reference counting @tpoterba, that can be reviewed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10001#issuecomment-775541037:46,rewrite,rewrite,46,https://hail.is,https://github.com/hail-is/hail/pull/10001#issuecomment-775541037,1,['rewrite'],['rewrite']
Modifiability,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:78,config,configuration,78,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683,2,['config'],['configuration']
Modifiability,"Before change:. ```; {""config"": {""cores"": 1, ""version"": ""0.2.57-e20c00f05c78"", ""timestamp"": ""2020-09-24 08:27:24.863298"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""hwe_normalized_pca_blanczos_small_data_10_iterations"", ""failed"": false, ""timed_out"": false, ""times"": [54.736854666000006, 46.213391341000005, 52.75462794499998]}]}; ```. After change: . ```; {""config"": {""cores"": 1, ""version"": ""0.2.57-c013f70fe868"", ""timestamp"": ""2020-09-24 08:32:23.991129"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""hwe_normalized_pca_blanczos_small_data_10_iterations"", ""failed"": false, ""timed_out"": false, ""times"": [28.998368115000005, 40.65512770199999, 28.816323178000005]}]}; ```. Obvious improvement, nearly 2x.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9498#issuecomment-698315046:23,config,config,23,https://hail.is,https://github.com/hail-is/hail/pull/9498#issuecomment-698315046,2,['config'],['config']
Modifiability,"Before we merge this, I'd like to have the new Artifact Registry in hail-vdc setup and have configured a cloud run job for the cleanup script. I think a daily run is good enough. https://github.com/GoogleCloudPlatform/gcr-cleaner/blob/main/docs/deploy-cloud-run.md",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12211#issuecomment-1255117035:92,config,configured,92,https://hail.is,https://github.com/hail-is/hail/pull/12211#issuecomment-1255117035,1,['config'],['configured']
Modifiability,"Better place to post things like this would be discuss.hail.is (because it's probably a configuration issue with your cluster and not a bug in hail). I'd guess you don't have BLAS installed on your cluster. If you check the hail.log file, do you have lines like. ```; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS; ```. ?. See here: https://hail.is/docs/0.2/getting_started.html#common-installation-issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7008#issuecomment-529008947:88,config,configuration,88,https://hail.is,https://github.com/hail-is/hail/issues/7008#issuecomment-529008947,1,['config'],['configuration']
Modifiability,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682:833,extend,extends,833,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682,1,['extend'],['extends']
Modifiability,But let's solve for Aus and Azure as a separate thread of work that doesn't block with PR. I imagine whatever we decide will either make this command need to take a domain or enhance this command to create a profile per-domain.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1663144322:175,enhance,enhance,175,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1663144322,1,['enhance'],['enhance']
Modifiability,"But only when ExtendedOrdering is used, which I think is pretty rare, now, just in `Interpret` (so eval, top-level expressions) and aggregators.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4871#issuecomment-443787719:14,Extend,ExtendedOrdering,14,https://hail.is,https://github.com/hail-is/hail/pull/4871#issuecomment-443787719,1,['Extend'],['ExtendedOrdering']
Modifiability,"Buy it, use it, break it, fix it; Trash it, change it, mail - upgrade it; Charge it, point it, zoom it, press it; Snap it, work it, quick - erase it; Write it, cut it, paste it, save it; Load it, check it, quick - rewrite it; Plug it, play it, burn it, rip it; Drag and drop it, zip - unzip it; Lock it, fill it, call it, find it; View it, code it, jam - unlock it; Surf it, scroll it, pause it, click it; Cross it, crack it, switch - update it; Name it, read it, tune it, print it; Scan it, send it, fax - rename it; Touch it, bring it, pay it, watch it; Turn it, leave it, start - format it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2036#issuecomment-318518817:214,rewrite,rewrite,214,https://hail.is,https://github.com/hail-is/hail/pull/2036#issuecomment-318518817,1,['rewrite'],['rewrite']
Modifiability,"Can confirm that at least locally, the environment variable caused a table to be written with packed integers, and the tests passed with the flag on (I assume).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7821#issuecomment-595913767:51,variab,variable,51,https://hail.is,https://github.com/hail-is/hail/pull/7821#issuecomment-595913767,1,['variab'],['variable']
Modifiability,"Can we make it a job and/or batch level configuration? The user obviously can do whatever they like with their tokens. However, since most users don't need them, I prefer our default to be to not expose them. I think we just need a new method and attribute on `Job` and `Batch` that mirrors, say, `image` or `memory`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-767739020:40,config,configuration,40,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-767739020,1,['config'],['configuration']
Modifiability,"Can you double check we don't need to explicitly tell nginx to use more than 1 core? I'm looking here:. https://www.nginx.com/blog/thread-pools-boost-performance-9x/#Configuring-Thread-Pools; https://www.nginx.com/blog/thread-pools-boost-performance-9x/#Benchmarking. Otherwise, I think this change is fine, although we do already have a minimum of two copies of internal-gateway at any time. Is this change better than increasing the number of copies of internal-gateway? I assume that the response time to increases in load will be faster with your proposed change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11876#issuecomment-1145004769:166,Config,Configuring-Thread-Pools,166,https://hail.is,https://github.com/hail-is/hail/pull/11876#issuecomment-1145004769,1,['Config'],['Configuring-Thread-Pools']
Modifiability,Can you respond to my concerns in the initial commit message? Specifically some math questions when computing resources and the AsyncWorkerPool usage and the temp variable in the SQL code.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9832#issuecomment-758019254:163,variab,variable,163,https://hail.is,https://github.com/hail-is/hail/pull/9832#issuecomment-758019254,1,['variab'],['variable']
Modifiability,Case class equals does a reference equality check as you expected. I switched to structural equality in the rewrite functions.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1778#issuecomment-302164339:108,rewrite,rewrite,108,https://hail.is,https://github.com/hail-is/hail/pull/1778#issuecomment-302164339,1,['rewrite'],['rewrite']
Modifiability,Changed to use the environment variable directly. I also updated the PR description with details on how to install mkl and make Hail load it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10770#issuecomment-897042440:31,variab,variable,31,https://hail.is,https://github.com/hail-is/hail/pull/10770#issuecomment-897042440,1,['variab'],['variable']
Modifiability,"Changes since last review:; - Method now takes expressions for call and (optionally) scores.; - Block matrix and table of scores annotated and collected from source.cols() sent to Python, processed using int indices, column names restored on python side (thanks @tpoterba); - Fixed bug that silently dropped `n_samples / block_size` proportion of pairs, Python test checks it; - Extended Python tests to compare k and scores paths, test counts, min_kinship, maf, block_size; - Tuned tolerances on comparison with R from Python; - Extended to general column key, removing unique key check, noted in docs; - MEMORY_AND_DISK caching as default (thanks @konradjk) on Scala side; - The diagonal fix meant phi is computed with parallelism up to the number of diagonal blocks, rather than parallelism 1. But that's still likely a bottleneck as phi requires computing and point-wise dividing two big gram matrices. I now write phi to disk and read it back in, which squares the parallelism up to the number of blocks in phi. I think this should also improve the stability of the many downstream calculations derived from phi, esp. if pre-emptibles are used. No longer cacheing phi, but I left caching on the other matrices. @konradjk let us know how this version compares next time you run it.; - Noted in FIXME room for further improvement when fusing blocks: `replace join with zipPartitions, throw away lower triangular blocks sooner, avoid the nulls`; - Updated docs accordingly; - Deleted a bunch of code in PCRelate and PCRelateSuite",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3211#issuecomment-376725104:379,Extend,Extended,379,https://hail.is,https://github.com/hail-is/hail/pull/3211#issuecomment-376725104,2,['Extend'],['Extended']
Modifiability,Citation for log4j1 programmatic configuration breaking log4j2: https://logging.apache.org/log4j/2.x/manual/migration.html#limitations-of-the-log4j-1-x-bridge,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941#issuecomment-1524328047:33,config,configuration,33,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1524328047,1,['config'],['configuration']
Modifiability,Closed because I want to rewrite it to take an environment VDS,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/929#issuecomment-253989005:25,rewrite,rewrite,25,https://hail.is,https://github.com/hail-is/hail/pull/929#issuecomment-253989005,1,['rewrite'],['rewrite']
Modifiability,Closing as this will evolve in separate branch for a while yet.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1697#issuecomment-298819643:21,evolve,evolve,21,https://hail.is,https://github.com/hail-is/hail/pull/1697#issuecomment-298819643,1,['evolve'],['evolve']
Modifiability,Closing this for now. Didn't realize the bootstrap dependence of not having a letsencrypt config. I don't think we can entirely delete this just yet (but could trivially move it to envoy),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12096#issuecomment-1226092951:90,config,config,90,https://hail.is,https://github.com/hail-is/hail/pull/12096#issuecomment-1226092951,1,['config'],['config']
Modifiability,"Closing this for now. I'll redo it in a way I think will be acceptable (or not, it's really just a small hill I care about, but I would like to point out that `gcloud` stores its credential and config data in `.config/`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-540740392:194,config,config,194,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-540740392,2,['config'],['config']
Modifiability,Closing this in favor of refactoring `TableStage` to do the whole right thing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8599#issuecomment-618680964:25,refactor,refactoring,25,https://hail.is,https://github.com/hail-is/hail/pull/8599#issuecomment-618680964,1,['refactor'],['refactoring']
Modifiability,Closing to PR build config with enabled ci2.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5842#issuecomment-487309837:20,config,config,20,https://hail.is,https://github.com/hail-is/hail/pull/5842#issuecomment-487309837,1,['config'],['config']
Modifiability,"Closing to refactor, will open updated PR shortly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3397#issuecomment-386605198:11,refactor,refactor,11,https://hail.is,https://github.com/hail-is/hail/pull/3397#issuecomment-386605198,1,['refactor'],['refactor']
Modifiability,Closing until Eigen PR is ready again and this is refactored accordingly.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2177#issuecomment-326646865:50,refactor,refactored,50,https://hail.is,https://github.com/hail-is/hail/pull/2177#issuecomment-326646865,1,['refactor'],['refactored']
Modifiability,Closing until I have time to rewrite using recursively passed EvalContexts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1637#issuecomment-298932795:29,rewrite,rewrite,29,https://hail.is,https://github.com/hail-is/hail/pull/1637#issuecomment-298932795,1,['rewrite'],['rewrite']
Modifiability,Closing while I refactor.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1011#issuecomment-257051213:16,refactor,refactor,16,https://hail.is,https://github.com/hail-is/hail/pull/1011#issuecomment-257051213,1,['refactor'],['refactor']
Modifiability,Closing while this evolves.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2965#issuecomment-369418966:19,evolve,evolves,19,https://hail.is,https://github.com/hail-is/hail/pull/2965#issuecomment-369418966,1,['evolve'],['evolves']
Modifiability,"Closing, will incorporate into refactor of linreg on dev.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2007#issuecomment-318852681:31,refactor,refactor,31,https://hail.is,https://github.com/hail-is/hail/pull/2007#issuecomment-318852681,1,['refactor'],['refactor']
Modifiability,"Closing, will make new PR after refactoring against dev.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2037#issuecomment-318852593:32,refactor,refactoring,32,https://hail.is,https://github.com/hail-is/hail/pull/2037#issuecomment-318852593,1,['refactor'],['refactoring']
Modifiability,"Code looks good. Can you update the VariantDataset.vep function documentation? In particular, note that plugin overrides human_ancestor and conservation file. plugin in cleaner. We should remove the latter when we start version 0.2: https://github.com/hail-is/hail/issues/1728",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1712#issuecomment-297784358:104,plugin,plugin,104,https://hail.is,https://github.com/hail-is/hail/pull/1712#issuecomment-297784358,2,['plugin'],['plugin']
Modifiability,Code looks good. It looks like there's something wrong with the gradle cpp test configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4594#issuecomment-433123340:80,config,configuration,80,https://hail.is,https://github.com/hail-is/hail/pull/4594#issuecomment-433123340,1,['config'],['configuration']
Modifiability,"Conceptually, if we're not just interested in missingness, then unboxedGT is useful only in route to nNonRefAlleles (and equal to it if we've split). E.g., the simplest way to extend regression to multi-allelic is to use nNonRefAlleles...though thinking about this further, it's upsettingly asymmetric in the case where the ref allele is the minor allele, so perhaps we should just force deliberate choice of splitting, esp if we're moving toward implementing that less painfully under the hood. Or do regression per alternate allele while maintaining multi-allelic form. (edited). We currently compute nNonRefAlleles from unboxedGT through GTPair, which allocates per genotype. For example, PCA currently requires splitting, but uses nNonRefAlleles. And IBD currently requires splitting but allocates per genotype via:; ```; def countRefs(gtIdx: Int): Int = {; val gt = Genotype.gtPair(gtIdx); indicator(gt.j == 0) + indicator(gt.k == 0); ```. So it may make sense to add `unboxedNNonRefAlleles` that avoids allocation, but doesn't require splitting for these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468:176,extend,extend,176,https://hail.is,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468,1,['extend'],['extend']
Modifiability,Configuration can now be stored in the metadata top-level JSON (e.g. split).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/368#issuecomment-236621515:0,Config,Configuration,0,https://hail.is,https://github.com/hail-is/hail/issues/368#issuecomment-236621515,1,['Config'],['Configuration']
Modifiability,Confirmed that the hailctl bit works:; ```; (base) dking@wm28c-761 hail % hailctl config set http/timeout_in_seconds 1234s; Error: bad value '1234s' for parameter <ConfigVariable.HTTP_TIMEOUT_IN_SECONDS: 'http/timeout_in_seconds'> should be a float or an int like 42.42 ; or 42; (base) dking@wm28c-761 hail % hailctl config set http/timeout_in_seconds 42 ; (base) dking@wm28c-761 hail % hailctl config set http/timeout_in_seconds 42.0; (base) dking@wm28c-761 hail % hailctl config set http/timeout_in_seconds 60 ; (base) dking@wm28c-761 hail % cat ~/.config/hail/config.ini ; [query]; backend = spark; jar_url = gs://hail-query-ger0g/jars/dking/uk4prwgezgva/5fc88d5a4b614454004226f5c77ea72efee1e38f.jar. [batch]; remote_tmpdir = gs://1-day/; billing_project = hail; backend = service. [aiocloudflare]. [global]; domain = hail.is. [gcs_requester_pays]; project = broad-ctsa. [http]; timeout_in_seconds = 60. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14206#issuecomment-1915030577:82,config,config,82,https://hail.is,https://github.com/hail-is/hail/pull/14206#issuecomment-1915030577,7,"['Config', 'config']","['ConfigVariable', 'config']"
Modifiability,"Cotton, mostly looks great, I haven't taken a look at the deployment configs yet. I would like to do that, and if you're ok with this from a time standpoint, spin up a locally deployed version to play with. . Nice work!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-528373663:69,config,configs,69,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-528373663,1,['config'],['configs']
Modifiability,"Currently the` nvidia-container-toolkit` is installed both in the vm startup script and in the worker docker image. The toolkit must be installed in the startup script to be able to configure docker with the command `nvidia-ctk runtime configure --runtime=docker`. This command cannot be run from Dockerfile.worker because it gets the error `""unable to flush config: unable to open /etc/docker/daemon.json for writing: open /etc/docker/daemon.json: no such file or directory""`.; The toolkit also has to be installed in Dockerfile.worker since that is where crun is invoked from. To execute the nvidia hook, the toolkit needs to be installed in that container. We could probably find a workaround to this if you would like but it only increased the worker image from 1.47Gb to 1.55Gb so it seems pretty small.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13430#issuecomment-1710796481:182,config,configure,182,https://hail.is,https://github.com/hail-is/hail/pull/13430#issuecomment-1710796481,3,['config'],"['config', 'configure']"
Modifiability,"Dan's OOO this week. It's a Python lint failure:; ```; + cd /io/repo; + make check-hail; make -C hail/python check; make[1]: Entering directory '/io/repo/hail/python'; python3 -m flake8 --config ../../setup.cfg hail; python3 -m flake8 --config ../../setup.cfg hailtop; hailtop/aiogoogle/auth/credentials.py:43:1: W293 blank line contains whitespace; hailtop/aiogoogle/auth/credentials.py:47:5: E303 too many blank lines (2); hailtop/aiogoogle/auth/credentials.py:105:1: E302 expected 2 blank lines, found 1; make[1]: Leaving directory '/io/repo/hail/python'; make[1]: *** [Makefile:12: check] Error 1; make: *** [Makefile:13: check-hail] Error 2; Status; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10647#issuecomment-876439207:188,config,config,188,https://hail.is,https://github.com/hail-is/hail/pull/10647#issuecomment-876439207,2,['config'],['config']
Modifiability,"DatabaseStep(Step):; self._name = database_name; self.admin_username = f'{database_name}-admin'; self.user_username = f'{database_name}-user'; + elif params.scope == 'dev':; + dev_username = params.code.config()['user']; + self._name = f'{dev_username}-{database_name}'; + self.admin_username = f'{dev_username}-{database_name}-admin'; + self.user_username = f'{dev_username}-{database_name}-user'; else:; assert params.scope == 'test'; self._name = f'{params.code.short_str()}-{database_name}-{self.token}'; @@ -1030,7 +1032,7 @@ class CreateDatabaseStep(Step):; @staticmethod; def from_json(params: StepParameters):; json = params.json; - return CreateDatabaseStep(; + return CreateDatabase2Step(; params,; json['databaseName'],; json['namespace'],; @@ -1111,12 +1113,12 @@ EOF; attributes={'name': self.name},; secrets=[; {; - 'namespace': self.database_server_config_namespace,; + 'namespace': self.namespace,; 'name': 'database-server-config',; 'mount_path': '/sql-config',; }; ],; - service_account={'namespace': DEFAULT_NAMESPACE, 'name': 'ci-agent'},; + service_account={'namespace': self.namespace, 'name': 'admin'},; input_files=input_files,; parents=[self.create_passwords_job] if self.create_passwords_job else self.deps_parents(),; network='private',; @@ -1125,42 +1127,4 @@ EOF; ); ; def cleanup(self, batch, scope, parents):; - if scope in ['deploy', 'dev'] or self.cant_create_database:; - return; -; - cleanup_script = f'''; -set -ex; -; -commands=$(mktemp); -; -cat >$commands <<EOF; -DROP DATABASE IF EXISTS \\`{self._name}\\`;; -DROP USER IF EXISTS '{self.admin_username}';; -DROP USER IF EXISTS '{self.user_username}';; -EOF; -; -until mysql --defaults-extra-file=/sql-config/sql-config.cnf <$commands; -do; - echo 'failed, will sleep 2 and retry'; - sleep 2; -done; -; -'''; -; - self.cleanup_job = batch.create_job(; - CI_UTILS_IMAGE,; - command=['bash', '-c', cleanup_script],; - attributes={'name': f'cleanup_{self.name}'},; - secrets=[; - {; - 'namespace': self.database_ser",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600:2408,config,config,2408,https://hail.is,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600,2,['config'],['config']
Modifiability,Depends -- we can extend the HWE aggregator to incorporate sample phenotype information or we can leave it to the user to filter out male samples.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/200#issuecomment-279521097:18,extend,extend,18,https://hail.is,https://github.com/hail-is/hail/issues/200#issuecomment-279521097,1,['extend'],['extend']
Modifiability,"Docs and a bunch of other updates are done. Ready for review!. Once it's in, I'll refactor linreg, logreg, and lmmreg commands to put the phenotype and covariate extraction logic in one place under stats.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1064#issuecomment-260219957:82,refactor,refactor,82,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-260219957,1,['refactor'],['refactor']
Modifiability,"Does the version of spark matter? such as apache spark 2.0.2 and the cloudera spark?; We use the cloudera hadoop,but for hail, the cloudera'spark can't work,so in the configuration we replaced the cloudera spark with the apache spark2.0.2,and this works in local mode,but have errors in cluster mode",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321241969:167,config,configuration,167,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321241969,2,['config'],['configuration']
Modifiability,Done in scalacheck rewrite.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/25#issuecomment-179404081:19,rewrite,rewrite,19,https://hail.is,https://github.com/hail-is/hail/issues/25#issuecomment-179404081,1,['rewrite'],['rewrite']
Modifiability,"Drat, it appears that pyspark doesn't work with Python 3.8. https://stackoverflow.com/a/58849063/342839. A simpler reproduction to demonstrate that this is a pyspark issues:. ```; snafu$ python -m pyspark.cloudpickle; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 185, in _run_module_as_main; mod_name, mod_spec, code = _get_module_details(mod_name, _Error); File ""/usr/lib/python3.8/runpy.py"", line 111, in _get_module_details; __import__(pkg_name); File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py"", line 51, in <module>; from pyspark.context import SparkContext; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py"", line 31, in <module>; from pyspark import accumulators; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py"", line 97, in <module>; from pyspark.serializers import read_int, PickleSerializer; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py"", line 71, in <module>; from pyspark import cloudpickle; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py"", line 145, in <module>; _cell_set_template_code = _make_cell_set_template_code(); File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py"", line 126, in _make_cell_set_template_code; return types.CodeType(; TypeError: an integer is required (got type bytes); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197#issuecomment-800647452:500,sandbox,sandbox,500,https://hail.is,https://github.com/hail-is/hail/issues/10197#issuecomment-800647452,6,['sandbox'],['sandbox']
Modifiability,"EDIT2:. OK, so, I'm not sure when this behavior changed but Make 4.0 wants a `\` to indicate that the recipe continues on the next line *but also* passes that backslash and newline to the shell. In Make 3.81, the `\` was also required but the newline and backslash *are not passed* to the shell. In other words: in 3.81, backslash-newline is always replaced with a space and in 4.0, backslash-newline is replaced with a space *except on recipe lines in which case it is necessary to indicate the recipe continues but it is also passed literally to the shell*. The docs page you linked directly addresses our use case and suggests we put the command inside of a make variable (thus triggering normal backslash-newline rules rather than the special recipe line rules). > Sometimes you want to split a long line inside of single quotes, but you dont want the backslash/newline to appear in the quoted content. This is often the case when passing scripts to languages such as Perl, where extraneous backslashes inside the script can change its meaning or even be a syntax error. One simple way of handling this is to place the quoted string, or even the entire command, into a make variable then use the variable in the recipe. In this situation the newline quoting rules for makefiles will be used, and the backslash/newline will be removed. If we rewrite our example above using this method:; > ; > ```; > HELLO = 'hello \; > world'; > ; > all : ; @echo $(HELLO); > ```; > ; > we will get output like this:; > ; > ```; > hello world; > ```; >; > If you like, you can also use target-specific variables (see [Target-specific Variable Values](https://www.gnu.org/software/make/manual/html_node/Target_002dspecific.html)) to obtain a tighter correspondence between the variable and the recipe that uses it. It seems to me like there are not any great choices. Putting the JSON into a Make variable seems too magical and likely to confuse a newbie editing this file. Using escaped double quotes is less leg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324:666,variab,variable,666,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324,1,['variab'],['variable']
Modifiability,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:1404,extend,extend,1404,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307,1,['extend'],['extend']
Modifiability,"Erm, sorry, the right place to look for a diff is: https://github.com/hail-is/hail/compare/main...danking:batch-cloud-agnostic-rewrite-all",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956506343:127,rewrite,rewrite-all,127,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956506343,1,['rewrite'],['rewrite-all']
Modifiability,Example error:. ```; Caused by: is.hail.relocated.org.json4s.MappingException: No usable value for value_parameter_names; No usable value for str; Did not find value which can be converted into java.lang.String; 	at is.hail.relocated.org.json4s.reflect.package$.fail(package.scala:53); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:638); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:689); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:688); 	at scala.PartialFunction.$anonfun$runWith$1$adapted(PartialFunction.scala:145); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at scala.collection.TraversableLike.collect(TraversableLike.scala:407); 	at scala.collection.TraversableLike.collect$(TraversableLike.scala:405); 	at scala.collection.AbstractTraversable.collect(Traversable.scala:108); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:688); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:767); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$extract$10(Extraction.scala:462); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$customOrElse$1(Extraction.scala:780); 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127); 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126); 	at scala.PartialFunction$$anon$1.applyOrElse(PartialFunction.scala:257); 	at is.hail.relocated.org.json4s.Extraction$.customOrElse(Extraction.scala:780); 	at is.hail.relocated.org.json4s.Extraction$.extract(Extraction.scala:454); 	at is.hail.relocated.org.json4s.Extraction$.org$json4s$Extraction$$extractDete,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14579#issuecomment-2163457890:698,adapt,adapted,698,https://hail.is,https://github.com/hail-is/hail/pull/14579#issuecomment-2163457890,1,['adapt'],['adapted']
Modifiability,"FWIW you don't actually need to parse it at the moment. This is my code that works:. ```; temp_file = 'hdfs:/kt.txt.bgz'; types_file = 'hdfs:/kt.types.txt'; kt.export(temp_file, types_file=types_file). with hail.hadoop_read(types_file) as f:; types = f.read(); kt = hc.import_keytable(temp_file, config=hail.TextTableConfig(types=types)); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1743#issuecomment-299075862:296,config,config,296,https://hail.is,https://github.com/hail-is/hail/issues/1743#issuecomment-299075862,1,['config'],['config']
Modifiability,"FWIW, when I added the MySQL pods I made sure to install the client config in them so you don't need the admin-pod for test namespaces, just `kssh db <NAMESPACE>` and `mysql` should work",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13150#issuecomment-1581414024:68,config,config,68,https://hail.is,https://github.com/hail-is/hail/pull/13150#issuecomment-1581414024,1,['config'],['config']
Modifiability,"FYI @danking There was also a Makefile bug: make creates make variables from the environment variables by default. Also, when make runs a rule, it makes the make variables available in the environment. That means PYTHONPATH in those Makefiles was getting set to some literal ${...} string, not the right value. I solved this by not assigning PYTHONPATH in the Makefile. https://www.gnu.org/software/make/manual/html_node/Environment.html. ""Every environment variable that make sees when it starts up is transformed into a make variable with the same name and value. However, an explicit assignment in the makefile, or with a command argument, overrides the environment."". ""When make runs a recipe, variables defined in the makefile are placed into the environment of each shell.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9394#issuecomment-685254706:62,variab,variables,62,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685254706,6,['variab'],"['variable', 'variables']"
Modifiability,"FYI, I had to add a default/gce-deploy-config for the deploy config for tasks running on batch2 workers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7507#issuecomment-553959196:39,config,config,39,https://hail.is,https://github.com/hail-is/hail/pull/7507#issuecomment-553959196,2,['config'],['config']
Modifiability,Fair point. I think asserting this behavior works on a local FS is good. I think a follow up to this PR that parameterized the other tests by filesystem is also great. That follow up PR can just leave the local-only test unchanged.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11227#issuecomment-1015881708:109,parameteriz,parameterized,109,https://hail.is,https://github.com/hail-is/hail/pull/11227#issuecomment-1015881708,1,['parameteriz'],['parameterized']
Modifiability,"First, @cristinaluengoagullo, thank you for your contribution! This is awesome. Second, I think this PR will need a little work before it can go in. Let me describe the situation:. We have now stopped work on Hail 0.1 and are now making only critical bug fixes. I think we can accept small feature additions, but we're optimizing for stability over features now. All new development has moved to master/0.2 beta. If you do make changes to 0.1, they should be forward ported to 0.2 if you want them to be carried forward. In addition, there are two problems with your PR:. 1. It is quite large. We prefer contributions to be single conceptual units. For example, a change to VEP should be separate from additions to the function registry. 2. The diff is somewhat confusing and I'm not 100% sure what is going on. It appears to include a large number of our own changes, it looks like from this commit: https://github.com/hail-is/hail/pull/3172/commits/e6f0b7f3a854f0fd64857876ab04375e570ba09f. However, given that the commit is under your name with a new commit message, I can't tell where those changes originated. Also, at least some (all?) of those changes already appear in 0.1, so I'm not sure why Github is displaying them, for example: https://github.com/hail-is/hail/pull/3172/files#diff-f11d07953ac5cd8bd8d4d3fd135a3efbR11. I think squashing your changes (and just your changes) and rebasing them onto the current 0.1 HEAD will fix the problem. Then we can take a closer look at the changes. Finally, it looks like your changed the VEP schema because you're invoking it with extra plugins. I think that's a no-go for us, but you could modify the VEP command (through an argument or in the properties file) to specify an alternate schema.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3172#issuecomment-377559347:1589,plugin,plugins,1589,https://hail.is,https://github.com/hail-is/hail/pull/3172#issuecomment-377559347,1,['plugin'],['plugins']
Modifiability,"Fixed, config and status now displayed in job page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7524#issuecomment-562856309:7,config,config,7,https://hail.is,https://github.com/hail-is/hail/issues/7524#issuecomment-562856309,1,['config'],['config']
Modifiability,"Fixed. Two outstanding issues:; 1. `RichVDS` cannot extend `AnyVal` due to implementation restrictions on value extensions nesting with other implicit conversions. Right now it is a standalone class.; 2. What do with `VSMSuite`? This seemed like it was designed to make sure all the implementations were synced, but that is no longer necessary.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/83#issuecomment-160690390:52,extend,extend,52,https://hail.is,https://github.com/hail-is/hail/pull/83#issuecomment-160690390,1,['extend'],['extend']
Modifiability,"For `hail` weve tried to be flexible, but We can move the lower bound or add an upper bound if necessary. Id this is services stuff thats just a mistake, definitely pin if youd like.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10349#issuecomment-824032425:29,flexible,flexible,29,https://hail.is,https://github.com/hail-is/hail/pull/10349#issuecomment-824032425,1,['flexible'],['flexible']
Modifiability,"For clarity: I removed PCanonicalBaseStruct because most method implementations were moved back to PBaseStruct, and I found its now-limited benefit to not be worth the drawback of a significantly more complex inheritance structure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7733#issuecomment-566778522:209,inherit,inheritance,209,https://hail.is,https://github.com/hail-is/hail/pull/7733#issuecomment-566778522,1,['inherit'],['inheritance']
Modifiability,"For dockerd, we probably need to use this: https://github.com/fluent-plugin-systemd/fluent-plugin-systemd",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9189#issuecomment-667117637:69,plugin,plugin-systemd,69,https://hail.is,https://github.com/hail-is/hail/pull/9189#issuecomment-667117637,2,['plugin'],['plugin-systemd']
Modifiability,"For flag style env variables, I think we should use present and non-empty. We could also go with a reasonable set of 'affirmative' values, like say (all case insensitive) `true`, `t`, `yes`, `y`, `1`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11746#issuecomment-1104272760:19,variab,variables,19,https://hail.is,https://github.com/hail-is/hail/pull/11746#issuecomment-1104272760,1,['variab'],['variables']
Modifiability,"Gah, OK, I think I have it now, but there was one more detail:. The gradle configuration `testCompileOnly` [1] *does not* inherit from the `shadow` configuration (as evidence see [this search](https://github.com/search?q=repo%3Ajohnrengelman%2Fshadow%20extendsFrom&type=code) of the shadow repo). We must explicitly request that `shadow` dependencies are included in the compile-time class path of the tests. This is as it should be: the things in `shadow` are things which are provided to us by our runtime environment. That's true of both the *test* runtime environment and the normal runtime environment. The Gradle Shadow plugin takes a different perspective by default, it suggests that `shadow` dependencies shouldn't be used in the tests at all. [1] NB: `testCompile` does not exist but you don't get an error if you try to use it, thanks for nothing gradle.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1710414563:75,config,configuration,75,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1710414563,4,"['config', 'inherit', 'plugin']","['configuration', 'inherit', 'plugin']"
Modifiability,Ghost does not respect the `X-Forwarded` headers. It should not have a `url` parameter but a `pathPrefix` and the protocol should be set from `X-Forwarded-Proto`. Thanks to this design bug we cannot test connectivity to the blog in PRs until the internal gateway is configured to use TLS. I'll revisit this PR when that happens.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8117#issuecomment-589741905:266,config,configured,266,https://hail.is,https://github.com/hail-is/hail/pull/8117#issuecomment-589741905,1,['config'],['configured']
Modifiability,"Good idea, I'll check. I feel like I initially found this in deep in a redhat tutorial, but ultimately found it again at the bottom of the [man page](https://man7.org/linux/man-pages/man8/xfs_quota.8.html). I was following this example:; ```; Enabling project quota on an XFS filesystem (restrict files in; log file directories to only using 1 gigabyte of space). # mount -o prjquota /dev/xvm/var /var; # echo 42:/var/log >> /etc/projects; # echo logfiles:42 >> /etc/projid; # xfs_quota -x -c 'project -s logfiles' /var; # xfs_quota -x -c 'limit -p bhard=1g logfiles' /var. Same as above without a need for configuration files. # rm -f /etc/projects /etc/projid; # mount -o prjquota /dev/xvm/var /var; # xfs_quota -x -c 'project -s -p /var/log 42' /var; # xfs_quota -x -c 'limit -p bhard=1g 42' /var; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10467#issuecomment-834771396:607,config,configuration,607,https://hail.is,https://github.com/hail-is/hail/pull/10467#issuecomment-834771396,1,['config'],['configuration']
Modifiability,"Good questions:. > How does Terraform know to import the module gsa_k8s_secret? I assume it just scans the directory. In short, yes. Every module is defined by a `main.tf`, `variables.tf`, and `outputs.tf`, and it finds it through the `source` path in the module blocks. When you run `terraform init` in the `infra` directory, it scans `main.tf`, sees that there are module references to the local filesystem, and sets up a watch on that directory (you can still make changes to `gsa_k8s_secret` without re-running `init`). > What does ${module.ci_gsa_secret....} do? Does module refer to that terraform file?. The way references work in terraform is a little bizarre. There are basically three classes of value that we use right now in terraform: resources, modules and variables. You declare instances of them like so (note that I'm using class and instance colloquially):. ```; resource <resource_class_name> <resource_instance_name> {; ; }. # I like to think of modules as being essentially unnamed collections of resources; module <module_instance_name> {; source = '/path/to/module'; ; }. variable <variable_name> {; ; }; ```. and then reference them like so:. ```; <resource_class_name>.<resource_instance_name>; module.<module_instance_name>; var.<variable_name>; ```. So module.ci_gsa_secret.... is how you would access `output`s of the module instance called `ci_gsa_secret`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10785#issuecomment-900398381:174,variab,variables,174,https://hail.is,https://github.com/hail-is/hail/pull/10785#issuecomment-900398381,3,['variab'],"['variable', 'variables']"
Modifiability,"Great feedback. Addressed comments, back to you. In the scope lists, I use the format: `variable (*Type*): description`, where the type is in italics but not a hyperlink, but I put a hyperlink in the description when it seemed appropriate. ```; *:ref:`foo`; ```. didn't format the hyperlink. Also, I don't think we can put hyperlinks in double-back-quote literal/code blocks. I didn't address the math stuff. I think we can merge this (and other doc migrations) when it is ready and fix that separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1225#issuecomment-271382041:88,variab,variable,88,https://hail.is,https://github.com/hail-is/hail/pull/1225#issuecomment-271382041,1,['variab'],['variable']
Modifiability,"Great, thank you! I've just checked out your branch and will do some manual testing this morning and let you know how that goes. I noticed that the VEP logic didn't have test coverage right now, but had a few ideas for some modest refactoring so unit tests are possible. I'll see if I can get that working!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3790#issuecomment-400314076:231,refactor,refactoring,231,https://hail.is,https://github.com/hail-is/hail/issues/3790#issuecomment-400314076,1,['refactor'],['refactoring']
Modifiability,HAIL_QUERY_BACKEND is unset. $ hailctl config get query/backend; batch. I will set it to spark,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12540#issuecomment-1363231471:39,config,config,39,https://hail.is,https://github.com/hail-is/hail/issues/12540#issuecomment-1363231471,1,['config'],['config']
Modifiability,"Hail does not support heterogeneous arrays: found list with elements of types [dtype('int32'), dtype('str')] . The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 584, in arg_check; return checker.check(arg, function_name, arg_name); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_typecheck.py"", line 80, in check; raise TypecheckFailure from e; hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in <module>; cli.main(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main; run(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file; runpy.run_path(target, run_name=""__main__""); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path; return _run_module_code(code, init_globals, run_name,; File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code; _run_code(code, mod_globals, init_globals,; File ""/home/edmund/.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:3044,adapt,adapter,3044,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['adapt'],['adapter']
Modifiability,"Having run through a test myself, it looks like we'll actually need to add back all the deleted config aside from the last bit that starts up the jupyter server. Aside from the last block that starts up jupyter, the rest is important logic to configure extensions and content management.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12788#issuecomment-1472586228:96,config,config,96,https://hail.is,https://github.com/hail-is/hail/pull/12788#issuecomment-1472586228,2,['config'],"['config', 'configure']"
Modifiability,"Heh. At least in my version of Docker, those are implicitly relative to the root not the WORKDIR:; ```; (base) dking@wm28c-761 /tmp % cat Dockerfile ; FROM ubuntu:20.04; WORKDIR /foo/bar; VOLUME baz; (base) dking@wm28c-761 /tmp % docker build -t foo . ; [+] Building 0.1s (6/6) FINISHED ; => [internal] load build definition from Dockerfile 0.0s; => => transferring dockerfile: 34B 0.0s; => [internal] load .dockerignore 0.0s; => => transferring context: 2B 0.0s; => [internal] load metadata for docker.io/library/ubuntu:20.04 0.0s; => [1/2] FROM docker.io/library/ubuntu:20.04 0.0s; => CACHED [2/2] WORKDIR /foo/bar 0.0s; => exporting to image 0.0s; => => exporting layers 0.0s; => => writing image sha256:217748640e5c53f72b8de9917010e5742fb8bef99a37dcb13ec59a903cb5834c 0.0s; => => naming to docker.io/library/foo 0.0s. Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them; (base) dking@wm28c-761 /tmp % docker run foo /bin/sh -c 'pwd && ls -l . && ls -l / && ls -l /baz'; /foo/bar; total 0; total 56; drwxr-xr-x 2 root root 4096 May 9 15:06 baz; lrwxrwxrwx 1 root root 7 Oct 19 2022 bin -> usr/bin; drwxr-xr-x 2 root root 4096 Apr 15 2020 boot; drwxr-xr-x 5 root root 340 May 9 15:06 dev; drwxr-xr-x 1 root root 4096 May 9 15:06 etc; drwxr-xr-x 3 root root 4096 May 9 15:01 foo; drwxr-xr-x 2 root root 4096 Apr 15 2020 home; lrwxrwxrwx 1 root root 7 Oct 19 2022 lib -> usr/lib; drwxr-xr-x 2 root root 4096 Oct 19 2022 media; drwxr-xr-x 2 root root 4096 Oct 19 2022 mnt; drwxr-xr-x 2 root root 4096 Oct 19 2022 opt; dr-xr-xr-x 238 root root 0 May 9 15:06 proc; drwx------ 2 root root 4096 Oct 19 2022 root; drwxr-xr-x 5 root root 4096 Oct 19 2022 run; lrwxrwxrwx 1 root root 8 Oct 19 2022 sbin -> usr/sbin; drwxr-xr-x 2 root root 4096 Oct 19 2022 srv; dr-xr-xr-x 13 root root 0 May 9 15:06 sys; drwxrwxrwt 2 root root 4096 Oct 19 2022 tmp; drwxr-xr-x 10 root root 4096 Oct 19 2022 usr; drwxr-xr-x 11 root root 4096 Oct 19 2022 var; total 0; (base) dki",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12990#issuecomment-1540332989:667,layers,layers,667,https://hail.is,https://github.com/hail-is/hail/pull/12990#issuecomment-1540332989,1,['layers'],['layers']
Modifiability,"Here is a Hail log.... I will work on getting the YARN container logs next. . more /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; ```; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:513,config,configuration,513,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['config'],['configuration']
Modifiability,"Here is an example of the trailing slash issue:. What kibana sees:. ""GET /monitoring/kibana/ui/fonts/inter_ui/Inter-UI-Bold.woff2 HTTP/1.1"" 200 94840 ""https://internal.hail.is/monitoring/kibana/app/kibana"". config:. ```; location /monitoring/kibana/ {; proxy_pass http://kibana/;; }; ```. It may not be inconsistent with trailing / on proxy_pass stripping the url, but it sure is confusing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540622757:207,config,config,207,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540622757,1,['config'],['config']
Modifiability,"Here's a larger rewrite of Github readme, ready for feedback. The gitter links reflect hail and hail-dev as we want them to be, so before merging we should rename hail to hail-dev and create hail. I also think it'd be good to give a bit more context for users on what ""pre-alpha, very active dev"" does and does not mean. In particular, that Hail is usable and tested now, but liable to change in non backward-compatible ways. Thoughts on including / wording this?. We should also consider moving the Roadmap somewhere on the forum. I think the development forum is a good place for more detailed instructions on collaboration (forking, etc) and best practices.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/699#issuecomment-243136925:16,rewrite,rewrite,16,https://hail.is,https://github.com/hail-is/hail/pull/699#issuecomment-243136925,1,['rewrite'],['rewrite']
Modifiability,"Here's a link with an absolute time window: https://cloudlogging.app.goo.gl/gXAWZpZtUiV8jphXA. This is the assertion's stack trace:; ```; at scala.Predef$.assert(Predef.scala:208); at is.hail.QoBOutputStreamManager.createOutputStream(QoBAppender.scala:38); at org.apache.logging.log4j.core.appender.OutputStreamManager.getOutputStream(OutputStreamManager.java:165); at org.apache.logging.log4j.core.appender.OutputStreamManager.writeToDestination(OutputStreamManager.java:250); at org.apache.logging.log4j.core.appender.OutputStreamManager.flushBuffer(OutputStreamManager.java:283); at org.apache.logging.log4j.core.appender.OutputStreamManager.flush(OutputStreamManager.java:294); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.directEncodeEvent(AbstractOutputStreamAppender.java:217); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.tryAppend(AbstractOutputStreamAppender.java:208); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:199); at org.apache.logging.log4j.core.config.AppenderControl.tryCallAppender(AppenderControl.java:161); ```. And the line of our code that triggers the logger appender:; ```; is.hail.JVMEntryway$2.run(JVMEntryway.java:139); ```. On that line, we should have already evaluated line 97:; ```; QoBOutputStreamManager.changeFileInAllAppenders(logFile);; ```; Which updates the filename for all `QoBOutputStreamManager`s. We should be the only ones allocating `QoBOutputStreamManager` (it has no magic annotations, we don't pass its constructor anywhere). We should only allocate `QoBOutputStreamManager` in its associated object. We always put it into the map in `getInstance`. We don't synchronize the other methods though, so that could be the issue? If we have a stale version of that map?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13242#issuecomment-1703383030:1083,config,config,1083,https://hail.is,https://github.com/hail-is/hail/issues/13242#issuecomment-1703383030,1,['config'],['config']
Modifiability,"Here's a typical interaction for a current 2.1.0 user:; ```bash; dking@wmb16-359 # gradle -Dspark.verison=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.781 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); 2.1.0; dking@wmb16-359 # gradle -Dspark.version=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 42. * What went wrong:; A problem occurred evaluating root project 'hail'.; > The spark version must now be explicitly specified in the `gradle.properties`; file. Do *not* specify it with `-Dspark.version`. This version *must* match the; version of the spark installed on the machine or cluster that will execute; hail. You can override the setting in `gradle.properties` with a command line; like:. ./gradlew -PsparkVersion=2.1.1 shadowJar. The previous implicit, default spark version was 2.0.2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.778 secs; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020:377,config,configure,377,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020,2,['config'],['configure']
Modifiability,"Here's an example:. https://github.com/hail-is/hail/blob/6f5d1d9b511a1b5c91908a70f1afde909ec3226e/hail/src/main/scala/is/hail/expr/types/physical/PBaseStruct.scala#L346. We use `srcStructOffset` multiple times without binding, and then don't bind a variable before calling a copyFromType. I think the invariant is basically that if you get a Code[T], you must not use it more than once, since you're inlining arbitrary code. I've tried to enforce this model in my implementations",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8099#issuecomment-586413200:249,variab,variable,249,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586413200,1,['variab'],['variable']
Modifiability,"Here's my proposed interface (names to be changed, I'm terrible at those). ```; case class WithSource[T](value: T, source: InputSource) {; def map[U](f: T => U): WithSource[U] = {; try {; copy[U](value = f(value)); } catch {; case e: Exception => source.wrapError(e); }; }; }. abstract class InputSource {; def wrapError(e: Exception): Nothing; }. case class TextSource(line: String, file: String, position: Option[Int]) extends InputSource {; def wrapError(e: Exception): Nothing = {; val msg = e match {; case _: FatalException => e.getMessage; case _ => s""caught $e""; }; val lineToPrint =; if (line.length > 62); line.take(59) + ""...""; else; line. log.error(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $line"""""".stripMargin); fatal(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $lineToPrint"""""".stripMargin); }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233012302:421,extend,extends,421,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233012302,1,['extend'],['extends']
Modifiability,Here's the output of grepping for `dmk9z` in the root of the hail repo:; ```; config.mk:HAIL_TEST_GCS_BUCKET := hail-test-dmk9z. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10807#issuecomment-905867503:78,config,config,78,https://hail.is,https://github.com/hail-is/hail/pull/10807#issuecomment-905867503,1,['config'],['config']
Modifiability,"Here's the terraform configurations in GCP and Azure:. - GCP: Batch has admin storage permissions, as granted here https://github.com/hail-is/hail/blob/1f5e1540c04abfde58ead1084841fec5aa6e0ed3/infra/gcp/main.tf#L415-L424. We also grant it a Viewer role on the query bucket after that which seems redundant. We should really not grant it global storage admin and instead give it admin for just the query bucket and other associated batch buckets. I checked in hail-vdc and batch does not have the global storage admin role, and it has the Viewer role on the query bucket. I've changed that role now to admin on the query bucket. - Azure: Story is simpler. The `query` storage container is part of the `batch` storage account. The batch SP has ownership over the `batch` storage account and by extension all of the containers inside it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011:21,config,configurations,21,https://hail.is,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011,1,['config'],['configurations']
Modifiability,"Hey @danking,; Thanks so much for the advice. Your team has been very helpful and responsive. - I made the adjustment to my `create_intervals` function; - Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. I tried your suggestion of writing the data to a table first, but my `cols` field doesn't contain the computed HWE values. These are contained within the `entries` field. Table description is below. I tried modifying the code to what is shown below but I'm still having the same issue. Also tried increasing the RAM to max available per CPU. One thing I noticed is the `mt_hwe_vals` variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'ancestry': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'filters': set<str>; 'variant_qc': struct {; gq_stats: struct {; mean: float64, ; stdev: float64, ; min: float64, ; max: float64; }, ; call_rate: float64, ; n_called: int64, ; n_not_called: int64, ; n_filtered: int64, ; n_het: int64, ; n_non_ref: int64, ; het_freq_hwe: float64, ; p_value_hwe: float64, ; p_value_excess_het: float64; }; 'info': struct {; AC: array<int32>, ; AF: array<float64>, ; AN: int32, ; homozygote_count: array<int32>; }; 'a_index': int32; 'was_split': bool; ----------------------------------------; Entry fields:; 'hwe': struct {; het_freq_hwe: float64, ; p_value: float64; }; ----------------------------------------; Column key: ['ancestry']; Row key: ['locus', 'alleles']; ----------------------------------------; ```. ```python; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)). # T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1674895492:649,variab,variable,649,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1674895492,1,['variab'],['variable']
Modifiability,"Hi @nawatts, your comments are very welcome, and I appreciate your perspective as a hailctl user. OK, mulling over your comments, I think I can address collectively by removing all hailctl options that pass through to gcloud. This removes the question of providing them twice, makes all the commands consistent. I think this also addresses the issue `hailctl dataproc submit` not supporting `--`, because you can specify it twice: once to break out of hailctl options, and once to break out of gcloud options to specify options the script being submitted: `hailctl dataproc submit --halictl-option -- --gcloud-options -- --script-options and-parameters`. What do you think?. > it would be nice if the --configuration/--gcloud-configuration argument was consistent across hailctl dataproc commands; > It would also be nice to standardize on kebab case for all arguments. Agree on both accounts, will fix. Thanks again!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-758080935:703,config,configuration,703,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758080935,2,['config'],['configuration']
Modifiability,"Hi Cotton,. Interesting that this is during tablet creation not while inserting data.; Looks like this is a known issue, but with no fix or workaround yet that I; can see:. https://issues.cloudera.org/plugins/servlet/mobile#issue/KUDU-383. Does it work if you retry, or delete the table and retry? I successfully; imported chr1 from 1k genomes on a 6 node cluster. This would create fewer; tablets though as it only covers one chromosome, so I should try with the; full dataset - I'll do that in the next few days when I'm back from; travelling. Thanks for trying it out. Do you have any more review comments for the PR?. Cheers,; Tom; On 11 Apr 2016 21:29, ""cseed"" notifications@github.com wrote:. Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to; 40m to fix a The requested number of tablets is over the permitted maximum; (100) error. I was able to write a small table. When I tried to write a; larger file (~900 exomes) and I got:. hail: writekudu: caught exception:; org.kududb.client.NonRecoverableException: Too many attempts:; KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6,; DeadlineTracker(timeout=10000, elapsed=7721),; Deferred@1490962783(state=PENDING, result=null, callback=(continuation; of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) ->; (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505),; errback=(continuation of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@11075008; 48) -> (continuation of Deferred@919337",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-208722298:201,plugin,plugins,201,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208722298,1,['plugin'],['plugins']
Modifiability,"Hi Tim,. What's the problem with this implementation? I've tested it and it works... On Wed, Sep 21, 2016 at 11:07 AM, Tim Poterba notifications@github.com; wrote:. > Laurent, I was totally wrong about being able to do this per-command --; > I'm really sorry. I thought that it would be possible to create a new; > configuration just for this command and use that, but this is only possible; > for HadoopConfigurations and not SparkContexts. Can you reopen the old; > PR? That model is our only option.; > ; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/826#issuecomment-248641543, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgcPW4xK16W3DlZfdE5U6RTcVmJthks5qsUhMgaJpZM4KC1O-; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/826#issuecomment-248643185:315,config,configuration,315,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248643185,1,['config'],['configuration']
Modifiability,"Hi Tim,; Sorry for the bother again. I am following this short tutorial as described [here](https://gnomad.broadinstitute.org/news/2021-09-using-the-gnomad-ancestry-principal-components-analysis-loadings-and-random-forest-classifier-on-your-dataset/). The code snippet was working properly with earlier. Now that I have installed hail from pip, I have this error while running RF model. ```; ht, rf_model = assign_population_pcs(; ... ht,; ... pc_cols=ht.scores,; ... fit=fit,; ... ). 2022-09-29 14:55:46 Hail: INFO: Coerced sorted dataset (0 + 1) / 1]; INFO (gnomad.sample_qc.ancestry 224): Found the following sample count after population assignment: sas: 1; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/bioinfoRD/ARCdata/Projects_AMT/conda_envs/hail/lib/python3.10/site-packages/gnomad/sample_qc/ancestry.py"", line 235, in assign_population_pcs; min_assignment_prob=min_prob, error_rate=error_rate; UnboundLocalError: local variable 'error_rate' referenced before assignment; ```. Could you please suggest what might be happening here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759:963,variab,variable,963,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759,1,['variab'],['variable']
Modifiability,"Hi Vlad, thanks for the PR! I'm afraid there are some internal migrations we're making that are probably not clear from just looking at the codebase. Are you up to date on our `main`? We've found working with `config.mk` cumbersome because it can be stale if you switch between different instances of Batch (e.g. one deployed in azure and the other in GCP). > DOCKER_ROOT_IMAGE used to build batch workers and benchmark. I've recently updated the scripts for building the batch worker VM image to query kubernetes directly and we should probably do the same for benchmark. > HAIL_TEST_GCS_BUCKET used to build query; KUBERNETES_SERVER_URL used to build amundsen. These services are both currently deleted in our `main`. > PROJECT, ZONE, REGION are probably not need, but might make sense to add for consistency. These will fail in an Azure deployment, and while we want to move away from `config.mk` entirely, we would at least want it to contain configurations that are valid across clouds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11371#issuecomment-1041941055:210,config,config,210,https://hail.is,https://github.com/hail-is/hail/pull/11371#issuecomment-1041941055,3,['config'],"['config', 'configurations']"
Modifiability,"Hi, sorry to leave this hanging - we aren't especially well-equipped to answer this kind of question, since it seems to be a problem with the ES config. We just convert the Hail Table to a Spark DataFrame and call `saveToEs`: ; ```scala; def export(df: spark.sql.DataFrame, host: String = ""localhost"", port: Int = 9200,; index: String, indexType: String, blockSize: Int = 1000,; config: Map[String, String], verbose: Boolean = true) {. // config docs: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html. val defaultConfig = Map(; ""es.nodes"" -> host,; ""es.port"" -> port.toString,; ""es.batch.size.entries"" -> blockSize.toString,; ""es.index.auto.create"" -> ""true""). val mergedConfig = if (config == null); defaultConfig; else; defaultConfig ++ config. if (verbose); println(s""Config ${ mergedConfig }""). df.saveToEs(s""${ index }/${ indexType }"", mergedConfig); }; ```. I'd try debugging entirely in Spark to see if you can isolate the issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584:145,config,config,145,https://hail.is,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584,7,"['Config', 'config']","['Config', 'config', 'configuration']"
Modifiability,"Hi, when we executed the command above, the results are as follows:; ```; [root@tele-1 ~]# PYSPARK_PYTHON=""ipython"" pyspark --conf spark.sql.files.openCostInBytes=1099511627776 --conf spark.sql.files.maxPartitionBytes=1099511627776 --conf spark.hadoop.parquet.block.size=1099511627776 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer; /usr/local/lib/python3.5/site-packages/IPython/core/history.py:228: UserWarning: IPython History requires SQLite, your history will not be saved; warn(""IPython History requires SQLite, your history will not be saved""); Python 3.5.2 (default, Jul 12 2017, 14:00:23) ; Type ""copyright"", ""credits"" or ""license"" for more information. IPython 5.1.0 -- An enhanced Interactive Python.; ? -> Introduction and overview of IPython's features.; %quickref -> Quick reference.; help -> Python's own help system.; object? -> Details about 'object', use 'object??' for extra details.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/09 12:51:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/09 12:51:55 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 12:51:55 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 12:51:55 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 12:51:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Welcome to; ____ __; / __/__ ___ _____/ /",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609:703,enhance,enhanced,703,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609,1,['enhance'],['enhanced']
Modifiability,"Hi,. I'm just curious if you tried black on a code that uses Hail query. As far as I see, PEP8 conflicts with the code style adopted in the Hail docs, e.g. black would remove spaces in named function arguments:. ```; - mt = mt.annotate_entries(GT = lgt_to_gt(mt.LGT, mt.LA)); + mt = mt.annotate_entries(GT=lgt_to_gt(mt.LGT, mt.LA)); ```. On the other hand, query can be seen as a DSL on top of Python, so the same guidelines probably don't need to be applied to it. Wondering if you had thoughts about lining the query code? We will be writing a lot of that in the nearest future in the Centre for Population Genomics, and would love to set up some style checks, or even automate that with a tool like black. And on black - are you considering automating code refactoring with black as part of the CI? Or you wanted to just do checks, alongside with pylint?. Vlad",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9931#issuecomment-768677633:760,refactor,refactoring,760,https://hail.is,https://github.com/hail-is/hail/pull/9931#issuecomment-768677633,1,['refactor'],['refactoring']
Modifiability,"Hicseed @cseed , I configured the java related to the Spark cluster, as follows. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-250697347:20,config,configured,20,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347,1,['config'],['configured']
Modifiability,"Hmm, looks like it's something like `spark.executorEnv.FOO=...` based on this: https://spark.apache.org/docs/latest/configuration.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583434753:116,config,configuration,116,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583434753,1,['config'],['configuration']
Modifiability,"Hmm. I trust the code now. I test against several R SKAT runs. I'm not sure I understand how we derive that Q is generalized chi-squared distributed. We use the residual phenotypes in the calculation of Q, but those are inverse-logit transformed normal variables. The derivation for the linear case doesn't apply, as far as I can tell. I assume the residuals are Bernoulli distributed? Maybe not. I guess the phenotypes are Bernoulli but the errors aren't? I'm not sure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12643#issuecomment-1419295599:253,variab,variables,253,https://hail.is,https://github.com/hail-is/hail/pull/12643#issuecomment-1419295599,1,['variab'],['variables']
Modifiability,Hmm. Ill have to sort this out tomorrow. Not sure whats going on with that. It seems like the shadowTestJar target is probably not correctly pulling in the testImolemebtation configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1709453126:177,config,configuration,177,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1709453126,1,['config'],['configuration']
Modifiability,"Hmmm... I guess we could just have it look for an environment variable, instead of asking for a flag on `HailContext`. Easy enough to set the environment variable in the `hailctl` init scripts. . You should maybe name it `HAIL_MKL_PATH` instead of `MKL_PATH`, so people know what this variable is for / doesn't conflict with other tools variables.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10770#issuecomment-897020574:62,variab,variable,62,https://hail.is,https://github.com/hail-is/hail/pull/10770#issuecomment-897020574,4,['variab'],"['variable', 'variables']"
Modifiability,"Hmph, ya this seems annoyingly complicated, and I'd prefer to make one command with opinionated but configurable defaults than have different commands. One thing that feels inconsistent here is what we do in the Batch interface. We don't have the equivalent of `HAIL_QUERY_BACKEND` and a user specifically has to create a `ServiceBackend` as opposed to relying on the environment dictating which model to use. I feel like it would be OK if we documented `hailctl batch submit` as ""distribute everything on batch by default""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12471#issuecomment-1324131496:100,config,configurable,100,https://hail.is,https://github.com/hail-is/hail/pull/12471#issuecomment-1324131496,1,['config'],['configurable']
Modifiability,"How's this?; ```; info(s""Running linear regression per row on $n samples for ${ y.cols } response ${ plural(y.cols, ""variable"") } y,\n""; + s"" with input variable x, intercept, and ${ k - 1 } additional ${ plural(k - 1, ""covariate"") }...""); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2777#issuecomment-359417991:117,variab,variable,117,https://hail.is,https://github.com/hail-is/hail/issues/2777#issuecomment-359417991,2,['variab'],['variable']
Modifiability,I *think* I've addressed all concerns. It would help if @danking could test all functions; I only have a partially configured environment.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4994#issuecomment-448388700:115,config,configured,115,https://hail.is,https://github.com/hail-is/hail/pull/4994#issuecomment-448388700,1,['config'],['configured']
Modifiability,"I actually think moving `root` into a `location` at the bottom of the file does not matter, but it seems easier to spot this way. I think the actual issue is that this server has never heard of `https://hail.is`, so the rewrite doesn't work?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4726#issuecomment-435889718:220,rewrite,rewrite,220,https://hail.is,https://github.com/hail-is/hail/pull/4726#issuecomment-435889718,1,['rewrite'],['rewrite']
Modifiability,I added `organization_domain` to the global-config in hail-vdc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11147#issuecomment-992708508:44,config,config,44,https://hail.is,https://github.com/hail-is/hail/pull/11147#issuecomment-992708508,1,['config'],['config']
Modifiability,"I addressed some comments. I still need to:; - expose hts_genotype_schema in python, and; - figure out what to rewrite instead of ""genotype"" in the VariantDataset docs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2480#issuecomment-347072699:111,rewrite,rewrite,111,https://hail.is,https://github.com/hail-is/hail/pull/2480#issuecomment-347072699,1,['rewrite'],['rewrite']
Modifiability,"I agree that we never carefully considered switching domains at the user-level. I'm mostly concerned with Broadies using the GCP instance for now. I think how to deal with domains really gets into the idea of having config profiles, which just makes all of this more complicated. Ergo, my gut is: if you're not a Broadie using Google, you have to do some extra work, but that set of users is very small.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1663143699:216,config,config,216,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1663143699,1,['config'],['config']
Modifiability,I also eliminated a useless internal function. I think this was an artifact leftover from a refactor.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6902#issuecomment-522672211:92,refactor,refactor,92,https://hail.is,https://github.com/hail-is/hail/pull/6902#issuecomment-522672211,1,['refactor'],['refactor']
Modifiability,I also made a bit of a restructuring to all `BlockMatrix` `Gen`erators because you can't use default arguments with overloaded methods but I wanted to parameterize everything by an element `Gen`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2366#issuecomment-339832499:151,parameteriz,parameterize,151,https://hail.is,https://github.com/hail-is/hail/pull/2366#issuecomment-339832499,1,['parameteriz'],['parameterize']
Modifiability,"I also prefer the second option in https://github.com/hail-is/hail/pull/9842#issuecomment-758128554. The more I think about this, the more problematic the notion of having an opaque list of ""arguments to pass through to gcloud"" seems. For example, `hailctl dataproc start` may run multiple gcloud commands: one to start the cluster and another to apply tags to the master node. In that case, we'd want to pass through extra args to the cluster start command, but not the apply tags command. Or `hailctl dataproc modify`, where we might want to accept extra args for `gcloud dataproc clusters update`. Those args shouldn't be passed through to the `gcloud compute ssh` commands, but options like `--project` or `--configuration` should. It seems like pass through arguments would be best handled on a case by case basis for each hailctl command. That would probably make any approach that required parsing those pass through options more cumbersome to use.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-758143032:713,config,configuration,713,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758143032,1,['config'],['configuration']
Modifiability,I believe that even a local cluster (2+ jvms) would be sufficient to reproduce this error. I just have no idea how to configure such a thing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5033#issuecomment-449493275:118,config,configure,118,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449493275,1,['config'],['configure']
Modifiability,"I believe the code is behaving as designed. The error message, however, leaves a lot to be desired. A few take-aways:; 1. Hail doesn't support heterogeneous arrays. In situations like these, using an array of tuples has the desired outcome.; 2. The variable `x` in `lambda x:` is already a hail expression and so you don't need to explictly capture it as a `literal`.; a. While support for using hail expressions with `literal` was added in https://github.com/hail-is/hail/pull/4086 (see the issue for motivation), it can only be used when that expression is self-contained (ie it's not dependent on another hail expression, eg referencing an element of a hail array expression or tuple expression etc).; b. Our evaluation strategy is to `eval` the expression, then broadcast the result in a `literal`.; c. `eval` correctly complains that that expression has free variables and so can't be evaluated.; d. This error is ugly and has little to do with what the user wanted to achieve. Off the top of my head, a couple of ways to proceed:; 1. The hardest (but backwards compatible) fix is to somehow provide a good error message that the `x` in `lambda x:` in this particular context is a hail expression containing a reference that you should not use with `literal`.; 2. Remove support for using hail expressions with literal.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624335413:249,variab,variable,249,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624335413,2,['variab'],"['variable', 'variables']"
Modifiability,"I believe the next step should be the minimal changes to move the global variables in `server/globals.py` to a database table(s). We also need to think about sql configuration. For now, I suppose we can just create a table manually, but we need a longer term strategy for this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-456848381:73,variab,variables,73,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456848381,2,"['config', 'variab']","['configuration', 'variables']"
Modifiability,"I borrowed the MissingArrayBuilder from #3458. I tried to unify this with ArrayBuilder with inheritance / trait, but was having trouble doing so.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3503#issuecomment-386721940:92,inherit,inheritance,92,https://hail.is,https://github.com/hail-is/hail/pull/3503#issuecomment-386721940,1,['inherit'],['inheritance']
Modifiability,I completely refactored this. You'll probably want to review the auth.py code from scratch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9658#issuecomment-720219339:13,refactor,refactored,13,https://hail.is,https://github.com/hail-is/hail/pull/9658#issuecomment-720219339,1,['refactor'],['refactored']
Modifiability,"I created a new multi-branch configuration that should be better for what we are trying to accomplish. This should fix issues 2 and 3. . For the reproducibility of errors, that will probably take both setting the random seed parameter in Hail for all random tests and getting Jenkins to give better error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/335#issuecomment-214377125:29,config,configuration,29,https://hail.is,https://github.com/hail-is/hail/issues/335#issuecomment-214377125,1,['config'],['configuration']
Modifiability,"I deleted the LinearRegressionFromHcsCommand and associated tests as it'd fallen out of sync with how hcs evolved for T2D, it's independent of the rest of the PR and I can add this functionality back later (at which point I imagine there will be other changes both to hcs and to the stats interfaces more generally).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/517#issuecomment-236385122:106,evolve,evolved,106,https://hail.is,https://github.com/hail-is/hail/pull/517#issuecomment-236385122,1,['evolve'],['evolved']
Modifiability,"I didn't mean disable all optimization, I meant optimizations that specifically transform `TableCount` (e.g. the TableCount rewrite rules, the pruner can't prune the input to TableCount, etc.). As you note, it will also have to modify the implementation to run the RDD. Except that regression returns a table but force count returns a number. We could have set of 6 opaque operations: {Table, MatrixTable} => {Table, MatrixTable, Value} (skipping ones that don't actually appear).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5015#issuecomment-448765387:124,rewrite,rewrite,124,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448765387,1,['rewrite'],['rewrite']
Modifiability,"I didn't set that to a value,and kept it by default.; I have no idea about which variables should be set to some value, is there a guide to show all the variables I should set ? I didn't see something like this in the hail website?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338462427:81,variab,variables,81,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338462427,2,['variab'],['variables']
Modifiability,"I didn't want to put too much in this PR but I dislike reassigning the loop variable, it only feels like a footgun. So will happily approve the follow-up PR that fixes violations of that rule. There a couple of rules that I would like to gradually follow more. w.r.t. convincing it is ok, I suppose I just trust the ruff developers (and the big projects that have already converted) that given we don't use any niche flake8 plugins we are not dropping any rules by moving over.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12967#issuecomment-1535600667:76,variab,variable,76,https://hail.is,https://github.com/hail-is/hail/pull/12967#issuecomment-1535600667,2,"['plugin', 'variab']","['plugins', 'variable']"
Modifiability,I dismissed them as wont fix because theyre leaking secrets as intended into config files.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12794#issuecomment-1505797153:79,config,config,79,https://hail.is,https://github.com/hail-is/hail/pull/12794#issuecomment-1505797153,1,['config'],['config']
Modifiability,"I don't know the exact rules about java/scala protected variables -- but `protected[ir]` means anything in `is.hail.expr.ir` should be able to see it. Which is the whole compiler, basically, so it doesn't do a lot. But I think that's preferable to requiring that InferPType needs to be in the IR file, or extend it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6594#issuecomment-515073290:56,variab,variables,56,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515073290,2,"['extend', 'variab']","['extend', 'variables']"
Modifiability,"I don't love what I've had to do with the deploy config stuff. That's in my opinion the most finicky part of this (has already broken multiple times) and it's mostly our fault, because we overload the `namespace` parameter with both identifying the namespace in Kubernetes and signifying whether the environment is prod or not. All I want really is to change the `domain` to a domain and path prefix, and not have the namespace have such an impact on routing. Like what if `namespace` didn't affect routing, but if the deploy config only gave a domain with no path e.g. `hail.is`, we use subdomains so `batch.hail.is`, but if we provided a domain with a path prefix like `internal.hail.is/dgoldste`, we make the batch root `internal.hail.is/dgoldste/batch`?. Alternative: Actually have and use a `base_path` in the deploy config. This would be used in dev and terra environments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13944#issuecomment-1785575616:49,config,config,49,https://hail.is,https://github.com/hail-is/hail/pull/13944#issuecomment-1785575616,3,['config'],['config']
Modifiability,"I don't think I actually understand how artifact and snapshot dependencies work in TeamCity. I thought a build by the main build configuration (the regular CI) would trigger a build of the docs build configuration. This was not the case and I'm not sure why. I've set up the docs build to trigger on any change to master. Unfortunately, we have to `compileScala` twice because these are separate builds. I'll add an issue to clean this up and make it more sensible. There's got to be a right way to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/733#issuecomment-244475645:129,config,configuration,129,https://hail.is,https://github.com/hail-is/hail/issues/733#issuecomment-244475645,2,['config'],['configuration']
Modifiability,"I feel a bit like a cheat here since there's been a fair bit of work since you last reviewed. Most of it was fixes of tiny bugs that the CI revealed. There was [one, kind of notable, change](https://github.com/hail-is/hail/pull/5194/commits/f95e4e0ff1cdd2865cf703aa27f780c7f162316c). I removed Spark from the Dockerfile. It is no longer necessary because the pip install will pull the correct version of Spark. To avoid pulling Spark on each PR build, I cache 2.2.0 (and all our other pip dependencies) in the hail conda env in the docker image. Doing this required that I move our requirements into a requirements file which is parameterized by spark version. A good follow up PR would be to either a) entirely remove dependency on conda or b) generate the conda `environment.yml` from `requirements.txt.in`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333:629,parameteriz,parameterized,629,https://hail.is,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333,1,['parameteriz'],['parameterized']
Modifiability,I fixed the code so that there's one interpolation step. What I did was use the job name plus a random token as the job directory name and the batch tmp directory is an environment variable. I'm happy with this setup. Then it will be easier for users to tell where files came from if they give jobs unique names.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10716#issuecomment-895316665:181,variab,variable,181,https://hail.is,https://github.com/hail-is/hail/pull/10716#issuecomment-895316665,1,['variab'],['variable']
Modifiability,"I forgot that we still had cron jobs running gcr-cleaner daily. This could have been conflicting with the new cleanup policy deletion settings. Let's reopen if this occurs again. Posting the job configurations here before I delete the jobs. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/batch-worker"",""us-docker.pkg.dev/hail-vdc/hail/benchmark"",""us-docker.pkg.dev/hail-vdc/hail/blog_nginx"",""us-docker.pkg.dev/hail-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-docker.pkg.dev/hail-vdc/hail/grafana"",""us-docker.pkg.dev/hail-vdc/hail/hail-base"",""us-docker.pkg.dev/hail-vdc/hail/hail-build"",""us-docker.pkg.dev/hail-vdc/hail/hail-buildkit"",""us-docker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python38"",""us-docker.pkg.dev/hail-vdc/hail/hail-ubuntu"",""us-docker.pkg.dev/hail-vdc/hail/memory"",""us-docker.pkg.dev/hail-vdc/hail/monitoring"",""us-docker.pkg.dev/hail-vdc/hail/notebook"",""us-docker.pkg.dev/hail-vdc/hail/notebook_nginx"",""us-docker.pkg.dev/hail-vdc/hail/prometheus"",""us-docker.pkg.dev/hail-vdc/hail/service-base"",""us-docker.pkg.dev/hail-vdc/hail/service-java-run-base"",""us-docker.pkg.dev/hail-vdc/hail/test-ci"",""us-docker.pkg.dev/hail-vdc/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545:195,config,configurations,195,https://hail.is,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545,1,['config'],['configurations']
Modifiability,I forgot the Java tests were split in the same way as the batch tests and had forgot to add the new env variable in each split.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10807#issuecomment-905458230:104,variab,variable,104,https://hail.is,https://github.com/hail-is/hail/pull/10807#issuecomment-905458230,1,['variab'],['variable']
Modifiability,"I got a bad local variable compilation error without the fields for the bindings. I suspect this is a locals/fields problem elsewhere but don't want to debug right now, so reverted.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7830#issuecomment-574725641:18,variab,variable,18,https://hail.is,https://github.com/hail-is/hail/pull/7830#issuecomment-574725641,1,['variab'],['variable']
Modifiability,I got a timeout!; ```; SocketTimeoutException: connect timed out. Java stack trace:; java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:254); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); 	at is.hail.io.fs.HadoopFSURL.<init>(HadoopFS.scala:76); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:88); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:85); 	at is.hail.io.fs.FS.exists(FS.scala:618); 	at is.hail.io.fs.FS.exists$(FS.scala:618); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:85); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3$adapted(LocalBackend.scala:223); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:823,config,configure,823,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,1,['config'],['configure']
Modifiability,"I guess I'm OK accepting this as-is, since I expect us to have the real fix, adaptive branching, in soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11784#issuecomment-1105467838:77,adapt,adaptive,77,https://hail.is,https://github.com/hail-is/hail/pull/11784#issuecomment-1105467838,1,['adapt'],['adaptive']
Modifiability,"I had exactly the same idea, but it causes problems with inheritance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1826#issuecomment-301948641:57,inherit,inheritance,57,https://hail.is,https://github.com/hail-is/hail/pull/1826#issuecomment-301948641,1,['inherit'],['inheritance']
Modifiability,I had problems with calling the Make rules that create batch worker images; but I see that you already remove them and created a script that doesn't depend on config.mk: https://github.com/hail-is/hail/blob/main/batch/gcp-create-worker-image.sh. Thanks for a response! I guess I don't have to worry about config.mk :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11371#issuecomment-1045522380:159,config,config,159,https://hail.is,https://github.com/hail-is/hail/pull/11371#issuecomment-1045522380,2,['config'],['config']
Modifiability,"I have commit `a451e1aaa5d1dd4cc055f8e7c1e261aa59eabeca`, I built the jar as `cd hail && ./gradlew shadowJar`. I have this file:; ```; (foo) # cat /tmp/failure.R ; data(mtcars); hail_jar <- ""/Users/bking/projects/hail/hail/build/libs/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sessionInfo(); sparklyr::invoke(ht, ""count""); ```. it generates this output:; ```; (foo) # Rscript /tmp/failure.R; R version 3.5.1 (2018-07-02); Platform: x86_64-apple-darwin17.6.0 (64-bit); Running under: macOS High Sierra 10.13.6. Matrix products: default; BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib; LAPACK: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib. locale:; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8. attached base packages:; [1] stats graphics grDevices utils datasets methods base . loaded via a namespace (and not attached):; [1] Rcpp_0.12.19 dbplyr_1.2.2 compiler_3.5.1 pillar_1.3.0 ; [5] later_0.7.5 bindr_0.1.1 r2d3_0.2.2 base64enc_0.1-3 ; [9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977:439,config,config,439,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977,3,['config'],['config']
Modifiability,I have found a solution. It is necessary to set the environmental variable SPARK_LOCAL_DIRS to the desired temp location. e.g. ```; export SPARK_LOCAL_DIRS=/local; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/902#issuecomment-251923422:66,variab,variable,66,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251923422,1,['variab'],['variable']
Modifiability,"I have the FET in the expr language done. Also in this branch is the linear regression code refactoring. I removed the docs for the group tests, but left the code for creating groups and the FET and linear regression group tests in this branch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/423#issuecomment-226914280:92,refactor,refactoring,92,https://hail.is,https://github.com/hail-is/hail/pull/423#issuecomment-226914280,1,['refactor'],['refactoring']
Modifiability,I haven't tested the refactored code yet -- would like to see if this was the refactoring you had in mind with Enums.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13224#issuecomment-1664685352:21,refactor,refactored,21,https://hail.is,https://github.com/hail-is/hail/pull/13224#issuecomment-1664685352,2,['refactor'],"['refactored', 'refactoring']"
Modifiability,I left in google_storage.py in for now because Benchmark uses it and I didn't want to rewrite that right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10331#issuecomment-820662160:86,rewrite,rewrite,86,https://hail.is,https://github.com/hail-is/hail/pull/10331#issuecomment-820662160,1,['rewrite'],['rewrite']
Modifiability,"I like this plan overall. Some comments to consider:. - I don't think batch should persist job intermediates which I think more as a pipe (|) or a temporary file in a traditional script. This means the downstream batch clients (CI, Pipeline) should be copying files they want to persist to something like gs. One thing we should be sure here is that the solution doesn't involve excessive copies, e.g. we don't want to generate batch steps that just copy from a temporary persisted location in gs to a permanent location in gs.; - We need a way to refer to individual inputs/outputs. A bioinformatics command might output a massive data file and a report file, and we want to run a command to process or format the report file, but we don't want to copy the (unused) massive data file unnecessarily. Copying everything is a fine start.; - This might be covered by ""parse and exec series of commands"", but we want to be able to specify a series of stacked containers to execute, e.g. in the case of Pipeline, a user command to execute followed by a pipeline-controlled container with Pipeline gs credentials to copy the output to Pipeline (or user) controlled bucket. Same for CI.; - In the long run, we're going to want to be able to control the size of the local disk for user jobs (e.g. a bioinformatics command that needs 1TB of scratch space) separate from the host node's local file system. This will go into the job configuration. We probably don't need this for CI.; - We should separate the database management from the job execution. Probably easiest to make this a monolithic service, but we could separate them. One will interact with batch users to update the database and watch the database, and the other to reconcile the database and k8s by running jobs and updating results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739:1422,config,configuration,1422,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739,1,['config'],['configuration']
Modifiability,"I looked over the code and it looks fine, but I'm having trouble understanding the bigger picture of what you're trying to accomplish. I see that you have a new step that creates a test database in the default namespace in the test scope. Then you create the database config secret from this new database. And then deploy_ci depends on it, which makes sense because it needs the secret to be able to create new databases. And this is all only in the test scope. It looks like you cleaned up the build database in the case of dev deploy, which is fine too. > we also create a ""test_instance"" database that will be used as the database instance inside the tests. I don't understand what you wrote here because test_instance database doesn't seem to be used at all. Aren't we still creating the same batch and ci databases? I don't see what the test_instance database is buying you except to be able to make the database config secret that doesn't have the root username and password. I also don't quite understand what's going on in the build_cant_create_database build step. Shouldn't those secrets already exist? Won't this fail?. I'm sorry if I'm missing something obvious.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906:268,config,config,268,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906,2,['config'],['config']
Modifiability,"I made `define_function` go through the CSE path, and it broke because CSE was assuming an IR with no free variables. I fixed CSE to take a list of free variables.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7009#issuecomment-531331340:107,variab,variables,107,https://hail.is,https://github.com/hail-is/hail/pull/7009#issuecomment-531331340,2,['variab'],['variables']
Modifiability,"I made a branch which I think fixes this (and makes the 'root' variable actually used), but will PR once I have testing going.; https://github.com/jbloom22/hail/tree/vep_csq_global",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4050#issuecomment-409401666:63,variab,variable,63,https://hail.is,https://github.com/hail-is/hail/issues/4050#issuecomment-409401666,1,['variab'],['variable']
Modifiability,I made some changes. I had to rewrite the audits to not fill up the temp disk space and account for a bug in billing that was fixed for job private instances #10069. I'll test this afternoon after I figure out how to revert my first attempt.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11996#issuecomment-1201421379:30,rewrite,rewrite,30,https://hail.is,https://github.com/hail-is/hail/pull/11996#issuecomment-1201421379,1,['rewrite'],['rewrite']
Modifiability,"I meant type `TEXT` compared to `VARCHAR`. The reason I did not pick a BIGINT representation is there are 34 regions in GCP right now. If there's ever more than 64, then we can't support that with a BIGINT representation. A varchar seemed more flexible as we can change the type later on and we're not restricted to 64 regions. However, restricting to 64 regions as just a fundamental batch limitation is always fine as well. I'm ambivalent on which way we choose as long as the field can be used in an index. Apparently, `TEXT` can be indexed, but you have to specify the first N characters that are indexable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1274923740:244,flexible,flexible,244,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1274923740,1,['flexible'],['flexible']
Modifiability,"I noticed something else, really long running jobs don't look great. That second blue line is a 6 minute job and it extends past the end of the page. Can we maybe put all of these divs into something that scrolls? Maybe a `div` with an `overflow: scroll`? I'm not sure exactly what should happen or how to do it, but I think we need a solution for this situation.; ![Screen Shot 2021-02-26 at 1 57 37 AM](https://user-images.githubusercontent.com/106194/109244152-17be8280-77d6-11eb-8f00-c124f9c2a5c7.png). I created this example with:; ```; import hailtop.batch as hb ; b = hb.Batch(backend=hb.ServiceBackend('test')) ; for i in range(1): ; b.new_job().command('sleep 360') ; b.run() ; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10060#issuecomment-786356423:116,extend,extends,116,https://hail.is,https://github.com/hail-is/hail/pull/10060#issuecomment-786356423,1,['extend'],['extends']
Modifiability,I propose saving the configuration question to a different PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9769#issuecomment-738084194:21,config,configuration,21,https://hail.is,https://github.com/hail-is/hail/pull/9769#issuecomment-738084194,1,['config'],['configuration']
Modifiability,I refactored the client code. Should be better now!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6245#issuecomment-498414073:2,refactor,refactored,2,https://hail.is,https://github.com/hail-is/hail/pull/6245#issuecomment-498414073,1,['refactor'],['refactored']
Modifiability,I see now what you are saying about the Structs -- the issue is the attributes on each Field object. I am tempted to go back to my original implementation where the types inherited from `HistoryMixin` and the `__init__` method was recorded when the object was initialized. @cseed didn't think it was necessary to have a Type have a history and suggested using `repr` to print how the object was created.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2060#issuecomment-322284636:171,inherit,inherited,171,https://hail.is,https://github.com/hail-is/hail/pull/2060#issuecomment-322284636,1,['inherit'],['inherited']
Modifiability,I still don't see what's wrong. TSet inherits from TIterable which does override it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2006#issuecomment-317562287:37,inherit,inherits,37,https://hail.is,https://github.com/hail-is/hail/issues/2006#issuecomment-317562287,1,['inherit'],['inherits']
Modifiability,"I suppose this is where I started getting entangled with the domain issue. If the Australians run a workshop, what should happen if their users run `hailctl batch init`? Should they have to supply some additional argument so that _if_ they're not authenticated they get sent somewhere other than `hail.is`? Maybe that's ok, seems kind of awkward though. While it doesn't work this way today, I imagine that we should ultimately configure `hailctl auth login` to accept a domain. I feel like that would make the AUS scenario slightly less awkward, and the tool more consistent, though I admit it is conceding some of our own convenience.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1663110567:428,config,configure,428,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1663110567,1,['config'],['configure']
Modifiability,I think I addressed most of the comments. I haven't tested the new code -- I don't want to do that until we're happy with it. I don't know that I like how this is turning out. I think we're conflating what `hailctl config init` should be which is intitializing an environment configuration file versus a quick start to using Hail Batch and QoB. My intention for this feature was to idiot proof the latter especially for the ATGU workshop and make it as few commands as possible. I worry that needing to run `hailctl auth login` before this is not a just run this single command and then you can get going with little effort. I don't think `hailctl config init` is the right place for what I have written. Maybe it should be `hailctl batch quick-start`???,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1662716382:215,config,config,215,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1662716382,3,['config'],"['config', 'configuration']"
Modifiability,"I think I'm on board with the RVDSpec stuff you've proposed. I still want to rewrite the names of the MatrixTableSpec and TableSpec, since these should have been abstract from the beginning.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4734#issuecomment-436689558:77,rewrite,rewrite,77,https://hail.is,https://github.com/hail-is/hail/pull/4734#issuecomment-436689558,1,['rewrite'],['rewrite']
Modifiability,I think it's safe. I was more worried about what the namespace should be and whether for developers it should have the service_namespace. I think the deploy config only tells you how to get the correct URL. There's nothing special about it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9437#issuecomment-691241075:157,config,config,157,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691241075,1,['config'],['config']
Modifiability,"I think the clear default answer is referential transparency. Whether you bind something in a python variable, or you inline that definition, should be semantically equivalent. Unless we come up with a compelling reason to break that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7418#issuecomment-548485790:101,variab,variable,101,https://hail.is,https://github.com/hail-is/hail/issues/7418#issuecomment-548485790,1,['variab'],['variable']
Modifiability,"I think the only two things I'm stuck on are:; (a) Do we want users to pass a VEPConfig instead of a `config` dictionary (and add documentation)?; (b) What is the best way to expose the VEP command interface so a user can customize it to their setup? I wanted to do something like this, but I don't see how to do this with the bash script being called with an argument `/bin/bash -c ""...."" csq` or `/bin/bash -c ""..."" vep`. ```python3; vep_85_grch37_command = '''; #!/bin/bash. if [ $VEP_CONSEQUENCE -ne 0 ]; then; vcf_or_json=""--vcf""; else; vcf_or_json=""--json""; fi. export VEP_COMMAND=/vep/vep \; ${VEP_INPUT_FILE:+--input_file $VEP_INPUT_FILE} \; --format vcf \; ${vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir=${VEP_DATA_DIR} \; --plugin LoF,human_ancestor_fa:${VEP_DATA_DIR}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:${VEP_DATA_DIR}/loftee_data/phylocsf_gerp.sql,gerp_file:${VEP_DATA_DIR}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT. exec vep.py ""$@""; '''. supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfig(; 'hail-qob-vep-grch37-us-central1',; ['us-central1'],; HAIL_GENETICS_QOB_VEP_GRCH37_IMAGE,; '/vep_data/',; {},; VEPConfig.default_vep_json_typ,; [""/bin/bash"", ""-c"", vep_85_grch37_command, ""vep""],; [""/bin/bash"", ""-c"", vep_85_grch37_command, ""csq_header""],; True,; 'gcp',; ),; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1498124947:102,config,config,102,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1498124947,2,"['config', 'plugin']","['config', 'plugin']"
Modifiability,"I think this could interact badly with Spark's partitioning logic. That would appear in the text file imports. We should probably rewrite the text file stuff the same way we rewrote import_bgen to use our own, sensible, partitioning logic. I guess the worst thing that happens is a small text file is broken into one partition per-line.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5698#issuecomment-476796791:130,rewrite,rewrite,130,https://hail.is,https://github.com/hail-is/hail/pull/5698#issuecomment-476796791,1,['rewrite'],['rewrite']
Modifiability,"I think this is a bad idea in the current model. However, we should explore IR visualization / execution enhancements",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1616#issuecomment-422389477:105,enhance,enhancements,105,https://hail.is,https://github.com/hail-is/hail/issues/1616#issuecomment-422389477,1,['enhance'],['enhancements']
Modifiability,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:295,config,config,295,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626,3,['config'],['config']
Modifiability,"I think this is ready for another look but I put a WIP label on it because I don't want to merge it today while there's still workshop things happening. After it goes in I'll run another scale test. The nginx config is mostly just lifted over from router, with a couple small changes. I wasn't able to just proxy to localhost because notebook couldn't figure out which subdomain the request was going to and everything would 404. Adding a Host header fixed the 404, but messed with the requests to notebook pods so instead I added `workshop.local` and `notebook.local` to `/etc/hosts` on the pod, which I kind of like, which did the trick. I needed two ssl configs, one for nginx and one for aiohttp to use internally, and ended up changing the `ssl-config-notebook` secret to `nginx` and creating a second `ssl-config-notebook-python`, but let me know if this seems off.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10204#issuecomment-806087347:209,config,config,209,https://hail.is,https://github.com/hail-is/hail/pull/10204#issuecomment-806087347,4,['config'],"['config', 'config-notebook', 'config-notebook-python', 'configs']"
Modifiability,"I think we should continue with another review and then a load test. I'm still a bit hesitant about the query change, but we can keep an eye on it. I'm still get errors with the typing:. ```; (venv) jigold@wm349-8c4 hail % make -C hail/python check; python3 -m flake8 --config ../../setup.cfg hail; python3 -m flake8 --config ../../setup.cfg hailtop; python3 -m pylint --rcfile ../../pylintrc hailtop --score=n; python3 -m mypy --config-file ../../setup.cfg hailtop; hailtop/batch/backend.py:481: error: Incompatible types in assignment (expression has type ""Union[str, List[str], None]"", variable has type ""Optional[List[str]]""); Found 1 error in 1 file (checked 146 source files); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1271807464:270,config,config,270,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1271807464,4,"['config', 'variab']","['config', 'config-file', 'variable']"
