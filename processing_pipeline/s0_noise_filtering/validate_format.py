import math
import os
import re
import shelve
import signal
import sys
from pathlib import Path
from typing import List

import dotenv
import pandas as pd
from langchain_ollama import ChatOllama
from loguru import logger
from pydantic import BaseModel
from tenacity import RetryError
from tqdm import tqdm

from constants.abs_paths import AbsDirPath
from constants.foldernames import FolderNames
from cfg.selected_repos import selected_repos
from utilities.utils import create_logger_path


# Load environment variables from .env file
dotenv.load_dotenv()


class OllamaFormatValidityResponse(BaseModel):
    to_eliminate: bool
    reason: str


class NoiseFilteringStage:
    stage_name = 's0'
    in_dir = AbsDirPath.OPTIMIZED_KEYWORDS
    out_dir = AbsDirPath.S0_NOISE_FILTERING
    cache_dir = AbsDirPath.CACHE / FolderNames.NOISE_FILTERING_DIR
    model_name = "deepseek-r1:8b"
    temperature = 0.0
    data_model = OllamaFormatValidityResponse

    def __init__(self, hostname: str, batch_size: int = 10):
        self.hostname = hostname
        self.batch_size = batch_size
        self.model_fields = list(self.data_model.model_fields.keys())
        self._init()

    def _init(self):
        AbsDirPath.LOGS.mkdir(exist_ok=True)
        os.makedirs(self.out_dir, exist_ok=True)
        os.makedirs(self.cache_dir, exist_ok=True)
        logger.add(create_logger_path(self.out_dir), mode="w")

        # Register the signal handler
        signal.signal(signal.SIGINT, self._cleanup_and_exit)

    @staticmethod
    def _cleanup_and_exit(signal_num, frame):
        print("Caught interrupt, cleaning up...")
        sys.exit(0)  # Triggers the context manager's cleanup

    # @retry(stop=stop_after_attempt(6), wait=wait_fixed(3), after=lambda retry_state: logger.warning(retry_state),
    #     reraise=True, )
    def request_ollama_chain(self, prompts: List[str]) -> List[BaseModel]:
        model = ChatOllama(model=self.model_name, temperature=self.temperature, base_url=self.hostname,
                           format=self.data_model.model_json_schema())
        batch_answers = model.batch(prompts)
        return [self.data_model.model_validate_json(answer.content) for answer in batch_answers]

    @staticmethod
    def to_prompt(x: pd.Series) -> str:
        return f"""
You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given **target content** should be filtered out. The goal is to identify and **keep** content that consists of meaningful human-written prose, explanation, or analysis intended for human readers, and to **filter out** content that is primarily non-prose programmatic or technical artifacts intended mainly for machines or formal structure.

## Instructions:
For each input, return:
1. `to_eliminate`: true or false — should this content be eliminated?
2. `reasoning`: Brief explanation of why the decision was made.

### Keep Content That:
- Is written for human readers and contains **significant natural language, explanation, commentary, analysis, or discussion**.
- Reflects **communication intended for developers or users**, such as thoughtful suggestions, analysis, critiques, or explanations of implementation/optimization strategies.
- Includes **scientific, academic, or detailed technical discussions**, even if highly formal or specialized (e.g., detailed explanations of model architecture, reasoning behind design choices, analysis of outcomes).
- **Crucially:** This content should be kept **even if it is embedded within or formatted as** technical artifacts (like code comments, string literals in config files, documentation sections within code) **as long as the natural language prose component is substantial and provides meaningful human-readable context or explanation.**

### Eliminate Content That:
- Is **primarily** composed of non-prose programmatic or technical artifacts, **lacking significant natural language explanation or discussion**.
- Consists mainly of:
 - **Pure executable code or formal syntax** (e.g., function bodies without comments, simple variable declarations, pure boolean logic like `if (x > 5) {{ y = 1; }}` without explanation).
 - **Program output, logs, or error traces:** Content generated by programs (like build tools, compilers, runtime environments) for diagnostic or reporting purposes, characterized by structured formats, timestamps, error codes, etc., and **distinguished by the absence of substantial human-authored explanations or narrative.**
 - **Formal configuration, data structures, or build specifications lacking explanatory comments/text** (e.g., pure YAML/JSON data structures, simple Makefile rules, compiler flags lists without descriptive text).
 - **Version control metadata lacking explanatory commit messages** (e.g., diff hunks, merge conflict markers, simple file path changes without a descriptive commit message).
 - **Formal API signatures or technical interface definitions without accompanying prose** (e.g., `def my_function(param1: int) -> str:` without a docstring explaining *what* the function does or *why*).

## Examples (for reference only – do not analyze):

### Example 1
**Content:** Build failed on ROOT-ubuntu2004/python3.; Running on root-ubuntu-2004-3.cern.ch:/home/sftnight/build/...; Failing tests:; - projectroot.test.test_stressgraphics_interpreted
**Answer:**
to_eliminate: true
reasoning: Consists entirely of build logs and test failures, which are diagnostic artifacts, not human-readable prose explaining a concept.

### Example 2
**Content:** recision><conversion specifier>`` where:. * ``#`` is an optional flag available for hex values (see; ``<conversion specifier>`` below) which requires the value matched to be; prefixed by ``0x``.; * ``.<precision>`` is an optional printf-style precision specifier in which; ``<precision>`` indicates the minimum number of digits that the value matched; must have, expecting leading zeros if needed. * ``<conversion specifier>`` is an optional scanf-style conversion specifier; to indicate what number format to match (e.g. hex number). Currently; accepted format specifiers are ``%u``, ``%d``, ``%x`` and ``%X``.
**Answer:**
to_eliminate: true
reasoning: Primarily a formal technical specification of syntax with only minimal natural language labeling, not a substantial explanation.

### Example 3
**Content:** I tested the new parallelization strategy. Simulation time dropped 30%, but memory usage increased. We may need more efficient data structures.
**Answer:**
to_eliminate: false
reasoning: Natural language explanation of performance trade-offs.

### Example 4
**Content:** The MemoryDef structure now keeps two operands: the defining access and the optimized access. This change allows faster walking of Def chains and enables caching.
**Answer:**
to_eliminate: false
reasoning: Explains technical design changes in natural language with rationale.

### Example 5
**Content:** We propose SPECTER, a document-level embedding model trained using citation graphs. It improves scientific document classification without task-specific fine-tuning.
**Answer:**
to_eliminate: false
reasoning: Describes an academic NLP model in natural language.

### Example 6
**Content:** # Configure the learning rate using an exponential decay.
**Answer:**
to_eliminate: false
reasoning: Although formatted as a code comment, the content is natural language providing a meaningful explanation of a technical strategy and its purpose.

---

## Now analyze ONLY the following content:

**Content to evaluate:**
{x['sentence']}
"""

    # TODO: remove existing random filters
    @staticmethod
    def filter_data(df: pd.DataFrame) -> pd.DataFrame:
        # df = df[df["quality_attribute"].isin(["Testability", "Energy Efficiency"])]
        # df["word_count"] = df["sentence"].apply(lambda x: len(re.sub(r"[\W_]+", " ", x).strip().split()))
        return df

    def verify_file_batched_llm(self, file_path: Path, res_filepath: Path):
        with shelve.open(self.cache_dir / file_path.stem) as db:
            if db.get("processed", False):
                logger.info(f"File {file_path.stem} already processed")
                return
            logger.info(f"Processing {file_path.stem}")

            try:
                df = pd.read_parquet(file_path)
            except Exception as e:
                logger.error(e)
                return

            last_idx = db.get("idx", 0)
            if last_idx > 0:
                logger.info(f"Continuing from {last_idx}")
                res_filepath = res_filepath.with_suffix(f".from_{last_idx}.parquet")

            df = self.filter_data(df)
            df = df.iloc[last_idx:].copy()

            prompt_field = f"{self.stage_name}_prompt"
            df[prompt_field] = df.apply(self.to_prompt, axis=1)

            for batch_n in tqdm(range(0, len(df), self.batch_size), total=math.ceil(len(df) / self.batch_size),
                          desc=f"Processing {file_path.stem} in batches of {self.batch_size}"):
                batch_end = batch_n + self.batch_size
                batch_df = df.iloc[batch_n:batch_end]
                prompts = batch_df[prompt_field].tolist()
                batch_index = batch_df.index

                llm_responses = self.process_batch(prompts)
                if llm_responses is None:
                    logger.error(f"Error processing batch {last_idx + batch_n}")
                    continue

                resulting_df = pd.DataFrame(llm_responses, columns=self.model_fields, index=batch_index)
                for field_name in self.model_fields:
                    df.loc[batch_index, field_name] = resulting_df[field_name]

                df.iloc[:batch_end].copy().to_parquet(res_filepath, engine='pyarrow', compression='snappy', index=False)
                db["idx"] = last_idx + batch_end

            db['processed'] = True
            logger.info(f"Processed {file_path.stem}")

    # TODO: RetryError does not exist anymore. Refactor. Think about the correct way to handle errors.
    #  What should happen if batch fails? Should we continue with the next file or something else?
    # When it fails it is likely not a problem with the file, but with the connection, thus no need to retry, just stop processing (after 3 more tries??)
    def process_batch(self, prompts):
        try:
            responses = self.request_ollama_chain(prompts)  # New batch query
            return [self.extract_response_data(r) for r in responses]
        except RetryError as error:
            logger.error(f"Retry error at batch, {error}")
            return None
        except Exception as e:
            logger.error(e)
            errors_for_termination = ["HTTPConnectionPool",
                                      "No connection could be made because the target machine actively refused it"]
            if any(error in str(e) for error in errors_for_termination):
                logger.error("HTTPConnectionPool error, exiting")
                exit(1)
            return None

    def extract_response_data(self, response: BaseModel):
        return tuple([getattr(response, field) for field in self.model_fields])

    def execute(self, only_files_containing_text: List[str] | None = None, reverse: bool = False):
        logger.info(f"Executing {self.stage_name} stage")
        only_files_containing_text = only_files_containing_text or []

        try:
            for file_path in self.in_dir.glob("*.parquet"):
                if any(repo.dotted_ref in file_path.stem for repo in selected_repos):
                    keep_processing = len(only_files_containing_text) == 0 or any(
                        text_to_test in file_path.stem for text_to_test in only_files_containing_text)
                    if keep_processing == reverse:
                        continue

                    res_filepath = self.out_dir / f"{file_path.stem}.parquet"
                    self.verify_file_batched_llm(file_path, res_filepath)
        except Exception as e:
            logger.error(e)
            raise e
        logger.info(f"Executing {self.stage_name} stage finished")


LOCAL_LLM_HOST = "http://localhost:11434"

if __name__ == "__main__":
    NoiseFilteringStage(hostname=LOCAL_LLM_HOST).execute([
        "root-project"
    ], reverse=True)
