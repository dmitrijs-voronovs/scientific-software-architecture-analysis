import itertools
import time

import dotenv
import pandas as pd
from pydantic import BaseModel

from constants.abs_paths import AbsDirPath
from constants.foldernames import FolderNames
from processing_pipeline.model.BaseStage import BaseStage

# Load environment variables from .env file
dotenv.load_dotenv()


class OllamaFormatValidityResponse(BaseModel):
    to_eliminate: bool
    reason: str


class NoiseFilteringStage(BaseStage):
    data_model = OllamaFormatValidityResponse
    temperature = 0.0
    model_name = "deepseek-r1:8b"
    cache_dir = AbsDirPath.CACHE / FolderNames.NOISE_FILTERING_DIR
    out_dir = AbsDirPath.S0_NOISE_FILTERING
    in_dir = AbsDirPath.OPTIMIZED_KEYWORDS
    stage_name = 's0'

    @staticmethod
    def to_prompt(x: pd.Series) -> str:
        return f"""
You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given **target content** should be filtered out. The goal is to identify and **keep** content that consists of meaningful human-written prose, explanation, or analysis intended for human readers, and to **filter out** content that is primarily non-prose programmatic or technical artifacts intended mainly for machines or formal structure.

## Instructions:
For each input, return:
1. `to_eliminate`: true or false — should this content be eliminated?
2. `reasoning`: Brief explanation of why the decision was made.

### Keep Content That:
- Is written for human readers and contains **significant natural language, explanation, commentary, analysis, or discussion**.
- Reflects **communication intended for developers or users**, such as thoughtful suggestions, analysis, critiques, or explanations of implementation/optimization strategies.
- Includes **scientific, academic, or detailed technical discussions**, even if highly formal or specialized (e.g., detailed explanations of model architecture, reasoning behind design choices, analysis of outcomes).
- **Crucially:** This content should be kept **even if it is embedded within or formatted as** technical artifacts (like code comments, string literals in config files, documentation sections within code) **as long as the natural language prose component is substantial and provides meaningful human-readable context or explanation.**

### Eliminate Content That:
- Is **primarily** composed of non-prose programmatic or technical artifacts, **lacking significant natural language explanation or discussion**.
- Consists mainly of:
 - **Pure executable code or formal syntax** (e.g., function bodies without comments, simple variable declarations, pure boolean logic like `if (x > 5) {{ y = 1; }}` without explanation).
 - **Program output, logs, or error traces:** Content generated by programs (like build tools, compilers, runtime environments) for diagnostic or reporting purposes, characterized by structured formats, timestamps, error codes, etc., and **distinguished by the absence of substantial human-authored explanations or narrative.**
 - **Formal configuration, data structures, or build specifications lacking explanatory comments/text** (e.g., pure YAML/JSON data structures, simple Makefile rules, compiler flags lists without descriptive text).
 - **Version control metadata lacking explanatory commit messages** (e.g., diff hunks, merge conflict markers, simple file path changes without a descriptive commit message).
 - **Formal API signatures or technical interface definitions without accompanying prose** (e.g., `def my_function(param1: int) -> str:` without a docstring explaining *what* the function does or *why*).

## Examples (for reference only – do not analyze):

### Example 1
**Content:** Build failed on ROOT-ubuntu2004/python3.; Running on root-ubuntu-2004-3.cern.ch:/home/sftnight/build/...; Failing tests:; - projectroot.test.test_stressgraphics_interpreted
**Answer:**
to_eliminate: true
reasoning: Consists entirely of build logs and test failures, which are diagnostic artifacts, not human-readable prose explaining a concept.

### Example 2
**Content:** recision><conversion specifier>`` where:. * ``#`` is an optional flag available for hex values (see; ``<conversion specifier>`` below) which requires the value matched to be; prefixed by ``0x``.; * ``.<precision>`` is an optional printf-style precision specifier in which; ``<precision>`` indicates the minimum number of digits that the value matched; must have, expecting leading zeros if needed. * ``<conversion specifier>`` is an optional scanf-style conversion specifier; to indicate what number format to match (e.g. hex number). Currently; accepted format specifiers are ``%u``, ``%d``, ``%x`` and ``%X``.
**Answer:**
to_eliminate: true
reasoning: Primarily a formal technical specification of syntax with only minimal natural language labeling, not a substantial explanation.

### Example 3
**Content:** I tested the new parallelization strategy. Simulation time dropped 30%, but memory usage increased. We may need more efficient data structures.
**Answer:**
to_eliminate: false
reasoning: Natural language explanation of performance trade-offs.

### Example 4
**Content:** The MemoryDef structure now keeps two operands: the defining access and the optimized access. This change allows faster walking of Def chains and enables caching.
**Answer:**
to_eliminate: false
reasoning: Explains technical design changes in natural language with rationale.

### Example 5
**Content:** We propose SPECTER, a document-level embedding model trained using citation graphs. It improves scientific document classification without task-specific fine-tuning.
**Answer:**
to_eliminate: false
reasoning: Describes an academic NLP model in natural language.

### Example 6
**Content:** # Configure the learning rate using an exponential decay.
**Answer:**
to_eliminate: false
reasoning: Although formatted as a code comment, the content is natural language providing a meaningful explanation of a technical strategy and its purpose.

---

## Now analyze ONLY the following content:

**Content to evaluate:**
{x['sentence']}
"""


LOCAL_LLM_HOST = "http://localhost:11434"


def main():
    # NoiseFilteringStage(hostname=LOCAL_LLM_HOST).execute(["root-project"], reverse=True)
    n_threads = [1, 5, 10, 15, 20]
    n_batches = [1, 5, 10, 15, 20, 50]
    results = []
    for threads, batches in itertools.product(n_threads, n_batches):
        start = time.time()
        NoiseFilteringStage(hostname=LOCAL_LLM_HOST, n_threads=threads, batch_size=batches).execute(["OpenGene.fastp"], reverse=False)
        end = time.time()
        results.append({"n_threads": threads, "n_batches": batches, "time": end - start})
    df = pd.DataFrame(results)
    df.to_csv(AbsDirPath.RES_S0_NOISE_FILTERING / "batch_thread_test.csv", index=False)



if __name__ == "__main__":
    main()
