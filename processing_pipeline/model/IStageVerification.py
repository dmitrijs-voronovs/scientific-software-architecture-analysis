from abc import ABC, abstractproperty, abstractmethod
from pydoc import html
from typing import Literal

import pandas as pd
from pydantic import BaseModel

from cfg.ModelName import ModelName
from constants.abs_paths import AbsDirPath
from processing_pipeline.model.CSVDFHandler import CSVDFHandler
from processing_pipeline.model.IBaseStage import IBaseStage


class OllamaFormatValidityResponse(BaseModel):
    correctness: Literal["correct", "partially correct", "incorrect"]
    reasoning: str


class IStageVerification(IBaseStage, ABC):
    temperature = 0.0
    model_name = ModelName.DEEPSEEK_8B
    in_dir = AbsDirPath.SAMPLES
    out_dir = AbsDirPath.SAMPLES_VERIFIED
    cache_dir = AbsDirPath.CACHE / "samples"
    DFHandler = CSVDFHandler()
    data_model = OllamaFormatValidityResponse

    stage_to_verify: IBaseStage

    @property
    def stage_prefix(self) -> str:
        return self.stage_to_verify.stage_name

    @property
    @abstractmethod
    def source_columns(self) -> list[str]:
        """A list of column names that contain the source text(s) for the AI."""
        pass

    @property
    def prompt_column(self) -> str:
        return self.stage_prefix + "_prompt"

    @property
    @abstractmethod
    def ai_output_columns(self) -> list[str]:
        """
        A list of the AI output column SUFFIXES to be verified.
        (e.g., ['to_eliminate', 'reasoning'] for stage 's0')
        """
        pass

    def get_system_prompt(self) -> str:
        """
        Returns the STATIC system prompt.
        It contains only the instructions, role, and output format.
        It does NOT contain any placeholders for data.
        """
        return """
You are a meticulous and pragmatic Quality Assurance specialist. Your task is to audit and verify an output generated by another AI model. Your goal is to be fair and accurate, focusing on whether the original AI made a reasonable decision based on the **primary substance and intent** of the source text.

### YOUR EVALUATION TASK

Your goal is to determine if the `<ai_output_to_verify>` is a **reasonable and correct** application of the rules in `<original_prompt>` to the content of `<source_text>`.

**1. Guiding Principle: Evaluate Holistically.** Before judging, ask yourself: What is the primary purpose of the `<source_text>`? Is it code, a log, a human discussion, or documentation? Do not let minor, incidental details override the text's main character. For example, a code block with a few minor comments is still primarily code.

2.  **Analyze the Goal:** First, read the `<original_prompt>` to understand the task the first AI was supposed to complete.
3.  **Analyze the Source:** Read the `<source_text>` to understand the content the AI was working with.
4.  **Scrutinize the Output:** Examine each field within `<ai_output_to_verify>`.
5.  **Synthesize and Judge:** Compare the AI's output with the source text and the prompt's rules. Did the AI's decision align with the **guiding principle** and the specific criteria?

### EVALUATION CRITERIA

- **`correct`**: The AI's decision in `<ai_output_to_verify>` correctly captures the **primary nature** of the `<source_text>` according to the rules in `<original_prompt>`. The reasoning provided is a plausible justification for this correct decision.

- **`partially correct`**: The AI's main decision is correct, BUT the `reasoning` is significantly flawed, irrelevant, or completely nonsensical.

- **`incorrect`**: The AI's main decision **fundamentally misrepresents the primary nature** of the `<source_text>`. For example, classifying a detailed technical explanation as "code" or classifying a program log as "human discussion". This verdict applies regardless of the quality of the reasoning.

### RESPONSE FORMAT

You **must** respond with a single, raw JSON object. Do not add any text, comments, or markdown formatting before or after the JSON.

```json
{{
    "evaluation": "correct" | "partially correct" | "incorrect",
    "reasoning": "Your detailed justification for the evaluation. Reference the rules from the <original_prompt>, the content from the <source_text>, and the AI's output to explain your verdict."
}}
"""

    def to_prompt(self, x: pd.Series) -> str:
        """
        Generates the DYNAMIC user prompt.
        It contains only the specific data for this one evaluation item, formatted as defined
        in the system prompt.
        """
        # 1. Prepare the source text block
        source_text_lines = [f"<{col}>{html.escape(str(x.get(col, 'N/A')))}</{col}>" for col in self.source_columns]
        source_text_str = "\n".join(source_text_lines)

        # 2. Prepare the original prompt string
        original_prompt_str = html.escape(x.get(self.prompt_column, 'N/A'))

        # 3. Prepare the AI output block
        ai_output_lines = []
        for col_suffix in self.ai_output_columns:
            full_col_name = f"{self.stage_prefix}_{col_suffix}"
            value = str(x.get(full_col_name, 'N/A'))
            ai_output_lines.append(f"    <{col_suffix}>{html.escape(value)}</{col_suffix}>")
        ai_output_block_str = "\n".join(ai_output_lines)

        # 4. Assemble the final data block for the user message
        return f"""Now, perform your evaluation based on the content within the <evaluation_data> block below.

<evaluation_data>
    <source_text>
    {source_text_str}
    </source_text>

    <original_prompt>
    {original_prompt_str}
    </original_prompt>
    
    <ai_output_to_verify>
    {ai_output_block_str}
    </ai_output_to_verify>
</evaluation_data>
"""


    @property
    def stage_name(self) -> str:
        return self.stage_to_verify.stage_name + '_v'

    def execute_verification(self):
        self.execute([self.stage_to_verify.stage_name])
