from abc import ABC, abstractproperty, abstractmethod
from pydoc import html
from typing import Literal

import pandas as pd
from pydantic import BaseModel

from cfg.ModelName import ModelName
from constants.abs_paths import AbsDirPath
from processing_pipeline.model.CSVDFHandler import CSVDFHandler
from processing_pipeline.model.IBaseStage import IBaseStage


class OllamaFormatValidityResponse(BaseModel):
    correctness: Literal["correct", "partially correct", "incorrect"]
    reasoning: str


class IStageVerification(IBaseStage, ABC):
    temperature = 0.0
    model_name = ModelName.DEEPSEEK_8B
    in_dir = AbsDirPath.SAMPLES
    out_dir = AbsDirPath.SAMPLES_VERIFIED
    cache_dir = AbsDirPath.CACHE / "samples"
    DFHandler = CSVDFHandler()
    data_model = OllamaFormatValidityResponse

    stage_to_verify = type[IBaseStage]

    @property
    def stage_prefix(self) -> str:
        return self.stage_to_verify.stage_name

    @property
    @abstractmethod
    def source_columns(self) -> list[str]:
        """A list of column names that contain the source text(s) for the AI."""
        pass

    @property
    def prompt_column(self) -> str:
        return self.stage_prefix + "_prompt"

    @property
    @abstractmethod
    def ai_output_columns(self) -> list[str]:
        """
        A list of the AI output column SUFFIXES to be verified.
        (e.g., ['to_eliminate', 'reasoning'] for stage 's0')
        """
        pass

    def get_system_prompt(self) -> str:
        """
        Returns the STATIC system prompt.
        It contains only the instructions, role, and output format.
        It does NOT contain any placeholders for data.
        """
        return """You are a hyper-critical and meticulous Quality Assurance specialist. Your task is to audit and verify an output generated by another AI model. You must be strict and follow the provided instructions exactly.

### YOUR EVALUATION TASK

Your goal is to determine if the `<ai_output_to_verify>` is a correct application of the rules in `<original_prompt>` to the content of `<source_text>`.

1.  **Analyze the Goal:** First, read the `<original_prompt>` to understand the task the first AI was supposed to complete.
2.  **Analyze the Source:** Read the `<source_text>` to understand the content the AI was working with.
3.  **Scrutinize the Output:** Examine each field within `<ai_output_to_verify>`.
4.  **Synthesize and Judge:** Compare the AI's output with the source text and the prompt's rules. Did the AI follow the instructions precisely? Is its reasoning grounded in the facts of the source text?

### EVALUATION CRITERIA

- **`correct`**: ALL fields in `<ai_output_to_verify>` are present, correct, and fully justified by the `<source_text>` according to the rules in the `<original_prompt>`. The reasoning must be accurate and relevant.
- **`partially correct`**: The main decision or classification is correct, BUT at least one part of the output is flawed. This includes weak, imprecise, irrelevant, or factually incorrect reasoning based on the `<source_text>`.
- **`incorrect`**: The main decision or classification is wrong. This verdict applies regardless of the quality of any other fields. If the primary conclusion is incorrect, the entire output is `incorrect`.

### RESPONSE FORMAT

You **must** respond with a single, raw JSON object. Do not add any text, comments, or markdown formatting before or after the JSON.

```json
{{
    "evaluation": "correct" | "partially correct" | "incorrect",
    "reasoning": "Your detailed justification for the evaluation. Reference the rules from the <original_prompt>, the content from the <source_text>, and the AI's output to explain your verdict."
}}
"""

    def to_prompt(self, x: pd.Series) -> str:
        """
        Generates the DYNAMIC user prompt.
        It contains only the specific data for this one evaluation item, formatted as defined
        in the system prompt.
        """
        # 1. Prepare the source text block
        source_text_lines = [f"<{col}>{html.escape(str(x.get(col, 'N/A')))}</{col}>" for col in self.source_columns]
        source_text_str = "\n".join(source_text_lines)

        # 2. Prepare the original prompt string
        original_prompt_str = html.escape(x.get(self.prompt_column, 'N/A'))

        # 3. Prepare the AI output block
        ai_output_lines = []
        for col_suffix in self.ai_output_columns:
            full_col_name = f"{self.stage_prefix}_{col_suffix}"
            value = str(x.get(full_col_name, 'N/A'))
            ai_output_lines.append(f"    <{col_suffix}>{html.escape(value)}</{col_suffix}>")
        ai_output_block_str = "\n".join(ai_output_lines)

        # 4. Assemble the final data block for the user message
        return f"""Now, perform your evaluation based on the content within the <evaluation_data> block below.

<evaluation_data>
    <source_text>
    {source_text_str}
    </source_text>

    <original_prompt>
    {original_prompt_str}
    </original_prompt>
    
    <ai_output_to_verify>
    {ai_output_block_str}
    </ai_output_to_verify>
</evaluation_data>
"""


    @property
    def stage_name(self) -> str:
        return self.stage_to_verify.stage_name + '_v'

    def execute_verification(self):
        self.execute([self.stage_to_verify.stage_name])
